#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
PDFæ‰¹æ”¹è„šæœ¬ - ä½¿ç”¨LangGraphå·¥ä½œæµè¿›è¡Œæ‰¹æ”¹å¹¶å®æ—¶è¿½è¸ªé—®é¢˜
"""

import os
import sys
import asyncio
import logging
from pathlib import Path
from datetime import datetime
from typing import Dict, Any, List
import json

# Windowsæ§åˆ¶å°UTF-8ç¼–ç æ”¯æŒ
if sys.platform == 'win32':
    import io
    sys.stdout = io.TextIOWrapper(sys.stdout.buffer, encoding='utf-8')
    sys.stderr = io.TextIOWrapper(sys.stderr.buffer, encoding='utf-8')

# æ·»åŠ é¡¹ç›®è·¯å¾„
project_root = Path(__file__).parent
sys.path.insert(0, str(project_root))

# é…ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[
        logging.StreamHandler(),
        logging.FileHandler('batch_correction.log', encoding='utf-8')
    ]
)
logger = logging.getLogger(__name__)

# å¯¼å…¥å¿…è¦çš„æ¨¡å—
from functions.langgraph.workflow_multimodal import run_multimodal_grading, get_multimodal_workflow
from functions.file_processor import process_multimodal_file


class PDFCorrectionTracker:
    """PDFæ‰¹æ”¹è¿½è¸ªå™¨ - å®æ—¶è¿½è¸ªæ‰¹æ”¹è¿›åº¦å’Œé—®é¢˜"""
    
    def __init__(self):
        self.start_time = None
        self.errors = []
        self.warnings = []
        self.progress_history = []
        
    def log_progress(self, step: str, progress: float, message: str = ""):
        """è®°å½•è¿›åº¦"""
        timestamp = datetime.now()
        progress_info = {
            'timestamp': timestamp.isoformat(),
            'step': step,
            'progress': progress,
            'message': message
        }
        self.progress_history.append(progress_info)
        
        # æ‰“å°è¿›åº¦
        progress_bar = "â–ˆ" * int(progress / 2) + "â–‘" * (50 - int(progress / 2))
        print(f"\r[{progress_bar}] {progress:.1f}% - {step} {message}", end='', flush=True)
        
        logger.info(f"è¿›åº¦æ›´æ–°: {step} - {progress:.1f}% - {message}")
    
    def log_error(self, step: str, error: str, details: Dict = None):
        """è®°å½•é”™è¯¯"""
        error_info = {
            'timestamp': datetime.now().isoformat(),
            'step': step,
            'error': error,
            'details': details or {}
        }
        self.errors.append(error_info)
        print(f"\nâŒ é”™è¯¯ [{step}]: {error}")
        logger.error(f"é”™è¯¯ [{step}]: {error}", extra={'details': details})
    
    def log_warning(self, step: str, warning: str, details: Dict = None):
        """è®°å½•è­¦å‘Š"""
        warning_info = {
            'timestamp': datetime.now().isoformat(),
            'step': step,
            'warning': warning,
            'details': details or {}
        }
        self.warnings.append(warning_info)
        print(f"\nâš ï¸  è­¦å‘Š [{step}]: {warning}")
        logger.warning(f"è­¦å‘Š [{step}]: {warning}", extra={'details': details})
    
    def print_summary(self):
        """æ‰“å°æ‘˜è¦"""
        print("\n" + "="*80)
        print("ğŸ“Š æ‰¹æ”¹æ‘˜è¦")
        print("="*80)
        
        if self.start_time:
            duration = (datetime.now() - self.start_time).total_seconds()
            print(f"â±ï¸  æ€»è€—æ—¶: {duration:.2f} ç§’")
        
        print(f"âœ… è¿›åº¦è®°å½•: {len(self.progress_history)} æ¡")
        print(f"âŒ é”™è¯¯æ•°é‡: {len(self.errors)} æ¡")
        print(f"âš ï¸  è­¦å‘Šæ•°é‡: {len(self.warnings)} æ¡")
        
        if self.errors:
            print("\nâŒ é”™è¯¯è¯¦æƒ…:")
            for i, err in enumerate(self.errors, 1):
                print(f"  {i}. [{err['step']}] {err['error']}")
        
        if self.warnings:
            print("\nâš ï¸  è­¦å‘Šè¯¦æƒ…:")
            for i, warn in enumerate(self.warnings, 1):
                print(f"  {i}. [{warn['step']}] {warn['warning']}")
        
        print("="*80)
    
    def save_report(self, output_path: str, result: Dict[str, Any]):
        """ä¿å­˜æ‰¹æ”¹æŠ¥å‘Š"""
        report = {
            'timestamp': datetime.now().isoformat(),
            'result': result,
            'progress_history': self.progress_history,
            'errors': self.errors,
            'warnings': self.warnings,
            'summary': {
                'total_progress_records': len(self.progress_history),
                'total_errors': len(self.errors),
                'total_warnings': len(self.warnings),
                'duration_seconds': (datetime.now() - self.start_time).total_seconds() if self.start_time else 0
            }
        }
        
        with open(output_path, 'w', encoding='utf-8') as f:
            json.dump(report, f, ensure_ascii=False, indent=2)
        
        logger.info(f"æŠ¥å‘Šå·²ä¿å­˜åˆ°: {output_path}")


async def monitor_workflow_progress(workflow, initial_state: Dict, task_id: str, tracker: PDFCorrectionTracker):
    """ç›‘æ§å·¥ä½œæµè¿›åº¦ - ä½¿ç”¨valuesæ¨¡å¼é¿å…å¹¶å‘æ›´æ–°é”™è¯¯"""
    config = {"configurable": {"thread_id": task_id}}
    
    try:
        # ä½¿ç”¨valuesæ¨¡å¼è·å–å®Œæ•´çŠ¶æ€ï¼Œé¿å…å¹¶å‘æ›´æ–°é”™è¯¯
        async for state_update in workflow.graph.astream(initial_state, config=config, stream_mode='values'):
            # state_updateæ ¼å¼: {node_name: state_dict} æˆ–ç›´æ¥æ˜¯state_dict
            if isinstance(state_update, dict):
                # æ£€æŸ¥æ˜¯å¦æ˜¯å®Œæ•´çŠ¶æ€å­—å…¸
                if 'task_id' in state_update:
                    state = state_update
                else:
                    # å¦‚æœæ˜¯èŠ‚ç‚¹æ›´æ–°å­—å…¸ï¼Œå–ç¬¬ä¸€ä¸ªå€¼
                    state = list(state_update.values())[0] if state_update else {}
                
                # æå–è¿›åº¦ä¿¡æ¯
                progress = state.get('progress_percentage', 0)
                current_step = state.get('current_step', 'processing')
                errors = state.get('errors', [])
                warnings = state.get('warnings', [])
                
                # è®°å½•è¿›åº¦
                tracker.log_progress(current_step, progress, f"çŠ¶æ€æ›´æ–°")
                
                # è®°å½•é”™è¯¯
                for error in errors:
                    if isinstance(error, dict):
                        tracker.log_error(
                            error.get('step', 'unknown'),
                            error.get('error', str(error)),
                            error
                        )
                    else:
                        tracker.log_error('unknown', str(error))
                
                # è®°å½•è­¦å‘Š
                for warning in warnings:
                    if isinstance(warning, dict):
                        tracker.log_warning(
                            warning.get('step', 'unknown'),
                            warning.get('warning', str(warning)),
                            warning
                        )
                    else:
                        tracker.log_warning('unknown', str(warning))
        
        return True
    except Exception as e:
        tracker.log_error('workflow_monitoring', f"ç›‘æ§å·¥ä½œæµå¤±è´¥: {str(e)}")
        import traceback
        tracker.log_error('workflow_monitoring', f"è¯¦ç»†é”™è¯¯:\n{traceback.format_exc()}")
        return False


async def correct_pdfs_with_tracking(
    question_pdf: str,
    answer_pdf: str,
    marking_pdf: str = None,
    strictness_level: str = "ä¸­ç­‰",
    language: str = "zh"
) -> Dict[str, Any]:
    """
    ä½¿ç”¨LangGraphå·¥ä½œæµæ‰¹æ”¹PDFæ–‡ä»¶ï¼Œå¹¶å®æ—¶è¿½è¸ªé—®é¢˜
    
    Args:
        question_pdf: é¢˜ç›®PDFæ–‡ä»¶è·¯å¾„
        answer_pdf: å­¦ç”Ÿä½œç­”PDFæ–‡ä»¶è·¯å¾„
        marking_pdf: æ‰¹æ”¹æ ‡å‡†PDFæ–‡ä»¶è·¯å¾„ï¼ˆå¯é€‰ï¼‰
        strictness_level: ä¸¥æ ¼ç¨‹åº¦
        language: è¯­è¨€
        
    Returns:
        æ‰¹æ”¹ç»“æœå­—å…¸
    """
    tracker = PDFCorrectionTracker()
    tracker.start_time = datetime.now()
    
    print("="*80)
    print("ğŸš€ å¼€å§‹PDFæ‰¹æ”¹ä»»åŠ¡")
    print("="*80)
    print(f"ğŸ“„ é¢˜ç›®æ–‡ä»¶: {question_pdf}")
    print(f"âœï¸  å­¦ç”Ÿä½œç­”: {answer_pdf}")
    if marking_pdf:
        print(f"ğŸ“Š æ‰¹æ”¹æ ‡å‡†: {marking_pdf}")
    print("="*80)
    
    # æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å­˜åœ¨
    files_to_check = {
        'é¢˜ç›®æ–‡ä»¶': question_pdf,
        'å­¦ç”Ÿä½œç­”': answer_pdf
    }
    if marking_pdf:
        files_to_check['æ‰¹æ”¹æ ‡å‡†'] = marking_pdf
    
    for file_type, file_path in files_to_check.items():
        if not Path(file_path).exists():
            tracker.log_error('file_validation', f"{file_type}ä¸å­˜åœ¨: {file_path}")
            return {
                'success': False,
                'error': f"{file_type}ä¸å­˜åœ¨: {file_path}",
                'errors': tracker.errors
            }
    
    tracker.log_progress('æ–‡ä»¶éªŒè¯', 5, "æ–‡ä»¶å­˜åœ¨æ€§æ£€æŸ¥é€šè¿‡")
    
    # å¤„ç†å¤šæ¨¡æ€æ–‡ä»¶
    try:
        tracker.log_progress('æ–‡ä»¶å¤„ç†', 10, "å¼€å§‹å¤„ç†PDFæ–‡ä»¶...")
        
        # PDFç›´æ¥ä½¿ç”¨Vision APIå¤„ç†ï¼Œä¸æå–æ–‡æœ¬
        question_mm = process_multimodal_file(question_pdf, prefer_vision=True)
        tracker.log_progress('æ–‡ä»¶å¤„ç†', 20, f"é¢˜ç›®æ–‡ä»¶å¤„ç†å®Œæˆ - ç±»å‹: {question_mm['modality_type']}")
        
        answer_mm = process_multimodal_file(answer_pdf, prefer_vision=True)
        tracker.log_progress('æ–‡ä»¶å¤„ç†', 30, f"å­¦ç”Ÿä½œç­”å¤„ç†å®Œæˆ - ç±»å‹: {answer_mm['modality_type']}")
        
        marking_mm = None
        if marking_pdf:
            marking_mm = process_multimodal_file(marking_pdf, prefer_vision=True)
            tracker.log_progress('æ–‡ä»¶å¤„ç†', 40, f"æ‰¹æ”¹æ ‡å‡†å¤„ç†å®Œæˆ - ç±»å‹: {marking_mm['modality_type']}")
        
        # é™é»˜å¤„ç†PDFç±»å‹ï¼Œä¸æ˜¾ç¤ºè½¬æ¢æç¤º
        # PDFä¼šæ ¹æ®å†…å®¹è‡ªåŠ¨é€‰æ‹©æ–‡æœ¬æˆ–å›¾ç‰‡æ¨¡å¼ï¼Œæ— éœ€è­¦å‘Š
        
    except Exception as e:
        tracker.log_error('æ–‡ä»¶å¤„ç†', f"æ–‡ä»¶å¤„ç†å¤±è´¥: {str(e)}", {'exception': str(e)})
        return {
            'success': False,
            'error': f"æ–‡ä»¶å¤„ç†å¤±è´¥: {str(e)}",
            'errors': tracker.errors
        }
    
    # ç”Ÿæˆä»»åŠ¡ID
    task_id = f"pdf_correction_{datetime.now().strftime('%Y%m%d_%H%M%S')}"
    user_id = "batch_user"
    
    tracker.log_progress('å·¥ä½œæµåˆå§‹åŒ–', 50, f"ä»»åŠ¡ID: {task_id}")
    
    # å‡†å¤‡æ–‡ä»¶åˆ—è¡¨
    question_files = [question_pdf]
    answer_files = [answer_pdf]
    marking_files = [marking_pdf] if marking_pdf else []
    
    # è¿è¡Œå¤šæ¨¡æ€æ‰¹æ”¹å·¥ä½œæµ
    try:
        tracker.log_progress('å·¥ä½œæµæ‰§è¡Œ', 55, "å¯åŠ¨LangGraphå¤šæ¨¡æ€å·¥ä½œæµ...")
        
        # è·å–å·¥ä½œæµå®ä¾‹ç”¨äºç›‘æ§
        workflow = get_multimodal_workflow()
        
        # åˆ›å»ºåˆå§‹çŠ¶æ€
        from functions.langgraph.state import GradingState
        # é‡è¦ï¼šç¡®ä¿æ–‡ä»¶æ­£ç¡®åˆ†ç¦»
        # question_files: é¢˜ç›®æ–‡ä»¶ï¼ˆå¦‚æœé¢˜ç›®å’Œç­”æ¡ˆåœ¨åŒä¸€æ–‡ä»¶ï¼Œåˆ™ä½¿ç”¨ç­”æ¡ˆæ–‡ä»¶ä½œä¸ºé¢˜ç›®å‚è€ƒï¼‰
        # answer_files: å­¦ç”Ÿä½œç­”æ–‡ä»¶
        # marking_files: æ‰¹æ”¹æ ‡å‡†æ–‡ä»¶
        
        logger.info(f"ğŸ“‹ æ–‡ä»¶éš”ç¦»æ£€æŸ¥:")
        logger.info(f"  é¢˜ç›®æ–‡ä»¶: {question_files}")
        logger.info(f"  å­¦ç”Ÿä½œç­”: {answer_files}")
        logger.info(f"  æ‰¹æ”¹æ ‡å‡†: {marking_files}")
        
        initial_state = GradingState(
            task_id=task_id,
            user_id=user_id,
            assignment_id=f"assignment_{task_id}",
            timestamp=datetime.now(),
            question_files=question_files,
            answer_files=answer_files,
            marking_files=marking_files,
            images=[],
            strictness_level=strictness_level,
            language=language,
            mode="auto",
            # åˆå§‹åŒ–å…¶ä»–å¿…è¦å­—æ®µ
            mm_tokens=[],
            student_info={},
            ocr_results={},
            image_regions={},
            preprocessed_images={},
            rubric_text="",
            rubric_struct={},
            rubric_data={},
            scoring_criteria=[],
            questions=[],
            batches=[],
            evaluations=[],
            scoring_results={},
            detailed_feedback=[],
            annotations=[],
            coordinate_annotations=[],
            error_regions=[],
            cropped_regions=[],
            knowledge_points=[],
            error_analysis={},
            learning_suggestions=[],
            difficulty_assessment={},
            total_score=0.0,
            section_scores={},
            student_evaluation={},
            class_evaluation={},
            export_payload={},
            final_report={},
            export_data={},
            visualization_data={},
            current_step="",
            progress_percentage=0.0,
            completion_status="pending",
            completed_at="",
            errors=[],
            step_results={},
            final_score=0.0,
            grade_level="",
            warnings=[],
            processing_time=0.0,
            model_versions={},
            quality_metrics={},
            # å¤šæ¨¡æ€æ–‡ä»¶ï¼ˆå·¥ä½œæµä¼šè‡ªåŠ¨å¤„ç†å¹¶å¡«å……ï¼‰
            question_multimodal_files=[question_mm] if 'question_mm' in locals() else [],
            answer_multimodal_files=[answer_mm] if 'answer_mm' in locals() else [],
            marking_multimodal_files=[marking_mm] if marking_mm and 'marking_mm' in locals() else [],
            question_understanding=None,
            answer_understanding=None,
            rubric_understanding=None,
            criteria_evaluations=[]
        )
        
        logger.info(f"âœ… åˆå§‹çŠ¶æ€åˆ›å»ºå®Œæˆ:")
        logger.info(f"  é¢˜ç›®å¤šæ¨¡æ€æ–‡ä»¶æ•°: {len(initial_state.get('question_multimodal_files', []))}")
        logger.info(f"  ç­”æ¡ˆå¤šæ¨¡æ€æ–‡ä»¶æ•°: {len(initial_state.get('answer_multimodal_files', []))}")
        logger.info(f"  æ‰¹æ”¹æ ‡å‡†å¤šæ¨¡æ€æ–‡ä»¶æ•°: {len(initial_state.get('marking_multimodal_files', []))}")
        
        # ç›´æ¥æ‰§è¡Œå·¥ä½œæµï¼ˆä¸ä½¿ç”¨ç›‘æ§ä»»åŠ¡ï¼Œé¿å…å¹¶å‘é—®é¢˜ï¼‰
        # å·¥ä½œæµå†…éƒ¨ä¼šæ›´æ–°è¿›åº¦ï¼Œæˆ‘ä»¬ç›´æ¥è·å–æœ€ç»ˆç»“æœ
        result = await run_multimodal_grading(
            task_id=task_id,
            user_id=user_id,
            question_files=question_files,
            answer_files=answer_files,
            marking_files=marking_files,
            strictness_level=strictness_level,
            language=language
        )
        
        # æ‰‹åŠ¨æ›´æ–°è¿›åº¦ï¼ˆåŸºäºç»“æœï¼‰
        if result.get('status') == 'completed':
            tracker.log_progress('æ‰¹æ”¹å®Œæˆ', 100, "æ‰¹æ”¹æˆåŠŸå®Œæˆ")
        else:
            tracker.log_progress('æ‰¹æ”¹å®Œæˆ', 90, f"çŠ¶æ€: {result.get('status')}")
        
        tracker.log_progress('å·¥ä½œæµå®Œæˆ', 100, "æ‰¹æ”¹ä»»åŠ¡å®Œæˆ")
        
        # æ£€æŸ¥ç»“æœ
        if result.get('status') == 'completed':
            print("\nâœ… æ‰¹æ”¹æˆåŠŸå®Œæˆï¼")
            print(f"ğŸ“Š æ€»åˆ†: {result.get('total_score', 'N/A')}")
            print(f"ğŸ“ ç­‰çº§: {result.get('grade_level', 'N/A')}")
        else:
            tracker.log_error('å·¥ä½œæµæ‰§è¡Œ', f"æ‰¹æ”¹æœªæˆåŠŸå®Œæˆï¼ŒçŠ¶æ€: {result.get('status')}")
        
        # åˆå¹¶è¿½è¸ªä¿¡æ¯åˆ°ç»“æœ
        result['tracking'] = {
            'progress_history': tracker.progress_history,
            'errors': tracker.errors,
            'warnings': tracker.warnings,
            'duration_seconds': (datetime.now() - tracker.start_time).total_seconds() if tracker.start_time else 0
        }
        
        return result
        
    except Exception as e:
        tracker.log_error('å·¥ä½œæµæ‰§è¡Œ', f"å·¥ä½œæµæ‰§è¡Œå¤±è´¥: {str(e)}", {'exception': str(e)})
        import traceback
        tracker.log_error('å·¥ä½œæµæ‰§è¡Œ', f"è¯¦ç»†é”™è¯¯ä¿¡æ¯:\n{traceback.format_exc()}")
        
        return {
            'success': False,
            'error': f"å·¥ä½œæµæ‰§è¡Œå¤±è´¥: {str(e)}",
            'errors': tracker.errors,
            'warnings': tracker.warnings
        }
    finally:
        tracker.print_summary()


async def main():
    """ä¸»å‡½æ•°"""
    # æ–‡ä»¶è·¯å¾„ï¼ˆç›¸å¯¹äºé¡¹ç›®æ ¹ç›®å½•ï¼‰
    project_root = Path(__file__).parent.parent
    
    question_pdf = project_root / "å­¦ç”Ÿä½œç­”.pdf"
    answer_pdf = project_root / "æ‰¹æ”¹æ ‡å‡†.pdf"
    
    # æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å­˜åœ¨
    if not question_pdf.exists():
        print(f"âŒ é”™è¯¯: æ‰¾ä¸åˆ°æ–‡ä»¶ {question_pdf}")
        print("è¯·ç¡®ä¿æ–‡ä»¶å­˜åœ¨äºé¡¹ç›®æ ¹ç›®å½•")
        return
    
    if not answer_pdf.exists():
        print(f"âŒ é”™è¯¯: æ‰¾ä¸åˆ°æ–‡ä»¶ {answer_pdf}")
        print("è¯·ç¡®ä¿æ–‡ä»¶å­˜åœ¨äºé¡¹ç›®æ ¹ç›®å½•")
        return
    
    # æ³¨æ„ï¼šæ ¹æ®æ–‡ä»¶åï¼Œçœ‹èµ·æ¥"å­¦ç”Ÿä½œç­”.pdf"åº”è¯¥æ˜¯ç­”æ¡ˆæ–‡ä»¶
    # "æ‰¹æ”¹æ ‡å‡†.pdf"åº”è¯¥æ˜¯è¯„åˆ†æ ‡å‡†æ–‡ä»¶
    # ä½†ç”¨æˆ·å¯èƒ½æŠŠé¢˜ç›®å’Œç­”æ¡ˆéƒ½æ”¾åœ¨äº†"å­¦ç”Ÿä½œç­”.pdf"ä¸­
    # è¿™é‡Œæˆ‘ä»¬å‡è®¾ï¼š
    # - question_pdf: é¢˜ç›®ï¼ˆå¦‚æœæœ‰å•ç‹¬çš„é¢˜ç›®æ–‡ä»¶ï¼‰
    # - answer_pdf: å­¦ç”Ÿä½œç­”
    # - marking_pdf: æ‰¹æ”¹æ ‡å‡†
    
    # æ ¹æ®å®é™…æ–‡ä»¶åè°ƒæ•´
    # å¦‚æœ"å­¦ç”Ÿä½œç­”.pdf"åŒ…å«é¢˜ç›®å’Œç­”æ¡ˆï¼Œæˆ‘ä»¬éœ€è¦è°ƒæ•´
    student_answer_pdf = project_root / "å­¦ç”Ÿä½œç­”.pdf"
    marking_scheme_pdf = project_root / "æ‰¹æ”¹æ ‡å‡†.pdf"
    
    # å¦‚æœæ²¡æœ‰å•ç‹¬çš„é¢˜ç›®æ–‡ä»¶ï¼Œæˆ‘ä»¬ä½¿ç”¨å­¦ç”Ÿä½œç­”æ–‡ä»¶ä½œä¸ºé¢˜ç›®å‚è€ƒ
    # æˆ–è€…éœ€è¦ä»å­¦ç”Ÿä½œç­”ä¸­æå–é¢˜ç›®
    question_pdf_path = student_answer_pdf  # ä¸´æ—¶ä½¿ç”¨ï¼Œå®é™…åº”è¯¥åˆ†å¼€
    
    print("ğŸ“‹ æ–‡ä»¶é…ç½®:")
    print(f"  é¢˜ç›®æ–‡ä»¶: {question_pdf_path}")
    print(f"  å­¦ç”Ÿä½œç­”: {student_answer_pdf}")
    print(f"  æ‰¹æ”¹æ ‡å‡†: {marking_scheme_pdf}")
    print()
    
    # æ£€æŸ¥APIå¯†é’¥é…ç½®ï¼ˆconfig.pyä¼šè‡ªåŠ¨åŠ è½½.envæ–‡ä»¶ï¼‰
    import os
    from config import OPENROUTER_API_KEY, LLM_API_KEY, LLM_PROVIDER
    
    api_key = OPENROUTER_API_KEY or LLM_API_KEY
    if not api_key:
        print("âš ï¸  è­¦å‘Š: æœªæ£€æµ‹åˆ°APIå¯†é’¥")
        print(f"å½“å‰LLM Provider: {LLM_PROVIDER}")
        print("è¯·è®¾ç½®ç¯å¢ƒå˜é‡:")
        print("  - OPENROUTER_API_KEY (å¦‚æœä½¿ç”¨OpenRouter)")
        print("  - LLM_API_KEY (é€šç”¨)")
        print("å¯ä»¥åœ¨ ai_correction/.env æ–‡ä»¶æˆ–é¡¹ç›®æ ¹ç›®å½• .env æ–‡ä»¶ä¸­æ·»åŠ ")
        print()
        print("ç»§ç»­æ‰§è¡Œæ‰¹æ”¹ï¼ˆå¯èƒ½ä¼šå¤±è´¥ï¼‰...")
        print()
    else:
        print(f"âœ… APIå¯†é’¥å·²é…ç½® (Provider: {LLM_PROVIDER})")
        print()
    
    # æ‰§è¡Œæ‰¹æ”¹
    result = await correct_pdfs_with_tracking(
        question_pdf=str(question_pdf_path),
        answer_pdf=str(student_answer_pdf),
        marking_pdf=str(marking_scheme_pdf) if marking_scheme_pdf.exists() else None,
        strictness_level="ä¸­ç­‰",
        language="zh"
    )
    
    # ä¿å­˜ç»“æœ
    output_dir = project_root / "correction_results"
    output_dir.mkdir(exist_ok=True)
    
    timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
    result_file = output_dir / f"correction_result_{timestamp}.json"
    
    # ä¿å­˜JSONç»“æœï¼ˆç¡®ä¿åŒ…å«æ‰€æœ‰å­—æ®µï¼‰
    # æ‰“å°resultçš„é”®ï¼Œç”¨äºè°ƒè¯•
    logger.info(f"ç»“æœå­—å…¸åŒ…å«çš„é”®: {list(result.keys())}")
    
    with open(result_file, 'w', encoding='utf-8') as f:
        json.dump(result, f, ensure_ascii=False, indent=2)
    
    print(f"\nğŸ’¾ ç»“æœå·²ä¿å­˜åˆ°: {result_file}")
    
    # ä¿å­˜æ–‡æœ¬æ ¼å¼çš„ç»“æœï¼ˆåŒ…å«è¯¦ç»†ä¿¡æ¯å’ŒAgentåä½œè¿‡ç¨‹ï¼‰
    text_result_file = output_dir / f"correction_result_{timestamp}.txt"
    with open(text_result_file, 'w', encoding='utf-8') as f:
        f.write("="*80 + "\n")
        f.write("PDFæ‰¹æ”¹ç»“æœ\n")
        f.write("="*80 + "\n\n")
        
        # æ·»åŠ æ‰¹æ”¹æ ‡å‡†è§£æç»“æœ
        if 'rubric_parsing_result' in result:
            f.write("="*80 + "\n")
            f.write("ğŸ“‹ æ‰¹æ”¹æ ‡å‡†è§£æç»“æœ\n")
            f.write("="*80 + "\n\n")
            rubric_result = result['rubric_parsing_result']
            f.write(f"æ ‡å‡†ID: {rubric_result.get('rubric_id', 'N/A')}\n")
            f.write(f"æ€»åˆ†: {rubric_result.get('total_points', 0)} åˆ†\n")
            f.write(f"è¯„åˆ†ç‚¹æ•°é‡: {rubric_result.get('criteria_count', 0)}\n\n")
            
            f.write("è¯„åˆ†ç‚¹è¯¦æƒ…:\n")
            f.write("-"*80 + "\n")
            for i, criterion in enumerate(rubric_result.get('criteria', []), 1):
                f.write(f"\nè¯„åˆ†ç‚¹ {i}:\n")
                f.write(f"  ID: {criterion.get('criterion_id', 'N/A')}\n")
                f.write(f"  é¢˜ç›®: {criterion.get('question_id', 'N/A')}\n")
                f.write(f"  æè¿°: {criterion.get('description', 'N/A')}\n")
                f.write(f"  åˆ†å€¼: {criterion.get('points', 0)} åˆ†\n")
                
                # æ·»åŠ è¯¦ç»†è¦æ±‚
                if criterion.get('detailed_requirements'):
                    f.write(f"  è¯¦ç»†è¦æ±‚: {criterion.get('detailed_requirements')}\n")
                
                # æ·»åŠ æ ‡å‡†ç­”æ¡ˆ
                if criterion.get('standard_answer'):
                    f.write(f"  æ ‡å‡†ç­”æ¡ˆ: {criterion.get('standard_answer')}\n")
                
                # æ·»åŠ è¯„åˆ†ç»†åˆ™
                if criterion.get('scoring_criteria'):
                    scoring = criterion.get('scoring_criteria', {})
                    f.write(f"  è¯„åˆ†ç»†åˆ™:\n")
                    if scoring.get('full_credit'):
                        f.write(f"    æ»¡åˆ†æ¡ä»¶: {scoring.get('full_credit')}\n")
                    if scoring.get('partial_credit'):
                        f.write(f"    éƒ¨åˆ†åˆ†æ¡ä»¶: {scoring.get('partial_credit')}\n")
                    if scoring.get('no_credit'):
                        f.write(f"    ä¸å¾—åˆ†æ¡ä»¶: {scoring.get('no_credit')}\n")
                
                # æ·»åŠ å¦ç±»è§£æ³•
                if criterion.get('alternative_methods'):
                    methods = criterion.get('alternative_methods', [])
                    if methods:
                        f.write(f"  å¦ç±»è§£æ³•:\n")
                        for method in methods:
                            f.write(f"    - {method}\n")
                
                f.write(f"  è¯„ä¼°æ–¹æ³•: {criterion.get('evaluation_method', 'N/A')}\n")
                
                if criterion.get('keywords'):
                    f.write(f"  å…³é”®è¯: {', '.join(criterion.get('keywords', []))}\n")
                if criterion.get('required_elements'):
                    f.write(f"  å¿…éœ€å…ƒç´ : {', '.join(criterion.get('required_elements', []))}\n")
                if criterion.get('common_mistakes'):
                    mistakes = criterion.get('common_mistakes', [])
                    if mistakes:
                        f.write(f"  å¸¸è§é”™è¯¯:\n")
                        for mistake in mistakes:
                            f.write(f"    - {mistake}\n")
            f.write("\n")
        
        # æ·»åŠ Agentåä½œè¿‡ç¨‹
        if 'agent_collaboration' in result:
            f.write("="*80 + "\n")
            f.write("ğŸ¤– Agentåä½œè¿‡ç¨‹\n")
            f.write("="*80 + "\n\n")
            collab = result['agent_collaboration']
            
            f.write("1. RubricInterpreterAgent (è¯„åˆ†æ ‡å‡†è§£æAgent):\n")
            rubric_info = collab.get('rubric_interpreter', {})
            f.write(f"   çŠ¶æ€: {rubric_info.get('status', 'N/A')}\n")
            f.write(f"   æå–è¯„åˆ†ç‚¹æ•°é‡: {rubric_info.get('criteria_extracted', 0)}\n")
            f.write(f"   æ€»åˆ†: {rubric_info.get('total_points', 0)} åˆ†\n\n")
            
            f.write("2. QuestionUnderstandingAgent (é¢˜ç›®ç†è§£Agent):\n")
            question_info = collab.get('question_understanding', {})
            f.write(f"   çŠ¶æ€: {question_info.get('status', 'N/A')}\n\n")
            
            f.write("3. AnswerUnderstandingAgent (ç­”æ¡ˆç†è§£Agent):\n")
            answer_info = collab.get('answer_understanding', {})
            f.write(f"   çŠ¶æ€: {answer_info.get('status', 'N/A')}\n\n")
            
            f.write("4. GradingWorkerAgent (æ‰¹æ”¹å·¥ä½œAgent):\n")
            grading_info = collab.get('grading_worker', {})
            f.write(f"   çŠ¶æ€: {grading_info.get('status', 'N/A')}\n")
            f.write(f"   æ‰¹æ”¹å­¦ç”Ÿæ•°é‡: {grading_info.get('students_graded', 0)}\n")
            f.write(f"   è¯„ä¼°æ•°é‡: {grading_info.get('evaluations_count', 0)}\n\n")
        
        # æ·»åŠ æ‰¹æ”¹ç»“æœ
        f.write("="*80 + "\n")
        f.write("ğŸ“Š æ‰¹æ”¹ç»“æœ\n")
        f.write("="*80 + "\n\n")
        
        f.write(f"ä»»åŠ¡ID: {result.get('task_id', 'N/A')}\n")
        f.write(f"çŠ¶æ€: {result.get('status', 'N/A')}\n")
        f.write(f"æ€»åˆ†: {result.get('total_score', 'N/A')}\n")
        f.write(f"ç­‰çº§: {result.get('grade_level', 'N/A')}\n\n")
        
        # æ·»åŠ è¯¦ç»†çš„è¯„åˆ†ç‚¹è¯„ä¼°
        criteria_evaluations = result.get('criteria_evaluations', [])
        if criteria_evaluations:
            f.write("="*80 + "\n")
            f.write("ğŸ“ è¯¦ç»†æ‰¹æ”¹è¯¦æƒ…ï¼ˆé€é¢˜é€é¡¹è¯„ä¼°ï¼‰\n")
            f.write("="*80 + "\n\n")
            
            # æŒ‰é¢˜ç›®åˆ†ç»„
            questions = {}
            for eval_item in criteria_evaluations:
                criterion_id = eval_item.get('criterion_id', '')
                # æå–é¢˜ç›®ç¼–å·ï¼ˆå¦‚Q1_C1 -> Q1ï¼‰
                question_id = criterion_id.split('_')[0] if '_' in criterion_id else 'UNKNOWN'
                if question_id not in questions:
                    questions[question_id] = []
                questions[question_id].append(eval_item)
            
            # æŒ‰é¢˜ç›®é¡ºåºè¾“å‡º
            sorted_questions = sorted(questions.items(), key=lambda x: x[0])
            
            for question_id, evals in sorted_questions:
                f.write(f"\nã€{question_id}ã€‘\n")
                f.write("-"*80 + "\n")
                
            for i, eval_item in enumerate(evals, 1):
                criterion_id = eval_item.get('criterion_id', 'N/A')
                score_earned = eval_item.get('score_earned', 0)
                max_score = eval_item.get('max_score', 0)
                satisfaction = eval_item.get('satisfaction_level', 'N/A')
                student_work = eval_item.get('student_work', '')
                justification = eval_item.get('justification', '')
                matched_criterion = eval_item.get('matched_criterion', '')
                feedback = eval_item.get('feedback', '')
                evidence = eval_item.get('evidence', [])
                
                f.write(f"\nè¯„åˆ†ç‚¹ {i} ({criterion_id}): {score_earned}/{max_score}åˆ† - {satisfaction}\n")
                if student_work:
                    f.write(f"  å­¦ç”Ÿä½œç­”: {student_work}\n")
                if matched_criterion:
                    f.write(f"  ç¬¦åˆæ ‡å‡†: {matched_criterion}\n")
                f.write(f"  è¯„åˆ†ç†ç”±: {justification}\n")
                if feedback and feedback != "æ— ":
                    f.write(f"  åé¦ˆæ„è§: {feedback}\n")
                if evidence:
                    f.write(f"  è¯æ®:\n")
                    for ev in evidence:
                        f.write(f"    - {ev}\n")
                f.write("\n")
        else:
            f.write("æš‚æ— è¯¦ç»†æ‰¹æ”¹è¯¦æƒ…\n")
        
        f.write("\nè¯¦ç»†åé¦ˆ:\n")
        f.write("-"*80 + "\n")
        feedback_list = result.get('detailed_feedback', [])
        if feedback_list:
            for i, feedback in enumerate(feedback_list, 1):
                if isinstance(feedback, dict):
                    f.write(f"{i}. {feedback.get('content', str(feedback))}\n")
                else:
                    f.write(f"{i}. {feedback}\n")
        else:
            f.write("æš‚æ— è¯¦ç»†åé¦ˆ\n")
        
        f.write("\n" + "="*80 + "\n")
        f.write("é”™è¯¯å’Œè­¦å‘Š\n")
        f.write("="*80 + "\n\n")
        
        errors = result.get('errors', [])
        if errors:
            f.write("é”™è¯¯:\n")
            for i, error in enumerate(errors, 1):
                if isinstance(error, dict):
                    f.write(f"  {i}. [{error.get('step', 'unknown')}] {error.get('error', str(error))}\n")
                else:
                    f.write(f"  {i}. {error}\n")
        else:
            f.write("æ— é”™è¯¯\n")
        
        warnings = result.get('warnings', [])
        if warnings:
            f.write("\nè­¦å‘Š:\n")
            for i, warning in enumerate(warnings, 1):
                if isinstance(warning, dict):
                    f.write(f"  {i}. [{warning.get('step', 'unknown')}] {warning.get('warning', str(warning))}\n")
                else:
                    f.write(f"  {i}. {warning}\n")
        else:
            f.write("\næ— è­¦å‘Š\n")
    
    print(f"ğŸ“„ æ–‡æœ¬ç»“æœå·²ä¿å­˜åˆ°: {text_result_file}")
    
    # æ‰“å°å…³é”®ç»“æœ
    print("\n" + "="*80)
    print("ğŸ“Š æ‰¹æ”¹ç»“æœæ‘˜è¦")
    print("="*80)
    print(f"çŠ¶æ€: {result.get('status', 'N/A')}")
    print(f"æ€»åˆ†: {result.get('total_score', 'N/A')}")
    print(f"ç­‰çº§: {result.get('grade_level', 'N/A')}")
    
    if result.get('errors'):
        print(f"\nâŒ å‘ç° {len(result['errors'])} ä¸ªé”™è¯¯")
    if result.get('warnings'):
        print(f"âš ï¸  å‘ç° {len(result['warnings'])} ä¸ªè­¦å‘Š")
    
    print("="*80)


if __name__ == "__main__":
    # è¿è¡Œä¸»å‡½æ•°
    asyncio.run(main())

