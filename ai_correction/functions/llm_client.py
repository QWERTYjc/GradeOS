#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
LLM å®¢æˆ·ç«¯ - æ”¯æŒ OpenRouter, Gemini, OpenAI
"""

import os
import sys
from pathlib import Path

# æ·»åŠ é¡¹ç›®è·¯å¾„
sys.path.insert(0, str(Path(__file__).parent.parent))

from config import (
    LLM_PROVIDER,
    OPENROUTER_API_KEY,
    OPENROUTER_BASE_URL,
    OPENROUTER_MODEL,
    GEMINI_API_KEY,
    GEMINI_MODEL,
    OPENAI_API_KEY,
    OPENAI_MODEL
)


class LLMClient:
    """ç»Ÿä¸€çš„ LLM å®¢æˆ·ç«¯"""
    
    def __init__(self, provider=None, api_key=None, model=None):
        self.provider = provider or LLM_PROVIDER
        self.api_key = api_key
        self.model = model
        
        # æ ¹æ® provider è®¾ç½®é»˜è®¤å€¼
        if self.provider == 'openrouter':
            self.api_key = self.api_key or OPENROUTER_API_KEY
            self.model = self.model or OPENROUTER_MODEL
            self.base_url = OPENROUTER_BASE_URL
        elif self.provider == 'gemini':
            self.api_key = self.api_key or GEMINI_API_KEY
            self.model = self.model or GEMINI_MODEL
        elif self.provider == 'openai':
            self.api_key = self.api_key or OPENAI_API_KEY
            self.model = self.model or OPENAI_MODEL
        
        print(f"ğŸ¤– LLM Client åˆå§‹åŒ–: provider={self.provider}, model={self.model}")
    
    def chat(self, messages, temperature=0.7, max_tokens=2000):
        """
        ç»Ÿä¸€çš„èŠå¤©æ¥å£
        
        Args:
            messages: æ¶ˆæ¯åˆ—è¡¨ [{"role": "user", "content": "..."}]
            temperature: æ¸©åº¦å‚æ•°
            max_tokens: æœ€å¤§ token æ•°
        
        Returns:
            str: LLM çš„å›å¤
        """
        if self.provider == 'openrouter':
            return self._chat_openrouter(messages, temperature, max_tokens)
        elif self.provider == 'gemini':
            return self._chat_gemini(messages, temperature, max_tokens)
        elif self.provider == 'openai':
            return self._chat_openai(messages, temperature, max_tokens)
        else:
            raise ValueError(f"ä¸æ”¯æŒçš„ LLM provider: {self.provider}")
    
    def _chat_openrouter(self, messages, temperature, max_tokens):
        """ä½¿ç”¨ OpenRouter API"""
        try:
            import requests
            
            headers = {
                "Authorization": f"Bearer {self.api_key}",
                "Content-Type": "application/json",
                "HTTP-Referer": "https://github.com/QWERTYjc/aiguru2.0",
                "X-Title": "AI Correction System"
            }
            
            data = {
                "model": self.model,
                "messages": messages,
                "temperature": temperature,
                "max_tokens": max_tokens
            }
            
            print(f"ğŸ“¡ è°ƒç”¨ OpenRouter API: model={self.model}")
            
            response = requests.post(
                f"{self.base_url}/chat/completions",
                headers=headers,
                json=data,
                timeout=60
            )
            
            response.raise_for_status()
            result = response.json()
            
            if 'choices' in result and len(result['choices']) > 0:
                content = result['choices'][0]['message']['content']
                print(f"âœ… OpenRouter å“åº”æˆåŠŸ: {len(content)} å­—ç¬¦")
                return content
            else:
                raise ValueError(f"OpenRouter å“åº”æ ¼å¼é”™è¯¯: {result}")
                
        except Exception as e:
            print(f"âŒ OpenRouter API è°ƒç”¨å¤±è´¥: {e}")
            raise
    
    def _chat_gemini(self, messages, temperature, max_tokens):
        """ä½¿ç”¨ Gemini API"""
        try:
            import google.generativeai as genai
            
            genai.configure(api_key=self.api_key)
            model = genai.GenerativeModel(self.model)
            
            # è½¬æ¢æ¶ˆæ¯æ ¼å¼
            prompt = "\n".join([f"{msg['role']}: {msg['content']}" for msg in messages])
            
            print(f"ğŸ“¡ è°ƒç”¨ Gemini API: model={self.model}")
            
            response = model.generate_content(
                prompt,
                generation_config={
                    'temperature': temperature,
                    'max_output_tokens': max_tokens
                }
            )
            
            content = response.text
            print(f"âœ… Gemini å“åº”æˆåŠŸ: {len(content)} å­—ç¬¦")
            return content
            
        except Exception as e:
            print(f"âŒ Gemini API è°ƒç”¨å¤±è´¥: {e}")
            raise
    
    def _chat_openai(self, messages, temperature, max_tokens):
        """ä½¿ç”¨ OpenAI API"""
        try:
            from openai import OpenAI
            
            client = OpenAI(api_key=self.api_key)
            
            print(f"ğŸ“¡ è°ƒç”¨ OpenAI API: model={self.model}")
            
            response = client.chat.completions.create(
                model=self.model,
                messages=messages,
                temperature=temperature,
                max_tokens=max_tokens
            )
            
            content = response.choices[0].message.content
            print(f"âœ… OpenAI å“åº”æˆåŠŸ: {len(content)} å­—ç¬¦")
            return content
            
        except Exception as e:
            print(f"âŒ OpenAI API è°ƒç”¨å¤±è´¥: {e}")
            raise


def get_llm_client(provider=None, api_key=None, model=None):
    """è·å– LLM å®¢æˆ·ç«¯å®ä¾‹"""
    return LLMClient(provider=provider, api_key=api_key, model=model)


# æµ‹è¯•ä»£ç 
if __name__ == '__main__':
    # æµ‹è¯• OpenRouter
    print("=" * 60)
    print("ğŸ§ª æµ‹è¯• LLM Client")
    print("=" * 60)
    
    client = get_llm_client()
    
    messages = [
        {"role": "user", "content": "è¯·ç”¨ä¸€å¥è¯ä»‹ç» Python ç¼–ç¨‹è¯­è¨€ã€‚"}
    ]
    
    try:
        response = client.chat(messages)
        print(f"\nğŸ“ LLM å›å¤:\n{response}\n")
        print("âœ… æµ‹è¯•æˆåŠŸï¼")
    except Exception as e:
        print(f"âŒ æµ‹è¯•å¤±è´¥: {e}")

