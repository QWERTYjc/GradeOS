#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
LLM å®¢æˆ·ç«¯ - æ”¯æŒ OpenRouter, Gemini, OpenAI
"""

import os
import sys
from pathlib import Path

# æ·»åŠ é¡¹ç›®è·¯å¾„
sys.path.insert(0, str(Path(__file__).parent.parent))

from config import (
    LLM_PROVIDER,
    OPENROUTER_API_KEY,
    OPENROUTER_BASE_URL,
    OPENROUTER_MODEL,
    GEMINI_API_KEY,
    GEMINI_MODEL,
    OPENAI_API_KEY,
    OPENAI_MODEL
)


class LLMClient:
    """ç»Ÿä¸€çš„ LLM å®¢æˆ·ç«¯"""
    
    def __init__(self, provider=None, api_key=None, model=None):
        self.provider = provider or LLM_PROVIDER
        self.api_key = api_key
        self.model = model
        
        # æ ¹æ® provider è®¾ç½®é»˜è®¤å€¼
        if self.provider == 'openrouter':
            self.api_key = self.api_key or OPENROUTER_API_KEY
            self.model = self.model or OPENROUTER_MODEL
            self.base_url = OPENROUTER_BASE_URL
        elif self.provider == 'gemini':
            self.api_key = self.api_key or GEMINI_API_KEY
            self.model = self.model or GEMINI_MODEL
        elif self.provider == 'openai':
            self.api_key = self.api_key or OPENAI_API_KEY
            self.model = self.model or OPENAI_MODEL
        
        print(f"LLM Client åˆå§‹åŒ–: provider={self.provider}, model={self.model}")
    
    def chat(self, messages, temperature=0.7, max_tokens=None, reasoning_effort=None):
        """
        ç»Ÿä¸€çš„èŠå¤©æ¥å£

        Args:
            messages: æ¶ˆæ¯åˆ—è¡¨ [{"role": "user", "content": "..."}]
            temperature: æ¸©åº¦å‚æ•°
            max_tokens: æœ€å¤§ token æ•°ï¼ˆNone è¡¨ç¤ºä¸é™åˆ¶ï¼Œä½¿ç”¨æ¨¡å‹é»˜è®¤æœ€å¤§å€¼ï¼‰
            reasoning_effort: æ€è€ƒå¼ºåº¦ï¼ˆä»… Gemini 2.5 æ¨¡å‹æ”¯æŒï¼‰ï¼š"low", "medium", "high"

        Returns:
            str: LLM çš„å›å¤
        """
        if self.provider == 'openrouter':
            return self._chat_openrouter(messages, temperature, max_tokens, reasoning_effort)
        elif self.provider == 'gemini':
            return self._chat_gemini(messages, temperature, max_tokens, reasoning_effort)
        elif self.provider == 'openai':
            return self._chat_openai(messages, temperature, max_tokens, reasoning_effort)
        else:
            raise ValueError(f"ä¸æ”¯æŒçš„ LLM provider: {self.provider}")
    
    def _chat_openrouter(self, messages, temperature, max_tokens, reasoning_effort):
        """ä½¿ç”¨ OpenRouter API"""
        try:
            import requests

            headers = {
                "Authorization": f"Bearer {self.api_key}",
                "Content-Type": "application/json",
                "HTTP-Referer": "https://github.com/QWERTYjc/aiguru2.0",
                "X-Title": "AI Correction System"
            }

            data = {
                "model": self.model,
                "messages": messages,
                "temperature": temperature
            }

            # åªåœ¨æŒ‡å®šäº† max_tokens æ—¶æ‰æ·»åŠ è¯¥å‚æ•°
            if max_tokens is not None:
                data["max_tokens"] = max_tokens

            # åªåœ¨æŒ‡å®šäº† reasoning_effort æ—¶æ‰æ·»åŠ è¯¥å‚æ•°ï¼ˆä»… Gemini 2.5 æ¨¡å‹æ”¯æŒï¼‰
            if reasoning_effort is not None:
                data["reasoning_effort"] = reasoning_effort

            print(f"è°ƒç”¨ OpenRouter API: model={self.model}, reasoning_effort={reasoning_effort}")

            # å¯¹äºå¤§å‹è§†è§‰æ¨¡å‹ï¼ˆå¦‚ Qwen3-VL-235Bï¼‰ï¼Œéœ€è¦æ›´é•¿çš„è¶…æ—¶æ—¶é—´
            timeout = 180 if 'vl' in self.model.lower() or 'vision' in self.model.lower() else 60

            response = requests.post(
                f"{self.base_url}/chat/completions",
                headers=headers,
                json=data,
                timeout=timeout
            )
            
            response.raise_for_status()
            result = response.json()
            
            if 'choices' in result and len(result['choices']) > 0:
                content = result['choices'][0]['message']['content']
                print(f"OpenRouter å“åº”æˆåŠŸ: {len(content)} å­—ç¬¦")
                return content
            else:
                raise ValueError(f"OpenRouter å“åº”æ ¼å¼é”™è¯¯: {result}")
                
        except Exception as e:
            print(f"OpenRouter API è°ƒç”¨å¤±è´¥: {e}")
            raise
    
    def _chat_gemini(self, messages, temperature, max_tokens, reasoning_effort):
        """ä½¿ç”¨ Gemini API"""
        try:
            import google.generativeai as genai

            genai.configure(api_key=self.api_key)
            model = genai.GenerativeModel(self.model)

            # è½¬æ¢æ¶ˆæ¯æ ¼å¼
            prompt = "\n".join([f"{msg['role']}: {msg['content']}" for msg in messages])

            print(f"è°ƒç”¨ Gemini API: model={self.model}")

            # æ„å»º generation_config
            generation_config = {'temperature': temperature}

            # åªåœ¨æŒ‡å®šäº† max_tokens æ—¶æ‰æ·»åŠ è¯¥å‚æ•°
            if max_tokens is not None:
                generation_config['max_output_tokens'] = max_tokens

            # æ³¨æ„ï¼šåŸç”Ÿ Gemini API ä¸æ”¯æŒ reasoning_effort å‚æ•°
            # è¯¥å‚æ•°ä»…åœ¨ OpenAI å…¼å®¹æ¥å£ä¸­æ”¯æŒ

            response = model.generate_content(
                prompt,
                generation_config=generation_config
            )

            content = response.text
            print(f"Gemini å“åº”æˆåŠŸ: {len(content)} å­—ç¬¦")
            return content

        except Exception as e:
            print(f"Gemini API è°ƒç”¨å¤±è´¥: {e}")
            raise

    def _chat_openai(self, messages, temperature, max_tokens, reasoning_effort):
        """ä½¿ç”¨ OpenAI API"""
        try:
            from openai import OpenAI

            client = OpenAI(api_key=self.api_key)

            print(f"è°ƒç”¨ OpenAI API: model={self.model}")

            # æ„å»ºè¯·æ±‚å‚æ•°
            params = {
                "model": self.model,
                "messages": messages,
                "temperature": temperature
            }

            # åªåœ¨æŒ‡å®šäº† max_tokens æ—¶æ‰æ·»åŠ è¯¥å‚æ•°
            if max_tokens is not None:
                params["max_tokens"] = max_tokens

            # OpenAI ä¸æ”¯æŒ reasoning_effort å‚æ•°ï¼ˆè¿™æ˜¯ Gemini ç‰¹æœ‰çš„ï¼‰

            response = client.chat.completions.create(**params)

            content = response.choices[0].message.content
            print(f"OpenAI å“åº”æˆåŠŸ: {len(content)} å­—ç¬¦")
            return content

        except Exception as e:
            print(f"OpenAI API è°ƒç”¨å¤±è´¥: {e}")
            raise


def get_llm_client(provider=None, api_key=None, model=None):
    """è·å– LLM å®¢æˆ·ç«¯å®ä¾‹"""
    return LLMClient(provider=provider, api_key=api_key, model=model)


# æµ‹è¯•ä»£ç 
if __name__ == '__main__':
    # æµ‹è¯• OpenRouter
    print("=" * 60)
    print("ğŸ§ª æµ‹è¯• LLM Client")
    print("=" * 60)
    
    client = get_llm_client()
    
    messages = [
        {"role": "user", "content": "è¯·ç”¨ä¸€å¥è¯ä»‹ç» Python ç¼–ç¨‹è¯­è¨€ã€‚"}
    ]
    
    try:
        response = client.chat(messages)
        print(f"\nLLM å›å¤:\n{response}\n")
        print("æµ‹è¯•æˆåŠŸï¼")
    except Exception as e:
        print(f"æµ‹è¯•å¤±è´¥: {e}")

