#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Gemini 3 Pro åŸç”Ÿ SDK å®¢æˆ·ç«¯
å®Œå…¨ç§»é™¤ Vision APIï¼Œä½¿ç”¨ Gemini åŸç”Ÿå¤šæ¨¡æ€èƒ½åŠ›
å‚è€ƒæ–‡æ¡£: https://ai.google.dev/gemini-api/docs/gemini-3
"""

import os
import sys
import logging
from pathlib import Path
from datetime import datetime
from typing import List, Dict, Any, Optional, Union

# æ·»åŠ é¡¹ç›®è·¯å¾„
sys.path.insert(0, str(Path(__file__).parent.parent))

from config import GEMINI_API_KEY, GEMINI_MODEL

logger = logging.getLogger(__name__)


class LLMClient:
    """Gemini 3 Pro åŸç”Ÿ SDK å®¢æˆ·ç«¯"""
    
    def __init__(self, provider=None, api_key=None, model=None, fallback_model=None):
        """
        åˆå§‹åŒ– Gemini 3 Pro å®¢æˆ·ç«¯
        
        Args:
            provider: å¿½ç•¥ï¼ˆå¼ºåˆ¶ä½¿ç”¨ Geminiï¼‰
            api_key: Gemini API å¯†é’¥
            model: æ¨¡å‹åç§°ï¼ˆé»˜è®¤ gemini-3-pro-previewï¼‰
            fallback_model: å¿½ç•¥
        """
        self.provider = "gemini"
        self.api_key = api_key or GEMINI_API_KEY
        self.model = model or GEMINI_MODEL or "gemini-3-pro-preview"
        self.last_call = None
        
        # åˆå§‹åŒ– Gemini SDK
        try:
            from google import genai
            from google.genai import types
            
            self.client = genai.Client(
                api_key=self.api_key,
                http_options={'api_version': 'v1beta'}  # ä½¿ç”¨ v1beta ä»¥æ”¯æŒæœ€æ–°åŠŸèƒ½
            )
            self.types = types
            logger.info(f"âœ… Gemini 3 Pro å®¢æˆ·ç«¯åˆå§‹åŒ–æˆåŠŸ: model={self.model}")
        except ImportError:
            error_msg = "âŒ è¯·å®‰è£… Google GenAI SDK: pip install google-genai"
            logger.error(error_msg)
            raise ImportError(error_msg)
        except Exception as e:
            logger.error(f"âŒ Gemini å®¢æˆ·ç«¯åˆå§‹åŒ–å¤±è´¥: {e}")
            raise
    
    def chat(
        self,
        messages: List[Dict[str, Any]],
        temperature: float = 1.0,  # Gemini 3 æ¨èä½¿ç”¨é»˜è®¤å€¼ 1.0
        max_tokens: Optional[int] = None,
        reasoning_effort: Optional[str] = None,  # å·²å¼ƒç”¨ï¼Œä½¿ç”¨ thinking_level
        timeout: Optional[int] = None,
        stream: bool = False,
        thinking_level: str = "high",  # Gemini 3 æ–°å‚æ•°
        files: Optional[List[str]] = None,  # PDF æ–‡ä»¶è·¯å¾„åˆ—è¡¨
        include_thoughts: bool = False  # æ˜¯å¦åŒ…å«æ€è€ƒè¿‡ç¨‹
    ) -> Union[str, Any]:
        """
        ç»Ÿä¸€çš„èŠå¤©æ¥å£ï¼ˆä½¿ç”¨ Gemini 3 Pro åŸç”Ÿ SDKï¼‰

        Args:
            messages: æ¶ˆæ¯åˆ—è¡¨ [{"role": "user", "content": "..."}]
            temperature: æ¸©åº¦å‚æ•°ï¼ˆGemini 3 æ¨èä½¿ç”¨é»˜è®¤å€¼ 1.0ï¼‰
            max_tokens: æœ€å¤§ token æ•°
            reasoning_effort: å·²å¼ƒç”¨ï¼Œä½¿ç”¨ thinking_level
            timeout: è¶…æ—¶æ—¶é—´ï¼ˆç§’ï¼‰
            stream: æ˜¯å¦ä½¿ç”¨æµå¼ä¼ è¾“
            thinking_level: æ€è€ƒç­‰çº§ ("low", "high")
            files: PDF æ–‡ä»¶è·¯å¾„åˆ—è¡¨ï¼ˆç”¨äºå¤šæ¨¡æ€è¾“å…¥ï¼‰
            include_thoughts: æ˜¯å¦åŒ…å«æ€è€ƒè¿‡ç¨‹ï¼ˆä»…åœ¨ stream=True æ—¶æœ‰æ•ˆï¼‰

        Returns:
            str æˆ– Generator: LLM çš„å›å¤
        """
        try:
            # è½¬æ¢ messages ä¸º Gemini æ ¼å¼
            contents = self._convert_messages_to_gemini_contents(messages, files)

            # æ„å»ºé…ç½®
            config = self._build_generation_config(
                temperature=temperature,
                max_tokens=max_tokens,
                thinking_level=thinking_level,
                include_thoughts=include_thoughts
            )

            logger.info(f"ğŸš€ è°ƒç”¨ Gemini 3 Pro: model={self.model}, thinking_level={thinking_level}, include_thoughts={include_thoughts}")

            if stream:
                # æµå¼ä¼ è¾“æ¨¡å¼
                return self._chat_stream(contents, config, include_thoughts)
            else:
                # éæµå¼æ¨¡å¼
                response = self.client.models.generate_content(
                    model=self.model,
                    contents=contents,
                    config=config
                )
                
                # æå–æ–‡æœ¬å†…å®¹
                text = response.text
                logger.info(f"âœ… Gemini å“åº”æˆåŠŸ: {len(text)} å­—ç¬¦")
                
                # è®°å½•è°ƒç”¨ä¿¡æ¯
                self._record_last_call(messages, text, temperature, max_tokens, thinking_level)
                
                return text
                
        except Exception as e:
            logger.error(f"âŒ Gemini API è°ƒç”¨å¤±è´¥: {e}")
            raise
    
    def _convert_messages_to_gemini_contents(
        self,
        messages: List[Dict[str, Any]],
        files: Optional[List[str]] = None
    ) -> List[Any]:
        """å°† OpenAI æ ¼å¼çš„ messages è½¬æ¢ä¸º Gemini çš„ contents æ ¼å¼"""
        contents = []
        
        for msg in messages:
            role = msg.get("role", "user")
            content = msg.get("content", "")
            
            # è·³è¿‡ system æ¶ˆæ¯
            if role == "system":
                continue
            
            # è½¬æ¢ role
            gemini_role = "model" if role == "assistant" else "user"
            
            # æ„å»º parts
            parts = []
            
            if isinstance(content, str):
                parts.append(self.types.Part(text=content))
            
            # æ·»åŠ  PDF æ–‡ä»¶
            if files and gemini_role == "user" and not contents:
                for file_path in files:
                    parts.append(self._upload_file(file_path))
            
            if parts:
                contents.append(self.types.Content(role=gemini_role, parts=parts))

        return contents

    def _upload_file(self, file_path: str) -> Any:
        """
        ä¸Šä¼ æ–‡ä»¶åˆ° Gemini APIï¼ˆä½¿ç”¨ File APIï¼‰

        Args:
            file_path: æ–‡ä»¶è·¯å¾„

        Returns:
            Gemini Part å¯¹è±¡
        """
        try:
            file_path = Path(file_path)

            # è¯»å–æ–‡ä»¶
            with open(file_path, 'rb') as f:
                file_data = f.read()

            # ç¡®å®š MIME ç±»å‹
            mime_type = self._get_mime_type(file_path)

            # ä½¿ç”¨ inline_data ç›´æ¥ä¼ é€’æ–‡ä»¶å†…å®¹
            import base64
            base64_data = base64.b64encode(file_data).decode('utf-8')

            logger.info(f"ğŸ“„ ä¸Šä¼ æ–‡ä»¶: {file_path.name}, MIME: {mime_type}, å¤§å°: {len(file_data)} bytes")

            return self.types.Part(
                inline_data=self.types.Blob(
                    mime_type=mime_type,
                    data=base64_data
                )
            )

        except Exception as e:
            logger.error(f"âŒ æ–‡ä»¶ä¸Šä¼ å¤±è´¥: {file_path}, é”™è¯¯: {e}")
            raise

    def _get_mime_type(self, file_path: Path) -> str:
        """è·å–æ–‡ä»¶çš„ MIME ç±»å‹"""
        suffix = file_path.suffix.lower()
        mime_types = {
            '.pdf': 'application/pdf',
            '.png': 'image/png',
            '.jpg': 'image/jpeg',
            '.jpeg': 'image/jpeg',
            '.gif': 'image/gif',
            '.webp': 'image/webp'
        }
        return mime_types.get(suffix, 'application/octet-stream')

    def _build_generation_config(
        self,
        temperature: float,
        max_tokens: Optional[int],
        thinking_level: str,
        include_thoughts: bool = False
    ) -> Any:
        """æ„å»º Gemini ç”Ÿæˆé…ç½®"""
        config_dict = {
            "temperature": temperature,
        }

        is_flash_model = "flash" in (self.model or "").lower()
        if thinking_level and not is_flash_model:
            config_dict["thinking_config"] = self.types.ThinkingConfig(
                thinking_level=thinking_level,
                include_thoughts=include_thoughts  # æ˜¯å¦åŒ…å«æ€è€ƒè¿‡ç¨‹
            )
        elif is_flash_model and thinking_level:
            logger.debug(f"Model {self.model} ä¸æ”¯æŒ thinking_levelï¼Œå·²è·³è¿‡æ€è€ƒé…ç½®")

        if max_tokens is not None:
            config_dict["max_output_tokens"] = max_tokens

        return self.types.GenerateContentConfig(**config_dict)

    def _chat_stream(self, contents: List[Any], config: Any, include_thoughts: bool = False) -> Any:
        """
        æµå¼ä¼ è¾“æ¨¡å¼

        Yields:
            Dict: {"type": "thought" | "text", "content": str}
        """
        try:
            response = self.client.models.generate_content_stream(
                model=self.model,
                contents=contents,
                config=config
            )

            for chunk in response:
                # æ£€æŸ¥æ˜¯å¦æœ‰æ€è€ƒå†…å®¹ï¼ˆGemini 3 Pro ç‰¹æ€§ï¼‰
                if include_thoughts and hasattr(chunk, 'candidates') and chunk.candidates:
                    candidate = chunk.candidates[0]
                    if hasattr(candidate, 'content') and hasattr(candidate.content, 'parts'):
                        for part in candidate.content.parts:
                            # æ€è€ƒå†…å®¹
                            if hasattr(part, 'thought') and part.thought:
                                yield {
                                    "type": "thought",
                                    "content": str(part.thought)
                                }
                            # æ™®é€šæ–‡æœ¬å†…å®¹
                            elif hasattr(part, 'text') and part.text:
                                yield {
                                    "type": "text",
                                    "content": part.text
                                }
                # å…¼å®¹æ—§ç‰ˆæœ¬ï¼šç›´æ¥è¿”å›æ–‡æœ¬
                elif hasattr(chunk, 'text') and chunk.text:
                    yield {
                        "type": "text",
                        "content": chunk.text
                    }

        except Exception as e:
            logger.error(f"âŒ Gemini æµå¼ API è°ƒç”¨å¤±è´¥: {e}")
            raise

    def _record_last_call(
        self,
        messages: List[Dict[str, Any]],
        response: str,
        temperature: float,
        max_tokens: Optional[int],
        thinking_level: str
    ):
        """è®°å½•æœ€è¿‘ä¸€æ¬¡è°ƒç”¨ä¿¡æ¯"""
        try:
            self.last_call = {
                "provider": "gemini",
                "model": self.model,
                "temperature": temperature,
                "max_tokens": max_tokens,
                "thinking_level": thinking_level,
                "message_count": len(messages),
                "response_preview": response[:1000] if response else None,
                "timestamp": datetime.now().isoformat()
            }
        except Exception:
            self.last_call = None


def get_llm_client(provider=None, api_key=None, model=None):
    """è·å– LLM å®¢æˆ·ç«¯å®ä¾‹"""
    return LLMClient(provider=provider, api_key=api_key, model=model)


# æµ‹è¯•ä»£ç 
if __name__ == '__main__':
    print("=" * 60)
    print("ğŸ§ª æµ‹è¯• Gemini 3 Pro Client")
    print("=" * 60)

    client = get_llm_client()

    messages = [
        {"role": "user", "content": "è¯·ç”¨ä¸€å¥è¯ä»‹ç» Python ç¼–ç¨‹è¯­è¨€ã€‚"}
    ]

    try:
        response = client.chat(messages)
        print(f"\nGemini å›å¤:\n{response}\n")
        print("âœ… æµ‹è¯•æˆåŠŸï¼")
    except Exception as e:
        print(f"âŒ æµ‹è¯•å¤±è´¥: {e}")
