#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ResultAggregator Agent - èšåˆæ‰¹æ”¹ç»“æœï¼Œç”Ÿæˆç»Ÿè®¡æ•°æ®
"""

from typing import Dict, Any, List
from collections import defaultdict


class ResultAggregatorAgent:
    """ç»“æœèšåˆ Agent"""
    
    def __init__(self):
        pass
    
    def aggregate(self, state: Dict[str, Any]) -> Dict[str, Any]:
        """
        èšåˆæ‰¹æ”¹ç»“æœ
        
        Args:
            state: åŒ…å« grading_results çš„çŠ¶æ€
            
        Returns:
            æ›´æ–°åçš„çŠ¶æ€ï¼Œæ·»åŠ  aggregated_results, statistics
        """
        try:
            grading_results = state.get('grading_results', [])
            student_info = state.get('student_info', {})
            
            # è®¡ç®—æ€»åˆ†
            total_score = sum(r['score'] for r in grading_results)
            max_total_score = sum(r['max_score'] for r in grading_results)
            
            # è®¡ç®—ç™¾åˆ†æ¯”
            percentage = (total_score / max_total_score * 100) if max_total_score > 0 else 0
            
            # ç¡®å®šç­‰çº§
            grade = self._calculate_grade(percentage)
            
            # åˆ†æé”™è¯¯ç±»å‹
            error_analysis = self._analyze_errors(grading_results)
            
            # çŸ¥è¯†ç‚¹åˆ†æ
            knowledge_analysis = self._analyze_knowledge_points(grading_results, state.get('questions', []))
            
            # ç”Ÿæˆæ€»ç»“
            summary = self._generate_summary(grading_results, total_score, max_total_score, grade)
            
            # èšåˆç»“æœ
            aggregated_results = {
                'student_info': student_info,
                'total_score': total_score,
                'max_score': max_total_score,
                'percentage': percentage,
                'grade': grade,
                'question_count': len(grading_results),
                'correct_count': sum(1 for r in grading_results if r['score'] >= r['max_score'] * 0.6),
                'error_analysis': error_analysis,
                'knowledge_analysis': knowledge_analysis,
                'summary': summary,
                'details': grading_results
            }
            
            # ç”Ÿæˆç»Ÿè®¡æ•°æ®
            statistics = self._generate_statistics(grading_results, state.get('questions', []))
            
            state.update({
                'aggregated_results': aggregated_results,
                'statistics': statistics,
                'aggregation_status': 'success'
            })
            
            return state
            
        except Exception as e:
            state.update({
                'aggregation_status': 'failed',
                'aggregation_errors': [str(e)]
            })
            return state
    
    def _calculate_grade(self, percentage: float) -> str:
        """è®¡ç®—ç­‰çº§"""
        if percentage >= 90:
            return 'A'
        elif percentage >= 80:
            return 'B'
        elif percentage >= 70:
            return 'C'
        elif percentage >= 60:
            return 'D'
        else:
            return 'F'
    
    def _analyze_errors(self, results: List[Dict]) -> Dict:
        """åˆ†æé”™è¯¯ç±»å‹"""
        error_types = defaultdict(int)
        error_questions = []
        
        for result in results:
            score_rate = result['score'] / result['max_score'] if result['max_score'] > 0 else 0
            
            if score_rate < 0.6:  # å¾—åˆ†ç‡ä½äº60%è§†ä¸ºé”™è¯¯
                error_questions.append({
                    'question_id': result['question_id'],
                    'score': result['score'],
                    'max_score': result['max_score'],
                    'feedback': result.get('feedback', '')
                })
                
                # ç»Ÿè®¡é”™è¯¯ç±»å‹
                strategy = result.get('strategy', 'unknown')
                error_types[strategy] += 1
        
        return {
            'total_errors': len(error_questions),
            'error_rate': len(error_questions) / len(results) if results else 0,
            'error_types': dict(error_types),
            'error_questions': error_questions
        }
    
    def _analyze_knowledge_points(self, results: List[Dict], questions: List[Dict]) -> Dict:
        """åˆ†æçŸ¥è¯†ç‚¹æŒæ¡æƒ…å†µ"""
        knowledge_points = defaultdict(lambda: {'total': 0, 'correct': 0, 'score': 0, 'max_score': 0})
        
        for result in results:
            question = next((q for q in questions if q['id'] == result['question_id']), None)
            if not question:
                continue
            
            # æå–çŸ¥è¯†ç‚¹ï¼ˆä»å…³é”®è¯ï¼‰
            keywords = question.get('analysis', {}).get('keywords', [])
            
            for keyword in keywords:
                knowledge_points[keyword]['total'] += 1
                knowledge_points[keyword]['score'] += result['score']
                knowledge_points[keyword]['max_score'] += result['max_score']
                
                if result['score'] >= result['max_score'] * 0.6:
                    knowledge_points[keyword]['correct'] += 1
        
        # è®¡ç®—æŒæ¡ç‡
        for kp in knowledge_points.values():
            kp['mastery_rate'] = kp['correct'] / kp['total'] if kp['total'] > 0 else 0
            kp['score_rate'] = kp['score'] / kp['max_score'] if kp['max_score'] > 0 else 0
        
        return dict(knowledge_points)
    
    def _generate_summary(self, results: List[Dict], total_score: int, max_score: int, grade: str) -> str:
        """ç”Ÿæˆæ€»ç»“"""
        correct_count = sum(1 for r in results if r['score'] >= r['max_score'] * 0.6)
        total_count = len(results)
        
        summary = f"""
## ğŸ“Š æ‰¹æ”¹æ€»ç»“

### åŸºæœ¬ä¿¡æ¯
- æ€»åˆ†ï¼š{total_score}/{max_score} åˆ†
- å¾—åˆ†ç‡ï¼š{total_score/max_score*100:.1f}%
- ç­‰çº§ï¼š{grade}
- ç­”å¯¹é¢˜æ•°ï¼š{correct_count}/{total_count}

### æ•´ä½“è¯„ä»·
"""
        
        if grade in ['A', 'B']:
            summary += "- âœ… æ•´ä½“è¡¨ç°ä¼˜ç§€ï¼Œç»§ç»­ä¿æŒï¼\n"
        elif grade == 'C':
            summary += "- âš ï¸ æ•´ä½“è¡¨ç°è‰¯å¥½ï¼Œè¿˜æœ‰æå‡ç©ºé—´ã€‚\n"
        else:
            summary += "- âŒ éœ€è¦åŠ å¼ºå­¦ä¹ ï¼Œå¤šåšç»ƒä¹ ã€‚\n"
        
        return summary
    
    def _generate_statistics(self, results: List[Dict], questions: List[Dict]) -> Dict:
        """ç”Ÿæˆç»Ÿè®¡æ•°æ®"""
        # æŒ‰é¢˜å‹ç»Ÿè®¡
        type_stats = defaultdict(lambda: {'count': 0, 'total_score': 0, 'max_score': 0})
        
        for result in results:
            question = next((q for q in questions if q['id'] == result['question_id']), None)
            if not question:
                continue
            
            q_type = question.get('type', 'unknown')
            type_stats[q_type]['count'] += 1
            type_stats[q_type]['total_score'] += result['score']
            type_stats[q_type]['max_score'] += result['max_score']
        
        # è®¡ç®—å¾—åˆ†ç‡
        for stats in type_stats.values():
            stats['score_rate'] = stats['total_score'] / stats['max_score'] if stats['max_score'] > 0 else 0
        
        # æŒ‰éš¾åº¦ç»Ÿè®¡
        difficulty_stats = defaultdict(lambda: {'count': 0, 'total_score': 0, 'max_score': 0})
        
        for result in results:
            question = next((q for q in questions if q['id'] == result['question_id']), None)
            if not question:
                continue
            
            difficulty = question.get('analysis', {}).get('difficulty', 'medium')
            difficulty_stats[difficulty]['count'] += 1
            difficulty_stats[difficulty]['total_score'] += result['score']
            difficulty_stats[difficulty]['max_score'] += result['max_score']
        
        # è®¡ç®—å¾—åˆ†ç‡
        for stats in difficulty_stats.values():
            stats['score_rate'] = stats['total_score'] / stats['max_score'] if stats['max_score'] > 0 else 0
        
        return {
            'by_type': dict(type_stats),
            'by_difficulty': dict(difficulty_stats),
            'total_questions': len(results),
            'average_score': sum(r['score'] for r in results) / len(results) if results else 0
        }


class RubricInterpreterAgent:
    """è¯„åˆ†æ ‡å‡†è§£é‡Š Agent"""
    
    def __init__(self):
        pass
    
    def interpret(self, state: Dict[str, Any]) -> Dict[str, Any]:
        """
        è§£é‡Šè¯„åˆ†æ ‡å‡†
        
        Args:
            state: åŒ…å« marking_scheme çš„çŠ¶æ€
            
        Returns:
            æ›´æ–°åçš„çŠ¶æ€ï¼Œæ·»åŠ  interpreted_rubric
        """
        try:
            marking_scheme = state.get('marking_scheme', {})
            
            if not marking_scheme:
                state.update({
                    'interpreted_rubric': {},
                    'rubric_status': 'empty'
                })
                return state
            
            # è§£æè¯„åˆ†æ ‡å‡†
            criteria = marking_scheme.get('criteria', [])
            
            # ç”Ÿæˆç»“æ„åŒ–çš„è¯„åˆ†æ ‡å‡†
            interpreted = {
                'total_points': sum(c['points'] for c in criteria),
                'criteria_count': len(criteria),
                'criteria': criteria,
                'raw_text': marking_scheme.get('raw_text', '')
            }
            
            state.update({
                'interpreted_rubric': interpreted,
                'rubric_status': 'success'
            })
            
            return state
            
        except Exception as e:
            state.update({
                'rubric_status': 'failed',
                'rubric_errors': [str(e)]
            })
            return state

