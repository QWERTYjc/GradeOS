#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
RubricInterpreterAgent - è¯„åˆ†æ ‡å‡†è§£æžAgent
è§£æžè¯„åˆ†æ ‡å‡†ï¼Œæå–è¯„åˆ†ç‚¹å’Œåˆ†å€¼
"""

import logging
import json
from typing import List, Dict, Any
from datetime import datetime

from ..state import GradingState
from ..multimodal_models import RubricUnderstanding, GradingCriterion
from ..prompts.multimodal_prompts import format_rubric_interpretation_prompt
from ...llm_client import get_llm_client

logger = logging.getLogger(__name__)


class RubricInterpreterAgent:
    """è¯„åˆ†æ ‡å‡†è§£æžAgent"""
    
    def __init__(self):
        self.name = "RubricInterpreterAgent"
        self.llm_client = get_llm_client()
    
    async def __call__(self, state: GradingState) -> GradingState:
        """æ‰§è¡Œè¯„åˆ†æ ‡å‡†è§£æž"""
        logger.info(f"ðŸ”„ {self.name} å¼€å§‹å¤„ç†...")
        
        try:
            state['current_step'] = "è¯„åˆ†æ ‡å‡†è§£æž"
            state['progress_percentage'] = 45.0
            
            # èŽ·å–è¯„åˆ†æ ‡å‡†æ–‡ä»¶
            marking_files = state.get('marking_multimodal_files', [])
            if not marking_files:
                logger.warning("æ²¡æœ‰è¯„åˆ†æ ‡å‡†æ–‡ä»¶ï¼Œä½¿ç”¨é»˜è®¤æ ‡å‡†")
                state['rubric_understanding'] = self._default_rubric()
                return state
            
            # å¤„ç†ç¬¬ä¸€ä¸ªè¯„åˆ†æ ‡å‡†æ–‡ä»¶
            marking_file = marking_files[0]
            modality_type = marking_file['modality_type']
            content = marking_file['content_representation']
            
            logger.info(f"å¤„ç†è¯„åˆ†æ ‡å‡†æ–‡ä»¶ï¼Œæ¨¡æ€ç±»åž‹: {modality_type}")
            
            # æå–æ–‡æœ¬å†…å®¹
            if modality_type == 'text':
                rubric_text = content['text']
            elif modality_type == 'pdf_text':
                rubric_text = content['text']
            else:
                rubric_text = ""
            
            # è§£æžè¯„åˆ†æ ‡å‡†
            if rubric_text:
                understanding = await self._interpret_rubric(rubric_text)
            else:
                understanding = self._default_rubric()
            
            # æ›´æ–°çŠ¶æ€
            state['rubric_understanding'] = understanding
            state['progress_percentage'] = 50.0
            
            logger.info(f"âœ… {self.name} å¤„ç†å®Œæˆï¼Œå…±{len(understanding.get('criteria', []))}ä¸ªè¯„åˆ†ç‚¹")
            return state
            
        except Exception as e:
            logger.error(f"{self.name} å¤±è´¥: {e}")
            state['errors'].append({
                'step': 'rubric_interpretation',
                'error': str(e),
                'timestamp': str(datetime.now())
            })
            state['rubric_understanding'] = self._default_rubric()
            return state
    
    async def _interpret_rubric(self, rubric_text: str) -> RubricUnderstanding:
        """è§£æžè¯„åˆ†æ ‡å‡†"""
        prompt = format_rubric_interpretation_prompt(rubric_text)
        
        messages = [
            {"role": "system", "content": "ä½ æ˜¯ä¸€ä½èµ„æ·±æ•™è‚²ä¸“å®¶ï¼Œæ“…é•¿è§£æžè¯„åˆ†æ ‡å‡†ã€‚"},
            {"role": "user", "content": prompt}
        ]
        
        try:
            response = self.llm_client.chat(messages, temperature=0.2, max_tokens=3000)
            return self._parse_rubric(response)
        except Exception as e:
            logger.error(f"LLMè°ƒç”¨å¤±è´¥: {e}")
            return self._parse_simple_rubric(rubric_text)
    
    def _parse_rubric(self, response: str) -> RubricUnderstanding:
        """è§£æžLLMå“åº”"""
        try:
            json_start = response.find('{')
            json_end = response.rfind('}') + 1
            if json_start >= 0 and json_end > json_start:
                result = json.loads(response[json_start:json_end])
                
                # è½¬æ¢criteriaä¸ºGradingCriterionç±»åž‹
                criteria = []
                for c in result.get('criteria', []):
                    criteria.append(GradingCriterion(
                        criterion_id=c.get('criterion_id', ''),
                        description=c.get('description', ''),
                        points=float(c.get('points', 0)),
                        evaluation_method=c.get('evaluation_method', 'semantic'),
                        keywords=c.get('keywords'),
                        required_elements=c.get('required_elements')
                    ))
                
                return RubricUnderstanding(
                    rubric_id=result.get('rubric_id', 'R1'),
                    criteria=criteria,
                    total_points=float(result.get('total_points', 0)),
                    grading_rules=result.get('grading_rules', {}),
                    strictness_guidance=result.get('strictness_guidance')
                )
        except Exception as e:
            logger.warning(f"JSONè§£æžå¤±è´¥: {e}")
        
        return self._default_rubric()
    
    def _parse_simple_rubric(self, rubric_text: str) -> RubricUnderstanding:
        """ç®€å•è§£æžè¯„åˆ†æ ‡å‡†ï¼ˆæ–‡æœ¬åˆ†æžï¼‰"""
        import re
        
        # å°è¯•æå–è¯„åˆ†ç‚¹å’Œåˆ†å€¼
        criteria = []
        total_points = 0.0
        
        # æŸ¥æ‰¾åŒ…å«åˆ†å€¼çš„è¡Œ
        lines = rubric_text.split('\n')
        for i, line in enumerate(lines):
            # åŒ¹é…æ¨¡å¼å¦‚ "1. xxx (5åˆ†)" æˆ– "è¯„åˆ†ç‚¹1ï¼šxxx 5åˆ†"
            patterns = [
                r'(\d+)[.ã€ï¼š:]\s*(.+?)\s*[ï¼ˆ(]?(\d+(?:\.\d+)?)\s*åˆ†[ï¼‰)]?',
                r'(.+?)\s*[ï¼ˆ(]?(\d+(?:\.\d+)?)\s*åˆ†[ï¼‰)]?'
            ]
            
            for pattern in patterns:
                match = re.search(pattern, line)
                if match:
                    groups = match.groups()
                    if len(groups) >= 2:
                        description = groups[-2] if len(groups) > 2 else groups[0]
                        points = float(groups[-1])
                        
                        criteria.append(GradingCriterion(
                            criterion_id=f"C{i+1}",
                            description=description.strip(),
                            points=points,
                            evaluation_method='semantic',
                            keywords=None,
                            required_elements=None
                        ))
                        total_points += points
                        break
        
        if not criteria:
            # å¦‚æžœæ²¡æœ‰æ‰¾åˆ°è¯„åˆ†ç‚¹ï¼Œåˆ›å»ºé»˜è®¤è¯„åˆ†ç‚¹
            criteria = [
                GradingCriterion(
                    criterion_id="C1",
                    description="ç­”æ¡ˆæ­£ç¡®æ€§",
                    points=100.0,
                    evaluation_method='semantic',
                    keywords=None,
                    required_elements=None
                )
            ]
            total_points = 100.0
        
        return RubricUnderstanding(
            rubric_id='R1',
            criteria=criteria,
            total_points=total_points,
            grading_rules={},
            strictness_guidance=None
        )
    
    def _default_rubric(self) -> RubricUnderstanding:
        """é»˜è®¤è¯„åˆ†æ ‡å‡†"""
        return RubricUnderstanding(
            rubric_id='R_DEFAULT',
            criteria=[
                GradingCriterion(
                    criterion_id="C1",
                    description="ç­”æ¡ˆå®Œæ•´æ€§å’Œæ­£ç¡®æ€§",
                    points=100.0,
                    evaluation_method='semantic',
                    keywords=None,
                    required_elements=None
                )
            ],
            total_points=100.0,
            grading_rules={'partial_credit': 'yes'},
            strictness_guidance=None
        )
