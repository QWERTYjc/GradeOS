#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
RubricInterpreterAgent - è¯„åˆ†æ ‡å‡†è§£æAgent
è§£æè¯„åˆ†æ ‡å‡†ï¼Œæå–è¯„åˆ†ç‚¹å’Œåˆ†å€¼
"""

import logging
import json
import os
from typing import List, Dict, Any
from datetime import datetime
from pathlib import Path

from ..state import GradingState
from ..multimodal_models import RubricUnderstanding, GradingCriterion
from ..prompts.multimodal_prompts import format_rubric_interpretation_prompt
from ...llm_client import LLMClient

logger = logging.getLogger(__name__)


class RubricInterpreterAgent:
    """è¯„åˆ†æ ‡å‡†è§£æAgent"""

    def __init__(self):
        self.name = "RubricInterpreterAgent"
        # ä½¿ç”¨ Gemini 3 Pro åŸç”Ÿ APIï¼Œæ”¯æŒçœŸæ­£çš„å¤šæ¨¡æ€ PDF å¤„ç†
        self.llm_client = LLMClient(
            provider='gemini',
            model='gemini-3-pro-preview'
        )
        self.reasoning_effort = None

    async def __call__(self, state: GradingState) -> GradingState:
        """æ‰§è¡Œè¯„åˆ†æ ‡å‡†è§£æ"""
        logger.info(f"{self.name} å¼€å§‹å¤„ç†...")

        state.setdefault('step_results', {})
        try:
            # è·å–è¯„åˆ†æ ‡å‡†æ–‡ä»¶
            marking_files = state.get('marking_multimodal_files', [])
            if not marking_files:
                logger.warning("æ²¡æœ‰è¯„åˆ†æ ‡å‡†æ–‡ä»¶ï¼Œä½¿ç”¨é»˜è®¤æ ‡å‡†")
                return {
                    'rubric_understanding': self._default_rubric()
                }

            # å¤„ç†ç¬¬ä¸€ä¸ªè¯„åˆ†æ ‡å‡†æ–‡ä»¶
            marking_file = marking_files[0]
            modality_type = marking_file['modality_type']
            content = marking_file['content_representation']

            logger.info(f"å¤„ç†è¯„åˆ†æ ‡å‡†æ–‡ä»¶ï¼Œæ¨¡æ€ç±»å‹: {modality_type}")

            # ????????????
            if modality_type in ['pdf', 'pdf_image', 'image']:
                pdf_file_path = marking_file.get('file_path') or content.get('file_path')
                if pdf_file_path:
                    file_type = "PDF" if modality_type in ['pdf', 'pdf_image'] else "å›¾ç‰‡"
                    logger.info(f"ğŸ“„ æ£€æµ‹åˆ° {file_type} è¯„åˆ†æ ‡å‡†ï¼Œå‡†å¤‡è§£æ: path={pdf_file_path}, pages={content.get('page_count', 'unknown')}")
                    
                    logger.info(f"ğŸ” ä½¿ç”¨ Gemini 3 Pro åŸç”Ÿå¤šæ¨¡æ€è§£æè¯„åˆ†æ ‡å‡† {file_type}: {pdf_file_path}")
                    rubric_understanding = await self._extract_and_parse_rubric_from_pdf(pdf_file_path)

                    criteria_num = len(rubric_understanding.get('criteria', []))
                    logger.info(f"Gemini è§£æå®Œæˆï¼Œæå–åˆ° {criteria_num} ä¸ªè¯„åˆ†ç‚¹")
                    self._record_step_trace(
                        state,
                        summary=f"Gemini è§£æ {file_type}ï¼Œæå– {criteria_num} ä¸ªè¯„åˆ†ç‚¹",
                        extra={
                            'criteria_count': criteria_num,
                            'total_points': rubric_understanding.get('total_points')
                        }
                    )
                    return {
                        'rubric_understanding': rubric_understanding,
                        'rubric_parsing_result': {
                            'rubric_id': rubric_understanding['rubric_id'],
                            'total_points': rubric_understanding['total_points'],
                            'criteria_count': len(rubric_understanding['criteria']),
                            'parsing_method': 'vision_api_pdf_direct'
                        }
                    }
                else:
                    logger.warning(f"è¯„åˆ†æ ‡å‡†ä¸º {modality_type} ä½†ç¼ºå°‘æ–‡ä»¶è·¯å¾„ï¼Œæ— æ³•è°ƒç”¨ Gemini è§£æ")
                    return {'rubric_understanding': self._default_rubric()}

            rubric_text = ""
            if modality_type == 'text':
                rubric_text = content['text']
            elif modality_type == 'pdf_text':
                rubric_text = content['text']

            # è§£æè¯„åˆ†æ ‡å‡†ï¼ˆæ–‡æœ¬ç±»å‹ï¼‰
            if rubric_text and len(rubric_text.strip()) > 10:
                understanding = await self._interpret_rubric(rubric_text)
            else:
                logger.warning("è¯„åˆ†æ ‡å‡†æ–‡æœ¬ä¸ºç©ºæˆ–è¿‡çŸ­ï¼Œä½¿ç”¨é»˜è®¤æ ‡å‡†")
                understanding = self._default_rubric()

            # è®°å½•è¯¦ç»†çš„è§£æç»“æœ
            criteria_count = len(understanding.get('criteria', []))
            total_points = understanding.get('total_points', 0)
            logger.info(f"{self.name} å¤„ç†å®Œæˆ")
            logger.info(f"   å…±è§£æå‡º {criteria_count} ä¸ªè¯„åˆ†ç‚¹")
            logger.info(f"   æ€»åˆ†: {total_points} åˆ†")

            # æ‰“å°æ¯ä¸ªè¯„åˆ†ç‚¹çš„è¯¦ç»†ä¿¡æ¯
            for i, criterion in enumerate(understanding.get('criteria', []), 1):
                logger.info(f"   è¯„åˆ†ç‚¹{i}: [{criterion.get('criterion_id', 'N/A')}] {criterion.get('description', 'N/A')[:50]}... ({criterion.get('points', 0)}åˆ†)")

            # ä¿å­˜åŸå§‹æ–‡æœ¬ç”¨äºè°ƒè¯•
            understanding['raw_rubric_text'] = rubric_text[:500]  # ä¿å­˜å‰500å­—ç¬¦ç”¨äºè°ƒè¯•

            # è®°å½•LLMè°ƒç”¨è½¨è¿¹
            self._record_step_trace(
                state,
                summary=f"è§£æè¯„åˆ†æ ‡å‡†ï¼Œå…± {criteria_count} ä¸ªè¯„åˆ†ç‚¹ï¼Œæ€»åˆ† {total_points}",
                extra={
                    'criteria_count': criteria_count,
                    'total_points': total_points,
                    'source': 'text' if rubric_text else 'pdf'
                }
            )

            # åªè¿”å›éœ€è¦æ›´æ–°çš„å­—æ®µï¼Œé¿å…å¹¶å‘æ›´æ–°å†²çª
            # æ³¨æ„ï¼šä¸è¿”å›progress_percentageå’Œcurrent_stepï¼Œå› ä¸ºå¹¶è¡ŒèŠ‚ç‚¹ä¼šå†²çª
            return {
                'rubric_understanding': understanding
            }

        except Exception as e:
            logger.error(f"{self.name} ??: {e}")
            return {
                'errors': [{
                    'step': 'rubric_interpretation',
                    'error': str(e),
                    'timestamp': str(datetime.now())
                }],
                'rubric_understanding': self._default_rubric()
            }


    def _get_llm_timeout(self) -> int:
        """è·å–LLMè¯·æ±‚çš„è¶…æ—¶æ—¶é—´ï¼ˆç§’ï¼‰"""
        try:
            return int(os.getenv("RUBRIC_LLM_TIMEOUT", os.getenv("LLM_REQUEST_TIMEOUT", "30")))
        except Exception:
            return getattr(self.llm_client, "default_timeout", 30)

    def _record_step_trace(self, state: GradingState, summary: str, extra: Dict[str, Any] | None = None):
        """è®°å½•LLMè°ƒç”¨è½¨è¿¹ï¼Œæ–¹ä¾¿å‰ç«¯å±•ç¤º"""
        try:
            trace = dict(self.llm_client.last_call or {})
            trace['summary'] = summary
            if extra:
                trace.update(extra)
            state['step_results'][self.name] = trace
        except Exception as err:
            logger.warning(f"{self.name} è®°å½•LLMè½¨è¿¹å¤±è´¥: {err}")

    async def _extract_and_parse_rubric_from_images(self, pages: List[Dict]) -> RubricUnderstanding:
        """???????????????????????????"""
        if not pages:
            return self._default_rubric()
        try:
            logger.warning('???????????????????????')
            return self._default_rubric()
        except Exception:
            return self._default_rubric()


    def _extract_text_from_pdf_local(self, pdf_file_path: str) -> str:
        """??????????PDF???????????Vision??"""
        try:
            import PyPDF2
            with open(pdf_file_path, 'rb') as f:
                reader = PyPDF2.PdfReader(f)
                texts = []
                for page in reader.pages:
                    page_text = page.extract_text() or ""
                    if page_text.strip():
                        texts.append(page_text)
            result = "\n".join(texts).strip()
            if not result:
                logger.warning("??PDF??????")
            else:
                logger.info(f"????PDF??????? {len(result)}")
            return result
        except Exception as e:
            logger.warning(f"??PDF??????: {e}")
            return ""

    async def _extract_text_from_pdf_file(self, pdf_file_path: str) -> str:
        """???PDF???????????????????????????"""
        try:
            local_text = self._extract_text_from_pdf_local(pdf_file_path)
            if local_text:
                return local_text
        except Exception as e:
            logger.error(f"PDF??????: {e}")
        return ""

    async def _extract_and_parse_rubric_from_pdf(self, pdf_file_path: str) -> RubricUnderstanding:
        """
        ä½¿ç”¨ Gemini 3 Pro åŸç”Ÿå¤šæ¨¡æ€èƒ½åŠ›è§£æè¯„åˆ†æ ‡å‡†ï¼ˆæ”¯æŒ PDF å’Œå›¾ç‰‡ï¼‰
        ä¸¥æ ¼ç¦æ­¢æ–‡æœ¬æå–ï¼Œå®Œå…¨ä¾èµ– Gemini åŸç”Ÿå¤šæ¨¡æ€èƒ½åŠ›
        """
        try:
            # æ£€æŸ¥æ–‡ä»¶ç±»å‹
            from pathlib import Path
            file_ext = Path(pdf_file_path).suffix.lower()
            is_image = file_ext in ['.jpg', '.jpeg', '.png', '.gif', '.bmp', '.webp']
            
            # ä½¿ç”¨ Gemini åŸç”Ÿå¤šæ¨¡æ€è§£æï¼ˆPDF æˆ–å›¾ç‰‡ï¼‰
            prompt = format_rubric_interpretation_prompt("")
            messages = [{"role": "user", "content": prompt}]
            
            file_type = "å›¾ç‰‡" if is_image else "PDF"
            logger.info(f"ğŸ“„ ä½¿ç”¨ Gemini 3 Pro åŸç”Ÿå¤šæ¨¡æ€è§£æ {file_type}: {pdf_file_path}")
            
            response = self.llm_client.chat(
                messages,
                temperature=0.2,
                max_tokens=8000,
                files=[pdf_file_path],
                thinking_level="high",
                timeout=self._get_llm_timeout()
            )
            rubric_understanding = self._parse_rubric(response, "")
            criteria_count = len(rubric_understanding.get('criteria', []))
            logger.info(f"âœ… Gemini 3 Pro æˆåŠŸè§£æ {file_type}ï¼Œæå–äº† {criteria_count} ä¸ªè¯„åˆ†ç‚¹")
            return rubric_understanding

        except Exception as e:
            logger.error(f"âŒ Gemini 3 Pro è§£æå¤±è´¥: {e}")
            logger.warning("âš ï¸ å›é€€åˆ°é»˜è®¤è¯„åˆ†æ ‡å‡†")
            return self._default_rubric()

    async def _interpret_rubric_in_batches(self, rubric_text: str) -> RubricUnderstanding:
        """åˆ†æ‰¹å¤„ç†è¯„åˆ†æ ‡å‡†ï¼ˆç”¨äºå¤„ç†é•¿æ–‡æœ¬ï¼‰"""
        logger.info("å¼€å§‹åˆ†æ‰¹å¤„ç†è¯„åˆ†æ ‡å‡†...")

        # ç­–ç•¥ï¼šæŒ‰é¢˜ç›®åˆ†æ‰¹å¤„ç†
        import re

        # è¯†åˆ«é¢˜ç›®ç¼–å·ï¼ˆæ”¯æŒå¤šç§æ ¼å¼ï¼šQ1, Question 1, é¢˜ç›®1, 1.ç­‰ï¼‰
        # ä¼˜å…ˆåŒ¹é…è¡Œé¦–çš„æ•°å­—+ç‚¹å·æ ¼å¼ï¼ˆå¦‚ "1.", "2."ï¼‰ï¼Œè¿™æ˜¯æœ€å¸¸è§çš„æ ¼å¼
        question_pattern = r'(?:^|\n)(\d+)\.\s'
        matches = list(re.finditer(question_pattern, rubric_text, re.MULTILINE))

        # å¦‚æœæ²¡æ‰¾åˆ°ï¼Œå°è¯•å…¶ä»–æ ¼å¼
        if not matches:
            question_pattern = r'(?:Q|Question\s+|é¢˜ç›®\s*)(\d+)'
            matches = list(re.finditer(question_pattern, rubric_text, re.IGNORECASE))

        if not matches:
            logger.warning("æœªæ‰¾åˆ°é¢˜ç›®ç¼–å·ï¼Œä½¿ç”¨ç®€å•è§£æ")
            return self._parse_simple_rubric(rubric_text)

        logger.info(f"è¯†åˆ«åˆ° {len(matches)} ä¸ªé¢˜ç›®æ ‡è®°")

        # æŒ‰é¢˜ç›®åˆ†å‰²æ–‡æœ¬
        question_texts = []
        for i, match in enumerate(matches):
            start = match.start()
            end = matches[i + 1].start() if i + 1 < len(matches) else len(rubric_text)
            question_id = f"Q{match.group(1)}"
            question_text = rubric_text[start:end].strip()
            question_texts.append((question_id, question_text))

        logger.info(f"åˆ†å‰²æˆ {len(question_texts)} ä¸ªé¢˜ç›®æ®µè½")

        # åˆ†æ‰¹å¤„ç†ï¼ˆæ¯æ‰¹å¤„ç†5é“é¢˜ï¼‰
        batch_size = 5
        all_criteria = []
        total_points = 0.0

        for batch_start in range(0, len(question_texts), batch_size):
            batch_end = min(batch_start + batch_size, len(question_texts))
            batch = question_texts[batch_start:batch_end]

            # åˆå¹¶è¿™ä¸€æ‰¹çš„æ–‡æœ¬
            batch_text = "\n\n".join([f"{qid}:\n{text}" for qid, text in batch])
            batch_qids = [qid for qid, _ in batch]

            logger.info(f"å¤„ç†æ‰¹æ¬¡ {batch_start//batch_size + 1}/{(len(question_texts) + batch_size - 1)//batch_size}: {batch_qids}")

            # è°ƒç”¨LLMè§£æè¿™ä¸€æ‰¹
            prompt = format_rubric_interpretation_prompt(batch_text)
            messages = [
                {"role": "system", "content": f"ä½ æ˜¯ä¸€ä½èµ„æ·±æ•™è‚²ä¸“å®¶ï¼Œæ“…é•¿è§£æè¯„åˆ†æ ‡å‡†ã€‚è¯·è§£æä»¥ä¸‹é¢˜ç›®çš„è¯„åˆ†æ ‡å‡†ï¼š{', '.join(batch_qids)}"},
                {"role": "user", "content": prompt}
            ]

            try:
                response = self.llm_client.chat(messages, temperature=0.2)
                batch_result = self._parse_rubric(response, batch_text)

                # åˆå¹¶ç»“æœ
                batch_criteria = batch_result.get('criteria', [])
                all_criteria.extend(batch_criteria)
                total_points += batch_result.get('total_points', 0)

                logger.info(f"???? {batch_start//batch_size + 1} ???????: {len(batch_criteria)} ???????")
            except Exception as e:
                logger.error(f"???? {batch_start//batch_size + 1} ???????: {e}")
                continue

        logger.info(f"åˆ†æ‰¹å¤„ç†å®Œæˆ: å…± {len(all_criteria)} ä¸ªè¯„åˆ†ç‚¹ï¼Œæ€»åˆ† {total_points}")

        return RubricUnderstanding(
            rubric_id='R1_BATCHED',
            criteria=all_criteria,
            total_points=total_points,
            grading_rules={'partial_credit': 'yes'},
            strictness_guidance=None
        )

    def _parse_rubric(self, response: str, rubric_text: str) -> RubricUnderstanding:
        """
        è§£æ LLM è¿”å›çš„è¯„åˆ†æ ‡å‡† JSON

        Args:
            response: LLM è¿”å›çš„å“åº”æ–‡æœ¬
            rubric_text: åŸå§‹è¯„åˆ†æ ‡å‡†æ–‡æœ¬ï¼ˆç”¨äºå¤‡ç”¨è§£æï¼‰

        Returns:
            RubricUnderstanding å¯¹è±¡
        """
        try:
            import json
            import re

            # æå– JSON éƒ¨åˆ† (æ”¯æŒ ```json ä»£ç å—)
            json_str = response

            # ç§»é™¤ markdown ä»£ç å—æ ‡è®°
            json_str = re.sub(r'```json\s*', '', json_str)
            json_str = re.sub(r'```\s*', '', json_str)

            # æŸ¥æ‰¾ JSON å¯¹è±¡
            json_start = json_str.find('{')
            json_end = json_str.rfind('}') + 1

            if json_start >= 0 and json_end > json_start:
                json_str = json_str[json_start:json_end]

                # å°è¯•ä¿®å¤å¸¸è§çš„ JSON æ ¼å¼é”™è¯¯
                # 1. ä¿®å¤æœªè½¬ä¹‰çš„æ¢è¡Œç¬¦
                json_str = json_str.replace('\n', '\\n')
                # 2. ä¿®å¤æœªè½¬ä¹‰çš„å¼•å· (åœ¨å­—ç¬¦ä¸²å€¼ä¸­)
                # è¿™ä¸ªæ¯”è¾ƒå¤æ‚,æš‚æ—¶è·³è¿‡

                try:
                    result = json.loads(json_str)
                except json.JSONDecodeError as e:
                    logger.warning(f"JSON è§£æå¤±è´¥ (ç¬¬ä¸€æ¬¡å°è¯•): {e}")
                    # å°è¯•ä½¿ç”¨ json5 æˆ–æ›´å®½æ¾çš„è§£æ
                    # å¦‚æœè¿˜æ˜¯å¤±è´¥,å°è¯•æå–éƒ¨åˆ†ä¿¡æ¯
                    logger.info("å°è¯•ä»å“åº”ä¸­æå–éƒ¨åˆ†è¯„åˆ†æ ‡å‡†ä¿¡æ¯...")
                    return self._extract_criteria_from_text(response, rubric_text)

                # è½¬æ¢ä¸º RubricUnderstanding æ ¼å¼
                criteria = []
                for c in result.get('criteria', []):
                    try:
                        criterion = GradingCriterion(
                            criterion_id=c.get('criterion_id', 'C1'),
                            question_id=c.get('question_id', ''),
                            description=c.get('description', ''),
                            detailed_requirements=c.get('detailed_requirements', ''),
                            points=float(c.get('points', 0)),
                            standard_answer=c.get('standard_answer', ''),
                            evaluation_method=c.get('evaluation_method', 'semantic'),
                            scoring_criteria=c.get('scoring_criteria', {}),
                            alternative_methods=c.get('alternative_methods', []),
                            keywords=c.get('keywords', []),
                            required_elements=c.get('required_elements', []),
                            common_mistakes=c.get('common_mistakes', [])
                        )
                        criteria.append(criterion)
                    except Exception as e:
                        logger.warning(f"è·³è¿‡æ— æ•ˆçš„è¯„åˆ†ç‚¹: {e}")
                        continue

                if not criteria:
                    logger.warning("æœªèƒ½ä» JSON ä¸­æå–ä»»ä½•è¯„åˆ†ç‚¹")
                    return self._extract_criteria_from_text(response, rubric_text)

                return RubricUnderstanding(
                    rubric_id=result.get('rubric_id', 'R1'),
                    criteria=criteria,
                    total_points=float(result.get('total_points', sum(c.points for c in criteria))),
                    grading_rules=result.get('grading_rules', {'partial_credit': 'yes'}),
                    strictness_guidance=result.get('strictness_guidance', '')
                )
            else:
                logger.warning("å“åº”ä¸­æœªæ‰¾åˆ° JSONï¼Œå°è¯•ä»æ–‡æœ¬ä¸­æå–")
                return self._extract_criteria_from_text(response, rubric_text)

        except Exception as e:
            logger.error(f"è¯„åˆ†æ ‡å‡†è§£æå¤±è´¥: {e}")
            return self._extract_criteria_from_text(response, rubric_text)

    def _extract_criteria_from_text(self, response: str, rubric_text: str) -> RubricUnderstanding:
        """
        ä» LLM å“åº”æ–‡æœ¬ä¸­æå–è¯„åˆ†æ ‡å‡† (å½“ JSON è§£æå¤±è´¥æ—¶ä½¿ç”¨)

        å°è¯•ä»å“åº”ä¸­æå– criterion_id, description, points ç­‰ä¿¡æ¯
        """
        try:
            import re

            criteria = []
            total_points = 0.0

            # å°è¯•åŒ¹é…è¯„åˆ†ç‚¹æ¨¡å¼
            # æ¨¡å¼ 1: "criterion_id": "Q1_C1", "description": "...", "points": 5
            pattern1 = r'"criterion_id"\s*:\s*"([^"]+)"[^}]*"description"\s*:\s*"([^"]+)"[^}]*"points"\s*:\s*(\d+(?:\.\d+)?)'
            matches1 = re.findall(pattern1, response, re.DOTALL)

            for criterion_id, description, points in matches1:
                # æå– question_id
                question_id_match = re.match(r'(Q\d+)_', criterion_id)
                question_id = question_id_match.group(1) if question_id_match else ''

                criterion = GradingCriterion(
                    criterion_id=criterion_id,
                    question_id=question_id,
                    description=description[:200],  # é™åˆ¶é•¿åº¦
                    points=float(points),
                    evaluation_method='semantic'
                )
                criteria.append(criterion)
                total_points += float(points)

            if criteria:
                logger.info(f"ä»æ–‡æœ¬ä¸­æå–äº† {len(criteria)} ä¸ªè¯„åˆ†ç‚¹")
                return RubricUnderstanding(
                    rubric_id='R_EXTRACTED',
                    criteria=criteria,
                    total_points=total_points,
                    grading_rules={'partial_credit': 'yes'},
                    strictness_guidance=None
                )
            else:
                logger.warning("æ— æ³•ä»æ–‡æœ¬ä¸­æå–è¯„åˆ†ç‚¹ï¼Œå°è¯•ç®€å•è§£æåŸå§‹æ–‡æœ¬")
                return self._parse_simple_rubric(rubric_text)

        except Exception as e:
            logger.error(f"ä»æ–‡æœ¬æå–è¯„åˆ†æ ‡å‡†å¤±è´¥: {e}")
            return self._parse_simple_rubric(rubric_text)

    def _parse_simple_rubric(self, rubric_text: str) -> RubricUnderstanding:
        """
        ç®€å•è§£æè¯„åˆ†æ ‡å‡†æ–‡æœ¬ï¼ˆå¤‡ç”¨æ–¹æ¡ˆï¼‰

        ä½¿ç”¨æ­£åˆ™è¡¨è¾¾å¼æå–é¢˜ç›®å’Œè¯„åˆ†ç‚¹
        """
        try:
            import re

            criteria = []
            total_points = 0.0

            # æŒ‰é¢˜ç›®åˆ†å‰²
            # åŒ¹é…æ ¼å¼: "é¢˜ç›®1ï¼ˆ10åˆ†ï¼‰" æˆ– "Q1 (10åˆ†)" æˆ– "1. (10åˆ†)"
            question_pattern = r'(?:é¢˜ç›®|Question|Q)?(\d+)[ï¼š:.\s]*[ï¼ˆ\(]?(\d+(?:\.\d+)?)\s*åˆ†[ï¼‰\)]?'
            question_matches = list(re.finditer(question_pattern, rubric_text, re.IGNORECASE))

            if question_matches:
                for i, match in enumerate(question_matches):
                    question_num = match.group(1)
                    question_points = float(match.group(2))
                    question_id = f"Q{question_num}"

                    # æå–è¯¥é¢˜ç›®çš„å†…å®¹ï¼ˆä»å½“å‰åŒ¹é…åˆ°ä¸‹ä¸€ä¸ªåŒ¹é…ä¹‹é—´çš„æ–‡æœ¬ï¼‰
                    start = match.end()
                    end = question_matches[i + 1].start() if i + 1 < len(question_matches) else len(rubric_text)
                    question_content = rubric_text[start:end].strip()

                    # æå–è¯„åˆ†ç‚¹
                    # åŒ¹é…æ ¼å¼: "- æè¿°ï¼ˆ5åˆ†ï¼‰" æˆ– "1. æè¿° (5åˆ†)"
                    criterion_pattern = r'[-â€¢\d+\.]\s*(.+?)[ï¼ˆ\(](\d+(?:\.\d+)?)\s*åˆ†[ï¼‰\)]'
                    criterion_matches = re.findall(criterion_pattern, question_content)

                    if criterion_matches:
                        for j, (desc, points) in enumerate(criterion_matches, 1):
                            criterion = GradingCriterion(
                                criterion_id=f"{question_id}_C{j}",
                                question_id=question_id,
                                description=desc.strip(),
                                points=float(points),
                                evaluation_method='semantic'
                            )
                            criteria.append(criterion)
                            total_points += float(points)
                    else:
                        # å¦‚æœæ²¡æœ‰æ‰¾åˆ°å…·ä½“è¯„åˆ†ç‚¹ï¼Œåˆ›å»ºä¸€ä¸ªé»˜è®¤è¯„åˆ†ç‚¹
                        criterion = GradingCriterion(
                            criterion_id=f"{question_id}_C1",
                            question_id=question_id,
                            description=f"é¢˜ç›®{question_num}æ•´ä½“è¯„åˆ†",
                            points=question_points,
                            evaluation_method='semantic'
                        )
                        criteria.append(criterion)
                        total_points += question_points

            if not criteria:
                logger.warning("æœªèƒ½è§£æå‡ºä»»ä½•è¯„åˆ†ç‚¹ï¼Œä½¿ç”¨é»˜è®¤æ ‡å‡†")
                return self._default_rubric()

            return RubricUnderstanding(
                rubric_id='R_SIMPLE',
                criteria=criteria,
                total_points=total_points,
                grading_rules={'partial_credit': 'yes'},
                strictness_guidance=None
            )

        except Exception as e:
            logger.error(f"ç®€å•è§£æå¤±è´¥: {e}")
            return self._default_rubric()

    async def _interpret_rubric(self, rubric_text: str) -> RubricUnderstanding:
        """
        è§£æè¯„åˆ†æ ‡å‡†æ–‡æœ¬ï¼ˆä½¿ç”¨ LLMï¼‰

        Args:
            rubric_text: è¯„åˆ†æ ‡å‡†æ–‡æœ¬

        Returns:
            RubricUnderstanding å¯¹è±¡
        """
        try:
            # å¦‚æœæ–‡æœ¬å¾ˆé•¿ï¼Œä½¿ç”¨åˆ†æ‰¹å¤„ç†
            if len(rubric_text) > 5000:
                return await self._interpret_rubric_in_batches(rubric_text)

            # æ„å»º Prompt
            prompt = format_rubric_interpretation_prompt(rubric_text)

            messages = [
                {"role": "system", "content": "ä½ æ˜¯ä¸€ä½èµ„æ·±æ•™è‚²ä¸“å®¶ï¼Œæ“…é•¿è§£æè¯„åˆ†æ ‡å‡†ã€‚"},
                {"role": "user", "content": prompt}
            ]

            # è°ƒç”¨ LLM
            response = self.llm_client.chat(
                messages,
                temperature=0.2,
                max_tokens=8000,
                timeout=self._get_llm_timeout()
            )

            # è§£æå“åº”
            return self._parse_rubric(response, rubric_text)

        except Exception as e:
            logger.error(f"LLM è§£æè¯„åˆ†æ ‡å‡†å¤±è´¥: {e}")
            return self._parse_simple_rubric(rubric_text)

    def _default_rubric(self) -> RubricUnderstanding:
        """é»˜è®¤è¯„åˆ†æ ‡å‡†"""
        return RubricUnderstanding(
            rubric_id='R_DEFAULT',
            criteria=[
                GradingCriterion(
                    criterion_id="C1",
                    description="ç­”æ¡ˆå®Œæ•´æ€§å’Œæ­£ç¡®æ€§",
                    points=100.0,
                    evaluation_method='semantic',
                    keywords=None,
                    required_elements=None
                )
            ],
            total_points=100.0,
            grading_rules={'partial_credit': 'yes'},
            strictness_guidance=None
        )
