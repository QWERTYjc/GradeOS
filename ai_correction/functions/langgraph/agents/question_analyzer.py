#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
QuestionAnalyzer Agent - åˆ†æé¢˜ç›®ç‰¹å¾ï¼Œè¯†åˆ«é¢˜å‹ã€éš¾åº¦ã€æ‰¹æ”¹ç­–ç•¥
"""

import sys
from pathlib import Path
from typing import Dict, Any, List

# æ·»åŠ é¡¹ç›®è·¯å¾„
sys.path.insert(0, str(Path(__file__).parent.parent.parent))

from functions.llm_client import get_llm_client


class QuestionAnalyzerAgent:
    """é¢˜ç›®åˆ†æ Agent"""
    
    # é¢˜å‹é…ç½®
    QUESTION_TYPES = {
        'choice': {
            'features': ['é€‰é¡¹', 'A.', 'B.', 'C.', 'D.'],
            'strategy': 'keyword_match',
            'expected_answer_length': 'short',
            'base_difficulty': 1
        },
        'fill': {
            'features': ['___', 'ç©ºç™½', 'å¡«ç©º'],
            'strategy': 'semantic',
            'expected_answer_length': 'short',
            'base_difficulty': 2
        },
        'essay': {
            'features': ['è®ºè¿°', 'åˆ†æ', 'è¯´æ˜', 'æè¿°', 'ç®€ç­”'],
            'strategy': 'rubric',
            'expected_answer_length': 'long',
            'base_difficulty': 4
        },
        'calculation': {
            'features': ['è®¡ç®—', 'æ±‚', 'è§£', 'è¯æ˜'],
            'strategy': 'step_by_step',
            'expected_answer_length': 'medium',
            'base_difficulty': 3
        }
    }
    
    def __init__(self):
        pass
    
    def analyze(self, state: Dict[str, Any]) -> Dict[str, Any]:
        """
        åˆ†æé¢˜ç›®ç‰¹å¾
        
        Args:
            state: åŒ…å« questions çš„çŠ¶æ€
            
        Returns:
            æ›´æ–°åçš„çŠ¶æ€ï¼Œquestions ä¸­æ·»åŠ  analysis å­—æ®µ
        """
        try:
            questions = state.get('questions', [])
            
            for question in questions:
                # åˆ†æé¢˜å‹
                q_type = question.get('type', 'unknown')
                
                # è¯„ä¼°éš¾åº¦
                difficulty = self._estimate_difficulty(question)
                
                # ç¡®å®šæ‰¹æ”¹ç­–ç•¥
                strategy = self._determine_strategy(q_type, difficulty)
                
                # æå–å…³é”®è¯
                keywords = self._extract_keywords(question['text'])
                
                # æ·»åŠ åˆ†æç»“æœ
                question['analysis'] = {
                    'difficulty': difficulty,
                    'strategy': strategy,
                    'keywords': keywords,
                    'expected_answer_length': self.QUESTION_TYPES.get(q_type, {}).get('expected_answer_length', 'medium')
                }
            
            state.update({
                'questions': questions,
                'analysis_status': 'success'
            })
            
            return state
            
        except Exception as e:
            state.update({
                'analysis_status': 'failed',
                'analysis_errors': [str(e)]
            })
            return state
    
    def _estimate_difficulty(self, question: Dict) -> str:
        """
        è¯„ä¼°é¢˜ç›®éš¾åº¦
        
        å› ç´ ï¼š
        1. é¢˜ç›®é•¿åº¦ï¼ˆé•¿ = éš¾ï¼‰
        2. å…³é”®è¯å¤æ‚åº¦
        3. é¢˜å‹åŸºç¡€éš¾åº¦
        """
        q_type = question.get('type', 'unknown')
        text = question.get('text', '')
        
        # åŸºç¡€éš¾åº¦
        base_difficulty = self.QUESTION_TYPES.get(q_type, {}).get('base_difficulty', 2)
        
        # é•¿åº¦å› ç´ 
        length_factor = 0
        if len(text) > 200:
            length_factor = 2
        elif len(text) > 100:
            length_factor = 1
        
        # å¤æ‚è¯æ±‡å› ç´ 
        complex_keywords = ['ç»¼åˆ', 'åˆ†æ', 'è¯„ä»·', 'è®ºè¯', 'æ¨å¯¼', 'è¯æ˜']
        complexity_factor = sum(1 for kw in complex_keywords if kw in text)
        
        # è®¡ç®—æ€»éš¾åº¦
        total_difficulty = base_difficulty + length_factor + complexity_factor
        
        # æ˜ å°„åˆ°éš¾åº¦ç­‰çº§
        if total_difficulty <= 2:
            return 'easy'
        elif total_difficulty <= 4:
            return 'medium'
        else:
            return 'hard'
    
    def _determine_strategy(self, q_type: str, difficulty: str) -> str:
        """
        ç¡®å®šæ‰¹æ”¹ç­–ç•¥
        
        ç­–ç•¥ï¼š
        - keyword_match: å…³é”®è¯åŒ¹é…ï¼ˆé€‰æ‹©é¢˜ã€å¡«ç©ºé¢˜ï¼‰
        - semantic: è¯­ä¹‰ç†è§£ï¼ˆå¡«ç©ºé¢˜ã€ç®€ç­”é¢˜ï¼‰
        - rubric: è¯„åˆ†æ ‡å‡†ï¼ˆè§£ç­”é¢˜ã€è®ºè¿°é¢˜ï¼‰
        - step_by_step: æ­¥éª¤åˆ†æï¼ˆè®¡ç®—é¢˜ã€è¯æ˜é¢˜ï¼‰
        """
        base_strategy = self.QUESTION_TYPES.get(q_type, {}).get('strategy', 'semantic')
        
        # æ ¹æ®éš¾åº¦è°ƒæ•´ç­–ç•¥
        if difficulty == 'hard' and base_strategy == 'keyword_match':
            return 'semantic'  # éš¾é¢˜ä½¿ç”¨è¯­ä¹‰ç†è§£
        
        return base_strategy
    
    def _extract_keywords(self, text: str) -> List[str]:
        """
        æå–å…³é”®è¯
        
        ç®€å•å®ç°ï¼šæå–åè¯ã€åŠ¨è¯ã€å½¢å®¹è¯
        """
        # è¿™é‡Œä½¿ç”¨ç®€å•çš„è§„åˆ™ï¼Œå®é™…å¯ä»¥ä½¿ç”¨ NLP åº“
        keywords = []
        
        # å¸¸è§å…³é”®è¯æ¨¡å¼
        important_words = [
            'è®¡ç®—', 'æ±‚', 'è§£', 'è¯æ˜', 'åˆ†æ', 'è¯´æ˜', 'æè¿°', 'è®ºè¿°',
            'æ¯”è¾ƒ', 'è¯„ä»·', 'æ€»ç»“', 'å½’çº³', 'æ¨å¯¼', 'åˆ¤æ–­', 'é€‰æ‹©'
        ]
        
        for word in important_words:
            if word in text:
                keywords.append(word)
        
        return keywords[:5]  # æœ€å¤šè¿”å›5ä¸ªå…³é”®è¯


class QuestionGraderAgent:
    """é¢˜ç›®æ‰¹æ”¹ Agent - é€é¢˜æ‰¹æ”¹"""
    
    def __init__(self, llm_client=None):
        """
        Args:
            llm_client: LLM å®¢æˆ·ç«¯ï¼ˆGemini/GPT/OpenRouterï¼‰
        """
        try:
            self.llm_client = llm_client or get_llm_client()
            print(f"ğŸ¯ QuestionGrader åˆå§‹åŒ–: LLM={self.llm_client.provider}")
        except Exception as e:
            print(f"âš ï¸ LLM åˆå§‹åŒ–å¤±è´¥ï¼Œå°†ä½¿ç”¨ç®€å•ç­–ç•¥: {e}")
            self.llm_client = None
    
    def grade(self, state: Dict[str, Any]) -> Dict[str, Any]:
        """
        é€é¢˜æ‰¹æ”¹
        
        Args:
            state: åŒ…å« questions, answers, marking_scheme çš„çŠ¶æ€
            
        Returns:
            æ›´æ–°åçš„çŠ¶æ€ï¼Œæ·»åŠ  grading_results
        """
        try:
            answers = state.get('answers', [])
            marking_scheme = state.get('marking_scheme', {})
            
            grading_results = []
            
            for answer in answers:
                question = answer.get('question', {})
                analysis = question.get('analysis', {})
                strategy = analysis.get('strategy', 'semantic')
                
                # æ ¹æ®ç­–ç•¥æ‰¹æ”¹
                if strategy == 'keyword_match':
                    result = self._grade_by_keywords(question, answer, marking_scheme)
                elif strategy == 'semantic':
                    result = self._grade_by_semantic(question, answer, marking_scheme)
                elif strategy == 'rubric':
                    result = self._grade_by_rubric(question, answer, marking_scheme)
                elif strategy == 'step_by_step':
                    result = self._grade_by_steps(question, answer, marking_scheme)
                else:
                    result = self._grade_by_semantic(question, answer, marking_scheme)
                
                grading_results.append(result)
            
            state.update({
                'grading_results': grading_results,
                'grading_status': 'success'
            })
            
            return state
            
        except Exception as e:
            state.update({
                'grading_status': 'failed',
                'grading_errors': [str(e)]
            })
            return state
    
    def _grade_by_keywords(self, question: Dict, answer: Dict, marking_scheme: Dict) -> Dict:
        """å…³é”®è¯åŒ¹é…æ‰¹æ”¹"""
        keywords = question.get('analysis', {}).get('keywords', [])
        answer_text = answer.get('text', '')
        
        # è®¡ç®—å…³é”®è¯åŒ¹é…åº¦
        matched_keywords = [kw for kw in keywords if kw in answer_text]
        match_rate = len(matched_keywords) / len(keywords) if keywords else 0
        
        # ç®€å•è¯„åˆ†
        score = int(match_rate * 10)
        
        return {
            'question_id': question['id'],
            'student_id': answer.get('student_id'),
            'score': score,
            'max_score': 10,
            'matched_keywords': matched_keywords,
            'feedback': f"å…³é”®è¯åŒ¹é…åº¦: {match_rate*100:.1f}%",
            'strategy': 'keyword_match'
        }
    
    def _grade_by_semantic(self, question: Dict, answer: Dict, marking_scheme: Dict) -> Dict:
        """è¯­ä¹‰ç†è§£æ‰¹æ”¹ï¼ˆä½¿ç”¨ç»†åˆ†è¯„åˆ†ç‚¹ï¼‰"""
        if not self.llm_client:
            print("âš ï¸ æ—  LLMï¼Œä½¿ç”¨å…³é”®è¯åŒ¹é…")
            return self._grade_by_keywords(question, answer, marking_scheme)

        try:
            # è·å–ç»†åˆ†è¯„åˆ†ç‚¹ - ä¿®å¤ï¼šcriteria æ˜¯åˆ—è¡¨ï¼Œä¸æ˜¯å­—å…¸
            criteria = marking_scheme.get('criteria', [])
            total_score = marking_scheme.get('total_score', 10)
            
            # å¦‚æœ criteria æ˜¯å­—å…¸æ ¼å¼ï¼ˆå…¼å®¹æ—§ç‰ˆæœ¬ï¼‰
            if isinstance(criteria, dict):
                scoring_points = criteria.get('scoring_points', [])
                total_score = criteria.get('total_score', total_score)
            # å¦‚æœ criteria æ˜¯åˆ—è¡¨æ ¼å¼ï¼ˆæ–°ç‰ˆæœ¬ï¼‰
            elif isinstance(criteria, list):
                scoring_points = criteria
                # è®¡ç®—æ€»åˆ†
                if scoring_points:
                    total_score = sum(point.get('points', point.get('score', 0)) for point in scoring_points)
            else:
                scoring_points = []

            # å¦‚æœæ²¡æœ‰ç»†åˆ†è¯„åˆ†ç‚¹ï¼Œä½¿ç”¨ç®€å•æ‰¹æ”¹
            if not scoring_points:
                return self._grade_simple_semantic(question, answer, total_score)

            # æ ¼å¼åŒ–è¯„åˆ†ç‚¹
            scoring_points_text = self._format_scoring_points(scoring_points)

            # æ„å»ºè¯¦ç»†çš„æ‰¹æ”¹æç¤ºè¯
            prompt = f"""ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„è€å¸ˆï¼Œæ­£åœ¨æ‰¹æ”¹å­¦ç”Ÿçš„ä½œä¸šã€‚è¯·ä¸¥æ ¼æŒ‰ç…§è¯„åˆ†æ ‡å‡†é€ç‚¹è¯„åˆ†ã€‚

ã€é¢˜ç›®ã€‘
{question.get('text', '')}

ã€å­¦ç”Ÿç­”æ¡ˆã€‘
{answer.get('text', '')}

ã€è¯„åˆ†æ ‡å‡†ã€‘ï¼ˆæ€»åˆ†ï¼š{total_score}åˆ†ï¼‰
{scoring_points_text}

ã€æ‰¹æ”¹è¦æ±‚ã€‘
1. **é€ç‚¹è¯„åˆ†**ï¼šå¯¹æ¯ä¸ªè¯„åˆ†ç‚¹å•ç‹¬è¯„åˆ†ï¼Œä¸è¦è·³è¿‡ä»»ä½•è¯„åˆ†ç‚¹
2. **ä¸¥æ ¼å¯¹ç…§**ï¼šä¸¥æ ¼å¯¹ç…§è¯„åˆ†æ ‡å‡†ï¼Œæ£€æŸ¥å­¦ç”Ÿç­”æ¡ˆæ˜¯å¦æ»¡è¶³æ¯ä¸ªè¯„åˆ†ç‚¹çš„è¦æ±‚
3. **è¯¦ç»†åˆ†æ**ï¼šå¯¹æ¯ä¸ªè¯„åˆ†ç‚¹ï¼Œè¯´æ˜å­¦ç”Ÿæ˜¯å¦è¾¾åˆ°è¦æ±‚ï¼Œä¸ºä»€ä¹ˆå¾—åˆ†æˆ–æ‰£åˆ†
4. **ç²¾å‡†è¯„åˆ†**ï¼šæ¯ä¸ªè¯„åˆ†ç‚¹çš„å¾—åˆ†å¿…é¡»åœ¨ 0 åˆ°è¯¥è¯„åˆ†ç‚¹æ»¡åˆ†ä¹‹é—´
5. **æ€»åˆ†è®¡ç®—**ï¼šæ€»åˆ† = æ‰€æœ‰è¯„åˆ†ç‚¹å¾—åˆ†ä¹‹å’Œ

ã€è¾“å‡ºæ ¼å¼ã€‘
è¯·ä»¥ JSON æ ¼å¼è¿”å›ï¼š
{{
    "total_score": æ€»åˆ†ï¼ˆæ‰€æœ‰è¯„åˆ†ç‚¹å¾—åˆ†ä¹‹å’Œï¼‰,
    "max_score": {total_score},
    "scoring_details": [
        {{
            "point_id": è¯„åˆ†ç‚¹ID,
            "point_name": "è¯„åˆ†ç‚¹åç§°",
            "max_score": è¯¥è¯„åˆ†ç‚¹æ»¡åˆ†,
            "score": è¯¥è¯„åˆ†ç‚¹å¾—åˆ†,
            "is_correct": true/false,
            "analysis": "è¯¦ç»†åˆ†æï¼šå­¦ç”Ÿç­”æ¡ˆä¸­æ˜¯å¦åŒ…å«è¯¥è¯„åˆ†ç‚¹è¦æ±‚çš„å†…å®¹",
            "evidence": "å­¦ç”Ÿç­”æ¡ˆä¸­çš„ç›¸å…³å†…å®¹ï¼ˆå¼•ç”¨åŸæ–‡ï¼‰",
            "reason": "å¾—åˆ†/æ‰£åˆ†åŸå› "
        }}
    ],
    "overall_feedback": "æ€»ä½“è¯„ä»·",
    "strengths": ["ä¼˜ç‚¹1", "ä¼˜ç‚¹2"],
    "weaknesses": ["ä¸è¶³1", "ä¸è¶³2"],
    "suggestions": ["æ”¹è¿›å»ºè®®1", "æ”¹è¿›å»ºè®®2"]
}}
"""

            print(f"ğŸ“¡ è°ƒç”¨ LLM æ‰¹æ”¹é¢˜ç›® {question.get('id')}")

            messages = [{"role": "user", "content": prompt}]
            response = self.llm_client.chat(messages, temperature=0.3)

            # è§£æ LLM å“åº”
            import json
            import re

            # æå– JSON
            json_match = re.search(r'\{.*\}', response, re.DOTALL)
            if json_match:
                result_data = json.loads(json_match.group())

                # æå–æ•°æ®
                total_score_result = result_data.get('total_score', 0)
                max_score_result = result_data.get('max_score', total_score)
                scoring_details = result_data.get('scoring_details', [])
                overall_feedback = result_data.get('overall_feedback', '')
                strengths = result_data.get('strengths', [])
                weaknesses = result_data.get('weaknesses', [])
                suggestions = result_data.get('suggestions', [])

                # æ„å»ºé”™è¯¯åˆ—è¡¨
                errors = []
                for detail in scoring_details:
                    if not detail.get('is_correct', False):
                        errors.append(f"{detail.get('point_name', '')}: {detail.get('reason', '')}")

                print(f"âœ… LLM æ‰¹æ”¹å®Œæˆ: å¾—åˆ†={total_score_result}/{max_score_result}")

                return {
                    'question_id': question['id'],
                    'student_id': answer.get('student_id'),
                    'score': total_score_result,
                    'max_score': max_score_result,
                    'feedback': overall_feedback,
                    'scoring_details': scoring_details,
                    'errors': errors,
                    'strengths': strengths,
                    'weaknesses': weaknesses,
                    'suggestions': suggestions,
                    'strategy': 'semantic'
                }
            else:
                # å¦‚æœæ— æ³•è§£æ JSONï¼Œä½¿ç”¨ç®€å•æ‰¹æ”¹
                print("âš ï¸ æ— æ³•è§£æ JSONï¼Œä½¿ç”¨ç®€å•æ‰¹æ”¹")
                return self._grade_simple_semantic(question, answer, total_score)

        except Exception as e:
            print(f"âŒ LLM æ‰¹æ”¹å¤±è´¥: {e}ï¼Œä½¿ç”¨å…³é”®è¯åŒ¹é…")
            return self._grade_by_keywords(question, answer, marking_scheme)

    def _format_scoring_points(self, scoring_points: List[Dict]) -> str:
        """æ ¼å¼åŒ–è¯„åˆ†ç‚¹ä¸ºæ–‡æœ¬"""
        lines = []
        for point in scoring_points:
            line = f"{point['id']}. {point['description']} ({point['score']}åˆ†)"
            if point.get('keywords'):
                line += f"\n   å…³é”®è¯ï¼š{', '.join(point['keywords'])}"
            lines.append(line)
        return '\n'.join(lines)

    def _grade_simple_semantic(self, question: Dict, answer: Dict, max_score: int) -> Dict:
        """ç®€å•è¯­ä¹‰æ‰¹æ”¹ï¼ˆæ— ç»†åˆ†è¯„åˆ†ç‚¹ï¼‰"""
        try:
            prompt = f"""è¯·æ‰¹æ”¹ä»¥ä¸‹ç­”æ¡ˆï¼Œå¹¶ä»¥ JSON æ ¼å¼è¿”å›ç»“æœï¼š

é¢˜ç›®ï¼š{question.get('text', '')}
å­¦ç”Ÿç­”æ¡ˆï¼š{answer.get('text', '')}

è¯·è¿”å› JSON æ ¼å¼ï¼š
{{
    "score": å¾—åˆ†ï¼ˆ0-{max_score}åˆ†çš„æ•´æ•°ï¼‰,
    "feedback": "è¯¦ç»†åé¦ˆ",
    "errors": ["é”™è¯¯ç‚¹1", "é”™è¯¯ç‚¹2"],
    "suggestions": ["æ”¹è¿›å»ºè®®1", "æ”¹è¿›å»ºè®®2"]
}}
"""

            messages = [{"role": "user", "content": prompt}]
            response = self.llm_client.chat(messages, temperature=0.3)

            import json
            import re

            json_match = re.search(r'\{.*\}', response, re.DOTALL)
            if json_match:
                result_data = json.loads(json_match.group())
                score = result_data.get('score', 0)
                feedback = result_data.get('feedback', 'ç­”æ¡ˆåŸºæœ¬æ­£ç¡®')
                errors = result_data.get('errors', [])
                suggestions = result_data.get('suggestions', [])
            else:
                score = 0
                feedback = response[:200]
                errors = []
                suggestions = []

            return {
                'question_id': question['id'],
                'student_id': answer.get('student_id'),
                'score': score,
                'max_score': max_score,
                'feedback': feedback,
                'errors': errors,
                'suggestions': suggestions,
                'strategy': 'semantic'
            }

        except Exception as e:
            print(f"âŒ ç®€å•æ‰¹æ”¹å¤±è´¥: {e}")
            return self._grade_by_keywords(question, answer, {})
    
    def _grade_by_rubric(self, question: Dict, answer: Dict, marking_scheme: Dict) -> Dict:
        """è¯„åˆ†æ ‡å‡†æ‰¹æ”¹"""
        criteria = marking_scheme.get('criteria', [])
        
        # ä½¿ç”¨ LLM æ ¹æ®è¯„åˆ†æ ‡å‡†æ‰¹æ”¹
        return self._grade_by_semantic(question, answer, marking_scheme)
    
    def _grade_by_steps(self, question: Dict, answer: Dict, marking_scheme: Dict) -> Dict:
        """æ­¥éª¤åˆ†ææ‰¹æ”¹"""
        # åˆ†æç­”æ¡ˆæ­¥éª¤
        return self._grade_by_semantic(question, answer, marking_scheme)

