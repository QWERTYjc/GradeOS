#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
GradingWorkerAgent - æ‰¹æ”¹å·¥ä½œAgent
èŒè´£ï¼šåŸºäºå®šåˆ¶åŒ–æ ‡å‡†å’Œé¢˜ç›®ä¸Šä¸‹æ–‡æ‰¹æ”¹å­¦ç”Ÿç­”æ¡ˆ
æ ¸å¿ƒä»·å€¼ï¼šæ¥æ”¶å‹ç¼©ç‰ˆè¯„åˆ†åŒ…å’Œä¸Šä¸‹æ–‡ï¼Œé«˜æ•ˆæ‰§è¡Œæ‰¹æ”¹ï¼Œæœ€å°åŒ–tokenæ¶ˆè€—
"""

import logging
import json
import os
from typing import Dict, Any, List
from datetime import datetime

from ...llm_client import LLMClient

logger = logging.getLogger(__name__)


class GradingWorkerAgent:
    """æ‰¹æ”¹å·¥ä½œAgent"""

    def __init__(self, llm_client=None):
        self.agent_name = "GradingWorkerAgent"
        # ä½¿ç”¨ Gemini 3 Pro åŸç”Ÿ APIï¼Œæ”¯æŒçœŸæ­£çš„å¤šæ¨¡æ€æ‰¹æ”¹
        self.llm_client = llm_client or LLMClient(
            provider='gemini',
            model='gemini-3-pro-preview'
        )
        try:
            self.llm_timeout = int(os.getenv("GRADING_LLM_TIMEOUT", os.getenv("LLM_REQUEST_TIMEOUT", "120")))
        except Exception:
            self.llm_timeout = 120
        self.reasoning_effort = None
    
    async def __call__(self, state: Dict[str, Any]) -> Dict[str, Any]:
        """æ‰§è¡Œæ‰¹æ”¹å·¥ä½œ"""
        logger.info(f"[{self.agent_name}] å¼€å§‹æ‰¹æ”¹ä½œä¸š...")
        
        try:
            state['current_step'] = "æ‰¹æ”¹ä½œä¸š"
            state['progress_percentage'] = 50.0
            state.setdefault('step_results', {})
            
            # è·å–æ‰¹æ¬¡ä¿¡æ¯
            batches_info = state.get('batches_info', [])
            batch_rubric_packages = state.get('batch_rubric_packages', {})
            question_context_packages = state.get('question_context_packages', {})
            answer_understanding = state.get('answer_understanding')
            
            if not batches_info:
                logger.warning("æ²¡æœ‰æ‰¹æ¬¡ä¿¡æ¯ï¼Œè·³è¿‡æ‰¹æ”¹")
                return {
                    'grading_results': [],
                    'total_score': 0
                }
            
            all_grading_results = []
            
            # å¹¶è¡Œå¤„ç†æ‰€æœ‰æ‰¹æ¬¡
            import asyncio
            
            async def grade_batch(batch):
                """æ‰¹æ”¹å•ä¸ªæ‰¹æ¬¡"""
                batch_id = batch.get('batch_id', 'default_batch')
                students = batch.get('students', [])
                question_ids = batch.get('question_ids', [])
                
                rubric_package = batch_rubric_packages.get(batch_id, {})
                context_package = question_context_packages.get(batch_id, {})
                
                logger.info("=" * 60)
                logger.info(f"[å¼€å§‹æ‰¹æ”¹æ‰¹æ¬¡] {batch_id}")
                logger.info(f"   é¢˜ç›®: {question_ids if question_ids else 'all'}")
                logger.info(f"   å­¦ç”Ÿæ•°é‡: {len(students)}")
                logger.info(f"   è¯„åˆ†ç‚¹æ•°é‡: {len(rubric_package.get('criteria', []))}")
                logger.info("=" * 60)
                
                batch_results = []
                # æ‰¹æ”¹è¯¥æ‰¹æ¬¡çš„å­¦ç”Ÿï¼ˆæ¯ä¸ªæ‰¹æ¬¡å¤„ç†ç›¸åŒçš„å­¦ç”Ÿï¼Œä½†åªæ‰¹æ”¹æŒ‡å®šçš„é¢˜ç›®ï¼‰
                for idx, student in enumerate(students, 1):
                    logger.info(f"   æ‰¹æ”¹å­¦ç”Ÿ {idx}/{len(students)}: {student.get('student_name', student.get('student_id', 'Unknown'))}")
                    result = await self._grade_student(
                        student,
                        rubric_package,
                        context_package,
                        answer_understanding,
                        state,  # ä¼ é€’stateä»¥ä¾¿è·å–answer_multimodal_files
                        question_ids  # ä¼ é€’é¢˜ç›®IDåˆ—è¡¨ï¼Œç”¨äºè¿‡æ»¤è¯„ä¼°ç»“æœ
                    )
                    batch_results.append(result)
                
                logger.info("=" * 60)
                logger.info(f"[æ‰¹æ¬¡å®Œæˆ] {batch_id}")
                logger.info(f"   å¤„ç†äº† {len(batch_results)} ä¸ªå­¦ç”Ÿ")
                total_eval_count = sum(len(r.get('evaluations', [])) for r in batch_results)
                logger.info(f"   æ€»è¯„ä¼°ç»“æœ: {total_eval_count} ä¸ª")
                logger.info("=" * 60)
                return batch_results
            
            # å¹¶è¡Œæ‰§è¡Œæ‰€æœ‰æ‰¹æ¬¡çš„æ‰¹æ”¹
            if len(batches_info) > 1:
                logger.info(f"å¹¶è¡Œå¤„ç† {len(batches_info)} ä¸ªæ‰¹æ¬¡...")
                try:
                    batch_results_list = await asyncio.gather(*[grade_batch(batch) for batch in batches_info])
                    # å±•å¹³ç»“æœåˆ—è¡¨
                    for i, batch_results in enumerate(batch_results_list):
                        logger.info(f"æ‰¹æ¬¡ {batches_info[i].get('batch_id')} è¿”å›äº† {len(batch_results)} ä¸ªç»“æœ")
                        all_grading_results.extend(batch_results)
                    logger.info(f"å¹¶è¡Œæ‰¹æ”¹å®Œæˆï¼Œæ€»å…± {len(all_grading_results)} ä¸ªå­¦ç”Ÿç»“æœ")
                except Exception as e:
                    logger.error(f"å¹¶è¡Œæ‰¹æ”¹å¤±è´¥: {e}", exc_info=True)
                    # å¦‚æœå¹¶è¡Œå¤±è´¥ï¼Œå°è¯•é¡ºåºå¤„ç†
                    logger.warning("å¹¶è¡Œæ‰¹æ”¹å¤±è´¥ï¼Œå°è¯•é¡ºåºå¤„ç†...")
                    for batch in batches_info:
                        try:
                            batch_results = await grade_batch(batch)
                            all_grading_results.extend(batch_results)
                        except Exception as batch_error:
                            logger.error(f"æ‰¹æ¬¡ {batch.get('batch_id')} å¤„ç†å¤±è´¥: {batch_error}", exc_info=True)
                            # ç»§ç»­å¤„ç†å…¶ä»–æ‰¹æ¬¡ï¼Œä¸ä¸­æ–­æ•´ä¸ªæµç¨‹
            else:
                # å•æ‰¹æ¬¡é¡ºåºå¤„ç†
                logger.info("å•æ‰¹æ¬¡é¡ºåºå¤„ç†...")
                for batch in batches_info:
                    batch_results = await grade_batch(batch)
                    all_grading_results.extend(batch_results)
            
            # è®¡ç®—æ€»åˆ†
            total_score = sum(r.get('total_score', 0) for r in all_grading_results) / len(all_grading_results) if all_grading_results else 0
            
            logger.info(f"   æ‰¹æ”¹äº† {len(all_grading_results)} ä¸ªå­¦ç”Ÿ")
            logger.info(f"   å¹³å‡åˆ†: {total_score:.1f}")
            
            # ç»Ÿè®¡è¯„ä¼°ç»“æœæ•°é‡
            total_evaluations = sum(len(r.get('evaluations', [])) for r in all_grading_results)
            logger.info(f"   æ€»è¯„ä¼°ç»“æœæ•°é‡: {total_evaluations}")
            
            # ç»Ÿè®¡é¢˜ç›®è¦†ç›–
            all_question_ids_in_results = set()
            for r in all_grading_results:
                for e in r.get('evaluations', []):
                    criterion_id = e.get('criterion_id', '')
                    if '_' in criterion_id:
                        qid = criterion_id.split('_')[0]
                        all_question_ids_in_results.add(qid)
            logger.info(f"   æ‰¹æ”¹è¦†ç›–çš„é¢˜ç›®: {sorted(all_question_ids_in_results)} ({len(all_question_ids_in_results)}é“é¢˜)")
            
            logger.info(f"[{self.agent_name}] æ‰¹æ”¹å®Œæˆ")
            
            # åªè¿”å›éœ€è¦æ›´æ–°çš„å­—æ®µ
            return {
                'grading_results': all_grading_results,
                'total_score': total_score,
                'progress_percentage': 80.0,
                'current_step': "æ‰¹æ”¹ä½œä¸š"
            }
            
        except Exception as e:
            error_msg = f"[{self.agent_name}] æ‰§è¡Œå¤±è´¥: {str(e)}"
            logger.error(error_msg)
            
            if 'errors' not in state:
                state['errors'] = []
            state['errors'].append({
                'agent': self.agent_name,
                'error': error_msg,
                'timestamp': str(datetime.now())
            })
            
            return state
    
    async def _grade_student(
        self,
        student: Dict[str, Any],
        rubric_package: Dict[str, Any],
        context_package: Dict[str, Any],
        answer_understanding: Dict[str, Any],
        state: Dict[str, Any] = None,
        question_ids: List[str] = None
    ) -> Dict[str, Any]:
        """æ‰¹æ”¹å•ä¸ªå­¦ç”Ÿï¼ˆåŸºäºå‹ç¼©ç‰ˆè¯„åˆ†åŒ…å’Œä¸Šä¸‹æ–‡ï¼‰"""
        
        student_id = student.get('student_id', '')
        student_name = student.get('name', '')
        
        # è·å–å‹ç¼©ç‰ˆè¯„åˆ†æ ‡å‡†
        compressed_criteria = rubric_package.get('compressed_criteria', [])
        decision_trees = rubric_package.get('decision_trees', {})
        quick_checks = rubric_package.get('quick_checks', {})
        total_points = rubric_package.get('total_points', 100)
        
        # æ£€æŸ¥æ˜¯å¦åªæœ‰é»˜è®¤è¯„åˆ†ç‚¹ï¼ˆè¯´æ˜æ‰¹æ”¹æ ‡å‡†è§£æå¤±è´¥ï¼‰
        # å¦‚æœåªæœ‰1ä¸ªè¯„åˆ†ç‚¹ä¸”åˆ†å€¼ä¸º100ï¼Œå¯èƒ½æ˜¯é»˜è®¤æ ‡å‡†
        if len(compressed_criteria) == 1 and compressed_criteria[0].get('pts', 0) == 100.0:
            logger.warning("æ£€æµ‹åˆ°é»˜è®¤è¯„åˆ†æ ‡å‡†ï¼Œæ‰¹æ”¹æ ‡å‡†è§£æå¯èƒ½å¤±è´¥")
            # å°è¯•ä»stateä¸­è·å–åŸå§‹çš„rubric_understanding
            if state:
                rubric_understanding = state.get('rubric_understanding')
                if rubric_understanding and rubric_understanding.get('criteria'):
                    criteria = rubric_understanding.get('criteria', [])
                    if len(criteria) > 1:
                        logger.info(f"ä»rubric_understandingä¸­æ¢å¤ {len(criteria)} ä¸ªè¯„åˆ†ç‚¹")
                        # é‡æ–°æ„å»ºcompressed_criteria
                        compressed_criteria = []
                        for criterion in criteria:
                            if isinstance(criterion, dict):
                                compressed_criteria.append({
                                    'id': criterion.get('criterion_id', ''),
                                    'desc': criterion.get('description', '')[:50],
                                    'pts': criterion.get('points', 0),
                                    'method': criterion.get('evaluation_method', 'semantic')
                                })
                        logger.info(f"å·²æ¢å¤ {len(compressed_criteria)} ä¸ªè¯„åˆ†ç‚¹åˆ°compressed_criteria")
        
        # è·å–å­¦ç”Ÿç­”æ¡ˆå†…å®¹
        # ä¼˜å…ˆä»åŸå§‹ç­”æ¡ˆæ–‡ä»¶è·å–å®Œæ•´æ–‡æœ¬ï¼Œè€Œä¸æ˜¯ä»ç†è§£ç»“æœè·å–ï¼ˆç†è§£ç»“æœå¯èƒ½è¢«LLMç®€åŒ–ï¼‰
        answer_text = ''
        answer_summary = ''
        
        # PDFç°åœ¨ç›´æ¥ä½¿ç”¨Vision APIå¤„ç†ï¼Œä¸æå–æ–‡æœ¬
        # ç­”æ¡ˆå†…å®¹å°†é€šè¿‡Vision APIåœ¨æ‰¹æ”¹æ—¶ç›´æ¥è·å–
        if state:
            answer_files = state.get('answer_multimodal_files', [])
            if answer_files:
                answer_file = answer_files[0]
                modality_type = answer_file.get('modality_type', '')
                content_rep = answer_file.get('content_representation', {})
                
                # å¦‚æœæ˜¯PDFå›¾ç‰‡æ ¼å¼ï¼Œç­”æ¡ˆå†…å®¹å°†é€šè¿‡Vision APIåœ¨æ‰¹æ”¹æ—¶è·å–
                if modality_type == 'pdf_image':
                    # ç­”æ¡ˆå†…å®¹å°†åœ¨æ‰¹æ”¹æç¤ºè¯ä¸­é€šè¿‡Vision APIè·å–
                    answer_text = ""  # ç•™ç©ºï¼Œè®©LLMé€šè¿‡Vision APIç›´æ¥æŸ¥çœ‹PDF
                    logger.info("PDFæ–‡ä»¶å°†ç›´æ¥é€šè¿‡Vision APIå¤„ç†ï¼Œä¸æå–æ–‡æœ¬")
                elif isinstance(content_rep, dict) and 'text' in content_rep:
                    # æ–‡æœ¬æ ¼å¼ï¼ˆéPDFï¼‰
                    raw_text = content_rep['text']
                    answer_text = self._filter_toc_content(raw_text)
                    logger.info(f"ä»åŸå§‹æ–‡ä»¶æå–ç­”æ¡ˆæ–‡æœ¬ï¼ŒåŸå§‹é•¿åº¦: {len(raw_text)} å­—ç¬¦ï¼Œè¿‡æ»¤å: {len(answer_text)} å­—ç¬¦")
        
        # å¦‚æœåŸå§‹æ–‡ä»¶æ²¡æœ‰æ–‡æœ¬ï¼Œå°è¯•ä»ç†è§£ç»“æœè·å–
        if not answer_text and answer_understanding:
            answer_text = answer_understanding.get('answer_text', '') or answer_understanding.get('answer_content', '')
            answer_summary = answer_understanding.get('summary', '') or answer_understanding.get('answer_summary', '')
            if answer_text:
                logger.info(f"ä»ç†è§£ç»“æœæå–ç­”æ¡ˆæ–‡æœ¬ï¼Œé•¿åº¦: {len(answer_text)} å­—ç¬¦")
        
        # å¦‚æœè¿˜æ˜¯æ²¡æœ‰ï¼Œä½¿ç”¨é»˜è®¤æ–‡æœ¬
        if not answer_text and not answer_summary:
            answer_text = "å­¦ç”Ÿç­”æ¡ˆå†…å®¹æœªæå–"
            logger.warning("æ— æ³•è·å–å­¦ç”Ÿç­”æ¡ˆå†…å®¹ï¼Œä½¿ç”¨é»˜è®¤æ–‡æœ¬")
        else:
            # è®°å½•ç­”æ¡ˆæ–‡æœ¬çš„å‰100ä¸ªå­—ç¬¦ç”¨äºè°ƒè¯•
            preview = (answer_text or answer_summary)[:100]
            logger.info(f"ç­”æ¡ˆæ–‡æœ¬é¢„è§ˆ: {preview}...")
        
        # è·å–é¢˜ç›®ä¸Šä¸‹æ–‡
        question_context = context_package.get('context_summary', '')
        
        if not compressed_criteria:
            logger.warning(f"æ²¡æœ‰è¯„åˆ†æ ‡å‡†ï¼Œè·³è¿‡æ‰¹æ”¹")
            return {
                'student_id': student_id,
                'student_name': student_name,
                'evaluations': [],
                'total_score': 0,
                'processing_time_ms': 0
            }
        
        # è·å–ç­”æ¡ˆæ–‡ä»¶ä¿¡æ¯ï¼ˆç”¨äº Gemini 3 Pro åŸç”Ÿå¤šæ¨¡æ€ï¼‰
        answer_file = None
        if state:
            answer_files = state.get('answer_multimodal_files', [])
            if answer_files:
                answer_file = answer_files[0]

        # æ„å»ºæ‰¹æ”¹æç¤ºè¯ï¼ˆGemini 3 Pro åŸç”Ÿç‰ˆæœ¬ï¼‰
        prompt_text, answer_file_path = self._build_grading_prompt(
            answer_text or answer_summary,
            question_context,
            compressed_criteria,
            decision_trees,
            total_points,
            answer_file
        )

        # ä¿®å¤ï¼šé»˜è®¤å€¼åº”è¯¥æ˜¯ "false"ï¼Œç¡®ä¿å¯ç”¨çœŸå®çš„ LLM æ‰¹æ”¹
        force_simple = os.getenv("SKIP_LLM_GRADING", "false").lower() == "true" or not getattr(self.llm_client, "api_key", None)
        logger.info(f"ğŸ” æ‰¹æ”¹æ¨¡å¼æ£€æŸ¥: SKIP_LLM_GRADING={os.getenv('SKIP_LLM_GRADING', 'false')}, force_simple={force_simple}, has_api_key={bool(getattr(self.llm_client, 'api_key', None))}")

        evaluations: List[Dict[str, Any]] = []
        if force_simple:
            logger.info("è·³è¿‡ LLM æ‰¹æ”¹ï¼Œä½¿ç”¨ç®€å•è§„åˆ™")
            evaluations = self._simple_grading(compressed_criteria, quick_checks)
        else:
            try:
                # æ„å»ºæ¶ˆæ¯ï¼ˆç®€å•æ–‡æœ¬æ ¼å¼ï¼‰
                messages = [
                    {
                        "role": "user",
                        "content": prompt_text
                    }
                ]

                logger.info(f"ğŸ“ è°ƒç”¨ Gemini 3 Pro æ‰¹æ”¹ {student_name} çš„ç­”æ¡ˆ...")
                if answer_file_path:
                    logger.info(f"ğŸ“„ åŒ…å« PDF æ–‡ä»¶: {answer_file_path}")

                num_criteria = len(compressed_criteria)
                logger.info(f"è¯„åˆ†ç‚¹æ•°é‡: {num_criteria}")

                # æ£€æŸ¥æ˜¯å¦å¯ç”¨æµå¼ä¼ è¾“
                use_streaming = os.getenv("USE_STREAMING", "true").lower() == "true"

                if use_streaming:
                    logger.info("ğŸŒŠ Streaming thoughts for UI; final resultå°†é€šè¿‡éæµå¼è·å–")
                    response = self._grade_with_streaming(
                        messages,
                        answer_file_path,
                        student_name,
                        state
                    )
                else:
                    response = self._grade_non_streaming(messages, answer_file_path, student_name)

                # è§£æ LLM å“åº”
                logger.info("ğŸ“Š å¼€å§‹è§£æ LLM å“åº”...")
                evaluations = self._parse_grading_response(response, compressed_criteria)
                logger.info(f"âœ… æˆåŠŸè§£æ {len(evaluations)} ä¸ªè¯„åˆ†ç‚¹")

            except Exception as e:
                logger.error(f"âŒ LLM æ‰¹æ”¹å¤±è´¥: {e}")
                logger.exception("è¯¦ç»†é”™è¯¯ä¿¡æ¯:")
                # å›é€€åˆ°ç®€å•æ‰¹æ”¹
                logger.warning("âš ï¸ å›é€€åˆ°ç®€å•æ‰¹æ”¹æ¨¡å¼")
                evaluations = self._simple_grading(compressed_criteria, quick_checks)

        # æ ¹æ® question_ids è¿‡æ»¤è¯„åˆ†ç»“æœï¼ˆåªè¿”å›å½“å‰æ‰¹æ¬¡çš„é¢˜ç›®ï¼‰
        if question_ids:
            if "UNKNOWN" in question_ids:
                filtered_evaluations = evaluations
            else:
                filtered_evaluations = []
                for eval_item in evaluations:
                    criterion_id = eval_item.get('criterion_id', '')
                    belongs_to_batch = False
                    for qid in question_ids:
                        if criterion_id.startswith(qid + "_") or criterion_id == qid:
                            belongs_to_batch = True
                            break
                    if belongs_to_batch:
                        filtered_evaluations.append(eval_item)
            evaluations = filtered_evaluations
            logger.info(f"ğŸ” è¿‡æ»¤åä¿ç•™ {len(evaluations)} ä¸ªè¯„åˆ†ç‚¹ï¼ˆé¢˜ç›®èŒƒå›´: {question_ids}ï¼‰")
        
        # è®¡ç®—æ€»åˆ†
        total_score = sum(e.get('score_earned', 0) for e in evaluations)

        # è®°å½•LLMè°ƒç”¨è½¨è¿¹
        trace = dict(self.llm_client.last_call or {})
        trace.update({
            'summary': f"{student_name or student_id} - {len(evaluations)} æ¡è¯„ä¼°ï¼Œå¾—åˆ† {total_score}",
            'student_id': student_id,
            'student_name': student_name,
            'question_ids': question_ids,
            'evaluation_count': len(evaluations),
            'score': total_score
        })
        self._record_llm_trace(state, trace)
        
        return {
            'student_id': student_id,
            'student_name': student_name,
            'evaluations': evaluations,
            'total_score': total_score,
            'processing_time_ms': 1000,
            'question_ids': question_ids  # è®°å½•å¤„ç†çš„é¢˜ç›®
        }

    def _grade_with_streaming(
        self,
        messages: List[Dict],
        answer_file_path: str,
        student_name: str,
        state: Dict
    ) -> str:
        """
        Stream thoughts for UI, then fetch final result without streaming
        """
        callback = state.get('streaming_callback') if state else None
        if callable(callback):
            self._stream_thoughts_preview(
                messages,
                answer_file_path,
                student_name,
                callback
            )
        else:
            logger.info("Streaming preview skipped: streaming_callback is missing or not callable")

        return self._grade_non_streaming(messages, answer_file_path, student_name)

    def _stream_thoughts_preview(
        self,
        messages: List[Dict],
        answer_file_path: str,
        student_name: str,
        callback
    ) -> None:
        """Stream only thought content for real-time display"""
        thought_buffer = ""
        text_preview = ""
        try:
            stream = self.llm_client.chat(
                messages,
                temperature=0.2,
                files=[answer_file_path] if answer_file_path else None,
                thinking_level="high",
                stream=True,
                include_thoughts=True,
                timeout=self.llm_timeout
            )

            for chunk in stream:
                chunk_type = chunk.get("type", "text")
                chunk_content = chunk.get("content", "")

                if chunk_type == "thought":
                    thought_buffer += chunk_content
                    callback({
                        "type": "thought",
                        "content": chunk_content,
                        "student": student_name
                    })
                    logger.debug(f"[thought] {chunk_content[:50]}...")
                elif chunk_type == "text":
                    text_preview += chunk_content

            logger.info(f"Streaming preview finished: thoughts {len(thought_buffer)} chars, text preview {len(text_preview)} chars")
        except Exception as e:
            logger.error(f"Streaming preview failed: {e}", exc_info=True)

    def _grade_non_streaming(
        self,
        messages: List[Dict],
        answer_file_path: str,
        student_name: str
    ) -> str:
        """Fetch full grading result without streaming to ease JSON parsing"""
        response = self.llm_client.chat(
            messages,
            temperature=0.2,
            files=[answer_file_path] if answer_file_path else None,
            thinking_level="high",
            stream=False,
            timeout=self.llm_timeout
        )
        logger.info(f"LLM non-stream response length: {len(response)}")
        return response

    def _build_grading_prompt(
        self,
        answer_text: str,
        question_context: str,
        compressed_criteria: List[Dict],
        decision_trees: Dict,
        total_points: float,
        answer_file: Dict[str, Any] = None
    ) -> tuple[str, str | None]:
        """
        æ„å»ºæ‰¹æ”¹æç¤ºè¯ - Gemini 3 Pro åŸç”Ÿç‰ˆæœ¬

        Returns:
            (prompt_text, answer_file_path): æç¤ºè¯æ–‡æœ¬å’Œç­”æ¡ˆæ–‡ä»¶è·¯å¾„ï¼ˆå¦‚æœæœ‰ï¼‰
        """
        
        criteria_text = "\n".join([
            f"{i+1}. [{c['id']}] {c['desc']} ({c['pts']}åˆ†)"
            for i, c in enumerate(compressed_criteria)
        ])
        
        answer_file_path = None

        # å¦‚æœç­”æ¡ˆæ–‡ä»¶æ˜¯PDFæ ¼å¼ï¼Œç›´æ¥ä¼ é€’æ–‡ä»¶è·¯å¾„ï¼ˆGemini 3 Pro åŸç”Ÿå¤„ç†ï¼‰
        if answer_file:
            modality_type = answer_file.get('modality_type', '')
            file_path = answer_file.get('file_path', '')

            if modality_type in ['pdf_image', 'pdf'] and file_path:
                answer_file_path = file_path
                logger.info(f"ğŸ“„ å°†ç›´æ¥ä¼ é€’ PDF æ–‡ä»¶ç»™ Gemini 3 Pro: {file_path}")

        prompt_text = f"""æ ¹æ®è¯„åˆ†æ ‡å‡†æ‰¹æ”¹å­¦ç”Ÿç­”æ¡ˆã€‚

ã€é¢˜ç›®ä¸Šä¸‹æ–‡ã€‘
{question_context}

ã€å­¦ç”Ÿç­”æ¡ˆã€‘
{"ï¼ˆç­”æ¡ˆåœ¨ PDF æ–‡ä»¶ä¸­ï¼Œè¯·ç›´æ¥æŸ¥çœ‹ï¼‰" if answer_file_path else answer_text}

ã€è¯„åˆ†æ ‡å‡†ã€‘
{criteria_text}

**å¿…é¡»è¯„ä¼°æ‰€æœ‰ {len(compressed_criteria)} ä¸ªè¯„åˆ†ç‚¹ï¼**

ã€æ ¸å¿ƒè¦æ±‚ã€‘
1. **é€é¡¹è¯„ä¼°**ï¼šæ¯ä¸ª criterion_id å•ç‹¬è¯„ä¼°
2. **è¯†åˆ«è§£é¢˜æ–¹æ³•**ï¼š
   - å¦‚æœ‰å¤šç§æ–¹æ³•ï¼ˆCase1/Case2, Method1/Method2ï¼‰ï¼Œè¯†åˆ«å­¦ç”Ÿä½¿ç”¨çš„æ–¹æ³•
   - åªè¯„ä¼°å­¦ç”Ÿä½¿ç”¨çš„æ–¹æ³•ï¼Œæœªä½¿ç”¨çš„æ–¹æ³•æ ‡è®°ä¸º"æœªæ»¡è¶³"ï¼ˆscore_earned=0ï¼‰
3. **è¯¦ç»†æè¿°**ï¼š
   - student_workï¼šå­¦ç”Ÿçš„å…¬å¼ã€æ­¥éª¤ã€ç»“æœã€ä½¿ç”¨çš„æ–¹æ³•
   - justificationï¼šä¸ºä»€ä¹ˆç»™è¿™ä¸ªåˆ†æ•°ï¼Œä¸æ ‡å‡†ç­”æ¡ˆå¯¹æ¯”
   - evidenceï¼šå…·ä½“è®¡ç®—å¼ã€ä¸­é—´ç»“æœã€æœ€ç»ˆç­”æ¡ˆ
   - feedbackï¼šå…·ä½“æ”¹è¿›å»ºè®®
4. **æ•°å­¦å…¬å¼è§„åˆ™**ï¼š
   - ç¦æ­¢åæ–œæ  LaTeXï¼ˆ\\frac, \\times ç­‰ï¼‰
   - ä½¿ç”¨ï¼ša/b, Ã—, Ï€, âˆšx, âˆ , â–³, â‰…

ã€JSON è¾“å‡ºæ ¼å¼ã€‘
{{
  "evaluations": [
    {{
      "criterion_id": "Q1_C1",
      "score_earned": 5,
      "max_score": 5,
      "is_met": true,
      "satisfaction_level": "å®Œå…¨æ»¡è¶³",
      "student_work": "å­¦ç”Ÿçš„å…¬å¼ã€æ­¥éª¤ã€ç»“æœã€ä½¿ç”¨çš„æ–¹æ³•",
      "justification": "ä¸ºä»€ä¹ˆç»™è¿™ä¸ªåˆ†æ•°ï¼Œä¸æ ‡å‡†ç­”æ¡ˆå¯¹æ¯”",
      "matched_criterion": "ç¬¦åˆè¯„åˆ†æ ‡å‡†çš„å“ªä¸€é¡¹",
      "feedback": "å…·ä½“æ”¹è¿›å»ºè®®",
      "evidence": ["å…·ä½“è®¡ç®—å¼", "ä¸­é—´ç»“æœ", "æœ€ç»ˆç­”æ¡ˆ"]
    }}
  ],
  "total_score": æ€»åˆ†,
  "overall_feedback": "æ€»ä½“è¯„ä»·",
  "question_by_question_feedback": {{"Q1": "é¢˜ç›®1æ‰¹æ”¹è¯´æ˜"}}
}}

ã€ç¤ºä¾‹ - å¤šç§æ–¹æ³•ã€‘
å­¦ç”Ÿä½¿ç”¨ Case1ï¼ˆå«ç†ç”±ï¼‰ï¼š
- Q8a_C1_Case1: score_earned=2, is_met=true, justification="å­¦ç”Ÿä½¿ç”¨ Case1ï¼Œç†ç”±å®Œæ•´"
- Q8a_C2_Case2: score_earned=0, is_met=false, justification="å­¦ç”Ÿæœªä½¿ç”¨ Case2"

ç¦æ­¢ï¼šæ¦‚æ‹¬æ€§æè¿°ã€åˆå¹¶è¯„åˆ†ç‚¹ã€å«ç³Šç†ç”±
æ­£ç¡®ï¼šè¯¦ç»†æè¿°ã€é€é¡¹è¯„ä¼°ã€è¯†åˆ«æ–¹æ³•"""

        return prompt_text, answer_file_path
    
    def _parse_grading_response(self, response: str, compressed_criteria: List[Dict]) -> List[Dict]:
        """è§£æLLMæ‰¹æ”¹å“åº”"""
        import re
        
        # å°è¯•æå–JSONï¼ˆæ”¯æŒå¤šè¡ŒJSONå’Œä»£ç å—ï¼‰
        json_start = response.find('{')
        json_end = response.rfind('}') + 1
        
        if json_start < 0 or json_end <= json_start:
            logger.warning("æœªæ‰¾åˆ°JSONå†…å®¹ï¼Œä½¿ç”¨ç®€åŒ–è¯„åˆ†")
            return self._simple_grading(compressed_criteria, {})
        
        json_str = response[json_start:json_end]
        
        # å¤šæ¬¡å°è¯•è§£æï¼Œé€æ­¥ä¿®å¤JSONé—®é¢˜
        for attempt in range(3):
            try:
                result = json.loads(json_str)
                evaluations = result.get('evaluations', [])
                
                if evaluations:
                    # è®°å½•è§£æç»“æœ
                    logger.info(f"JSONè§£ææˆåŠŸï¼è§£æåˆ° {len(evaluations)} ä¸ªè¯„åˆ†ç‚¹è¯„ä¼°")
                    for i, eval_item in enumerate(evaluations, 1):
                        criterion_id = eval_item.get('criterion_id', 'N/A')
                        score = eval_item.get('score_earned', 0)
                        max_score = eval_item.get('max_score', 0)
                        student_work = eval_item.get('student_work', '')
                        matched_criterion = eval_item.get('matched_criterion', '')
                        justification = eval_item.get('justification', '')[:50]
                        logger.info(f"  è¯„ä¼°{i}: [{criterion_id}] {score}/{max_score}åˆ† - {justification}...")
                        if student_work:
                            logger.info(f"    å­¦ç”Ÿä½œç­”: {student_work[:80]}...")
                        if matched_criterion:
                            logger.info(f"    ç¬¦åˆæ ‡å‡†: {matched_criterion[:80]}...")
                    
                    # æ£€æŸ¥æ˜¯å¦æœ‰question_by_question_feedback
                    if 'question_by_question_feedback' in result:
                        q_feedback = result['question_by_question_feedback']
                        logger.info(f"é€é¢˜åé¦ˆ: {len(q_feedback)} é“é¢˜")
                        for q_id, feedback in q_feedback.items():
                            logger.info(f"  {q_id}: {feedback[:100]}...")
                    
                    # æ£€æŸ¥æ˜¯å¦æ‰€æœ‰è¯„åˆ†ç‚¹éƒ½è¢«è¯„ä¼°äº†
                    evaluated_criterion_ids = {e.get('criterion_id') for e in evaluations if e.get('criterion_id')}
                    all_criterion_ids = {c.get('id') for c in compressed_criteria if c.get('id')}
                    missing_criterion_ids = all_criterion_ids - evaluated_criterion_ids
                    
                    if missing_criterion_ids:
                        logger.warning(f"å‘ç° {len(missing_criterion_ids)} ä¸ªæœªè¯„ä¼°çš„è¯„åˆ†ç‚¹: {sorted(missing_criterion_ids)}")
                        # ä¸ºç¼ºå¤±çš„è¯„åˆ†ç‚¹åˆ›å»ºé»˜è®¤è¯„ä¼°é¡¹
                        for criterion_id in missing_criterion_ids:
                            # æ‰¾åˆ°å¯¹åº”çš„è¯„åˆ†ç‚¹ä¿¡æ¯
                            criterion_info = None
                            for c in compressed_criteria:
                                if c.get('id') == criterion_id:
                                    criterion_info = c
                                    break
                            
                            if criterion_info:
                                missing_eval = {
                                    'criterion_id': criterion_id,
                                    'score_earned': 0,
                                    'max_score': criterion_info.get('pts', 0),
                                    'is_met': False,
                                    'satisfaction_level': 'æœªè¯„ä¼°',
                                    'justification': f'è¯¥è¯„åˆ†ç‚¹æœªåœ¨LLMå“åº”ä¸­æ‰¾åˆ°ï¼Œå¯èƒ½æ˜¯å“åº”è¢«æˆªæ–­',
                                    'matched_criterion': criterion_info.get('desc', '')[:100],
                                    'student_work': 'æœªè¯„ä¼°',
                                    'feedback': 'è¯·æ£€æŸ¥æ‰¹æ”¹ç»“æœæ˜¯å¦å®Œæ•´',
                                    'evidence': []
                                }
                                evaluations.append(missing_eval)
                                logger.info(f"ä¸ºç¼ºå¤±çš„è¯„åˆ†ç‚¹ {criterion_id} åˆ›å»ºé»˜è®¤è¯„ä¼°é¡¹")
                    
                    # ç¡®ä¿æ‰€æœ‰è¯„ä¼°é¡¹éƒ½åŒ…å«å¿…è¦å­—æ®µ
                    for eval_item in evaluations:
                        # ç¡®ä¿æœ‰max_scoreå­—æ®µ
                        if 'max_score' not in eval_item:
                            # ä»compressed_criteriaä¸­æŸ¥æ‰¾å¯¹åº”çš„åˆ†å€¼
                            criterion_id = eval_item.get('criterion_id', '')
                            for c in compressed_criteria:
                                if c.get('id') == criterion_id:
                                    eval_item['max_score'] = c.get('pts', 0)
                                    break
                        # ç¡®ä¿æœ‰matched_criterionå­—æ®µï¼ˆå¦‚æœæ²¡æœ‰ï¼Œä»justificationä¸­æå–æˆ–ä½¿ç”¨é»˜è®¤å€¼ï¼‰
                        if 'matched_criterion' not in eval_item or not eval_item.get('matched_criterion'):
                            # å°è¯•ä»justificationä¸­æå–ï¼Œæˆ–ä½¿ç”¨description
                            justification = eval_item.get('justification', '')
                            if justification:
                                # ç®€å•æå–ï¼šå–justificationçš„å‰100å­—ç¬¦ä½œä¸ºmatched_criterion
                                eval_item['matched_criterion'] = justification[:100]
                            else:
                                eval_item['matched_criterion'] = 'å·²è¯„ä¼°'
                        # ç¡®ä¿æœ‰student_workå­—æ®µ
                        if 'student_work' not in eval_item or not eval_item.get('student_work'):
                            # ä»evidenceä¸­æå–æˆ–ä½¿ç”¨justification
                            evidence = eval_item.get('evidence', [])
                            if evidence:
                                eval_item['student_work'] = 'ï¼›'.join(evidence[:3])  # å–å‰3ä¸ªè¯æ®
                            else:
                                eval_item['student_work'] = eval_item.get('justification', 'å·²è¯„ä¼°')[:200]
                    
                    logger.info(f"æœ€ç»ˆè¯„ä¼°é¡¹æ•°é‡: {len(evaluations)}ï¼Œè¯„åˆ†ç‚¹æ•°é‡: {len(compressed_criteria)}")
                    return evaluations
                else:
                    logger.warning("JSONè§£ææˆåŠŸä½†evaluationsä¸ºç©º")
                    break
                    
            except json.JSONDecodeError as e:
                error_pos = getattr(e, 'pos', None)
                error_msg = str(e)
                
                if attempt < 2:  # å‰ä¸¤æ¬¡å°è¯•ä¿®å¤
                    logger.warning(f"JSONè§£æå¤±è´¥ï¼ˆå°è¯•{attempt+1}/3ï¼‰: {error_msg}")
                    
                    # ä¿®å¤ç­–ç•¥1: ä¿®å¤LaTeXæ•°å­¦å…¬å¼ä¸­çš„åæ–œæ 
                    # åœ¨å­—ç¬¦ä¸²å€¼ä¸­ï¼ŒLaTeXå‘½ä»¤å¦‚ \frac, \sqrt éœ€è¦è½¬ä¹‰
                    if 'Invalid \\escape' in error_msg or '\\escape' in error_msg:
                        # ä½¿ç”¨æ›´ç®€å•ä½†æœ‰æ•ˆçš„æ–¹æ³•ï¼šåœ¨å­—ç¬¦ä¸²å€¼ä¸­ä¿®å¤åæ–œæ 
                        # éå†JSONå­—ç¬¦ä¸²ï¼Œæ‰¾åˆ°å­—ç¬¦ä¸²å€¼å¹¶ä¿®å¤å…¶ä¸­çš„åæ–œæ 
                        fixed_parts = []
                        i = 0
                        in_string = False
                        escape_next = False
                        
                        while i < len(json_str):
                            char = json_str[i]
                            
                            if escape_next:
                                # å¦‚æœä¸‹ä¸€ä¸ªå­—ç¬¦æ˜¯è½¬ä¹‰å­—ç¬¦
                                if char not in '\\/bfnrtu"':
                                    # ä¸æ˜¯åˆæ³•çš„JSONè½¬ä¹‰ï¼Œéœ€è¦åŒè½¬ä¹‰
                                    fixed_parts.append('\\\\' + char)
                                else:
                                    fixed_parts.append('\\' + char)
                                escape_next = False
                            elif char == '\\':
                                # é‡åˆ°åæ–œæ ï¼Œæ£€æŸ¥ä¸‹ä¸€ä¸ªå­—ç¬¦
                                if i + 1 < len(json_str):
                                    next_char = json_str[i + 1]
                                    if next_char not in '\\/bfnrtu"':
                                        # ä¸æ˜¯åˆæ³•çš„JSONè½¬ä¹‰ï¼Œéœ€è¦åŒè½¬ä¹‰
                                        fixed_parts.append('\\\\')
                                        escape_next = True
                                    else:
                                        fixed_parts.append('\\')
                                        escape_next = True
                                else:
                                    fixed_parts.append('\\')
                            elif char == '"':
                                # æ£€æŸ¥æ˜¯å¦æ˜¯è½¬ä¹‰çš„å¼•å·
                                if i > 0 and json_str[i-1] == '\\' and (i < 2 or json_str[i-2] != '\\'):
                                    # è½¬ä¹‰çš„å¼•å·ï¼Œä¿æŒåŸæ ·
                                    fixed_parts.append(char)
                                else:
                                    # å­—ç¬¦ä¸²å¼€å§‹æˆ–ç»“æŸ
                                    in_string = not in_string
                                    fixed_parts.append(char)
                            else:
                                fixed_parts.append(char)
                            
                            i += 1
                        
                        json_str = ''.join(fixed_parts)
                    
                    # ä¿®å¤ç­–ç•¥2: ä¿®å¤æœªè½¬ä¹‰çš„å¼•å·
                    elif 'Expecting' in error_msg and 'delimiter' in error_msg:
                        # å°è¯•ä¿®å¤å¸¸è§çš„åˆ†éš”ç¬¦é—®é¢˜
                        json_str = json_str.replace(',,', ',').replace('{,', '{').replace(',}', '}')
                    
                    # ä¿®å¤ç­–ç•¥3: ç§»é™¤ä»£ç å—æ ‡è®°
                    json_str = json_str.replace('```json', '').replace('```', '')
                    
                    logger.info(f"å°è¯•ä¿®å¤JSONï¼ˆé”™è¯¯ä½ç½®: {error_pos}ï¼‰...")
                else:
                    # æœ€åä¸€æ¬¡å°è¯•å¤±è´¥ï¼Œè®°å½•è¯¦ç»†ä¿¡æ¯
                    logger.error(f"JSONè§£ææœ€ç»ˆå¤±è´¥: {error_msg}")
                    logger.error(f"é”™è¯¯ä½ç½®: {error_pos}")
                    logger.error(f"é”™è¯¯ä½ç½®é™„è¿‘çš„å†…å®¹: {json_str[max(0, (error_pos or 0)-100):(error_pos or 0)+100] if error_pos else 'N/A'}")
                    logger.error(f"å“åº”å†…å®¹å‰1000å­—ç¬¦: {response[:1000]}")
                    break
            except Exception as e:
                logger.error(f"è§£æå¤±è´¥: {e}")
                logger.error(f"å“åº”å†…å®¹å‰500å­—ç¬¦: {response[:500]}")
                break
        
        # å¦‚æœæ‰€æœ‰å°è¯•éƒ½å¤±è´¥ï¼Œå°è¯•ä½¿ç”¨æ›´å®½æ¾çš„æ–¹æ³•æå–evaluations
        logger.warning("æ‰€æœ‰JSONè§£æå°è¯•å¤±è´¥ï¼Œå°è¯•ä½¿ç”¨å®½æ¾æ–¹æ³•æå–evaluations...")
        
        try:
            # æ–¹æ³•1: ä½¿ç”¨æ­£åˆ™è¡¨è¾¾å¼æå–evaluationsæ•°ç»„ä¸­çš„å¯¹è±¡
            # æŸ¥æ‰¾evaluationsæ•°ç»„çš„å¼€å§‹å’Œç»“æŸ
            eval_start = json_str.find('"evaluations"')
            if eval_start < 0:
                eval_start = json_str.find("'evaluations'")
            
            if eval_start >= 0:
                # æ‰¾åˆ°æ•°ç»„å¼€å§‹
                array_start = json_str.find('[', eval_start)
                if array_start >= 0:
                    # æ‰¾åˆ°åŒ¹é…çš„å³æ‹¬å·
                    bracket_count = 0
                    array_end = array_start
                    for i in range(array_start, len(json_str)):
                        if json_str[i] == '[':
                            bracket_count += 1
                        elif json_str[i] == ']':
                            bracket_count -= 1
                            if bracket_count == 0:
                                array_end = i + 1
                                break
                    
                    if array_end > array_start:
                        eval_array_str = json_str[array_start:array_end]
                        # å°è¯•æå–æ¯ä¸ªevaluationå¯¹è±¡
                        evaluations = []
                        i = 0
                        while i < len(eval_array_str):
                            if eval_array_str[i] == '{':
                                # æ‰¾åˆ°åŒ¹é…çš„}
                                brace_count = 0
                                obj_start = i
                                obj_end = i
                                for j in range(i, len(eval_array_str)):
                                    if eval_array_str[j] == '{':
                                        brace_count += 1
                                    elif eval_array_str[j] == '}':
                                        brace_count -= 1
                                        if brace_count == 0:
                                            obj_end = j + 1
                                            break
                                
                                if obj_end > obj_start:
                                    obj_str = eval_array_str[obj_start:obj_end]
                                    # å°è¯•è§£æè¿™ä¸ªå¯¹è±¡
                                    try:
                                        # ä¿®å¤å¸¸è§çš„JSONé—®é¢˜
                                        obj_str_fixed = obj_str
                                        # ä¿®å¤å•å¼•å·
                                        obj_str_fixed = obj_str_fixed.replace("'", '"')
                                        # ä¿®å¤Pythonå¸ƒå°”å€¼
                                        obj_str_fixed = obj_str_fixed.replace('True', 'true').replace('False', 'false').replace('None', 'null')
                                        # ä¿®å¤LaTeXè½¬ä¹‰ï¼šåœ¨å­—ç¬¦ä¸²å€¼ä¸­åŒè½¬ä¹‰åæ–œæ 
                                        # ç®€å•æ–¹æ³•ï¼šæ‰¾åˆ°æ‰€æœ‰ "key": "value" æ¨¡å¼ï¼Œä¿®å¤valueä¸­çš„åæ–œæ 
                                        import re
                                        # åŒ¹é… "key": "value" æ¨¡å¼
                                        def fix_value_escape(match):
                                            key_part = match.group(1)
                                            value_part = match.group(2)
                                            # ä¿®å¤valueä¸­çš„åæ–œæ ï¼ˆä½†ä¿ç•™åˆæ³•è½¬ä¹‰ï¼‰
                                            fixed_value = ''
                                            j = 0
                                            while j < len(value_part):
                                                if value_part[j] == '\\' and j + 1 < len(value_part):
                                                    next_char = value_part[j + 1]
                                                    if next_char not in '\\/bfnrtu"':
                                                        fixed_value += '\\\\' + next_char
                                                        j += 2
                                                    else:
                                                        fixed_value += '\\' + next_char
                                                        j += 2
                                                else:
                                                    fixed_value += value_part[j]
                                                    j += 1
                                            return f'"{key_part}": "{fixed_value}"'
                                        
                                        # å°è¯•ç›´æ¥è§£æ
                                        obj = json.loads(obj_str_fixed)
                                        if 'criterion_id' in obj:
                                            evaluations.append(obj)
                                    except Exception as parse_err:
                                        # å¦‚æœè§£æå¤±è´¥ï¼Œå°è¯•æ‰‹åŠ¨æå–å­—æ®µ
                                        try:
                                            # ä½¿ç”¨æ­£åˆ™è¡¨è¾¾å¼æå–å…³é”®å­—æ®µ
                                            criterion_id_match = re.search(r'"criterion_id"\s*:\s*"([^"]+)"', obj_str)
                                            score_match = re.search(r'"score_earned"\s*:\s*(\d+(?:\.\d+)?)', obj_str)
                                            max_score_match = re.search(r'"max_score"\s*:\s*(\d+(?:\.\d+)?)', obj_str)
                                            justification_match = re.search(r'"justification"\s*:\s*"([^"]+)"', obj_str)
                                            
                                            if criterion_id_match:
                                                student_work_match = re.search(r'"student_work"\s*:\s*"([^"]+)"', obj_str)
                                                matched_criterion_match = re.search(r'"matched_criterion"\s*:\s*"([^"]+)"', obj_str)
                                                evidence_match = re.search(r'"evidence"\s*:\s*\[([^\]]+)\]', obj_str)
                                                
                                                eval_obj = {
                                                    'criterion_id': criterion_id_match.group(1),
                                                    'score_earned': float(score_match.group(1)) if score_match else 0,
                                                    'max_score': float(max_score_match.group(1)) if max_score_match else 0,
                                                    'is_met': True,
                                                    'satisfaction_level': 'å®Œå…¨æ»¡è¶³',
                                                    'justification': justification_match.group(1) if justification_match else 'å·²è¯„ä¼°',
                                                    'feedback': 'æ— ',
                                                    'evidence': [],
                                                    'student_work': student_work_match.group(1) if student_work_match else (justification_match.group(1)[:200] if justification_match else 'å·²è¯„ä¼°'),
                                                    'matched_criterion': matched_criterion_match.group(1) if matched_criterion_match else (justification_match.group(1)[:100] if justification_match else 'å·²è¯„ä¼°')
                                                }
                                                evaluations.append(eval_obj)
                                        except:
                                            pass
                                    
                                    i = obj_end
                                else:
                                    i += 1
                            else:
                                i += 1
                        
                        if evaluations:
                            logger.info(f"ä½¿ç”¨å®½æ¾æ–¹æ³•æå–æˆåŠŸï¼è·å¾— {len(evaluations)} ä¸ªè¯„åˆ†ç‚¹è¯„ä¼°")
                            return evaluations
            
            # æ–¹æ³•2: ä»åŸå§‹å“åº”ä¸­ç›´æ¥æœç´¢evaluationå¯¹è±¡
            # æŸ¥æ‰¾æ‰€æœ‰åŒ…å«criterion_idçš„è¡Œ
            criterion_pattern = r'"criterion_id"\s*:\s*"([^"]+)"'
            criterion_matches = list(re.finditer(criterion_pattern, response))
            
            if criterion_matches:
                logger.info(f"åœ¨å“åº”ä¸­æ‰¾åˆ° {len(criterion_matches)} ä¸ªcriterion_idï¼Œå°è¯•æå–...")
                evaluations = []
                for match in criterion_matches:
                    criterion_id = match.group(1)
                    # åœ¨åŒ¹é…ä½ç½®é™„è¿‘æŸ¥æ‰¾å…¶ä»–å­—æ®µ
                    start_pos = max(0, match.start() - 200)
                    end_pos = min(len(response), match.end() + 1000)
                    region = response[start_pos:end_pos]
                    
                    # æå–å­—æ®µ
                    score_match = re.search(r'"score_earned"\s*:\s*(\d+(?:\.\d+)?)', region)
                    max_score_match = re.search(r'"max_score"\s*:\s*(\d+(?:\.\d+)?)', region)
                    justification_match = re.search(r'"justification"\s*:\s*"([^"]+)"', region)
                    student_work_match = re.search(r'"student_work"\s*:\s*"([^"]+)"', region)
                    matched_criterion_match = re.search(r'"matched_criterion"\s*:\s*"([^"]+)"', region)
                    
                    eval_obj = {
                        'criterion_id': criterion_id,
                        'score_earned': float(score_match.group(1)) if score_match else 0,
                        'max_score': float(max_score_match.group(1)) if max_score_match else 0,
                        'is_met': True,
                        'satisfaction_level': 'å®Œå…¨æ»¡è¶³',
                        'justification': justification_match.group(1) if justification_match else 'å·²è¯„ä¼°',
                        'feedback': 'æ— ',
                        'evidence': [],
                        'student_work': student_work_match.group(1) if student_work_match else (justification_match.group(1)[:200] if justification_match else 'å·²è¯„ä¼°'),
                        'matched_criterion': matched_criterion_match.group(1) if matched_criterion_match else (justification_match.group(1)[:100] if justification_match else 'å·²è¯„ä¼°')
                    }
                    evaluations.append(eval_obj)
                
                if evaluations:
                    logger.info(f"ä»å“åº”ä¸­ç›´æ¥æå–æˆåŠŸï¼è·å¾— {len(evaluations)} ä¸ªè¯„åˆ†ç‚¹è¯„ä¼°")
                    return evaluations
                    
        except Exception as e:
            logger.warning(f"å®½æ¾æå–æ–¹æ³•ä¹Ÿå¤±è´¥: {e}")
        
        # æœ€åä½¿ç”¨ç®€åŒ–è¯„åˆ†
        logger.warning("æ‰€æœ‰è§£ææ–¹æ³•å¤±è´¥ï¼Œä½¿ç”¨ç®€åŒ–è¯„åˆ†ä½œä¸ºå¤‡é€‰")
        return self._simple_grading(compressed_criteria, {})
    
    def _record_llm_trace(self, state: Dict[str, Any], trace: Dict[str, Any]):
        """è®°å½•LLMè°ƒç”¨è¯¦æƒ…ä¾›å‰ç«¯å±•ç¤º"""
        try:
            if 'step_results' not in state:
                state['step_results'] = {}
            agent_entry = state['step_results'].setdefault(self.agent_name, {'llm_calls': []})
            agent_entry.setdefault('llm_calls', [])
            agent_entry['llm_calls'].append(trace)
        except Exception as err:
            logger.warning(f"{self.agent_name} è®°å½•LLMè½¨è¿¹å¤±è´¥: {err}")
    
    def _filter_toc_content(self, text: str) -> str:
        """è¿‡æ»¤æ‰ç›®å½•é¡µå†…å®¹"""
        if not text:
            return text
        
        # ç›®å½•é¡µå…³é”®è¯
        toc_keywords = [
            'table of contents',
            'ç›®å½•',
            'contents',
            'ç›® å½•',
            'paper 1 exemplar',
            'paper 2 exemplar',
            'level 5',
            'level 6'
        ]
        
        # æŒ‰è¡Œåˆ†å‰²æ–‡æœ¬
        lines = text.split('\n')
        filtered_lines = []
        skip_mode = False
        
        for line in lines:
            line_lower = line.lower().strip()
            
            # æ£€æŸ¥æ˜¯å¦æ˜¯ç›®å½•é¡µæ ‡é¢˜è¡Œ
            if any(keyword in line_lower for keyword in toc_keywords):
                skip_mode = True
                continue
            
            # å¦‚æœé‡åˆ°å®é™…å†…å®¹ï¼ˆåŒ…å«æ•°å­—ã€å…¬å¼ã€ä¸­æ–‡ç­‰ï¼‰ï¼Œåœæ­¢è·³è¿‡
            if skip_mode:
                # æ£€æŸ¥æ˜¯å¦æ˜¯å®é™…å†…å®¹è¡Œï¼ˆåŒ…å«æ•°å­—ã€ä¸­æ–‡ã€å…¬å¼ç¬¦å·ç­‰ï¼‰
                has_content = any([
                    any(c.isdigit() for c in line),  # åŒ…å«æ•°å­—
                    any('\u4e00' <= c <= '\u9fff' for c in line),  # åŒ…å«ä¸­æ–‡
                    any(c in line for c in ['=', '+', '-', 'Ã—', 'Ã·', '(', ')']),  # åŒ…å«å…¬å¼ç¬¦å·
                    len(line.strip()) > 20  # é•¿è¡Œå¯èƒ½æ˜¯å†…å®¹
                ])
                
                if has_content:
                    skip_mode = False
                    filtered_lines.append(line)
                # å¦åˆ™ç»§ç»­è·³è¿‡ï¼ˆå¯èƒ½æ˜¯ç›®å½•é¡¹ï¼‰
            else:
                filtered_lines.append(line)
        
        result = '\n'.join(filtered_lines).strip()
        
        # å¦‚æœè¿‡æ»¤åå†…å®¹å¤ªå°‘ï¼Œè¿”å›åŸå§‹æ–‡æœ¬
        if len(result) < len(text) * 0.3:
            logger.warning("è¿‡æ»¤åå†…å®¹è¿‡å°‘ï¼Œä½¿ç”¨åŸå§‹æ–‡æœ¬")
            return text
        
        return result
    
    def _simple_grading(self, compressed_criteria: List[Dict], quick_checks: Dict) -> List[Dict]:
        """ç®€åŒ–è¯„åˆ†ï¼ˆé™çº§æ–¹æ¡ˆï¼‰"""
        evaluations = []
        for criterion in compressed_criteria:
            cid = criterion['id']
            pts = criterion['pts']
            desc = criterion.get('desc', '')
            score_earned = pts * 0.8  # é»˜è®¤80%åˆ†æ•°
            
            evaluations.append({
                'criterion_id': cid,
                'score_earned': score_earned,
                'max_score': pts,
                'is_met': score_earned >= pts * 0.5,
                'satisfaction_level': 'éƒ¨åˆ†æ»¡è¶³',
                'justification': f"åŸºäºå¿«é€Ÿæ£€æŸ¥: {quick_checks.get(cid, 'é»˜è®¤è¯„åˆ†')}",
                'feedback': 'è¯·æŸ¥çœ‹è¯¦ç»†æ‰¹æ”¹ç»“æœ',
                'evidence': ['ç­”æ¡ˆå·²æ£€æŸ¥'],
                'student_work': quick_checks.get(cid, 'å·²æ£€æŸ¥å­¦ç”Ÿä½œç­”'),
                'matched_criterion': desc[:100] if desc else 'å·²è¯„ä¼°'
            })
        return evaluations
