#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
å¤šæ¨¡æ€æ‰¹æ”¹å·¥ä½œæµ - workflow_multimodal.py (é‡æ„ç‰ˆ)
ç‰¹æ€§ï¼š
1. æ·±åº¦åä½œçš„8ä¸ªAgentæ¶æ„
2. åŸºäºå­¦ç”Ÿçš„æ‰¹æ¬¡ç®¡ç†
3. Tokenæè‡´ä¼˜åŒ–ï¼ˆä¸€æ¬¡ç†è§£ï¼Œå¤šæ¬¡ä½¿ç”¨ï¼‰
4. å¹¶è¡Œå¤„ç†ç­–ç•¥
"""

import logging
from typing import Dict, Any
from datetime import datetime

from langgraph.graph import StateGraph, END
from langgraph.checkpoint.memory import MemorySaver

from .state import GradingState

# ğŸ†• å¯¼å…¥æ·±åº¦åä½œçš„Agent
from .agents.orchestrator_agent import OrchestratorAgent
from .agents.student_detection_agent import StudentDetectionAgent
from .agents.batch_planning_agent import BatchPlanningAgent
from .agents.rubric_master_agent import RubricMasterAgent
from .agents.question_context_agent import QuestionContextAgent
from .agents.grading_worker_agent import GradingWorkerAgent
from .agents.result_aggregator_agent import ResultAggregatorAgent
from .agents.class_analysis_agent import ClassAnalysisAgent

# ä¿ç•™ç°æœ‰çš„ç†è§£Agentï¼ˆä¸æ–°æ¶æ„å…¼å®¹ï¼‰
from .agents.multimodal_input_agent import MultiModalInputAgent
from .agents.question_understanding_agent import QuestionUnderstandingAgent
from .agents.answer_understanding_agent import AnswerUnderstandingAgent
from .agents.rubric_interpreter_agent import RubricInterpreterAgent

logger = logging.getLogger(__name__)


class MultiModalGradingWorkflow:
    """
    å¤šæ¨¡æ€æ‰¹æ”¹å·¥ä½œæµï¼ˆé‡æ„ç‰ˆï¼‰
    
    æ‰§è¡Œæµç¨‹ï¼ˆæ·±åº¦åä½œæ¶æ„ï¼‰ï¼š
    0. OrchestratorAgent - ä»»åŠ¡ç¼–æ’
    1. MultiModalInputAgent - å¤šæ¨¡æ€æ–‡ä»¶å¤„ç†
    2. å¹¶è¡Œæ‰§è¡Œï¼š
       - QuestionUnderstandingAgent - é¢˜ç›®ç†è§£
       - AnswerUnderstandingAgent - ç­”æ¡ˆç†è§£
       - RubricInterpreterAgent - è¯„åˆ†æ ‡å‡†è§£æ
    3. StudentDetectionAgent - å­¦ç”Ÿä¿¡æ¯è¯†åˆ«ï¼ˆå¯é€‰ï¼‰
    4. BatchPlanningAgent - æ‰¹æ¬¡è§„åˆ’
    5. RubricMasterAgent - è¯„åˆ†æ ‡å‡†ä¸»æ§ï¼ˆç”Ÿæˆå‹ç¼©åŒ…ï¼‰
    6. QuestionContextAgent - é¢˜ç›®ä¸Šä¸‹æ–‡ï¼ˆç”Ÿæˆå‹ç¼©åŒ…ï¼‰
    7. GradingWorkerAgent - æ‰¹æ”¹å·¥ä½œï¼ˆåŸºäºå‹ç¼©åŒ…ï¼‰
    8. ResultAggregatorAgent - ç»“æœèšåˆ
    9. ClassAnalysisAgent - ç­çº§åˆ†æï¼ˆå¯é€‰ï¼‰
    """
    
    def __init__(self):
        self.graph = None
        self.checkpointer = MemorySaver()
        self._build_workflow()
    
    def _build_workflow(self):
        """æ„å»ºå·¥ä½œæµå›¾"""
        logger.info("æ„å»ºæ·±åº¦åä½œå¤šæ¨¡æ€æ‰¹æ”¹å·¥ä½œæµ...")
        
        # åˆ›å»ºçŠ¶æ€å›¾
        workflow = StateGraph(GradingState)
        
        # æ·»åŠ AgentèŠ‚ç‚¹
        workflow.add_node("orchestrator", OrchestratorAgent())
        workflow.add_node("multimodal_input", MultiModalInputAgent())
        workflow.add_node("question_understanding", QuestionUnderstandingAgent())
        workflow.add_node("answer_understanding", AnswerUnderstandingAgent())
        workflow.add_node("rubric_interpretation", RubricInterpreterAgent())
        workflow.add_node("student_detection", StudentDetectionAgent())
        workflow.add_node("batch_planning", BatchPlanningAgent())
        workflow.add_node("rubric_master", RubricMasterAgent())
        workflow.add_node("question_context", QuestionContextAgent())
        workflow.add_node("grading_worker", GradingWorkerAgent())
        workflow.add_node("result_aggregator", ResultAggregatorAgent())
        workflow.add_node("class_analysis", ClassAnalysisAgent())
        workflow.add_node("finalize", self._finalize_results)
        
        # è®¾ç½®å…¥å£ç‚¹
        workflow.set_entry_point("orchestrator")
        
        # å®šä¹‰æ‰§è¡Œæµç¨‹
        # 0. ç¼–æ’ -> å¤šæ¨¡æ€è¾“å…¥
        workflow.add_edge("orchestrator", "multimodal_input")
        
        # 1. å¤šæ¨¡æ€è¾“å…¥ -> å¹¶è¡Œç†è§£
        workflow.add_edge("multimodal_input", "question_understanding")
        workflow.add_edge("multimodal_input", "answer_understanding")
        workflow.add_edge("multimodal_input", "rubric_interpretation")
        
        # 2. ç†è§£å®Œæˆ -> å­¦ç”Ÿè¯†åˆ«ï¼ˆæ³¨ï¼šLangGraphä¼šç­‰å¾…å¹¶è¡ŒèŠ‚ç‚¹å®Œæˆï¼‰
        workflow.add_edge("question_understanding", "student_detection")
        workflow.add_edge("answer_understanding", "student_detection")
        workflow.add_edge("rubric_interpretation", "student_detection")
        
        # 3. å­¦ç”Ÿè¯†åˆ« -> æ‰¹æ¬¡è§„åˆ’
        workflow.add_edge("student_detection", "batch_planning")
        
        # 4. æ‰¹æ¬¡è§„åˆ’ -> å¹¶è¡Œç”Ÿæˆå‹ç¼©åŒ…
        workflow.add_edge("batch_planning", "rubric_master")
        workflow.add_edge("batch_planning", "question_context")
        
        # 5. å‹ç¼©åŒ…ç”Ÿæˆå®Œæˆ -> æ‰¹æ”¹å·¥ä½œ
        workflow.add_edge("rubric_master", "grading_worker")
        workflow.add_edge("question_context", "grading_worker")
        
        # 6. æ‰¹æ”¹å®Œæˆ -> ç»“æœèšåˆ
        workflow.add_edge("grading_worker", "result_aggregator")
        
        # 7. ç»“æœèšåˆ -> ç­çº§åˆ†æ
        workflow.add_edge("result_aggregator", "class_analysis")
        
        # 8. ç­çº§åˆ†æ -> å®Œæˆ
        workflow.add_edge("class_analysis", "finalize")
        
        # 9. æœ€ç»ˆåŒ– -> ç»“æŸ
        workflow.add_edge("finalize", END)
        
        # ç¼–è¯‘å›¾
        self.graph = workflow.compile(checkpointer=self.checkpointer)
        
        logger.info("æ·±åº¦åä½œå¤šæ¨¡æ€æ‰¹æ”¹å·¥ä½œæµæ„å»ºå®Œæˆ")
    
    async def execute(self, initial_state: GradingState, progress_callback=None) -> GradingState:
        """
        æ‰§è¡Œå·¥ä½œæµ
        
        Args:
            initial_state: åˆå§‹çŠ¶æ€ï¼ˆåŒ…å«æ–‡ä»¶è·¯å¾„ç­‰ä¿¡æ¯ï¼‰
            
        Returns:
            æœ€ç»ˆçŠ¶æ€ï¼ˆåŒ…å«æ‰¹æ”¹ç»“æœï¼‰
        """
        logger.info(f"å¼€å§‹æ‰§è¡Œå¤šæ¨¡æ€æ‰¹æ”¹å·¥ä½œæµï¼Œä»»åŠ¡ID: {initial_state.get('task_id', 'unknown')}")
        logger.info(f"æ–‡ä»¶ä¿¡æ¯ - é¢˜ç›®æ–‡ä»¶: {len(initial_state.get('question_files', []))}, "
                   f"ç­”æ¡ˆæ–‡ä»¶: {len(initial_state.get('answer_files', []))}, "
                   f"æ‰¹æ”¹æ ‡å‡†æ–‡ä»¶: {len(initial_state.get('marking_files', []))}")
        
        try:
            # åˆå§‹åŒ–å¿…è¦å­—æ®µ
            if 'errors' not in initial_state:
                initial_state['errors'] = []
            if 'step_results' not in initial_state:
                initial_state['step_results'] = {}
            if 'warnings' not in initial_state:
                initial_state['warnings'] = []
            if 'question_multimodal_files' not in initial_state:
                initial_state['question_multimodal_files'] = []
            if 'answer_multimodal_files' not in initial_state:
                initial_state['answer_multimodal_files'] = []
            if 'marking_multimodal_files' not in initial_state:
                initial_state['marking_multimodal_files'] = []
            if 'criteria_evaluations' not in initial_state:
                initial_state['criteria_evaluations'] = []
            # ğŸ†• æ·±åº¦åä½œç›¸å…³å­—æ®µ
            if 'students_info' not in initial_state:
                initial_state['students_info'] = []
            if 'batches_info' not in initial_state:
                initial_state['batches_info'] = []
            if 'batch_rubric_packages' not in initial_state:
                initial_state['batch_rubric_packages'] = {}
            if 'question_context_packages' not in initial_state:
                initial_state['question_context_packages'] = {}
            if 'grading_results' not in initial_state:
                initial_state['grading_results'] = []
            if 'student_reports' not in initial_state:
                initial_state['student_reports'] = []
            if 'class_analysis' not in initial_state:
                initial_state['class_analysis'] = {}
            
            # è®¾ç½®åˆå§‹çŠ¶æ€
            initial_state['current_step'] = "åˆå§‹åŒ–"
            initial_state['progress_percentage'] = 0.0
            initial_state['completion_status'] = "in_progress"
            
            # æ‰§è¡Œå·¥ä½œæµ
            config = {"configurable": {"thread_id": initial_state.get('task_id', 'default')}}
            
            final_state = None
            async for state in self.graph.astream(initial_state, config):
                # æ›´æ–°çŠ¶æ€
                if state:
                    final_state = state
                    # è·å–å½“å‰èŠ‚ç‚¹åç§°
                    current_node = list(state.keys())[0] if state else "unknown"
                    logger.info(f"[å½“å‰èŠ‚ç‚¹] {current_node}")
                    
                    # è®°å½•èŠ‚ç‚¹æ‰§è¡Œçš„è¯¦ç»†ä¿¡æ¯
                    state_value = list(state.values())[0] if isinstance(state, dict) and state else state
                    if isinstance(state_value, dict):
                        progress = state_value.get('progress_percentage', 0)
                        current_step = state_value.get('current_step', 'å¤„ç†ä¸­...')
                        logger.info(f"   æ­¥éª¤: {current_step}, è¿›åº¦: {progress:.1f}%")
                    
                    # è°ƒç”¨è¿›åº¦å›è°ƒ
                    if progress_callback:
                        try:
                            # è·å–å®é™…çš„çŠ¶æ€å€¼
                            state_value = list(state.values())[0] if isinstance(state, dict) and state else state
                            if isinstance(state_value, dict):
                                # ç¡®ä¿è¿›åº¦å›è°ƒè¢«è°ƒç”¨
                                progress_callback(state_value, current_node)
                                logger.debug(f"è¿›åº¦å›è°ƒå·²è°ƒç”¨: {current_node}, è¿›åº¦: {state_value.get('progress_percentage', 0)}%")
                        except Exception as e:
                            logger.warning(f"è¿›åº¦å›è°ƒå¤±è´¥: {e}", exc_info=True)
            
            # æ ‡è®°å®Œæˆ
            if final_state:
                # è·å–æœ€ç»ˆçŠ¶æ€å€¼
                final_result = list(final_state.values())[0] if final_state else initial_state
                final_result['completion_status'] = "completed"
                final_result['completed_at'] = str(datetime.now())
                final_result['progress_percentage'] = 100.0
                
                logger.info(f"å·¥ä½œæµæ‰§è¡Œå®Œæˆï¼Œæ€»åˆ†: {final_result.get('total_score', 0)}")
                return final_result
            else:
                raise Exception("å·¥ä½œæµæ‰§è¡Œå¤±è´¥ï¼Œæœªè¿”å›æœ€ç»ˆçŠ¶æ€")
                
        except Exception as e:
            error_msg = f"å·¥ä½œæµæ‰§è¡Œå¤±è´¥: {str(e)}"
            logger.error(error_msg)
            
            initial_state['completion_status'] = "failed"
            initial_state['errors'].append({
                'step': 'workflow_execution',
                'error': error_msg,
                'timestamp': str(datetime.now())
            })
            
            return initial_state
    
    def _finalize_results(self, state: GradingState) -> GradingState:
        """
        æœ€ç»ˆåŒ–ç»“æœ
        
        Args:
            state: å½“å‰çŠ¶æ€
            
        Returns:
            æ›´æ–°åçš„çŠ¶æ€
        """
        logger.info("æœ€ç»ˆåŒ–æ‰¹æ”¹ç»“æœ...")
        
        try:
            state['current_step'] = "æœ€ç»ˆåŒ–ç»“æœ"
            state['progress_percentage'] = 100.0
            state['completion_status'] = "completed"
            state['completed_at'] = str(datetime.now())
            
            # æå–æ‰¹æ”¹ç»“æœåˆ°æœ€ç»ˆè¾“å‡ºæ ¼å¼
            student_reports = state.get('student_reports', [])
            if student_reports:
                # å–ç¬¬ä¸€ä¸ªå­¦ç”Ÿçš„æŠ¥å‘Šï¼ˆå•å­¦ç”Ÿæ‰¹æ”¹ï¼‰
                first_report = student_reports[0]
                state['detailed_feedback'] = first_report.get('detailed_feedback', '')
                state['criteria_evaluations'] = first_report.get('evaluations', [])
                state['grade_level'] = first_report.get('grade_level', '')
                state['total_score'] = first_report.get('total_score', state.get('total_score', 0))
                
                # å¦‚æœdetailed_feedbackæ˜¯å­—ç¬¦ä¸²ï¼Œè½¬æ¢ä¸ºåˆ—è¡¨
                if isinstance(state['detailed_feedback'], str):
                    state['detailed_feedback'] = [state['detailed_feedback']] if state['detailed_feedback'] else []
            else:
                # å¦‚æœæ²¡æœ‰å­¦ç”ŸæŠ¥å‘Šï¼Œå°è¯•ä»grading_resultsç›´æ¥æå–
                logger.warning("æ²¡æœ‰å­¦ç”ŸæŠ¥å‘Šï¼Œå°è¯•ä»grading_resultsç›´æ¥æå–è¯„ä¼°ç»“æœ")
                grading_results = state.get('grading_results', [])
                if grading_results:
                    # åˆå¹¶æ‰€æœ‰æ‰¹æ¬¡çš„è¯„ä¼°ç»“æœ
                    all_evaluations = []
                    total_score = 0
                    for result in grading_results:
                        evaluations = result.get('evaluations', [])
                        all_evaluations.extend(evaluations)
                        total_score += result.get('total_score', 0)
                    
                    if all_evaluations:
                        state['criteria_evaluations'] = all_evaluations
                        state['total_score'] = total_score / len(grading_results) if grading_results else 0
                        logger.info(f"ä»grading_resultsæå–äº† {len(all_evaluations)} ä¸ªè¯„ä¼°ç»“æœ")
                    else:
                        logger.warning("grading_resultsä¸­æ²¡æœ‰è¯„ä¼°ç»“æœ")
                else:
                    logger.warning("grading_resultsä¸ºç©ºï¼Œæ— æ³•æå–è¯„ä¼°ç»“æœ")
            
            # æ·»åŠ æ‰¹æ”¹æ ‡å‡†è§£æç»“æœåˆ°æœ€ç»ˆè¾“å‡º
            rubric_understanding = state.get('rubric_understanding')
            criteria_evaluations = state.get('criteria_evaluations', [])
            
            # å¦‚æœrubric_understandingå­˜åœ¨ä¸”ä¸ä¸ºNoneï¼Œä½¿ç”¨å®ƒ
            # å¦åˆ™ï¼Œä»criteria_evaluationsä¸­æå–ä¿¡æ¯
            if rubric_understanding is not None:
                criteria_count = len(rubric_understanding.get('criteria', []))
                # å¦‚æœåªæœ‰1ä¸ªé»˜è®¤è¯„åˆ†ç‚¹ï¼Œå°è¯•ä»criteria_evaluationsä¸­æå–
                if criteria_count == 1 and rubric_understanding.get('criteria', [{}])[0].get('points', 0) == 100.0:
                    # ä»criteria_evaluationsä¸­æå–è¯„åˆ†ç‚¹ä¿¡æ¯
                    if criteria_evaluations:
                        # æŒ‰é¢˜ç›®åˆ†ç»„
                        questions = {}
                        for eval_item in criteria_evaluations:
                            criterion_id = eval_item.get('criterion_id', '')
                            question_id = criterion_id.split('_')[0] if '_' in criterion_id else 'UNKNOWN'
                            if question_id not in questions:
                                questions[question_id] = []
                            questions[question_id].append(eval_item)
                        
                        # æ„å»ºè¯„åˆ†ç‚¹åˆ—è¡¨
                        extracted_criteria = []
                        total_points = 0.0
                        for question_id, evals in sorted(questions.items()):
                            for eval_item in evals:
                                criterion_id = eval_item.get('criterion_id', '')
                                max_score = eval_item.get('max_score', 0)
                                total_points += max_score
                                
                                # ä»æ‰¹æ”¹ç»“æœä¸­æå–æ›´è¯¦ç»†çš„ä¿¡æ¯
                                criterion_dict = {
                                    'criterion_id': criterion_id,
                                    'question_id': question_id,
                                    'description': eval_item.get('matched_criterion', '')[:100] if eval_item.get('matched_criterion') else (eval_item.get('justification', '')[:100] if eval_item.get('justification') else 'å·²è¯„ä¼°'),
                                    'points': max_score,
                                    'evaluation_method': 'semantic',
                                    'keywords': None,
                                    'required_elements': None,
                                    'detailed_requirements': eval_item.get('justification', '')[:200] if eval_item.get('justification') else None,
                                    'standard_answer': None,  # æ— æ³•ä»æ‰¹æ”¹ç»“æœä¸­æå–æ ‡å‡†ç­”æ¡ˆ
                                    'scoring_criteria': {
                                        'full_credit': f'å¾—{max_score}åˆ†ï¼š{eval_item.get("matched_criterion", "ç¬¦åˆè¯„åˆ†æ ‡å‡†")}',
                                        'partial_credit': f'å¾—éƒ¨åˆ†åˆ†ï¼šéƒ¨åˆ†ç¬¦åˆè¯„åˆ†æ ‡å‡†',
                                        'no_credit': 'ä¸å¾—åˆ†ï¼šä¸ç¬¦åˆè¯„åˆ†æ ‡å‡†'
                                    } if eval_item.get('matched_criterion') else None,
                                    'alternative_methods': None,  # æ— æ³•ä»æ‰¹æ”¹ç»“æœä¸­æå–å¦ç±»è§£æ³•
                                    'common_mistakes': None
                                }
                                # ç§»é™¤Noneå€¼
                                criterion_dict = {k: v for k, v in criterion_dict.items() if v is not None}
                                extracted_criteria.append(criterion_dict)
                        
                        if extracted_criteria:
                            state['rubric_parsing_result'] = {
                                'rubric_id': f"EXTRACTED_FROM_EVALUATIONS",
                                'total_points': total_points,
                                'criteria_count': len(extracted_criteria),
                                'criteria': extracted_criteria,
                                'grading_rules': {},
                                'strictness_guidance': 'ä»æ‰¹æ”¹ç»“æœä¸­æå–'
                            }
                            logger.info(f"   ä»æ‰¹æ”¹ç»“æœä¸­æå–äº† {len(extracted_criteria)} ä¸ªè¯„åˆ†ç‚¹")
                        else:
                            # ä½¿ç”¨é»˜è®¤çš„
                            state['rubric_parsing_result'] = {
                                'rubric_id': rubric_understanding.get('rubric_id', 'N/A'),
                                'total_points': rubric_understanding.get('total_points', 0),
                                'criteria_count': criteria_count,
                                'criteria': [
                                    {
                                        'criterion_id': c.get('criterion_id', 'N/A'),
                                        'description': c.get('description', 'N/A'),
                                        'points': c.get('points', 0),
                                        'evaluation_method': c.get('evaluation_method', 'N/A'),
                                        'keywords': c.get('keywords', []),
                                        'required_elements': c.get('required_elements', [])
                                    }
                                    for c in rubric_understanding.get('criteria', [])
                                ],
                                'grading_rules': rubric_understanding.get('grading_rules', {}),
                                'strictness_guidance': rubric_understanding.get('strictness_guidance')
                            }
                else:
                    # ä½¿ç”¨rubric_understanding
                    state['rubric_parsing_result'] = {
                        'rubric_id': rubric_understanding.get('rubric_id', 'N/A'),
                        'total_points': rubric_understanding.get('total_points', 0),
                        'criteria_count': criteria_count,
                        'criteria': [
                            {
                                'criterion_id': c.get('criterion_id', 'N/A'),
                                'question_id': c.get('question_id', ''),
                                'description': c.get('description', 'N/A'),
                                'detailed_requirements': c.get('detailed_requirements', ''),
                                'points': c.get('points', 0),
                                'standard_answer': c.get('standard_answer', ''),
                                'evaluation_method': c.get('evaluation_method', 'N/A'),
                                'scoring_criteria': c.get('scoring_criteria', {}),
                                'alternative_methods': c.get('alternative_methods', []),
                                'keywords': c.get('keywords', []),
                                'required_elements': c.get('required_elements', []),
                                'common_mistakes': c.get('common_mistakes', [])
                            }
                            for c in rubric_understanding.get('criteria', [])
                        ],
                        'grading_rules': rubric_understanding.get('grading_rules', {}),
                        'strictness_guidance': rubric_understanding.get('strictness_guidance')
                    }
                logger.info(f"   æ‰¹æ”¹æ ‡å‡†è§£æç»“æœå·²æ·»åŠ åˆ°è¾“å‡º")
            else:
                # å¦‚æœæ²¡æœ‰rubric_understandingï¼Œä»criteria_evaluationsä¸­æå–
                if criteria_evaluations:
                    questions = {}
                    for eval_item in criteria_evaluations:
                        criterion_id = eval_item.get('criterion_id', '')
                        question_id = criterion_id.split('_')[0] if '_' in criterion_id else 'UNKNOWN'
                        if question_id not in questions:
                            questions[question_id] = []
                        questions[question_id].append(eval_item)
                    
                    extracted_criteria = []
                    total_points = 0.0
                    for question_id, evals in sorted(questions.items()):
                        for eval_item in evals:
                            criterion_id = eval_item.get('criterion_id', '')
                            max_score = eval_item.get('max_score', 0)
                            total_points += max_score
                            
                            extracted_criteria.append({
                                'criterion_id': criterion_id,
                                'description': eval_item.get('justification', '')[:100] if eval_item.get('justification') else 'å·²è¯„ä¼°',
                                'points': max_score,
                                'evaluation_method': 'semantic',
                                'keywords': None,
                                'required_elements': None,
                                'question_id': question_id
                            })
                    
                    if extracted_criteria:
                        state['rubric_parsing_result'] = {
                            'rubric_id': f"EXTRACTED_FROM_EVALUATIONS",
                            'total_points': total_points,
                            'criteria_count': len(extracted_criteria),
                            'criteria': extracted_criteria,
                            'grading_rules': {},
                            'strictness_guidance': 'ä»æ‰¹æ”¹ç»“æœä¸­æå–'
                        }
                        logger.info(f"   ä»æ‰¹æ”¹ç»“æœä¸­æå–äº† {len(extracted_criteria)} ä¸ªè¯„åˆ†ç‚¹")
            
            # æ·»åŠ Agentåä½œè¿‡ç¨‹ä¿¡æ¯
            state['agent_collaboration'] = {
                'rubric_interpreter': {
                    'status': 'completed',
                    'criteria_extracted': len(state.get('rubric_parsing_result', {}).get('criteria', [])) if 'rubric_parsing_result' in state else (len(rubric_understanding.get('criteria', [])) if rubric_understanding else 0),
                    'total_points': state.get('rubric_parsing_result', {}).get('total_points', 0) if 'rubric_parsing_result' in state else (rubric_understanding.get('total_points', 0) if rubric_understanding else 0)
                },
                'question_understanding': {
                    'status': 'completed' if state.get('question_understanding') else 'pending'
                },
                'answer_understanding': {
                    'status': 'completed' if state.get('answer_understanding') else 'pending'
                },
                'grading_worker': {
                    'status': 'completed',
                    'students_graded': len(student_reports),
                    'evaluations_count': len(criteria_evaluations)
                }
            }
            
            logger.info(f"   Agentåä½œä¿¡æ¯å·²æ·»åŠ åˆ°è¾“å‡º")
            
            # ç”Ÿæˆæ‘˜è¦
            summary = state.get('summary', {})
            total_score = state.get('total_score', 0)
            
            logger.info(f"æ‰¹æ”¹å®Œæˆ")
            logger.info(f"   æ€»åˆ†: {total_score}")
            logger.info(f"   å­¦ç”Ÿæ•°: {summary.get('total_students', 0)}")
            logger.info(f"   å¹³å‡åˆ†: {summary.get('average_score', 0):.1f}")
            logger.info(f"   è¯¦ç»†åé¦ˆæ•°é‡: {len(state.get('detailed_feedback', []))}")
            logger.info(f"   è¯„åˆ†ç‚¹æ•°é‡: {len(state.get('criteria_evaluations', []))}")
            
            return state
            
        except Exception as e:
            logger.error(f"æœ€ç»ˆåŒ–å¤±è´¥: {e}")
            state['errors'].append({
                'step': 'finalize',
                'error': str(e),
                'timestamp': str(datetime.now())
            })
            return state


# åˆ›å»ºå…¨å±€å·¥ä½œæµå®ä¾‹
_workflow_instance = None

def get_multimodal_workflow() -> MultiModalGradingWorkflow:
    """è·å–å¤šæ¨¡æ€å·¥ä½œæµå®ä¾‹ï¼ˆå•ä¾‹æ¨¡å¼ï¼‰"""
    global _workflow_instance
    if _workflow_instance is None:
        _workflow_instance = MultiModalGradingWorkflow()
    return _workflow_instance


async def run_multimodal_grading(
    task_id: str,
    user_id: str,
    question_files: list,
    answer_files: list,
    marking_files: list,
    strictness_level: str = "ä¸­ç­‰",
    language: str = "zh",
    target_questions: list | None = None,
    scope_description: str | None = "",
    scope_warnings: list | None = None,
    progress_callback=None,
    streaming_callback=None
) -> Dict[str, Any]:
    """
    è¿è¡Œå¤šæ¨¡æ€æ‰¹æ”¹å·¥ä½œæµï¼ˆä¾¿æ·å‡½æ•°ï¼‰

    Args:
        task_id: ä»»åŠ¡ID
        user_id: ç”¨æˆ·ID
        question_files: é¢˜ç›®æ–‡ä»¶è·¯å¾„åˆ—è¡¨
        answer_files: ç­”æ¡ˆæ–‡ä»¶è·¯å¾„åˆ—è¡¨
        marking_files: è¯„åˆ†æ ‡å‡†æ–‡ä»¶è·¯å¾„åˆ—è¡¨
        strictness_level: ä¸¥æ ¼ç¨‹åº¦
        language: è¯­è¨€
        progress_callback: è¿›åº¦å›è°ƒå‡½æ•°
        streaming_callback: æµå¼å†…å®¹å›è°ƒå‡½æ•°

    Returns:
        æ‰¹æ”¹ç»“æœå­—å…¸
    """
    # åˆ›å»ºåˆå§‹çŠ¶æ€
    initial_state = GradingState(
        task_id=task_id,
        user_id=user_id,
        assignment_id=f"assignment_{task_id}",
        timestamp=datetime.now(),
        question_files=question_files,
        answer_files=answer_files,
        marking_files=marking_files,
        images=[],
        strictness_level=strictness_level,
        language=language,
        mode="efficient",
        target_questions=target_questions or [],
        scope_description=scope_description or "",
        scope_warnings=scope_warnings or [],
        streaming_callback=streaming_callback,  # æ·»åŠ æµå¼å›è°ƒ
        # åˆå§‹åŒ–å¿…è¦å­—æ®µ
        mm_tokens=[],
        student_info={},
        ocr_results={},
        image_regions={},
        preprocessed_images={},
        rubric_text="",
        rubric_struct={},
        rubric_data={},
        scoring_criteria=[],
        questions=[],
        batches=[],
        evaluations=[],
        scoring_results={},
        detailed_feedback=[],
        annotations=[],
        coordinate_annotations=[],
        error_regions=[],
        cropped_regions=[],
        knowledge_points=[],
        error_analysis={},
        learning_suggestions=[],
        difficulty_assessment={},
        total_score=0.0,
        section_scores={},
        student_evaluation={},
        class_evaluation={},
        export_payload={},
        final_report={},
        export_data={},
        visualization_data={},
        current_step="",
        progress_percentage=0.0,
        completion_status="pending",
        completed_at="",
        errors=[],
        step_results={},
        final_score=0.0,
        grade_level="",
        warnings=[],
        processing_time=0.0,
        model_versions={},
        quality_metrics={},
        student_alias_map={},
        graded_questions=[],
        skipped_questions=[]
        # å¤šæ¨¡æ€å­—æ®µ
        question_multimodal_files=[],
        answer_multimodal_files=[],
        marking_multimodal_files=[],
        question_understanding=None,
        answer_understanding=None,
        rubric_understanding=None,
        criteria_evaluations=[]
    )
    
    # è·å–å·¥ä½œæµå®ä¾‹å¹¶æ‰§è¡Œ
    workflow = get_multimodal_workflow()
    final_state = await workflow.execute(initial_state, progress_callback=progress_callback)
    
    # è¿”å›ç»“æœï¼ˆåŒ…å«æ‰€æœ‰é‡è¦å­—æ®µï¼‰
    result = {
        'task_id': final_state.get('task_id'),
        'status': final_state.get('completion_status'),
        'total_score': final_state.get('total_score'),
        'grade_level': final_state.get('grade_level'),
        'detailed_feedback': final_state.get('detailed_feedback'),
        'criteria_evaluations': final_state.get('criteria_evaluations', []),
        'errors': final_state.get('errors', []),
        'warnings': final_state.get('warnings', []),
        'student_reports': final_state.get('student_reports', []),
        'step_results': final_state.get('step_results', {})
    }
    
    # æ·»åŠ æ‰¹æ”¹æ ‡å‡†è§£æç»“æœï¼ˆå¿…é¡»åŒ…å«ï¼‰
    if 'rubric_parsing_result' in final_state and final_state['rubric_parsing_result']:
        result['rubric_parsing_result'] = final_state['rubric_parsing_result']
        logger.info(f"å·²æ·»åŠ rubric_parsing_resultåˆ°ç»“æœ")
    else:
        logger.warning("final_stateä¸­æ²¡æœ‰rubric_parsing_resultï¼Œå°è¯•ä»rubric_understandingæ„å»º")
        # å¦‚æœrubric_parsing_resultä¸å­˜åœ¨ï¼Œå°è¯•ä»rubric_understandingæ„å»º
        rubric_understanding = final_state.get('rubric_understanding')
        if rubric_understanding:
            result['rubric_parsing_result'] = {
                'rubric_id': rubric_understanding.get('rubric_id', 'unknown'),
                'total_points': rubric_understanding.get('total_points', 0),
                'criteria_count': len(rubric_understanding.get('criteria', [])),
                'criteria': rubric_understanding.get('criteria', [])
            }
            logger.info(f"ä»rubric_understandingæ„å»ºäº†rubric_parsing_resultï¼ŒåŒ…å« {len(rubric_understanding.get('criteria', []))} ä¸ªè¯„åˆ†ç‚¹")
    
    # æ·»åŠ Agentåä½œè¿‡ç¨‹ä¿¡æ¯ï¼ˆå¿…é¡»åŒ…å«ï¼‰
    if 'agent_collaboration' in final_state:
        result['agent_collaboration'] = final_state['agent_collaboration']
        logger.info(f"å·²æ·»åŠ agent_collaborationåˆ°ç»“æœ")
    else:
        logger.warning("final_stateä¸­æ²¡æœ‰agent_collaboration")
    
    # è°ƒè¯•ï¼šæ‰“å°final_stateçš„é”®
    logger.info(f"final_stateåŒ…å«çš„é”®: {list(final_state.keys())[:20]}...")
    
    # æ·»åŠ æ‰¹æ”¹æ ‡å‡†ç†è§£ï¼ˆåŸå§‹æ•°æ®ï¼‰
    if 'rubric_understanding' in final_state and final_state.get('rubric_understanding'):
        rubric_understanding = final_state['rubric_understanding']
        # è½¬æ¢ä¸ºå¯åºåˆ—åŒ–çš„æ ¼å¼
        if isinstance(rubric_understanding, dict):
            result['rubric_understanding'] = {
                'rubric_id': rubric_understanding.get('rubric_id'),
                'total_points': rubric_understanding.get('total_points'),
                'criteria_count': len(rubric_understanding.get('criteria', [])),
                'criteria': [
                    {
                        'criterion_id': c.get('criterion_id'),
                        'description': c.get('description'),
                        'points': c.get('points'),
                        'evaluation_method': c.get('evaluation_method'),
                        'keywords': c.get('keywords'),
                        'required_elements': c.get('required_elements')
                    }
                    for c in rubric_understanding.get('criteria', [])
                ]
            }
    
    return result
