"""æ‰¹é‡æäº¤ API è·¯ç”± - æ”¯æŒå¤šå­¦ç”Ÿåˆå·ä¸Šä¼ """

import uuid
import logging
import tempfile
import json
import asyncio
from pathlib import Path
from typing import List, Optional, Dict, Any

from fastapi import APIRouter, HTTPException, UploadFile, File, Form, WebSocket, WebSocketDisconnect
from pydantic import BaseModel, Field
import fitz
from PIL import Image
from io import BytesIO
import os

from src.models.enums import SubmissionStatus
from src.services.student_identification import StudentIdentificationService
from src.services.rubric_parser import RubricParserService
from src.services.strict_grading import StrictGradingService
from src.services.cached_grading import CachedGradingService

# LangGraph æ™ºèƒ½ä½“ï¼ˆè‡ªæˆ‘ä¿®æ­£æ‰¹æ”¹ï¼‰
from src.agents.grading_agent import GradingAgent
from src.services.gemini_reasoning import GeminiReasoningClient

# è‡ªæˆ‘æˆé•¿ç³»ç»Ÿç»„ä»¶
from src.services.exemplar_memory import ExemplarMemory
from src.services.prompt_assembler import PromptAssembler
from src.services.calibration import CalibrationService
from src.services.grading_logger import GradingLogger, get_grading_logger
from src.models.grading_log import GradingLog


logger = logging.getLogger(__name__)
router = APIRouter(prefix="/batch", tags=["æ‰¹é‡æäº¤"])

# å­˜å‚¨æ´»è·ƒçš„ WebSocket è¿æ¥
active_connections: Dict[str, List[WebSocket]] = {}


class BatchSubmissionResponse(BaseModel):
    """æ‰¹é‡æäº¤å“åº”"""
    batch_id: str = Field(..., description="æ‰¹æ¬¡ ID")
    status: SubmissionStatus = Field(..., description="çŠ¶æ€")
    total_pages: int = Field(..., description="æ€»é¡µæ•°")
    estimated_completion_time: int = Field(..., description="é¢„è®¡å®Œæˆæ—¶é—´ï¼ˆç§’ï¼‰")


class BatchStatusResponse(BaseModel):
    """æ‰¹é‡çŠ¶æ€æŸ¥è¯¢å“åº”"""
    batch_id: str
    exam_id: str
    status: str
    total_students: int = Field(0, description="è¯†åˆ«åˆ°çš„å­¦ç”Ÿæ•°")
    completed_students: int = Field(0, description="å·²å®Œæˆæ‰¹æ”¹çš„å­¦ç”Ÿæ•°")
    unidentified_pages: int = Field(0, description="æœªè¯†åˆ«å­¦ç”Ÿçš„é¡µæ•°")
    results: Optional[dict] = Field(None, description="æ‰¹æ”¹ç»“æœ")


def _pdf_to_images(pdf_path: str, dpi: int = 150) -> List[bytes]:
    """å°† PDF è½¬æ¢ä¸ºå›¾åƒåˆ—è¡¨"""
    pdf_doc = fitz.open(pdf_path)
    images = []
    
    for page_num in range(len(pdf_doc)):
        page = pdf_doc[page_num]
        mat = fitz.Matrix(dpi/72, dpi/72)
        pix = page.get_pixmap(matrix=mat)
        img = Image.frombytes("RGB", [pix.width, pix.height], pix.samples)
        
        img_bytes = BytesIO()
        img.save(img_bytes, format='PNG')
        images.append(img_bytes.getvalue())
    
    pdf_doc.close()
    return images


async def run_real_grading_workflow(
    batch_id: str, 
    rubric_images: List[bytes], 
    answer_images: List[bytes],
    api_key: str
):
    """
    çœŸå®æ‰¹æ”¹å·¥ä½œæµï¼Œé€šè¿‡ WebSocket æ¨é€è¿›åº¦
    
    æ­£ç¡®çš„å·¥ä½œæµæ­¥éª¤ï¼ˆæŒ‰è®¾è®¡æ–‡æ¡£ï¼‰ï¼š
    1. Intake - æ¥æ”¶æ–‡ä»¶
    2. Preprocess - é¢„å¤„ç†ï¼ˆå·²å®Œæˆ PDF è½¬å›¾åƒï¼‰
    3. Rubric Parse - è§£æè¯„åˆ†æ ‡å‡†
    4. Grading - å›ºå®šåˆ†æ‰¹å¹¶è¡Œæ‰¹æ”¹ï¼ˆ10å¼ å›¾ç‰‡ä¸€æ‰¹ï¼Œä¸éœ€è¦å…ˆè¯†åˆ«å­¦ç”Ÿï¼‰
    5. Segment - æ‰¹æ”¹åå­¦ç”Ÿåˆ†å‰²ï¼ˆåŸºäºæ‰¹æ”¹ç»“æœæ™ºèƒ½åˆ¤æ–­å­¦ç”Ÿè¾¹ç•Œï¼‰
    6. Review - æ±‡æ€»å®¡æ ¸
    7. Export - å¯¼å‡ºç»“æœ
    """
    import asyncio
    
    # ç­‰å¾… WebSocket è¿æ¥å»ºç«‹
    await asyncio.sleep(2.0)
    
    logger.info(f"å¼€å§‹çœŸå®æ‰¹æ”¹å·¥ä½œæµ: batch_id={batch_id}, rubric_pages={len(rubric_images)}, answer_pages={len(answer_images)}")
    
    # å­˜å‚¨æœ€ç»ˆç»“æœ
    all_results = []
    
    try:
        # === Step 1: Intake ===
        await broadcast_progress(batch_id, {
            "type": "workflow_update",
            "nodeId": "intake",
            "status": "running",
            "message": "æ¥æ”¶æ–‡ä»¶ä¸­..."
        })
        await asyncio.sleep(0.3)
        await broadcast_progress(batch_id, {
            "type": "workflow_update",
            "nodeId": "intake",
            "status": "completed",
            "message": f"æ¥æ”¶å®Œæˆï¼šè¯„åˆ†æ ‡å‡† {len(rubric_images)} é¡µï¼Œå­¦ç”Ÿä½œç­” {len(answer_images)} é¡µ"
        })
        
        # === Step 2: Preprocess (å·²å®Œæˆ) ===
        await broadcast_progress(batch_id, {
            "type": "workflow_update",
            "nodeId": "preprocess",
            "status": "running",
            "message": "æ­£åœ¨é¢„å¤„ç†å›¾åƒ..."
        })
        await asyncio.sleep(0.3)
        await broadcast_progress(batch_id, {
            "type": "workflow_update",
            "nodeId": "preprocess",
            "status": "completed",
            "message": "é¢„å¤„ç†å®Œæˆ"
        })
        
        # === Step 3: Parse Rubric (è¯„åˆ†æ ‡å‡†è§£æ - å¯¹åº”å‰ç«¯ rubric_parse èŠ‚ç‚¹) ===
        await broadcast_progress(batch_id, {
            "type": "workflow_update",
            "nodeId": "rubric_parse",
            "status": "running",
            "message": "æ­£åœ¨è§£æè¯„åˆ†æ ‡å‡†..."
        })
        
        # åˆå§‹åŒ–è¯„åˆ†æ ‡å‡†è§£ææœåŠ¡
        rubric_parser = RubricParserService(api_key=api_key)
        parsed_rubric = await rubric_parser.parse_rubric(rubric_images)
        rubric_context = rubric_parser.format_rubric_context(parsed_rubric)
        
        # å‘é€ rubric_parsed äº‹ä»¶ï¼ˆå¯¹åº”è®¾è®¡æ–‡æ¡£ StreamEventï¼‰
        await broadcast_progress(batch_id, {
            "type": "rubric_parsed",
            "totalQuestions": parsed_rubric.total_questions,
            "totalScore": parsed_rubric.total_score
        })
        
        await broadcast_progress(batch_id, {
            "type": "workflow_update",
            "nodeId": "rubric_parse",
            "status": "completed",
            "message": f"è¯„åˆ†æ ‡å‡†è§£æå®Œæˆï¼š{parsed_rubric.total_questions} é“é¢˜ï¼Œæ»¡åˆ† {parsed_rubric.total_score} åˆ†"
        })
        
        # === Step 4: å›ºå®šåˆ†æ‰¹å¹¶è¡Œæ‰¹æ”¹ (å¯¹åº”å‰ç«¯ grading èŠ‚ç‚¹) ===
        await broadcast_progress(batch_id, {
            "type": "workflow_update",
            "nodeId": "grading",
            "status": "running",
            "message": f"å¼€å§‹å›ºå®šåˆ†æ‰¹æ‰¹æ”¹ï¼Œå…± {len(answer_images)} é¡µ..."
        })
        
        # åˆå§‹åŒ–è‡ªæˆ‘æˆé•¿ç³»ç»Ÿç»„ä»¶ï¼ˆä¼˜é›…é™çº§ï¼šæ•°æ®åº“ä¸å¯ç”¨æ—¶ä½¿ç”¨ Noneï¼‰
        exemplar_memory = None
        prompt_assembler = None
        calibration_service = None
        calibration_profile = None
        grading_logger = None
        
        try:
            # 1. åˆ¤ä¾‹è®°å¿†åº“ - æ£€ç´¢ç›¸ä¼¼æ‰¹æ”¹ç¤ºä¾‹ç”¨äº few-shot å­¦ä¹ 
            exemplar_memory = ExemplarMemory()
            
            # 2. åŠ¨æ€æç¤ºè¯æ‹¼è£…å™¨ - æ ¹æ®ä¸Šä¸‹æ–‡æ„å»ºæœ€ä¼˜æç¤ºè¯
            prompt_assembler = PromptAssembler()
            
            # 3. æ ¡å‡†æœåŠ¡ - åŠ è½½æ•™å¸ˆä¸ªæ€§åŒ–è¯„åˆ†é…ç½®
            calibration_service = CalibrationService()
            teacher_id = "default_teacher"
            calibration_profile = await calibration_service.get_or_create_profile(teacher_id)
            
            # 4. æ‰¹æ”¹æ—¥å¿—æœåŠ¡ - è®°å½•æ‰¹æ”¹è¿‡ç¨‹ç”¨äºåç»­åˆ†æ
            grading_logger = get_grading_logger()
            
            logger.info(f"è‡ªæˆ‘æˆé•¿ç»„ä»¶åˆå§‹åŒ–æˆåŠŸ: exemplar_memory={exemplar_memory is not None}, "
                        f"calibration_profile={calibration_profile.profile_id if calibration_profile else None}")
        except Exception as init_err:
            logger.warning(f"è‡ªæˆ‘æˆé•¿ç»„ä»¶åˆå§‹åŒ–å¤±è´¥ï¼ˆé™çº§æ¨¡å¼ï¼‰: {init_err}")
        
        # åˆå§‹åŒ– LangGraph æ‰¹æ”¹æ™ºèƒ½ä½“ï¼ˆæ”¯æŒè‡ªæˆ‘ä¿®æ­£ï¼‰
        reasoning_client = GeminiReasoningClient(api_key=api_key)
        # æ³¨æ„ï¼šæ¯ä¸ªå¹¶è¡Œæ‰¹æ¬¡ä¼šåˆ›å»ºè‡ªå·±çš„ GradingAgent å®ä¾‹ï¼Œé¿å…çŠ¶æ€å†²çª
        logger.info("ä½¿ç”¨ LangGraph GradingAgentï¼ˆè‡ªæˆ‘ä¿®æ­£æ¨¡å¼ï¼‰")
        
        # æŒ‰ 10 å¼ ä¸€ç»„åˆ†æ‰¹
        BATCH_SIZE = 10
        batches = [answer_images[i:i + BATCH_SIZE] for i in range(0, len(answer_images), BATCH_SIZE)]
        total_batches = len(batches)
        
        logger.info(f"åˆ†æ‰¹å®Œæˆï¼šå…± {total_batches} ä¸ªæ‰¹æ¬¡")
        
        # å­˜å‚¨æ‰€æœ‰é¡µé¢çš„æ‰¹æ”¹ç»“æœ
        all_page_results = []
        success_count = 0
        failure_count = 0
        
        # åˆ›å»ºæ‰¹æ¬¡ Agentï¼ˆç”¨äºå‰ç«¯æ˜¾ç¤ºï¼‰
        grading_agents = []
        for batch_idx in range(total_batches):
            agent_id = f"batch_{batch_idx}"
            grading_agents.append({
                "id": agent_id,
                "label": f"æ‰¹æ¬¡ {batch_idx + 1}",
                "status": "pending"
            })
        
        await broadcast_progress(batch_id, {
            "type": "parallel_agents_created",
            "parentNodeId": "grading",
            "agents": grading_agents
        })
        
        # å‘é€æ‰¹æ¬¡è¿›åº¦
        await broadcast_progress(batch_id, {
            "type": "batch_start",
            "batchIndex": 0,
            "totalBatches": total_batches
        })
        
        # å®šä¹‰å•ä¸ªæ‰¹æ¬¡çš„å¤„ç†å‡½æ•°
        async def process_single_batch(batch_idx: int, batch_images: list) -> dict:
            """å¤„ç†å•ä¸ªæ‰¹æ¬¡ï¼Œè¿”å›è¯¥æ‰¹æ¬¡çš„ç»“æœ"""
            agent_id = f"batch_{batch_idx}"
            batch_results = []
            batch_success = 0
            batch_failure = 0
            
            # æ¯ä¸ªæ‰¹æ¬¡åˆ›å»ºç‹¬ç«‹çš„ LangGraph æ‰¹æ”¹æ™ºèƒ½ä½“ï¼ˆé¿å…å¹¶å‘çŠ¶æ€å†²çªï¼‰
            batch_reasoning_client = GeminiReasoningClient(api_key=api_key)
            batch_grading_agent = GradingAgent(reasoning_client=batch_reasoning_client)
            
            # æ›´æ–°æ‰¹æ¬¡çŠ¶æ€ä¸ºè¿è¡Œä¸­
            await broadcast_progress(batch_id, {
                "type": "agent_update",
                "agentId": agent_id,
                "status": "running",
                "message": f"æ­£åœ¨æ‰¹æ”¹ç¬¬ {batch_idx + 1} æ‰¹ï¼ˆLangGraph è‡ªæˆ‘ä¿®æ­£æ¨¡å¼ï¼‰...",
                "logs": [f"å¼€å§‹å¤„ç†æ‰¹æ¬¡ {batch_idx + 1}ï¼Œå…± {len(batch_images)} é¡µï¼Œä½¿ç”¨ LangGraph Agent"]
            })
            
            try:
                # æ‰¹æ”¹å½“å‰æ‰¹æ¬¡çš„æ‰€æœ‰é¡µé¢
                for page_offset, page_image in enumerate(batch_images):
                    page_index = batch_idx * BATCH_SIZE + page_offset
                    
                    try:
                        # === è‡ªæˆ‘æˆé•¿ï¼šæ‰¹æ”¹å‰æ£€ç´¢ç›¸ä¼¼åˆ¤ä¾‹ ===
                        similar_exemplars = []
                        if exemplar_memory is not None:
                            import hashlib
                            page_hash = hashlib.md5(page_image).hexdigest()
                            
                            try:
                                similar_exemplars = await exemplar_memory.retrieve_similar(
                                    question_image_hash=page_hash,
                                    question_type="general",
                                    top_k=3,
                                    min_similarity=0.7
                                )
                                logger.debug(f"é¡µé¢ {page_index} æ‰¾åˆ° {len(similar_exemplars)} ä¸ªç›¸ä¼¼åˆ¤ä¾‹")
                            except Exception as ex:
                                logger.warning(f"åˆ¤ä¾‹æ£€ç´¢å¤±è´¥ï¼ˆç»§ç»­æ‰¹æ”¹ï¼‰: {ex}")
                        
                        # === è‡ªæˆ‘æˆé•¿ï¼šåŠ¨æ€æ‹¼è£…æç¤ºè¯ ===
                        if prompt_assembler is not None:
                            try:
                                assembled_prompt = prompt_assembler.assemble(
                                    question_type="general",
                                    rubric=rubric_context,
                                    exemplars=similar_exemplars,
                                    error_patterns=[],
                                    previous_confidence=None,
                                    calibration=calibration_profile
                                )
                                # åç»­å¯å°† assembled_prompt ä¼ é€’ç»™ GradingAgent
                            except Exception as ex:
                                logger.warning(f"æç¤ºè¯æ‹¼è£…å¤±è´¥ï¼ˆä½¿ç”¨é»˜è®¤ï¼‰: {ex}")
                        
                        # å°†å›¾åƒè½¬æ¢ä¸º base64ï¼ˆLangGraph Agent éœ€è¦ï¼‰
                        import base64
                        image_b64 = base64.b64encode(page_image).decode('utf-8')
                        
                        # ğŸš€ ä½¿ç”¨ LangGraph GradingAgent æ‰¹æ”¹ï¼ˆæ”¯æŒå¾ªç¯è‡ªæˆ‘ä¿®æ­£ï¼‰
                        thread_id = f"{batch_id}_page_{page_index}"
                        grading_state = await batch_grading_agent.run(
                            question_image=image_b64,
                            rubric=rubric_context,
                            max_score=parsed_rubric.total_score,
                            standard_answer=None,
                            thread_id=thread_id
                        )
                        
                        # ä» LangGraph çŠ¶æ€æå–ç»“æœ
                        final_score = grading_state.get("final_score", 0.0)
                        max_score = grading_state.get("max_score", parsed_rubric.total_score)
                        confidence = grading_state.get("confidence", 0.0)
                        feedback = grading_state.get("student_feedback", "")
                        revision_count = grading_state.get("revision_count", 0)
                        
                        # æ„é€ å…¼å®¹çš„ç»“æœå¯¹è±¡
                        class LangGraphResult:
                            def __init__(self, state, page_idx):
                                self.total_score = state.get("final_score", 0.0)
                                self.max_total_score = state.get("max_score", 0.0)
                                self.question_results = []
                                # åˆ›å»ºå•ä¸ªé¢˜ç›®ç»“æœ
                                class QuestionResult:
                                    def __init__(self, state, page_idx):
                                        self.question_id = f"q_{page_idx}"
                                        self.awarded_score = state.get("final_score", 0.0)
                                        self.max_score = state.get("max_score", 0.0)
                                        self.confidence = state.get("confidence", 0.0)
                                        self.overall_feedback = state.get("student_feedback", "")
                                        self.is_correct = state.get("final_score", 0) > 0
                                        self.scoring_point_results = []
                                self.question_results.append(QuestionResult(state, page_idx))
                        
                        result = LangGraphResult(grading_state, page_index)
                        
                        # === è‡ªæˆ‘æˆé•¿ï¼šè®°å½•æ‰¹æ”¹æ—¥å¿— ===
                        if grading_logger is not None:
                            try:
                                log_entry = GradingLog(
                                    submission_id=batch_id,
                                    question_id=f"q_{page_index}",
                                    extracted_answer="",
                                    extraction_confidence=confidence,
                                    evidence_snippets=[],
                                    normalized_answer=None,
                                    normalization_rules_applied=[],
                                    match_result=final_score > 0,
                                    match_failure_reason=None,
                                    score=final_score,
                                    max_score=max_score,
                                    confidence=confidence,
                                    reasoning_trace=grading_state.get("reasoning_trace", [])
                                )
                                await grading_logger.log_grading(log_entry)
                            except Exception as ex:
                                logger.warning(f"æ‰¹æ”¹æ—¥å¿—è®°å½•å¤±è´¥: {ex}")
                        
                        batch_results.append({
                            "page_index": page_index,
                            "result": result,
                            "question_ids": [f"q_{page_index}"],
                            "success": True,
                            "revision_count": revision_count  # è®°å½•è‡ªæˆ‘ä¿®æ­£æ¬¡æ•°
                        })
                        batch_success += 1
                        
                        # æ¨é€é¡µé¢å®Œæˆäº‹ä»¶
                        await broadcast_progress(batch_id, {
                            "type": "page_complete",
                            "pageIndex": page_index,
                            "success": True,
                            "score": final_score,
                            "maxScore": max_score,
                            "revisionCount": revision_count  # å‰ç«¯å¯æ˜¾ç¤ºä¿®æ­£æ¬¡æ•°
                        })
                        
                    except Exception as e:
                        logger.error(f"é¡µé¢ {page_index} æ‰¹æ”¹å¤±è´¥: {e}")
                        batch_results.append({
                            "page_index": page_index,
                            "result": None,
                            "question_ids": [],
                            "success": False,
                            "error": str(e)
                        })
                        batch_failure += 1
                
                # æ›´æ–°æ‰¹æ¬¡è¿›åº¦
                await broadcast_progress(batch_id, {
                    "type": "agent_update",
                    "agentId": agent_id,
                    "status": "completed",
                    "progress": 100,
                    "message": f"æ‰¹æ¬¡ {batch_idx + 1} å®Œæˆ",
                    "logs": [f"æ‰¹æ¬¡ {batch_idx + 1} å¤„ç†å®Œæˆ"]
                })
                
                # æ¨é€æ‰¹æ¬¡å®Œæˆäº‹ä»¶
                await broadcast_progress(batch_id, {
                    "type": "batch_complete",
                    "batchIndex": batch_idx,
                    "totalBatches": total_batches,
                    "successCount": batch_success,
                    "failureCount": batch_failure
                })
                
                return {
                    "batch_idx": batch_idx,
                    "results": batch_results,
                    "success_count": batch_success,
                    "failure_count": batch_failure,
                    "status": "completed"
                }
                
            except Exception as e:
                logger.error(f"æ‰¹æ¬¡ {batch_idx} å¤„ç†å¤±è´¥: {e}")
                await broadcast_progress(batch_id, {
                    "type": "agent_update",
                    "agentId": agent_id,
                    "status": "failed",
                    "message": f"æ‰¹æ¬¡å¤±è´¥: {str(e)}",
                    "logs": [f"é”™è¯¯: {str(e)}"]
                })
                return {
                    "batch_idx": batch_idx,
                    "results": batch_results,
                    "success_count": batch_success,
                    "failure_count": batch_failure,
                    "status": "failed",
                    "error": str(e)
                }
        
        # ğŸš€ å¹¶è¡Œå¤„ç†æ‰€æœ‰æ‰¹æ¬¡
        logger.info(f"å¼€å§‹å¹¶è¡Œå¤„ç† {total_batches} ä¸ªæ‰¹æ¬¡...")
        batch_tasks = [
            process_single_batch(idx, batch_images) 
            for idx, batch_images in enumerate(batches)
        ]
        batch_results_list = await asyncio.gather(*batch_tasks, return_exceptions=True)
        
        # æ±‡æ€»æ‰€æœ‰æ‰¹æ¬¡çš„ç»“æœ
        for batch_result in batch_results_list:
            if isinstance(batch_result, Exception):
                logger.error(f"æ‰¹æ¬¡å¤„ç†å¼‚å¸¸: {batch_result}")
                continue
            if isinstance(batch_result, dict):
                all_page_results.extend(batch_result.get("results", []))
                success_count += batch_result.get("success_count", 0)
                failure_count += batch_result.get("failure_count", 0)
        
        # æŒ‰é¡µé¢ç´¢å¼•æ’åºç»“æœ
        all_page_results.sort(key=lambda x: x.get("page_index", 0))
        
        await broadcast_progress(batch_id, {
            "type": "workflow_update",
            "nodeId": "grading",
            "status": "completed",
            "message": f"å›ºå®šåˆ†æ‰¹æ‰¹æ”¹å®Œæˆï¼šæˆåŠŸ {success_count} é¡µï¼Œå¤±è´¥ {failure_count} é¡µ"
        })
        
        # === Step 5: æ‰¹æ”¹åå­¦ç”Ÿåˆ†å‰² (å¯¹åº”å‰ç«¯ segment èŠ‚ç‚¹) ===
        await broadcast_progress(batch_id, {
            "type": "workflow_update",
            "nodeId": "segment",
            "status": "running",
            "message": "æ­£åœ¨åŸºäºæ‰¹æ”¹ç»“æœåˆ†æå­¦ç”Ÿè¾¹ç•Œ..."
        })
        
        # åŸºäºé¢˜ç›®åºåˆ—å¾ªç¯æ£€æµ‹å­¦ç”Ÿè¾¹ç•Œ
        boundaries = []
        current_start = 0
        last_max_question = 0
        student_count = 0
        
        for i, page_result in enumerate(all_page_results):
            question_ids = page_result.get("question_ids", [])
            if not question_ids:
                continue
            
            try:
                first_q = int(question_ids[0])
                
                # æ£€æµ‹å¾ªç¯ï¼šé¢˜ç›®ç¼–å·å›é€€åˆ°è¾ƒå°å€¼ï¼ˆå¦‚ä» 5 å›åˆ° 1ï¼‰ï¼Œè¯´æ˜æ¢äº†å­¦ç”Ÿ
                if first_q < last_max_question and first_q <= 2:
                    if i > current_start:
                        student_count += 1
                        boundaries.append({
                            "studentKey": f"å­¦ç”Ÿ{student_count}",
                            "startPage": current_start,
                            "endPage": i - 1,
                            "confidence": 0.7,  # åŸºäºå¾ªç¯æ£€æµ‹çš„ç½®ä¿¡åº¦è¾ƒä½
                            "needsConfirmation": True
                        })
                    current_start = i
                    last_max_question = first_q
                else:
                    for q_id in question_ids:
                        try:
                            q_num = int(q_id)
                            last_max_question = max(last_max_question, q_num)
                        except ValueError:
                            pass
            except (ValueError, IndexError):
                pass
        
        # æ·»åŠ æœ€åä¸€ä¸ªå­¦ç”Ÿ
        if current_start < len(all_page_results):
            student_count += 1
            boundaries.append({
                "studentKey": f"å­¦ç”Ÿ{student_count}",
                "startPage": current_start,
                "endPage": len(all_page_results) - 1,
                "confidence": 0.7,
                "needsConfirmation": True
            })
        
        num_students = len(boundaries)
        
        # å‘é€å­¦ç”Ÿè¾¹ç•Œæ£€æµ‹ç»“æœ
        await broadcast_progress(batch_id, {
            "type": "student_identified",
            "boundaries": boundaries
        })
        
        await broadcast_progress(batch_id, {
            "type": "workflow_update",
            "nodeId": "segment",
            "status": "completed",
            "message": f"å­¦ç”Ÿåˆ†å‰²å®Œæˆï¼šæ£€æµ‹åˆ° {num_students} åå­¦ç”Ÿï¼ˆåŸºäºé¢˜ç›®åºåˆ—å¾ªç¯ï¼‰"
        })
        
        # === Step 6: æŒ‰å­¦ç”Ÿèšåˆç»“æœ ===
        all_results = []
        for boundary in boundaries:
            student_pages = [
                pr for pr in all_page_results 
                if pr["success"] and boundary["startPage"] <= pr["page_index"] <= boundary["endPage"]
            ]
            
            total_score = sum(pr["result"].total_score for pr in student_pages if pr.get("result"))
            max_score = sum(pr["result"].max_total_score for pr in student_pages if pr.get("result"))
            
            # èšåˆæ‰€æœ‰é¡µé¢çš„é¢˜ç›®ç»“æœ
            student_question_results = []
            for pr in student_pages:
                if pr.get("result") and pr["result"].question_results:
                    # å°† Pydantic æ¨¡å‹è½¬æ¢ä¸ºå­—å…¸
                    q_results = [
                        q.dict() if hasattr(q, "dict") else q 
                        for q in pr["result"].question_results
                    ]
                    student_question_results.extend(q_results)
            
            # æŒ‰é¢˜ç›® ID æ’åº
            try:
                student_question_results.sort(key=lambda x: float(x.get("question_id", 0)) if isinstance(x, dict) else float(x.question_id))
            except:
                pass

            all_results.append({
                "studentName": boundary["studentKey"],
                "total_score": total_score,
                "max_score": max_score,
                "page_range": (boundary["startPage"], boundary["endPage"]),
                "questionResults": student_question_results
            })
        
        # === Step 7: Review ===
        await broadcast_progress(batch_id, {
            "type": "workflow_update",
            "nodeId": "review",
            "status": "running",
            "message": "æ­£åœ¨æ±‡æ€»å®¡æ ¸ç»“æœ..."
        })
        await asyncio.sleep(0.5)
        
        # è®¡ç®—ç»Ÿè®¡ä¿¡æ¯
        if all_results:
            scores = [r["total_score"] for r in all_results]
            avg_score = sum(scores) / len(scores) if scores else 0
            max_total = all_results[0]["max_score"] if all_results else 100
            
            await broadcast_progress(batch_id, {
                "type": "workflow_update",
                "nodeId": "review",
                "status": "completed",
                "message": f"å®¡æ ¸å®Œæˆï¼Œå¹³å‡åˆ† {avg_score:.1f}/{max_total}"
            })
        else:
            await broadcast_progress(batch_id, {
                "type": "workflow_update",
                "nodeId": "review",
                "status": "completed",
                "message": "å®¡æ ¸å®Œæˆ"
            })
        
        # === Step 8: Export ===
        await broadcast_progress(batch_id, {
            "type": "workflow_update",
            "nodeId": "export",
            "status": "running",
            "message": "æ­£åœ¨å¯¼å‡ºç»“æœ..."
        })
        await asyncio.sleep(0.3)
        await broadcast_progress(batch_id, {
            "type": "workflow_update",
            "nodeId": "export",
            "status": "completed",
            "message": "å¯¼å‡ºå®Œæˆ"
        })
        
        # å·¥ä½œæµå®Œæˆï¼Œå‘é€æœ€ç»ˆç»“æœ
        await broadcast_progress(batch_id, {
            "type": "workflow_completed",
            "message": f"æ‰¹æ”¹å·¥ä½œæµå®Œæˆï¼Œå…±å¤„ç† {num_students} åå­¦ç”Ÿ",
            "results": [
                {
                    "studentName": r["studentName"],
                    "score": r["total_score"],
                    "maxScore": r["max_score"],
                    "questionResults": [
                        {
                            "questionId": str(q.get("question_id")) if isinstance(q, dict) else str(q.question_id),
                            "score": q.get("awarded_score") if isinstance(q, dict) else q.awarded_score,
                            "maxScore": q.get("max_score") if isinstance(q, dict) else q.max_score,
                            "feedback": q.get("overall_feedback") if isinstance(q, dict) else q.overall_feedback,
                            "scoringPoints": [
                                {
                                    "description": sp.get("description") if isinstance(sp, dict) else sp.description,
                                    "score": sp.get("awarded_score") if isinstance(sp, dict) else sp.awarded_score,
                                    "maxScore": sp.get("max_score") if isinstance(sp, dict) else sp.max_score,
                                    "isCorrect": sp.get("is_correct") if isinstance(sp, dict) else sp.is_correct,
                                    "explanation": sp.get("explanation") if isinstance(sp, dict) else sp.explanation
                                }
                                for sp in (q.get("scoring_point_results") if isinstance(q, dict) else q.scoring_point_results) or []
                            ] if (q.get("scoring_point_results") if isinstance(q, dict) else q.scoring_point_results) else []
                        }
                        for q in r["questionResults"]
                    ]
                } for r in all_results
            ]
        })
        
    except Exception as e:
        logger.error(f"æ‰¹æ”¹å·¥ä½œæµå¤±è´¥: {str(e)}", exc_info=True)
        await broadcast_progress(batch_id, {
            "type": "workflow_error",
            "message": f"æ‰¹æ”¹å¤±è´¥: {str(e)}"
        })


@router.post("/submit", response_model=BatchSubmissionResponse)
async def submit_batch(
    exam_id: Optional[str] = Form(None, description="è€ƒè¯• ID"),
    rubrics: List[UploadFile] = File(..., description="è¯„åˆ†æ ‡å‡† PDF"),
    files: List[UploadFile] = File(..., description="å­¦ç”Ÿä½œç­” PDF"),
    api_key: Optional[str] = Form(None, description="Gemini API Key"),
    auto_identify: bool = Form(True, description="æ˜¯å¦è‡ªåŠ¨è¯†åˆ«å­¦ç”Ÿèº«ä»½")
):
    """
    æ‰¹é‡æäº¤è¯•å·å¹¶è¿›è¡Œæ‰¹æ”¹
    
    æ”¯æŒä¸Šä¼ åŒ…å«å¤šä¸ªå­¦ç”Ÿä½œä¸šçš„æ–‡ä»¶ï¼ˆå¦‚æ•´ç­æ‰«æçš„ PDFï¼‰ï¼Œ
    ç³»ç»Ÿä¼šè‡ªåŠ¨è¯†åˆ«æ¯é¡µæ‰€å±çš„å­¦ç”Ÿå¹¶åˆ†åˆ«æ‰¹æ”¹ã€‚
    
    Args:
        exam_id: è€ƒè¯• ID
        rubric_file: è¯„åˆ†æ ‡å‡† PDF æ–‡ä»¶
        answer_file: å­¦ç”Ÿä½œç­” PDF æ–‡ä»¶
        api_key: Gemini API Key
        auto_identify: æ˜¯å¦å¯ç”¨è‡ªåŠ¨å­¦ç”Ÿè¯†åˆ«ï¼ˆé»˜è®¤å¼€å¯ï¼‰
        
    Returns:
        BatchSubmissionResponse: æ‰¹æ¬¡ä¿¡æ¯
    """

    if not api_key:
        api_key = os.getenv("GEMINI_API_KEY")
        # Allow request even without API key for testing
        # if not api_key:
        #     raise HTTPException(status_code=400, detail="API Key not provided and GEMINI_API_KEY env var not set")

    if not exam_id:
        exam_id = str(uuid.uuid4())

    batch_id = str(uuid.uuid4())
    
    logger.info(
        f"æ”¶åˆ°æ‰¹é‡æäº¤: "
        f"batch_id={batch_id}, "
        f"exam_id={exam_id}, "
        f"auto_identify={auto_identify}"
    )
    
    temp_dir = None
    try:
        # åˆ›å»ºä¸´æ—¶ç›®å½•
        temp_dir = tempfile.mkdtemp()
        temp_path = Path(temp_dir)
        
        # ä¿å­˜ä¸Šä¼ çš„æ–‡ä»¶ (Taking the first file from the list for now)
        rubric_path = temp_path / "rubric.pdf"
        answer_path = temp_path / "answer.pdf"
        
        rubric_content = await rubrics[0].read()
        answer_content = await files[0].read()
        
        with open(rubric_path, "wb") as f:
            f.write(rubric_content)
        with open(answer_path, "wb") as f:
            f.write(answer_content)
        
        # è½¬æ¢ PDF ä¸ºå›¾åƒ
        logger.info(f"è½¬æ¢ PDF ä¸ºå›¾åƒ: batch_id={batch_id}")
        loop = asyncio.get_event_loop()
        rubric_images = await loop.run_in_executor(None, _pdf_to_images, str(rubric_path), 150)
        answer_images = await loop.run_in_executor(None, _pdf_to_images, str(answer_path), 150)
        
        total_pages = len(answer_images)
        
        logger.info(
            f"PDF è½¬æ¢å®Œæˆ: "
            f"batch_id={batch_id}, "
            f"rubric_pages={len(rubric_images)}, "
            f"answer_pages={total_pages}"
        )
        
        # ä¼°ç®—æ—¶é—´ï¼šæ¯é¡µ 30 ç§’
        estimated_time = total_pages * 30
        
        # å¯åŠ¨åå°çœŸå®æ‰¹æ”¹ä»»åŠ¡
        asyncio.create_task(run_real_grading_workflow(
            batch_id=batch_id,
            rubric_images=rubric_images,
            answer_images=answer_images,
            api_key=api_key or ""
        ))
        
        # è¿”å›å“åº”
        return BatchSubmissionResponse(
            batch_id=batch_id,
            status=SubmissionStatus.UPLOADED,
            total_pages=total_pages,
            estimated_completion_time=estimated_time
        )
        
    except Exception as e:
        logger.error(f"æ‰¹é‡æäº¤å¤±è´¥: {str(e)}", exc_info=True)
        raise HTTPException(status_code=500, detail=str(e))
    finally:
        # ä¸æ¸…ç†ä¸´æ—¶æ–‡ä»¶ï¼Œè®©åå°ä»»åŠ¡ä½¿ç”¨ï¼ˆåå°ä»»åŠ¡ä¼šæ¸…ç†ï¼‰
        pass


@router.get("/status/{batch_id}", response_model=BatchStatusResponse)
async def get_batch_status(batch_id: str):
    """
    æŸ¥è¯¢æ‰¹é‡æ‰¹æ”¹çŠ¶æ€
    
    Args:
        batch_id: æ‰¹æ¬¡ ID
        
    Returns:
        BatchStatusResponse: æ‰¹æ¬¡çŠ¶æ€å’Œè¿›åº¦
    """
    # TODO: ä» Temporal æŸ¥è¯¢å·¥ä½œæµçŠ¶æ€
    # handle = temporal_client.get_workflow_handle(f"batch_{batch_id}")
    # progress = await handle.query(BatchGradingWorkflow.get_progress)
    
    return BatchStatusResponse(
        batch_id=batch_id,
        exam_id="",
        status="processing",
        total_students=0,
        completed_students=0,
        unidentified_pages=0
    )


@router.get("/results/{batch_id}")
async def get_batch_results(batch_id: str):
    """
    è·å–æ‰¹é‡æ‰¹æ”¹ç»“æœ
    
    è¿”å›æ¯ä¸ªå­¦ç”Ÿçš„æ‰¹æ”¹ç»“æœæ±‡æ€»ã€‚
    
    Args:
        batch_id: æ‰¹æ¬¡ ID
        
    Returns:
        dict: åŒ…å«æ‰€æœ‰å­¦ç”Ÿæ‰¹æ”¹ç»“æœçš„å­—å…¸
    """
    # TODO: ä»æ•°æ®åº“æˆ– Temporal è·å–ç»“æœ
    return {
        "batch_id": batch_id,
        "students": []
    }


@router.post("/grade-sync")
async def grade_batch_sync(
    rubric_file: UploadFile = File(..., description="è¯„åˆ†æ ‡å‡† PDF"),
    answer_file: UploadFile = File(..., description="å­¦ç”Ÿä½œç­” PDF"),
    api_key: Optional[str] = Form(None, description="Gemini API Key"),
    total_score: int = Form(105, description="æ€»åˆ†"),
    total_questions: int = Form(19, description="æ€»é¢˜æ•°")
):
    """
    åŒæ­¥æ‰¹æ”¹ï¼ˆç”¨äºæµ‹è¯•ï¼‰
    
    å®Œæ•´çš„æ‰¹æ”¹æµç¨‹ï¼š
    1. è§£æè¯„åˆ†æ ‡å‡†
    2. è¯†åˆ«å­¦ç”Ÿè¾¹ç•Œ
    3. é€é¢˜æ‰¹æ”¹
    4. è¿”å›è¯¦ç»†ç»“æœ
    
    Args:
        rubric_file: è¯„åˆ†æ ‡å‡† PDF
        answer_file: å­¦ç”Ÿä½œç­” PDF
        api_key: Gemini API Key
        total_score: æ€»åˆ†
        total_questions: æ€»é¢˜æ•°
        
    Returns:
        dict: åŒ…å«æ‰€æœ‰å­¦ç”Ÿçš„æ‰¹æ”¹ç»“æœ
    """

    if not api_key:
        api_key = os.getenv("GEMINI_API_KEY")
        if not api_key:
            raise HTTPException(status_code=400, detail="API Key not provided and GEMINI_API_KEY env var not set")

    temp_dir = None
    try:
        # åˆ›å»ºä¸´æ—¶ç›®å½•
        temp_dir = tempfile.mkdtemp()
        temp_path = Path(temp_dir)
        
        # ä¿å­˜ä¸Šä¼ çš„æ–‡ä»¶
        rubric_path = temp_path / "rubric.pdf"
        answer_path = temp_path / "answer.pdf"
        
        rubric_content = await rubric_file.read()
        answer_content = await answer_file.read()
        
        with open(rubric_path, "wb") as f:
            f.write(rubric_content)
        with open(answer_path, "wb") as f:
            f.write(answer_content)
        
        # è½¬æ¢ PDF ä¸ºå›¾åƒ
        logger.info("è½¬æ¢ PDF ä¸ºå›¾åƒ...")
        rubric_images = _pdf_to_images(str(rubric_path), dpi=150)
        answer_images = _pdf_to_images(str(answer_path), dpi=150)
        
        logger.info(
            f"PDF è½¬æ¢å®Œæˆ: "
            f"rubric_pages={len(rubric_images)}, "
            f"answer_pages={len(answer_images)}"
        )
        
        # ===== æ­¥éª¤ 1: è§£æè¯„åˆ†æ ‡å‡† =====
        logger.info("è§£æè¯„åˆ†æ ‡å‡†...")
        rubric_parser = RubricParserService(api_key=api_key)
        parsed_rubric = await rubric_parser.parse_rubric(
            rubric_images,
            expected_total_score=total_score
        )
        
        logger.info(
            f"è¯„åˆ†æ ‡å‡†è§£æå®Œæˆ: "
            f"é¢˜ç›®æ•°={parsed_rubric.total_questions}, "
            f"æ€»åˆ†={parsed_rubric.total_score}"
        )
        
        rubric_context = rubric_parser.format_rubric_context(parsed_rubric)
        
        # ===== æ­¥éª¤ 2: è¯†åˆ«å­¦ç”Ÿè¾¹ç•Œ =====
        logger.info("è¯†åˆ«å­¦ç”Ÿè¾¹ç•Œ...")
        id_service = StudentIdentificationService(api_key=api_key)
        segmentation_result = await id_service.segment_batch_document(answer_images)
        student_groups = id_service.group_pages_by_student(segmentation_result)
        
        logger.info(f"è¯†åˆ«åˆ° {len(student_groups)} åå­¦ç”Ÿ")
        
        # ===== æ­¥éª¤ 3: æ‰¹æ”¹æ¯ä¸ªå­¦ç”Ÿ =====
        logger.info("å¼€å§‹æ‰¹æ”¹...")
        grading_service = StrictGradingService(api_key=api_key)
        all_results = []
        
        for idx, (student_key, page_indices) in enumerate(student_groups.items(), 1):
            logger.info(f"æ­£åœ¨æ‰¹æ”¹ {student_key}...")
            
            # æ¨é€è¿›åº¦
            await broadcast_progress(
                batch_id,
                {
                    "type": "progress",
                    "stage": "grading",
                    "current_student": idx,
                    "total_students": len(student_groups),
                    "student_name": student_key,
                    "percentage": int(idx / len(student_groups) * 100)
                }
            )
            
            # è·å–è¯¥å­¦ç”Ÿçš„é¡µé¢
            student_pages = [answer_images[i] for i in page_indices]
            
            # æ‰¹æ”¹
            result = await grading_service.grade_student(
                student_pages=student_pages,
                rubric=parsed_rubric,
                rubric_context=rubric_context,
                student_name=student_key
            )
            result.page_range = (min(page_indices), max(page_indices))
            all_results.append(result)
            
            logger.info(
                f"{student_key} æ‰¹æ”¹å®Œæˆ: "
                f"{result.total_score}/{result.max_total_score} åˆ†"
            )
        
        # ===== æ­¥éª¤ 4: æ ¼å¼åŒ–ç»“æœ =====
        # æ¨é€å®Œæˆé€šçŸ¥
        await broadcast_progress(
            batch_id,
            {
                "type": "progress",
                "stage": "formatting",
                "percentage": 95
            }
        )
        
        response_data = {
            "status": "completed",
            "total_students": len(all_results),
            "students": []
        }
        
        for result in all_results:
            student_data = {
                "name": result.student_name,
                "page_range": {
                    "start": result.page_range[0] + 1,
                    "end": result.page_range[1] + 1
                },
                "total_score": result.total_score,
                "max_score": result.max_total_score,
                "percentage": round(result.total_score / result.max_total_score * 100, 1),
                "questions_graded": len(result.question_results),
                "details": []
            }
            
            # æ·»åŠ æ¯é¢˜çš„è¯¦ç»†ç»“æœ
            for q_result in result.question_results:
                question_detail = {
                    "question_id": q_result.question_id,
                    "score": q_result.awarded_score,
                    "max_score": q_result.max_score,
                    "scoring_point_results": [
                        {
                            "description": sp.description,
                            "max_score": sp.max_score,
                            "awarded_score": sp.awarded_score,
                            "is_correct": sp.is_correct,
                            "explanation": sp.explanation
                        }
                        for sp in q_result.scoring_point_results
                    ],
                    "used_alternative_solution": q_result.used_alternative_solution,
                    "confidence": q_result.confidence
                }
                student_data["details"].append(question_detail)
            
            response_data["students"].append(student_data)
        
        # æ¨é€å®Œæˆé€šçŸ¥
        await broadcast_progress(
            batch_id,
            {
                "type": "completed",
                "percentage": 100,
                "total_students": len(all_results),
                "message": "æ‰¹æ”¹å®Œæˆ"
            }
        )
        
        return response_data
        
    except Exception as e:
        logger.error(f"æ‰¹æ”¹å¤±è´¥: {str(e)}", exc_info=True)
        raise HTTPException(status_code=500, detail=str(e))
    finally:
        # æ¸…ç†ä¸´æ—¶æ–‡ä»¶
        if temp_dir:
            import shutil
            try:
                shutil.rmtree(temp_dir)
            except Exception as e:
                logger.warning(f"æ¸…ç†ä¸´æ—¶æ–‡ä»¶å¤±è´¥: {str(e)}")



# ==================== WebSocket å®æ—¶æ¨é€ ====================

@router.websocket("/ws/{batch_id}")
async def websocket_batch_progress(websocket: WebSocket, batch_id: str):
    """
    WebSocket å®æ—¶æ¨é€æ‰¹æ”¹è¿›åº¦
    
    å®¢æˆ·ç«¯è¿æ¥åï¼Œç³»ç»Ÿä¼šå®æ—¶æ¨é€ä»¥ä¸‹äº‹ä»¶ï¼š
    - "progress": æ‰¹æ”¹è¿›åº¦æ›´æ–°
    - "completed": æ‰¹æ”¹å®Œæˆ
    - "error": æ‰¹æ”¹å‡ºé”™
    
    Args:
        batch_id: æ‰¹æ¬¡ ID
    """
    await websocket.accept()
    
    # æ³¨å†Œè¿æ¥
    if batch_id not in active_connections:
        active_connections[batch_id] = []
    active_connections[batch_id].append(websocket)
    
    logger.info(f"WebSocket è¿æ¥å·²å»ºç«‹: batch_id={batch_id}")
    
    try:
        # ä¿æŒè¿æ¥æ‰“å¼€ï¼Œç­‰å¾…å®¢æˆ·ç«¯æ¶ˆæ¯
        while True:
            data = await websocket.receive_text()
            # å¯ä»¥å¤„ç†å®¢æˆ·ç«¯å‘é€çš„å‘½ä»¤ï¼Œä¾‹å¦‚å–æ¶ˆæ‰¹æ”¹
            if data == "cancel":
                logger.info(f"æ”¶åˆ°å–æ¶ˆè¯·æ±‚: batch_id={batch_id}")
                await websocket.send_json({
                    "type": "info",
                    "message": "å–æ¶ˆè¯·æ±‚å·²æ”¶åˆ°ï¼Œæ­£åœ¨åœæ­¢æ‰¹æ”¹..."
                })
    
    except WebSocketDisconnect:
        logger.info(f"WebSocket è¿æ¥å·²æ–­å¼€: batch_id={batch_id}")
        active_connections[batch_id].remove(websocket)
        if not active_connections[batch_id]:
            del active_connections[batch_id]
    
    except Exception as e:
        logger.error(f"WebSocket é”™è¯¯: batch_id={batch_id}, error={str(e)}")
        if batch_id in active_connections and websocket in active_connections[batch_id]:
            active_connections[batch_id].remove(websocket)


async def broadcast_progress(batch_id: str, message: Dict[str, Any]):
    """
    å¹¿æ’­æ‰¹æ”¹è¿›åº¦åˆ°æ‰€æœ‰è¿æ¥çš„å®¢æˆ·ç«¯
    
    Args:
        batch_id: æ‰¹æ¬¡ ID
        message: æ¶ˆæ¯å†…å®¹
    """
    if batch_id not in active_connections:
        return
    
    disconnected = []
    for websocket in active_connections[batch_id]:
        try:
            await websocket.send_json(message)
        except Exception as e:
            logger.error(f"å‘é€æ¶ˆæ¯å¤±è´¥: {str(e)}")
            disconnected.append(websocket)
    
    # æ¸…ç†æ–­å¼€çš„è¿æ¥
    for websocket in disconnected:
        active_connections[batch_id].remove(websocket)



@router.post("/grade-cached")
async def grade_batch_cached(
    rubric_file: UploadFile = File(..., description="è¯„åˆ†æ ‡å‡† PDF"),
    answer_file: UploadFile = File(..., description="å­¦ç”Ÿä½œç­” PDF"),
    api_key: Optional[str] = Form(None, description="Gemini API Key"),
    total_score: int = Form(105, description="æ€»åˆ†"),
    total_questions: int = Form(19, description="æ€»é¢˜æ•°"),
    batch_id: Optional[str] = Form(None, description="æ‰¹æ¬¡ ID (å¯é€‰ï¼Œç”¨äºå‰ç«¯é¢„ç”Ÿæˆ)")
):
    """
    ä¼˜åŒ–çš„æ‰¹æ”¹ç«¯ç‚¹ - ä½¿ç”¨ Context Caching
    
    ç›¸æ¯” /grade-syncï¼Œæ­¤ç«¯ç‚¹ä½¿ç”¨ Gemini Context Caching æŠ€æœ¯ï¼š
    - è¯„åˆ†æ ‡å‡†åªè®¡è´¹ä¸€æ¬¡
    - åç»­å­¦ç”Ÿæ‰¹æ”¹å…è´¹ä½¿ç”¨ç¼“å­˜
    - èŠ‚çœçº¦ 25% çš„ Token æˆæœ¬
    
    é€‚ç”¨åœºæ™¯ï¼š
    - æ‰¹æ”¹å¤šä¸ªå­¦ç”Ÿï¼ˆ2+ ä¸ªå­¦ç”Ÿï¼‰
    - åŒä¸€ä»½è¯„åˆ†æ ‡å‡†
    - éœ€è¦é™ä½æˆæœ¬
    
    Args:
        rubric_file: è¯„åˆ†æ ‡å‡† PDF
        answer_file: å­¦ç”Ÿä½œç­” PDF
        api_key: Gemini API Key
        total_score: æ€»åˆ†
        total_questions: æ€»é¢˜æ•°
        
    Returns:
        dict: åŒ…å«æ‰€æœ‰å­¦ç”Ÿçš„æ‰¹æ”¹ç»“æœ + Token èŠ‚çœä¿¡æ¯
    """

    if not api_key:
        api_key = os.getenv("GEMINI_API_KEY")
        if not api_key:
            raise HTTPException(status_code=400, detail="API Key not provided and GEMINI_API_KEY env var not set")

    if not batch_id:
        batch_id = str(uuid.uuid4())
    temp_dir = None
    
    try:
        # åˆ›å»ºä¸´æ—¶ç›®å½•
        temp_dir = tempfile.mkdtemp()
        temp_path = Path(temp_dir)
        
        # ä¿å­˜ä¸Šä¼ çš„æ–‡ä»¶
        rubric_path = temp_path / "rubric.pdf"
        answer_path = temp_path / "answer.pdf"
        
        rubric_content = await rubric_file.read()
        answer_content = await answer_file.read()
        
        with open(rubric_path, "wb") as f:
            f.write(rubric_content)
        with open(answer_path, "wb") as f:
            f.write(answer_content)
        
        # è½¬æ¢ PDF ä¸ºå›¾åƒ
        logger.info(f"è½¬æ¢ PDF ä¸ºå›¾åƒ: batch_id={batch_id}")
        rubric_images = _pdf_to_images(str(rubric_path), dpi=150)
        answer_images = _pdf_to_images(str(answer_path), dpi=150)
        
        logger.info(
            f"PDF è½¬æ¢å®Œæˆ: "
            f"batch_id={batch_id}, "
            f"rubric_pages={len(rubric_images)}, "
            f"answer_pages={len(answer_images)}"
        )
        
        # æ¨é€è¿›åº¦
        await broadcast_progress(
            batch_id,
            {
                "type": "progress",
                "stage": "parsing_rubric",
                "percentage": 10
            }
        )
        
        # ===== æ­¥éª¤ 1: è§£æè¯„åˆ†æ ‡å‡† =====
        logger.info("è§£æè¯„åˆ†æ ‡å‡†...")
        rubric_parser = RubricParserService(api_key=api_key)
        parsed_rubric = await rubric_parser.parse_rubric(
            rubric_images,
            expected_total_score=total_score
        )
        
        logger.info(
            f"è¯„åˆ†æ ‡å‡†è§£æå®Œæˆ: "
            f"é¢˜ç›®æ•°={parsed_rubric.total_questions}, "
            f"æ€»åˆ†={parsed_rubric.total_score}"
        )
        
        rubric_context = rubric_parser.format_rubric_context(parsed_rubric)
        
        # æ¨é€è¿›åº¦
        await broadcast_progress(
            batch_id,
            {
                "type": "progress",
                "stage": "identifying_students",
                "percentage": 20
            }
        )
        
        # ===== æ­¥éª¤ 2: è¯†åˆ«å­¦ç”Ÿè¾¹ç•Œ =====
        logger.info("è¯†åˆ«å­¦ç”Ÿè¾¹ç•Œ...")
        id_service = StudentIdentificationService(api_key=api_key)
        segmentation_result = await id_service.segment_batch_document(answer_images)
        student_groups = id_service.group_pages_by_student(segmentation_result)
        
        logger.info(f"è¯†åˆ«åˆ° {len(student_groups)} åå­¦ç”Ÿ")
        
        # æ¨é€è¿›åº¦
        await broadcast_progress(
            batch_id,
            {
                "type": "progress",
                "stage": "creating_cache",
                "percentage": 30
            }
        )
        
        # ===== æ­¥éª¤ 3: åˆ›å»ºè¯„åˆ†æ ‡å‡†ç¼“å­˜ =====
        logger.info("åˆ›å»ºè¯„åˆ†æ ‡å‡†ç¼“å­˜...")
        cached_service = CachedGradingService(api_key=api_key)
        await cached_service.create_rubric_cache(parsed_rubric, rubric_context)
        
        cache_info = cached_service.get_cache_info()
        logger.info(f"ç¼“å­˜åˆ›å»ºæˆåŠŸ: {cache_info['cache_name']}")
        
        # æ¨é€è¿›åº¦
        await broadcast_progress(
            batch_id,
            {
                "type": "progress",
                "stage": "grading",
                "percentage": 40,
                "total_students": len(student_groups)
            }
        )
        
        # ===== æ­¥éª¤ 4: ä½¿ç”¨ç¼“å­˜æ‰¹æ”¹æ¯ä¸ªå­¦ç”Ÿ =====
        logger.info("å¼€å§‹æ‰¹æ”¹ï¼ˆä½¿ç”¨ç¼“å­˜ï¼‰...")
        all_results = []
        
        for idx, (student_key, page_indices) in enumerate(student_groups.items(), 1):
            logger.info(f"æ­£åœ¨æ‰¹æ”¹ {student_key}ï¼ˆä½¿ç”¨ç¼“å­˜ï¼‰...")
            
            # æ¨é€è¿›åº¦
            await broadcast_progress(
                batch_id,
                {
                    "type": "progress",
                    "stage": "grading",
                    "current_student": idx,
                    "total_students": len(student_groups),
                    "student_name": student_key,
                    "percentage": 40 + int(idx / len(student_groups) * 50)
                }
            )
            
            # è·å–è¯¥å­¦ç”Ÿçš„é¡µé¢
            student_pages = [answer_images[i] for i in page_indices]
            
            # ä½¿ç”¨ç¼“å­˜æ‰¹æ”¹
            result = await cached_service.grade_student_with_cache(
                student_pages=student_pages,
                student_name=student_key
            )
            result.page_range = (min(page_indices), max(page_indices))
            all_results.append(result)
            
            logger.info(
                f"{student_key} æ‰¹æ”¹å®Œæˆ: "
                f"{result.total_score}/{result.max_total_score} åˆ†"
            )
        
        # ===== æ­¥éª¤ 5: æ ¼å¼åŒ–ç»“æœ =====
        await broadcast_progress(
            batch_id,
            {
                "type": "progress",
                "stage": "formatting",
                "percentage": 95
            }
        )
        
        response_data = {
            "status": "completed",
            "total_students": len(all_results),
            "optimization": {
                "method": "context_caching",
                "cache_info": cache_info,
                "token_savings": {
                    "description": "ä½¿ç”¨ Context Caching èŠ‚çœçº¦ 25% Token",
                    "estimated_savings_per_student": "çº¦ 15,000-20,000 tokens",
                    "cost_savings_per_student": "çº¦ $0.04-0.05"
                }
            },
            "students": []
        }
        
        for result in all_results:
            student_data = {
                "name": result.student_name,
                "page_range": {
                    "start": result.page_range[0] + 1,
                    "end": result.page_range[1] + 1
                },
                "total_score": result.total_score,
                "max_score": result.max_total_score,
                "percentage": round(result.total_score / result.max_total_score * 100, 1),
                "questions_graded": len(result.question_results),
                "details": []
            }
            
            # æ·»åŠ æ¯é¢˜çš„è¯¦ç»†ç»“æœ
            for q_result in result.question_results:
                question_detail = {
                    "question_id": q_result.question_id,
                    "score": q_result.awarded_score,
                    "max_score": q_result.max_score,
                    "scoring_point_results": [
                        {
                            "description": sp.description,
                            "max_score": sp.max_score,
                            "awarded_score": sp.awarded_score,
                            "is_correct": sp.is_correct,
                            "explanation": sp.explanation
                        }
                        for sp in q_result.scoring_point_results
                    ],
                    "used_alternative_solution": q_result.used_alternative_solution,
                    "confidence": q_result.confidence
                }
                student_data["details"].append(question_detail)
            
            response_data["students"].append(student_data)
        
        # æ¸…ç†ç¼“å­˜
        cached_service.delete_cache()
        logger.info("ç¼“å­˜å·²æ¸…ç†")
        
        # æ¨é€å®Œæˆé€šçŸ¥
        await broadcast_progress(
            batch_id,
            {
                "type": "completed",
                "percentage": 100,
                "total_students": len(all_results),
                "message": "æ‰¹æ”¹å®Œæˆï¼ˆä½¿ç”¨ç¼“å­˜ä¼˜åŒ–ï¼‰"
            }
        )
        
        return response_data
        
    except Exception as e:
        logger.error(f"æ‰¹æ”¹å¤±è´¥: {str(e)}", exc_info=True)
        raise HTTPException(status_code=500, detail=str(e))
    finally:
        # æ¸…ç†ä¸´æ—¶æ–‡ä»¶
        if temp_dir:
            import shutil
            try:
                shutil.rmtree(temp_dir)
            except Exception as e:
                logger.warning(f"æ¸…ç†ä¸´æ—¶æ–‡ä»¶å¤±è´¥: {str(e)}")
