# ç”Ÿäº§ç¯å¢ƒéƒ¨ç½²æŒ‡å—

<cite>
**æœ¬æ–‡æ¡£å¼•ç”¨çš„æ–‡ä»¶**
- [vercel.json](file://vercel.json)
- [urgent_fix_deployment.py](file://urgent_fix_deployment.py)
- [ai_correction/main.py](file://ai_correction/main.py)
- [ai_correction/requirements.txt](file://ai_correction/requirements.txt)
- [ai_correction/config.py](file://ai_correction/config.py)
- [ai_correction/init_database.py](file://ai_correction/init_database.py)
- [ai_correction/functions/database/migration.py](file://ai_correction/functions/database/migration.py)
- [ai_correction/local_runner.py](file://ai_correction/local_runner.py)
- [ai_correction/docs/DEPLOYMENT_GUIDE.md](file://ai_correction/docs/DEPLOYMENT_GUIDE.md)
- [ai_correction/docs/ENVIRONMENT_VARIABLES.md](file://ai_correction/docs/ENVIRONMENT_VARIABLES.md)
</cite>

## ç›®å½•
1. [æ¦‚è¿°](#æ¦‚è¿°)
2. [éƒ¨ç½²å¹³å°é€‰æ‹©](#éƒ¨ç½²å¹³å°é€‰æ‹©)
3. [Verceléƒ¨ç½²é…ç½®](#verceléƒ¨ç½²é…ç½®)
4. [Railwayéƒ¨ç½²æµç¨‹](#railwayéƒ¨ç½²æµç¨‹)
5. [Dockeréƒ¨ç½²æ–¹æ¡ˆ](#dockeréƒ¨ç½²æ–¹æ¡ˆ)
6. [ç”Ÿäº§ç¯å¢ƒé…ç½®](#ç”Ÿäº§ç¯å¢ƒé…ç½®)
7. [æ•°æ®åº“é…ç½®](#æ•°æ®åº“é…ç½®)
8. [Redisç¼“å­˜é›†æˆ](#redisç¼“å­˜é›†æˆ)
9. [å®‰å…¨è®¾ç½®](#å®‰å…¨è®¾ç½®)
10. [ç´§æ€¥ä¿®å¤æµç¨‹](#ç´§æ€¥ä¿®å¤æµç¨‹)
11. [éƒ¨ç½²åéªŒè¯](#éƒ¨ç½²åéªŒè¯)
12. [ç›‘æ§é…ç½®](#ç›‘æ§é…ç½®)
13. [æ•…éšœæ’é™¤](#æ•…éšœæ’é™¤)

## æ¦‚è¿°

AIæ™ºèƒ½æ‰¹æ”¹ç³»ç»Ÿæ”¯æŒå¤šç§ç”Ÿäº§ç¯å¢ƒéƒ¨ç½²æ–¹æ¡ˆï¼ŒåŒ…æ‹¬Vercelã€Railwayå’ŒDockerå®¹å™¨åŒ–éƒ¨ç½²ã€‚æœ¬æŒ‡å—è¯¦ç»†è¯´æ˜äº†æ¯ç§éƒ¨ç½²æ–¹å¼çš„é…ç½®è¦æ±‚ã€ç¯å¢ƒå˜é‡è®¾ç½®å’Œæœ€ä½³å®è·µã€‚

### ç³»ç»Ÿæ¶æ„ç‰¹ç‚¹
- åŸºäºStreamlitçš„Webç•Œé¢
- LangGraphå¤šæ¨¡æ€AIæ‰¹æ”¹å¼•æ“
- æ”¯æŒå¤šç§LLMæä¾›å•†ï¼ˆOpenAIã€Geminiã€OpenRouterï¼‰
- å¯æ‰©å±•çš„æ•°æ®åº“æ¶æ„
- å®æ—¶æ‰¹æ”¹å’Œå†å²è®°å½•ç®¡ç†

## éƒ¨ç½²å¹³å°é€‰æ‹©

### å¹³å°å¯¹æ¯”

| ç‰¹æ€§ | Vercel | Railway | Docker |
|------|--------|---------|--------|
| éƒ¨ç½²å¤æ‚åº¦ | ç®€å• | ä¸­ç­‰ | ä¸­ç­‰ |
| æ•°æ®åº“æ”¯æŒ | æœ‰é™ | PostgreSQL | è‡ªå®šä¹‰ |
| æ‰©å±•æ€§ | æœ‰é™ | é«˜ | é«˜ |
| æˆæœ¬ | å…è´¹/ä»˜è´¹ | ä»˜è´¹ | è‡ªå»º |
| ç›‘æ§ | åŸºç¡€ | ä¼˜ç§€ | éœ€é…ç½® |

### æ¨èéƒ¨ç½²æ–¹æ¡ˆ
- **å°å‹é¡¹ç›®**: Vercelï¼ˆå¿«é€Ÿéƒ¨ç½²ï¼Œæˆæœ¬ä½ï¼‰
- **ä¸­å‹é¡¹ç›®**: Railwayï¼ˆå¹³è¡¡æ€§èƒ½å’Œæˆæœ¬ï¼‰
- **å¤§å‹é¡¹ç›®**: Dockerï¼ˆå®Œå…¨æ§åˆ¶ï¼Œé«˜æ‰©å±•æ€§ï¼‰

## Verceléƒ¨ç½²é…ç½®

### vercel.jsoné…ç½®è¯¦è§£

åŸºäºé¡¹ç›®ä¸­çš„vercel.jsonæ–‡ä»¶ï¼ŒVerceléƒ¨ç½²é…ç½®å¦‚ä¸‹ï¼š

```json
{
  "rewrites": [{"source": "/(.*)", "destination": "/index.html"}]
}
```

è¯¥é…ç½®å®ç°äº†SPAï¼ˆå•é¡µåº”ç”¨ï¼‰è·¯ç”±é‡å†™ï¼Œç¡®ä¿æ‰€æœ‰è·¯ç”±éƒ½æŒ‡å‘Streamlitåº”ç”¨çš„å…¥å£ç‚¹ã€‚

### æ„å»ºå‘½ä»¤é…ç½®

Vercelè‡ªåŠ¨æ£€æµ‹requirements.txtæ–‡ä»¶å¹¶æ‰§è¡Œï¼š
```bash
pip install -r requirements.txt
```

### å¼€å‘å‘½ä»¤é…ç½®

```bash
streamlit run main.py
```

### å®‰è£…å‘½ä»¤é…ç½®

```bash
pip install -r requirements.txt
```

### Vercelç¯å¢ƒå˜é‡é…ç½®

| å˜é‡å | æè¿° | ç¤ºä¾‹å€¼ |
|--------|------|--------|
| `DATABASE_URL` | PostgreSQLæ•°æ®åº“è¿æ¥å­—ç¬¦ä¸² | `postgresql://user:pass@host:5432/dbname` |
| `OPENAI_API_KEY` | OpenAI APIå¯†é’¥ | `sk-xxx` |
| `ENVIRONMENT` | è¿è¡Œç¯å¢ƒ | `production` |
| `DEFAULT_MODE` | é»˜è®¤æ‰¹æ”¹æ¨¡å¼ | `professional` |

### éƒ¨ç½²æ­¥éª¤

1. **è¿æ¥GitHubä»“åº“**
   ```bash
   vercel link
   vercel deploy
   ```

2. **é…ç½®ç¯å¢ƒå˜é‡**
   ```bash
   vercel env add DATABASE_URL production
   vercel env add OPENAI_API_KEY production
   ```

3. **éƒ¨ç½²åº”ç”¨**
   ```bash
   vercel --prod
   ```

## Railwayéƒ¨ç½²æµç¨‹

### å‰ç½®è¦æ±‚
- Railwayè´¦å·
- GitHubä»“åº“è¿æ¥
- PostgreSQLæ•°æ®åº“
- Redisç¼“å­˜ï¼ˆå¯é€‰ï¼‰

### ç¯å¢ƒå˜é‡é…ç½®

åŸºäºç´§æ€¥ä¿®å¤è„šæœ¬ä¸­çš„é…ç½®ï¼Œç”Ÿäº§ç¯å¢ƒéœ€è¦ä»¥ä¸‹ç¯å¢ƒå˜é‡ï¼š

```bash
# æ•°æ®åº“é…ç½®
DATABASE_URL=postgresql://postgres:sfraebGPmjkZtWpAsHqeHrxUrxuDSQFz@postgres.railway.internal:5432/railway
REDIS_URL=redis://default:fXZjFSKZfAfkTiqBfomlFHzcddmZZLLv@redis.railway.internal:6379

# åº”ç”¨é…ç½®
ENVIRONMENT=production
DEFAULT_MODE=professional
SECRET_KEY=aiguru2-production-secret-key-2024
PORT=8000

# LLMé…ç½®
OPENAI_API_KEY=${OPENAI_API_KEY}
OPENROUTER_API_KEY=${OPENROUTER_API_KEY}

# Firebaseé…ç½®ï¼ˆå¯é€‰ï¼‰
FIREBASE_PROJECT_ID=${FIREBASE_PROJECT_ID}
FIREBASE_CLIENT_EMAIL=${FIREBASE_CLIENT_EMAIL}
FIREBASE_PRIVATE_KEY=${FIREBASE_PRIVATE_KEY}
```

### å¯åŠ¨å‘½ä»¤é…ç½®

```bash
streamlit run main.py --server.port=$PORT
```

### æ•°æ®åº“è¿ç§»ç­–ç•¥

1. **åˆå§‹åŒ–æ•°æ®åº“**
```bash
python functions/database/migration.py init
```

2. **åˆ›å»ºè¿ç§»è„šæœ¬**
```bash
python functions/database/migration.py create -m "åˆå§‹æ•°æ®åº“ç»“æ„"
```

3. **æ‰§è¡Œå‡çº§**
```bash
python functions/database/migration.py upgrade
```

### Railwayéƒ¨ç½²è‡ªåŠ¨åŒ–

ç´§æ€¥ä¿®å¤è„šæœ¬å±•ç¤ºäº†å®Œæ•´çš„Railwayéƒ¨ç½²æµç¨‹ï¼š

```mermaid
flowchart TD
A["æ£€æŸ¥Railwayè¿æ¥"] --> B["æ¸…ç†å¤±è´¥æœåŠ¡"]
B --> C["é‡æ–°é“¾æ¥é¡¹ç›®"]
C --> D["è®¾ç½®åç«¯ç¯å¢ƒå˜é‡"]
D --> E["éƒ¨ç½²åç«¯æœåŠ¡"]
E --> F["è®¾ç½®å‰ç«¯ç¯å¢ƒå˜é‡"]
F --> G["éƒ¨ç½²å‰ç«¯æœåŠ¡"]
G --> H["æ£€æŸ¥æœ€ç»ˆçŠ¶æ€"]
H --> I["éƒ¨ç½²å®Œæˆ"]
E --> J{"åç«¯éƒ¨ç½²æˆåŠŸ?"}
J --> |å¦| K["æ£€æŸ¥æ—¥å¿—"]
J --> |æ˜¯| F
G --> L{"å‰ç«¯éƒ¨ç½²æˆåŠŸ?"}
L --> |å¦| M["æ£€æŸ¥å‰ç«¯æ—¥å¿—"]
L --> |æ˜¯| H
```

**å›¾è¡¨æ¥æº**
- [urgent_fix_deployment.py](file://urgent_fix_deployment.py#L43-L111)

**ç« èŠ‚æ¥æº**
- [urgent_fix_deployment.py](file://urgent_fix_deployment.py#L1-L112)

## Dockeréƒ¨ç½²æ–¹æ¡ˆ

### Dockerfileé…ç½®

```dockerfile
FROM python:3.9-slim

# è®¾ç½®å·¥ä½œç›®å½•
WORKDIR /app

# å¤åˆ¶ä¾èµ–æ–‡ä»¶
COPY requirements.txt .

# å®‰è£…ä¾èµ–
RUN pip install --no-cache-dir -r requirements.txt

# å¤åˆ¶åº”ç”¨æ–‡ä»¶
COPY . .

# åˆ›å»ºæ•°æ®ç›®å½•
RUN mkdir -p uploads data logs

# æš´éœ²ç«¯å£
EXPOSE 8501

# å¥åº·æ£€æŸ¥
HEALTHCHECK --interval=30s --timeout=10s --start-period=5s --retries=3 \
    CMD python -c "import requests; requests.get('http://localhost:8501/_stcore/health')" || exit 1

# å¯åŠ¨å‘½ä»¤
CMD ["streamlit", "run", "main.py", "--server.address=0.0.0.0", "--server.port=8501"]
```

### docker-compose.ymlé…ç½®

```yaml
version: '3.8'

services:
  app:
    build: .
    ports:
      - "8501:8501"
    environment:
      - DATABASE_URL=postgresql://postgres:password@db:5432/ai_correction
      - OPENAI_API_KEY=${OPENAI_API_KEY}
      - ENVIRONMENT=production
      - DEFAULT_MODE=professional
    volumes:
      - ./uploads:/app/uploads
      - ./data:/app/data
      - ./logs:/app/logs
    depends_on:
      - db
      - redis
    restart: unless-stopped
    
  db:
    image: postgres:13-alpine
    environment:
      - POSTGRES_DB=ai_correction
      - POSTGRES_USER=postgres
      - POSTGRES_PASSWORD=password
    volumes:
      - postgres_data:/var/lib/postgresql/data
    restart: unless-stopped
    
  redis:
    image: redis:7-alpine
    ports:
      - "6379:6379"
    volumes:
      - redis_data:/data
    restart: unless-stopped

volumes:
  postgres_data:
  redis_data:
```

### Dockeréƒ¨ç½²æ­¥éª¤

1. **æ„å»ºé•œåƒ**
```bash
docker-compose build
```

2. **å¯åŠ¨æœåŠ¡**
```bash
docker-compose up -d
```

3. **æ•°æ®åº“åˆå§‹åŒ–**
```bash
docker-compose exec app python init_database.py
```

4. **æ•°æ®åº“è¿ç§»**
```bash
docker-compose exec app python functions/database/migration.py upgrade
```

5. **æŸ¥çœ‹æ—¥å¿—**
```bash
docker-compose logs -f app
```

## ç”Ÿäº§ç¯å¢ƒé…ç½®

### æ ¸å¿ƒé…ç½®å‚æ•°

åŸºäºç¯å¢ƒå˜é‡æ–‡æ¡£ï¼Œç”Ÿäº§ç¯å¢ƒçš„å…³é”®é…ç½®ï¼š

| é…ç½®é¡¹ | æ¨èå€¼ | è¯´æ˜ |
|--------|--------|------|
| `ENVIRONMENT` | `production` | ç”Ÿäº§ç¯å¢ƒæ ‡è¯† |
| `DEFAULT_MODE` | `professional` | é»˜è®¤æ‰¹æ”¹æ¨¡å¼ |
| `LOG_LEVEL` | `WARNING` | æ—¥å¿—çº§åˆ« |
| `MAX_PARALLEL_WORKERS` | `8` | å¹¶è¡Œå¤„ç†æ•°é‡ |
| `EFFICIENT_MODE_THRESHOLD` | `8000` | é«˜æ•ˆæ¨¡å¼é˜ˆå€¼ |
| `PROFESSIONAL_MODE_THRESHOLD` | `5000` | ä¸“ä¸šæ¨¡å¼é˜ˆå€¼ |

### æ€§èƒ½ä¼˜åŒ–é…ç½®

```bash
# å¹¶è¡Œå¤„ç†ä¼˜åŒ–
MAX_PARALLEL_WORKERS=8
MAX_RETRIES=5
REQUEST_TIMEOUT=60

# ç¼“å­˜é…ç½®
ENABLE_CACHE=true
CACHE_TTL=3600

# æ€§èƒ½ç›‘æ§
CHECKPOINT_INTERVAL=120
```

### æ‰¹æ”¹æ¨¡å¼é…ç½®

```mermaid
flowchart LR
A["ç”¨æˆ·é€‰æ‹©æ¨¡å¼"] --> B{"æ¨¡å¼ç±»å‹"}
B --> |é«˜æ•ˆæ¨¡å¼| C["é«˜æ•ˆæ¨¡å¼é˜ˆå€¼<br/>6000 token"]
B --> |ä¸“ä¸šæ¨¡å¼| D["ä¸“ä¸šæ¨¡å¼é˜ˆå€¼<br/>4000 token"]
C --> E["èŠ‚çœ66% token<br/>å¿«é€Ÿæ‰¹æ”¹"]
D --> F["è¯¦ç»†åˆ†æ<br/>æ·±åº¦åé¦ˆ"]
E --> G["é€‚åˆå¤§é‡ä½œä¸š"]
F --> H["é€‚åˆé‡è¦è€ƒè¯•"]
```

**å›¾è¡¨æ¥æº**
- [ai_correction/docs/ENVIRONMENT_VARIABLES.md](file://ai_correction/docs/ENVIRONMENT_VARIABLES.md#L60-L85)

**ç« èŠ‚æ¥æº**
- [ai_correction/docs/ENVIRONMENT_VARIABLES.md](file://ai_correction/docs/ENVIRONMENT_VARIABLES.md#L184-L291)

## æ•°æ®åº“é…ç½®

### PostgreSQLç”Ÿäº§é…ç½®

```bash
# Railway PostgreSQLé…ç½®
DATABASE_URL=postgresql://postgres:sfraebGPmjkZtWpAsHqeHrxUrxuDSQFz@postgres.railway.internal:5432/railway

# è‡ªå»ºPostgreSQLé…ç½®
DATABASE_URL=postgresql://user:password@host:5432/ai_correction
```

### æ•°æ®åº“è¿æ¥æ± é…ç½®

```python
# SQLAlchemyè¿æ¥æ± é…ç½®
from sqlalchemy.pool import QueuePool

engine = create_engine(
    DATABASE_URL,
    poolclass=QueuePool,
    pool_size=10,
    max_overflow=20,
    pool_pre_ping=True,
    pool_recycle=3600
)
```

### æ•°æ®åº“è¿ç§»ç®¡ç†

```bash
# åˆå§‹åŒ–Alembic
python functions/database/migration.py init

# åˆ›å»ºè¿ç§»è„šæœ¬
python functions/database/migration.py create -m "æ·»åŠ ç”¨æˆ·è¡¨"

# æ‰§è¡Œå‡çº§
python functions/database/migration.py upgrade

# æŸ¥çœ‹å½“å‰ç‰ˆæœ¬
python functions/database/migration.py current

# é™çº§åˆ°ä¸Šä¸€ç‰ˆæœ¬
python functions/database/migration.py downgrade -r -1
```

### æ•°æ®åº“å¤‡ä»½ç­–ç•¥

```bash
# PostgreSQLå¤‡ä»½
pg_dump $DATABASE_URL > backup_$(date +%Y%m%d_%H%M%S).sql

# SQLiteå¤‡ä»½
cp ai_correction.db backup_$(date +%Y%m%d_%H%M%S).db
```

**ç« èŠ‚æ¥æº**
- [ai_correction/functions/database/migration.py](file://ai_correction/functions/database/migration.py#L1-L247)
- [ai_correction/init_database.py](file://ai_correction/init_database.py#L1-L112)

## Redisç¼“å­˜é›†æˆ

### Redisé…ç½®é€‰é¡¹

```bash
# Railway Redisé…ç½®
REDIS_URL=redis://default:fXZjFSKZfAfkTiqBfomlFHzcddmZZLLv@redis.railway.internal:6379

# è‡ªå»ºRedisé…ç½®
REDIS_URL=redis://localhost:6379
```

### ç¼“å­˜é…ç½®å®ç°

```python
# config.pyä¸­çš„ç¼“å­˜é…ç½®
ENABLE_CACHE = os.getenv('ENABLE_CACHE', 'true').lower() == 'true'
CACHE_TTL = int(os.getenv('CACHE_TTL', '3600'))  # 1å°æ—¶
```

### ç¼“å­˜ä½¿ç”¨åœºæ™¯

1. **ç”¨æˆ·ä¼šè¯ç¼“å­˜**
2. **æ‰¹æ”¹ç»“æœç¼“å­˜**
3. **LLMå“åº”ç¼“å­˜**
4. **æ–‡ä»¶é¢„è§ˆç¼“å­˜**

### ç¼“å­˜ç›‘æ§

```python
# ç¼“å­˜å‘½ä¸­ç‡ç›‘æ§
import redis

def monitor_cache_hit_rate():
    r = redis.from_url(REDIS_URL)
    stats = r.info('stats')
    hit_rate = stats.get('keyspace_hits', 0) / (stats.get('keyspace_hits', 0) + stats.get('keyspace_misses', 0))
    return hit_rate
```

## å®‰å…¨è®¾ç½®

### APIå¯†é’¥ç®¡ç†

```bash
# ç¯å¢ƒå˜é‡é…ç½®
OPENAI_API_KEY=${OPENAI_API_KEY}
GEMINI_API_KEY=${GEMINI_API_KEY}
OPENROUTER_API_KEY=${OPENROUTER_API_KEY}

# ä¸è¦æäº¤åˆ°Git
echo ".env.local" >> .gitignore
echo ".env.production" >> .gitignore
```

### æ•°æ®åº“å®‰å…¨

```bash
# å¼ºå¯†ç é…ç½®
DATABASE_PASSWORD=complex_secure_password_2024

# SSLè¿æ¥ï¼ˆç”Ÿäº§ç¯å¢ƒï¼‰
DATABASE_URL=postgresql://user:password@host:5432/dbname?sslmode=require
```

### æ–‡ä»¶ä¸Šä¼ å®‰å…¨

```python
# æ–‡ä»¶å¤§å°é™åˆ¶
MAX_FILE_SIZE = 10 * 1024 * 1024  # 10MB

# å…è®¸çš„æ–‡ä»¶ç±»å‹
ALLOWED_EXTENSIONS = ['.txt', '.md', '.pdf', '.jpg', '.png']
```

### CORSé…ç½®

```python
# Streamlit CORSé…ç½®
st.set_page_config(
    page_title="AIæ™ºèƒ½æ‰¹æ”¹ç³»ç»Ÿ",
    page_icon="ğŸ¤–",
    layout="wide",
    initial_sidebar_state="expanded",
    menu_items={
        'Get Help': 'https://github.com/your-org/ai-correction/issues',
        'Report a Bug': 'https://github.com/your-org/ai-correction/issues',
        'About': "AIæ™ºèƒ½æ‰¹æ”¹ç³»ç»Ÿ - Powered by LangGraph"
    }
)
```

### å®‰å…¨æœ€ä½³å®è·µ

1. **å®šæœŸè½®æ¢å¯†é’¥**
2. **ä½¿ç”¨å¯†é’¥ç®¡ç†æœåŠ¡**
3. **é™åˆ¶æ•°æ®åº“è®¿é—®IP**
4. **å¯ç”¨SSL/TLSåŠ å¯†**
5. **å®šæœŸå®‰å…¨å®¡è®¡**

## ç´§æ€¥ä¿®å¤æµç¨‹

### urgent_fix_deployment.pyåŠŸèƒ½

ç´§æ€¥ä¿®å¤è„šæœ¬æä¾›äº†å®Œæ•´çš„Railwayéƒ¨ç½²ä¿®å¤æµç¨‹ï¼š

```mermaid
sequenceDiagram
participant User as ç”¨æˆ·
participant Script as ç´§æ€¥ä¿®å¤è„šæœ¬
participant Railway as Railwayå¹³å°
participant DB as æ•°æ®åº“
participant Redis as Redisç¼“å­˜
User->>Script : æ‰§è¡Œç´§æ€¥ä¿®å¤
Script->>Railway : æ£€æŸ¥è¿æ¥çŠ¶æ€
Script->>Railway : æ¸…ç†å¤±è´¥æœåŠ¡
Script->>Railway : é‡æ–°é“¾æ¥é¡¹ç›®
Script->>Railway : è®¾ç½®ç¯å¢ƒå˜é‡
Script->>Railway : éƒ¨ç½²åç«¯æœåŠ¡
Script->>DB : éªŒè¯æ•°æ®åº“è¿æ¥
Script->>Redis : éªŒè¯ç¼“å­˜è¿æ¥
Script->>Railway : éƒ¨ç½²å‰ç«¯æœåŠ¡
Script->>User : æŠ¥å‘Šéƒ¨ç½²çŠ¶æ€
```

**å›¾è¡¨æ¥æº**
- [urgent_fix_deployment.py](file://urgent_fix_deployment.py#L43-L111)

### æ‰§è¡Œæ–¹å¼

```bash
# æ‰§è¡Œç´§æ€¥ä¿®å¤
python urgent_fix_deployment.py

# æˆ–è€…æ‰‹åŠ¨æ‰§è¡Œå…³é”®æ­¥éª¤
# 1. æ£€æŸ¥RailwayçŠ¶æ€
railway whoami

# 2. æ¸…ç†å¤±è´¥çš„æœåŠ¡
railway service

# 3. é‡æ–°éƒ¨ç½²
railway up --detach
```

### ä¿®å¤æ£€æŸ¥æ¸…å•

- [ ] Railwayè´¦æˆ·éªŒè¯
- [ ] é¡¹ç›®é“¾æ¥çŠ¶æ€
- [ ] ç¯å¢ƒå˜é‡é…ç½®
- [ ] æ•°æ®åº“è¿æ¥æµ‹è¯•
- [ ] æœåŠ¡å¥åº·æ£€æŸ¥
- [ ] å‰ç«¯å¯ç”¨æ€§éªŒè¯

**ç« èŠ‚æ¥æº**
- [urgent_fix_deployment.py](file://urgent_fix_deployment.py#L1-L112)

## éƒ¨ç½²åéªŒè¯

### è‡ªåŠ¨åŒ–éªŒè¯è„šæœ¬

åŸºäºlocal_runner.pyçš„åŠŸèƒ½ï¼Œéƒ¨ç½²åéªŒè¯åŒ…æ‹¬ï¼š

```bash
# 1. ä¾èµ–æ£€æŸ¥
python -c "import streamlit, langgraph, sqlalchemy"

# 2. æ•°æ®åº“è¿æ¥æµ‹è¯•
python -c "
from functions.database.models import check_database_connection
print('æ•°æ®åº“è¿æ¥:', check_database_connection())
"

# 3. LLM APIæµ‹è¯•
python -c "
import openai
openai.api_key = 'test'
try:
    openai.ChatCompletion.create(model='gpt-3.5-turbo', messages=[{'role': 'user', 'content': 'test'}], max_tokens=5)
    print('LLM APIè¿æ¥æ­£å¸¸')
except:
    print('LLM APIè¿æ¥å¤±è´¥')
"

# 4. å·¥ä½œæµæµ‹è¯•
python -c "
from functions.langgraph.workflow_new import get_production_workflow
workflow = get_production_workflow()
print('å·¥ä½œæµåŠ è½½æˆåŠŸ:', workflow is not None)
"
```

### æ‰‹åŠ¨éªŒè¯æ­¥éª¤

1. **åº”ç”¨å¯åŠ¨éªŒè¯**
   - è®¿é—®åº”ç”¨ä¸»é¡µ
   - æ£€æŸ¥é¡µé¢åŠ è½½
   - éªŒè¯å¯¼èˆªåŠŸèƒ½

2. **åŠŸèƒ½æµ‹è¯•**
   - æ–‡ä»¶ä¸Šä¼ åŠŸèƒ½
   - æ‰¹æ”¹æµç¨‹æµ‹è¯•
   - å†å²è®°å½•æŸ¥çœ‹

3. **æ€§èƒ½æµ‹è¯•**
   - å“åº”æ—¶é—´æµ‹è¯•
   - å¹¶å‘å¤„ç†èƒ½åŠ›
   - å†…å­˜ä½¿ç”¨æƒ…å†µ

### å¥åº·æ£€æŸ¥ç«¯ç‚¹

```python
# æ·»åŠ å¥åº·æ£€æŸ¥è·¯ç”±
@app.route('/health')
def health_check():
    checks = {
        'database': check_database_connection(),
        'llm': test_llm_api(),
        'redis': check_redis_connection(),
        'dependencies': check_dependencies()
    }
    
    status = 'healthy' if all(checks.values()) else 'unhealthy'
    return {
        'status': status,
        'checks': checks,
        'timestamp': datetime.now().isoformat()
    }
```

**ç« èŠ‚æ¥æº**
- [ai_correction/local_runner.py](file://ai_correction/local_runner.py#L1-L205)

## ç›‘æ§é…ç½®

### æ—¥å¿—ç³»ç»Ÿé…ç½®

```bash
# æ—¥å¿—çº§åˆ«é…ç½®
LOG_LEVEL=WARNING
LOG_FILE=/var/log/ai_correction.log

# æ—¥å¿—è½®è½¬é…ç½®
LOG_ROTATION_MAX_SIZE=100MB
LOG_ROTATION_BACKUP_COUNT=10
```

### ç›‘æ§æŒ‡æ ‡

| æŒ‡æ ‡ç±»å‹ | ç›‘æ§å†…å®¹ | é˜ˆå€¼å»ºè®® |
|----------|----------|----------|
| æ€§èƒ½æŒ‡æ ‡ | å“åº”æ—¶é—´ | < 5ç§’ |
| èµ„æºæŒ‡æ ‡ | CPUä½¿ç”¨ç‡ | < 80% |
| èµ„æºæŒ‡æ ‡ | å†…å­˜ä½¿ç”¨ç‡ | < 85% |
| é”™è¯¯æŒ‡æ ‡ | é”™è¯¯ç‡ | < 1% |
| ä¸šåŠ¡æŒ‡æ ‡ | æ‰¹æ”¹æˆåŠŸç‡ | > 95% |

### ç›‘æ§å·¥å…·é›†æˆ

```python
# PrometheusæŒ‡æ ‡æ”¶é›†
from prometheus_client import Counter, Histogram, Gauge

# è¯·æ±‚è®¡æ•°å™¨
request_counter = Counter('web_requests_total', 'Total web requests', ['method', 'endpoint'])
response_time = Histogram('web_request_duration_seconds', 'Web request duration')
active_connections = Gauge('web_active_connections', 'Active web connections')

# LLM APIç›‘æ§
llm_requests = Counter('llm_requests_total', 'LLM API requests')
llm_errors = Counter('llm_errors_total', 'LLM API errors')
llm_response_time = Histogram('llm_response_duration_seconds', 'LLM response time')
```

### å‘Šè­¦é…ç½®

```yaml
# AlertManageré…ç½®ç¤ºä¾‹
groups:
  - name: ai-correction-alerts
    rules:
      - alert: HighErrorRate
        expr: rate(http_requests_total{status=~"5.."}[5m]) > 0.1
        for: 2m
        labels:
          severity: critical
        annotations:
          summary: "High error rate detected"
          
      - alert: HighResponseTime
        expr: histogram_quantile(0.95, http_request_duration_seconds) > 5
        for: 5m
        labels:
          severity: warning
        annotations:
          summary: "High response time detected"
```

## æ•…éšœæ’é™¤

### å¸¸è§éƒ¨ç½²é—®é¢˜

#### 1. æ•°æ®åº“è¿æ¥å¤±è´¥

**ç—‡çŠ¶**: åº”ç”¨å¯åŠ¨æ—¶æŠ¥æ•°æ®åº“è¿æ¥é”™è¯¯

**è§£å†³æ–¹æ¡ˆ**:
```bash
# æ£€æŸ¥æ•°æ®åº“è¿æ¥
python -c "
from functions.database.models import check_database_connection
print('æ•°æ®åº“è¿æ¥:', check_database_connection())
"

# é‡æ–°åˆå§‹åŒ–æ•°æ®åº“
python init_database.py

# æ‰§è¡Œæ•°æ®åº“è¿ç§»
python functions/database/migration.py upgrade
```

#### 2. LLM APIå¯†é’¥æ— æ•ˆ

**ç—‡çŠ¶**: æ‰¹æ”¹åŠŸèƒ½æŠ¥APIå¯†é’¥é”™è¯¯

**è§£å†³æ–¹æ¡ˆ**:
```bash
# éªŒè¯APIå¯†é’¥
python -c "
import openai
openai.api_key = '$OPENAI_API_KEY'
try:
    openai.ChatCompletion.create(model='gpt-3.5-turbo', messages=[{'role': 'user', 'content': 'test'}], max_tokens=5)
    print('APIå¯†é’¥æœ‰æ•ˆ')
except Exception as e:
    print('APIå¯†é’¥æ— æ•ˆ:', e)
"
```

#### 3. å†…å­˜ä¸è¶³

**ç—‡çŠ¶**: åº”ç”¨å´©æºƒæˆ–å“åº”ç¼“æ…¢

**è§£å†³æ–¹æ¡ˆ**:
```bash
# å‡å°‘å¹¶è¡Œå¤„ç†æ•°é‡
MAX_PARALLEL_WORKERS=4

# å¢åŠ è™šæ‹Ÿå†…å­˜
# å¯¹äºDocker: ä¿®æ”¹docker-compose.ymlçš„memoryé™åˆ¶
```

#### 4. æ–‡ä»¶ä¸Šä¼ å¤±è´¥

**ç—‡çŠ¶**: æ–‡ä»¶ä¸Šä¼ æŠ¥é”™æˆ–æ— æ³•å¤„ç†

**è§£å†³æ–¹æ¡ˆ**:
```bash
# æ£€æŸ¥æ–‡ä»¶å¤§å°é™åˆ¶
MAX_FILE_SIZE=50*1024*1024  # 50MB

# æ£€æŸ¥å…è®¸çš„æ–‡ä»¶ç±»å‹
ALLOWED_EXTENSIONS=['.txt', '.pdf', '.jpg', '.png', '.docx']
```

### è°ƒè¯•å·¥å…·

```python
# è°ƒè¯•ä¿¡æ¯æ”¶é›†
def collect_debug_info():
    import platform
    import psutil
    import sys
    
    debug_info = {
        'platform': platform.platform(),
        'python_version': platform.python_version(),
        'memory_usage': psutil.virtual_memory().percent,
        'disk_usage': psutil.disk_usage('/').percent,
        'dependencies': {
            'streamlit': streamlit.__version__,
            'langgraph': langgraph.__version__,
            'sqlalchemy': sqlalchemy.__version__
        }
    }
    
    return debug_info
```

### æ€§èƒ½ä¼˜åŒ–å»ºè®®

1. **æ•°æ®åº“ä¼˜åŒ–**
   - ä½¿ç”¨ç´¢å¼•ä¼˜åŒ–æŸ¥è¯¢
   - é…ç½®è¿æ¥æ± 
   - å®šæœŸæ¸…ç†å†å²æ•°æ®

2. **ç¼“å­˜ä¼˜åŒ–**
   - å¯ç”¨Redisç¼“å­˜
   - ä¼˜åŒ–ç¼“å­˜ç­–ç•¥
   - ç›‘æ§ç¼“å­˜å‘½ä¸­ç‡

3. **å¹¶å‘ä¼˜åŒ–**
   - è°ƒæ•´å¹¶è¡Œworkeræ•°é‡
   - ä½¿ç”¨å¼‚æ­¥å¤„ç†
   - ä¼˜åŒ–èµ„æºåˆ†é…

### å¤‡ä»½å’Œæ¢å¤

```bash
# å®šæœŸå¤‡ä»½è„šæœ¬
#!/bin/bash
BACKUP_DIR="/backup/ai-correction"
DATE=$(date +%Y%m%d_%H%M%S)

# æ•°æ®åº“å¤‡ä»½
pg_dump $DATABASE_URL > "$BACKUP_DIR/db_$DATE.sql"

# æ–‡ä»¶å¤‡ä»½
tar -czf "$BACKUP_DIR/files_$DATE.tar.gz" uploads/

# æ—¥å¿—å¤‡ä»½
cp -r logs/ "$BACKUP_DIR/logs_$DATE/"

# æ¸…ç†æ—§å¤‡ä»½ï¼ˆä¿ç•™30å¤©ï¼‰
find "$BACKUP_DIR" -name "*.sql" -mtime +30 -delete
find "$BACKUP_DIR" -name "*.tar.gz" -mtime +30 -delete
```

**ç« èŠ‚æ¥æº**
- [ai_correction/docs/DEPLOYMENT_GUIDE.md](file://ai_correction/docs/DEPLOYMENT_GUIDE.md#L250-L309)

## æ€»ç»“

æœ¬éƒ¨ç½²æŒ‡å—æ¶µç›–äº†AIæ™ºèƒ½æ‰¹æ”¹ç³»ç»Ÿåœ¨ç”Ÿäº§ç¯å¢ƒä¸­çš„å®Œæ•´éƒ¨ç½²æµç¨‹ï¼ŒåŒ…æ‹¬ï¼š

1. **å¤šç§éƒ¨ç½²å¹³å°é€‰æ‹©**ï¼šVercelã€Railwayå’ŒDocker
2. **è¯¦ç»†çš„é…ç½®è¯´æ˜**ï¼šç¯å¢ƒå˜é‡ã€æ•°æ®åº“ã€ç¼“å­˜ç­‰
3. **å®‰å…¨æœ€ä½³å®è·µ**ï¼šAPIå¯†é’¥ç®¡ç†ã€æ•°æ®åº“å®‰å…¨ã€æ–‡ä»¶ä¸Šä¼ å®‰å…¨
4. **ç›‘æ§å’Œç»´æŠ¤**ï¼šæ—¥å¿—é…ç½®ã€æ€§èƒ½ç›‘æ§ã€æ•…éšœæ’é™¤
5. **ç´§æ€¥ä¿®å¤æµç¨‹**ï¼šè‡ªåŠ¨åŒ–ä¿®å¤è„šæœ¬å’Œæ‰‹åŠ¨æ“ä½œæŒ‡å—

é€šè¿‡éµå¾ªæœ¬æŒ‡å—ï¼Œæ‚¨å¯ä»¥æˆåŠŸéƒ¨ç½²ä¸€ä¸ªé«˜æ€§èƒ½ã€å¯æ‰©å±•çš„AIæ™ºèƒ½æ‰¹æ”¹ç³»ç»Ÿï¼Œæ»¡è¶³æ•™è‚²æœºæ„çš„æ‰¹æ”¹éœ€æ±‚ã€‚