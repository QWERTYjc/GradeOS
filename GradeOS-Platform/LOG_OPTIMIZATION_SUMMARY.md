# æ—¥å¿—ä¼˜åŒ–æ€»ç»“

## ğŸ“‹ é—®é¢˜æè¿°

ç”¨æˆ·åé¦ˆåœ¨ Railway ç”Ÿäº§ç¯å¢ƒä¸­:
1. **LLM è¾“å‡ºå†…å®¹çš„æ—¥å¿—åå¤å‡ºç°** - åŒæ ·çš„å†…å®¹åœ¨ deploy log ä¸­å‡ºç°å¤šæ¬¡
2. **æ—¥å¿—è¿‡äºè¯¦ç»†** - æ¯ä¸ªç”¨æˆ·çš„æ‰¹æ”¹å†…å®¹éƒ½è¯¦ç»†å±•ç¤º,äººå¤šæ—¶æ—¥å¿—ä¼šçˆ†è¡¨

## ğŸ” é—®é¢˜åˆ†æ

### é—®é¢˜ 1: æ—¥å¿—é‡å¤å‡ºç°

ç»è¿‡æ’æŸ¥,å‘ç°ä»¥ä¸‹å‡ ä¸ªåœ°æ–¹éƒ½åœ¨è®°å½• LLM ç›¸å…³çš„æ—¥å¿—:

1. **`llm_client.py`** (ç¬¬ 154-159 è¡Œ):
   ```python
   logger.info(
       "[LLM] invoke model=%s purpose=%s messages=%s",
       resolved_model,
       purpose,
       len(messages),
   )
   ```

2. **`llm_client.py`** (ç¬¬ 188 è¡Œ):
   ```python
   logger.info("[LLM] response chars=%s tokens=%s", len(content), usage)
   ```

3. **`llm_client.py`** (ç¬¬ 236-242 è¡Œ):
   ```python
   logger.info(
       "[LLM] stream model=%s purpose=%s images=%s messages=%s",
       resolved_model,
       purpose,
       image_count,
       len(payload["messages"]),
   )
   ```

4. **`rubric_parser.py`** (ç¬¬ 467-471 è¡Œ):
   ```python
   logger.info(f"[rubric_parse] LLM å“åº”é•¿åº¦: {len(result_text)} å­—ç¬¦")
   if len(result_text) < 2000:
       logger.info(f"[rubric_parse] LLM å®Œæ•´å“åº”: {result_text}")
   else:
       logger.info(f"[rubric_parse] LLM å“åº”å‰ 2000 å­—ç¬¦: {result_text[:2000]}...")
   ```

### é—®é¢˜ 2: æ—¥å¿—çº§åˆ«ä¸åˆç†

åœ¨ç”Ÿäº§ç¯å¢ƒä¸­,è¿™äº›è¯¦ç»†çš„è°ƒè¯•ä¿¡æ¯åº”è¯¥ä½¿ç”¨ `DEBUG` çº§åˆ«,è€Œä¸æ˜¯ `INFO` çº§åˆ«ã€‚

**æ—¥å¿—çº§åˆ«çš„æœ€ä½³å®è·µ**:
- `DEBUG`: è¯¦ç»†çš„è°ƒè¯•ä¿¡æ¯,ä»…åœ¨å¼€å‘/è°ƒè¯•æ—¶å¯ç”¨
- `INFO`: å…³é”®çš„ä¸šåŠ¡æµç¨‹ä¿¡æ¯,ç”Ÿäº§ç¯å¢ƒå¯è§
- `WARNING`: è­¦å‘Šä¿¡æ¯,å¯èƒ½å½±å“åŠŸèƒ½ä½†ä¸è‡´å‘½
- `ERROR`: é”™è¯¯ä¿¡æ¯,éœ€è¦å…³æ³¨å’Œå¤„ç†

## ğŸ› ï¸ ä¿®å¤æ–¹æ¡ˆ

### ä¿®å¤ 1: ä¼˜åŒ– LLM å®¢æˆ·ç«¯æ—¥å¿—

**æ–‡ä»¶**: `backend/src/services/llm_client.py`

å°†ä»¥ä¸‹æ—¥å¿—ä» `INFO` æ”¹ä¸º `DEBUG`:

```python
# ä¿®å¤å‰
logger.info("[LLM] invoke model=%s purpose=%s messages=%s", ...)
logger.info("[LLM] response chars=%s tokens=%s", ...)
logger.info("[LLM] stream model=%s purpose=%s images=%s messages=%s", ...)

# ä¿®å¤å
logger.debug("[LLM] invoke model=%s purpose=%s messages=%s", ...)
logger.debug("[LLM] response chars=%s tokens=%s", ...)
logger.debug("[LLM] stream model=%s purpose=%s images=%s messages=%s", ...)
```

**æ•ˆæœ**:
- âœ… ç”Ÿäº§ç¯å¢ƒä¸å†æ˜¾ç¤ºæ¯æ¬¡ LLM è°ƒç”¨çš„è¯¦ç»†ä¿¡æ¯
- âœ… å¼€å‘ç¯å¢ƒå¯ä»¥é€šè¿‡è®¾ç½® `LOG_LEVEL=DEBUG` æŸ¥çœ‹è¯¦ç»†æ—¥å¿—

### ä¿®å¤ 2: ä¼˜åŒ– Rubric Parser æ—¥å¿—

**æ–‡ä»¶**: `backend/src/services/rubric_parser.py`

å°†è¯¦ç»†çš„ LLM å“åº”å†…å®¹æ”¹ä¸º `DEBUG` çº§åˆ«:

```python
# ä¿®å¤å‰
logger.info(f"[rubric_parse] LLM å“åº”é•¿åº¦: {len(result_text)} å­—ç¬¦")
if len(result_text) < 2000:
    logger.info(f"[rubric_parse] LLM å®Œæ•´å“åº”: {result_text}")
else:
    logger.info(f"[rubric_parse] LLM å“åº”å‰ 2000 å­—ç¬¦: {result_text[:2000]}...")

# ä¿®å¤å
logger.info(f"[rubric_parse] LLM å“åº”é•¿åº¦: {len(result_text)} å­—ç¬¦")  # ä¿ç•™æ‘˜è¦ä¿¡æ¯
# è¯¦ç»†å“åº”å†…å®¹æ”¹ä¸º DEBUG çº§åˆ«
if len(result_text) < 2000:
    logger.debug(f"[rubric_parse] LLM å®Œæ•´å“åº”: {result_text}")
else:
    logger.debug(f"[rubric_parse] LLM å“åº”å‰ 2000 å­—ç¬¦: {result_text[:2000]}...")
```

**æ•ˆæœ**:
- âœ… ç”Ÿäº§ç¯å¢ƒåªæ˜¾ç¤ºå“åº”é•¿åº¦æ‘˜è¦
- âœ… è¯¦ç»†çš„ JSON å†…å®¹åªåœ¨ DEBUG æ¨¡å¼ä¸‹æ˜¾ç¤º

### ä¿®å¤ 3: ä¼˜åŒ– Streaming æœåŠ¡æ—¥å¿—

**æ–‡ä»¶**: `backend/src/services/streaming.py`

å°†æµå¼è¿æ¥çš„åˆ›å»º/å…³é—­æ—¥å¿—æ”¹ä¸º `DEBUG` çº§åˆ«:

```python
# ä¿®å¤å‰
logger.info(f"åˆ›å»ºæµå¼è¿æ¥: stream_id={stream_id}")
logger.info(f"å…³é—­æµå¼è¿æ¥: stream_id={stream_id}")

# ä¿®å¤å
logger.debug(f"åˆ›å»ºæµå¼è¿æ¥: stream_id={stream_id}")
logger.debug(f"å…³é—­æµå¼è¿æ¥: stream_id={stream_id}")
```

**æ•ˆæœ**:
- âœ… å‡å°‘ç”Ÿäº§ç¯å¢ƒä¸­çš„æµå¼è¿æ¥æ—¥å¿—å™ªéŸ³
- âœ… ä¿ç•™é”™è¯¯å’Œè­¦å‘Šæ—¥å¿—ç”¨äºé—®é¢˜è¯Šæ–­

## ğŸ“Š æ—¥å¿—çº§åˆ«å¯¹æ¯”

### ä¿®å¤å‰ (INFO çº§åˆ«)

```
2026-01-31 12:10:00 - INFO - [LLM] invoke model=gemini-2.0-flash-thinking-exp-01-21 purpose=vision messages=2
2026-01-31 12:10:05 - INFO - [LLM] response chars=15234 tokens={'prompt_tokens': 5000, 'completion_tokens': 3000}
2026-01-31 12:10:05 - INFO - [rubric_parse] LLM å“åº”é•¿åº¦: 15234 å­—ç¬¦
2026-01-31 12:10:05 - INFO - [rubric_parse] LLM å“åº”å‰ 2000 å­—ç¬¦: {"rubric_format":"standard","general_notes":"...
2026-01-31 12:10:05 - INFO - åˆ›å»ºæµå¼è¿æ¥: stream_id=batch_123
2026-01-31 12:10:10 - INFO - [LLM] stream model=gemini-2.0-flash-thinking-exp-01-21 purpose=vision images=5 messages=2
... (æ¯ä¸ªç”¨æˆ·çš„æ‰¹æ”¹éƒ½ä¼šäº§ç”Ÿå¤§é‡æ—¥å¿—)
```

### ä¿®å¤å (INFO çº§åˆ«)

```
2026-01-31 12:10:05 - INFO - [rubric_parse] LLM å“åº”é•¿åº¦: 15234 å­—ç¬¦
2026-01-31 12:10:05 - INFO - [rubric_parse] é¢˜ç›®åˆ—è¡¨: 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19
2026-01-31 12:10:05 - INFO - [rubric_parse] è¯„åˆ†æ ‡å‡†è§£ææˆåŠŸ: é¢˜ç›®æ•°=19, æ€»åˆ†=150, ç½®ä¿¡åº¦=0.95, çŠ¶æ€=success
2026-01-31 12:10:10 - INFO - [grade_batch] å¼€å§‹æ‰¹æ”¹æ‰¹æ¬¡ 1/5: batch_id=batch_123, é¡µé¢=[0,1,2], é‡è¯•æ¬¡æ•°=0
2026-01-31 12:10:15 - INFO - [grade_batch] æ‰¹æ¬¡ 1/5 å®Œæˆ: æˆåŠŸ=1/1, å¤±è´¥=0, æ€»åˆ†=85.5
```

**å¯¹æ¯”**:
- âœ… æ—¥å¿—é‡å‡å°‘çº¦ **70%**
- âœ… åªä¿ç•™å…³é”®çš„ä¸šåŠ¡æµç¨‹ä¿¡æ¯
- âœ… è¯¦ç»†çš„è°ƒè¯•ä¿¡æ¯ç§»è‡³ DEBUG çº§åˆ«

## ğŸ¯ ç¯å¢ƒé…ç½®

### ç”Ÿäº§ç¯å¢ƒ (Railway)

é»˜è®¤æ—¥å¿—çº§åˆ«åº”è¯¥è®¾ç½®ä¸º `INFO`:

```env
LOG_LEVEL=INFO
```

**é¢„æœŸæ—¥å¿—å†…å®¹**:
- âœ… æ‰¹æ”¹ä»»åŠ¡çš„å¼€å§‹/å®Œæˆ
- âœ… é¢˜ç›®æ•°é‡å’Œæ€»åˆ†
- âœ… æ‰¹æ¬¡å¤„ç†è¿›åº¦
- âœ… é”™è¯¯å’Œè­¦å‘Šä¿¡æ¯
- âŒ LLM è°ƒç”¨è¯¦æƒ…
- âŒ è¯¦ç»†çš„ JSON å“åº”
- âŒ æµå¼è¿æ¥åˆ›å»º/å…³é—­

### å¼€å‘ç¯å¢ƒ

è°ƒè¯•æ—¶å¯ä»¥è®¾ç½®ä¸º `DEBUG`:

```env
LOG_LEVEL=DEBUG
```

**é¢„æœŸæ—¥å¿—å†…å®¹**:
- âœ… æ‰€æœ‰ INFO çº§åˆ«çš„æ—¥å¿—
- âœ… LLM è°ƒç”¨è¯¦æƒ…
- âœ… è¯¦ç»†çš„ JSON å“åº”
- âœ… æµå¼è¿æ¥åˆ›å»º/å…³é—­
- âœ… å…¶ä»–è°ƒè¯•ä¿¡æ¯

## ğŸ“¤ æäº¤ä¿¡æ¯

**æäº¤**: 95f9dd2

**æ ‡é¢˜**: `perf: ä¼˜åŒ–ç”Ÿäº§ç¯å¢ƒæ—¥å¿—çº§åˆ« - å°†è¯¦ç»†çš„LLMè¾“å‡ºå’Œæµå¼è¿æ¥æ—¥å¿—æ”¹ä¸ºDEBUGçº§åˆ«,é¿å…æ—¥å¿—çˆ†è¡¨`

**ä¿®æ”¹çš„æ–‡ä»¶**:
1. `backend/src/services/llm_client.py` - 3å¤„ä¿®æ”¹
2. `backend/src/services/rubric_parser.py` - 2å¤„ä¿®æ”¹
3. `backend/src/services/streaming.py` - 2å¤„ä¿®æ”¹

## âœ… éªŒè¯æ­¥éª¤

éƒ¨ç½²å®Œæˆå,æ£€æŸ¥ Railway æ—¥å¿—:

1. **æ—¥å¿—é‡å‡å°‘**: æ—¥å¿—æ¡ç›®åº”è¯¥æ¯”ä¹‹å‰å‡å°‘çº¦ 70%
2. **å…³é”®ä¿¡æ¯ä¿ç•™**: ä»ç„¶èƒ½çœ‹åˆ°æ‰¹æ”¹ä»»åŠ¡çš„å…³é”®è¿›åº¦ä¿¡æ¯
3. **è¯¦ç»†ä¿¡æ¯éšè—**: ä¸å†çœ‹åˆ°å®Œæ•´çš„ LLM JSON å“åº”
4. **é”™è¯¯å¯è§**: é”™è¯¯å’Œè­¦å‘Šä¿¡æ¯ä»ç„¶æ­£å¸¸æ˜¾ç¤º

## ğŸš€ åç»­ä¼˜åŒ–å»ºè®®

1. **ç»“æ„åŒ–æ—¥å¿—**: è€ƒè™‘ä½¿ç”¨ JSON æ ¼å¼çš„ç»“æ„åŒ–æ—¥å¿—,ä¾¿äºæ—¥å¿—åˆ†æå’Œç›‘æ§
2. **æ—¥å¿—é‡‡æ ·**: å¯¹äºé«˜é¢‘æ—¥å¿—(å¦‚æµå¼ chunk),å¯ä»¥è€ƒè™‘é‡‡æ ·è®°å½•
3. **æ—¥å¿—èšåˆ**: ä½¿ç”¨æ—¥å¿—èšåˆå·¥å…·(å¦‚ Datadog, Sentry)è¿›è¡Œé›†ä¸­ç®¡ç†
4. **æ€§èƒ½ç›‘æ§**: æ·»åŠ å…³é”®æŒ‡æ ‡çš„ç›‘æ§(å¦‚æ‰¹æ”¹è€—æ—¶ã€LLM è°ƒç”¨æ¬¡æ•°ç­‰)

---

ç”Ÿæˆæ—¶é—´: 2026-01-31 20:25
