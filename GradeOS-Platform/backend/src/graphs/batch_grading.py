"""æ‰¹é‡æ‰¹æ”¹ Graph ç¼–è¯‘

å®ç°æ‰¹é‡è¯•å·æ‰¹æ”¹æµç¨‹ï¼Œæ”¯æŒï¼š
- å›¾åƒé¢„å¤„ç†
- è¯„åˆ†æ ‡å‡†è§£æ
- å¯é…ç½®åˆ†æ‰¹å¹¶è¡Œæ‰¹æ”¹ï¼ˆä¸é¢„å…ˆåˆ†å‰²å­¦ç”Ÿï¼‰
- æ‰¹æ”¹å‰ç´¢å¼•ï¼ˆé¢˜ç›®ä¿¡æ¯ä¸å­¦ç”Ÿè¯†åˆ«ï¼‰
- ç»“æœå®¡æ ¸
- å¯¼å‡ºç»“æœ
- æ‰¹æ¬¡å¤±è´¥é‡è¯•ä¸é”™è¯¯éš”ç¦»
- å®æ—¶è¿›åº¦æŠ¥å‘Š

å·¥ä½œæµï¼š
æ¥æ”¶æ–‡ä»¶ â†’ å›¾åƒé¢„å¤„ç† â†’ ç´¢å¼•å±‚ â†’ è§£æè¯„åˆ†æ ‡å‡† â†’ å¯é…ç½®åˆ†æ‰¹æ‰¹æ”¹ â†’ ç´¢å¼•èšåˆ â†’ ç»“æœå®¡æ ¸ â†’ å¯¼å‡ºç»“æœ

éªŒè¯ï¼šéœ€æ±‚ 3.1, 3.2, 3.3, 3.4, 5.1, 5.4, 10.1
"""

import logging
import os
import asyncio
from typing import Optional, List, Dict, Any, Literal, Tuple
from datetime import datetime
from dataclasses import dataclass, field

from langgraph.graph import StateGraph, END
from langgraph.types import Send
from langgraph.checkpoint.postgres.aio import AsyncPostgresSaver

from src.graphs.state import BatchGradingGraphState


logger = logging.getLogger(__name__)


# ==================== æ‰¹æ¬¡é…ç½® ====================


@dataclass
class BatchConfig:
    """
    æ‰¹æ¬¡é…ç½®ç±»
    
    æ”¯æŒé…ç½®æ‰¹æ¬¡å¤§å°å’Œå¹¶å‘æ•°é‡ã€‚
    
    Requirements: 3.1, 10.1
    """
    batch_size: int = 10  # æ¯æ‰¹å¤„ç†çš„é¡µé¢æ•°é‡
    max_concurrent_workers: int = 5  # æœ€å¤§å¹¶å‘ Worker æ•°é‡
    max_retries: int = 2  # æ‰¹æ¬¡å¤±è´¥æœ€å¤§é‡è¯•æ¬¡æ•°
    retry_delay: float = 1.0  # é‡è¯•å»¶è¿Ÿï¼ˆç§’ï¼‰
    
    @classmethod
    def from_env(cls) -> "BatchConfig":
        """ä»ç¯å¢ƒå˜é‡åŠ è½½é…ç½®"""
        return cls(
            batch_size=int(os.getenv("GRADING_BATCH_SIZE", "10")),
            max_concurrent_workers=int(os.getenv("GRADING_MAX_WORKERS", "5")),
            max_retries=int(os.getenv("GRADING_MAX_RETRIES", "2")),
            retry_delay=float(os.getenv("GRADING_RETRY_DELAY", "1.0")),
        )


# å…¨å±€æ‰¹æ¬¡é…ç½®
_batch_config: Optional[BatchConfig] = None


def get_batch_config() -> BatchConfig:
    """è·å–æ‰¹æ¬¡é…ç½®"""
    global _batch_config
    if _batch_config is None:
        _batch_config = BatchConfig.from_env()
    return _batch_config


def set_batch_config(config: BatchConfig) -> None:
    """è®¾ç½®æ‰¹æ¬¡é…ç½®"""
    global _batch_config
    _batch_config = config
    logger.info(
        f"æ‰¹æ¬¡é…ç½®å·²æ›´æ–°: batch_size={config.batch_size}, "
        f"max_workers={config.max_concurrent_workers}, "
        f"max_retries={config.max_retries}"
    )


# ==================== è¿›åº¦æŠ¥å‘Š ====================


@dataclass
class BatchProgress:
    """
    æ‰¹æ¬¡è¿›åº¦ä¿¡æ¯
    
    Requirements: 3.4
    """
    batch_id: str
    total_batches: int
    completed_batches: int = 0
    failed_batches: int = 0
    in_progress_batches: int = 0
    total_pages: int = 0
    processed_pages: int = 0
    failed_pages: int = 0
    current_stage: str = "initialized"
    percentage: float = 0.0
    batch_details: Dict[int, Dict[str, Any]] = field(default_factory=dict)
    timestamps: Dict[str, str] = field(default_factory=dict)
    
    def update_batch_status(
        self, 
        batch_index: int, 
        status: str, 
        pages_processed: int = 0,
        pages_failed: int = 0,
        error: Optional[str] = None
    ) -> None:
        """æ›´æ–°å•ä¸ªæ‰¹æ¬¡çŠ¶æ€"""
        self.batch_details[batch_index] = {
            "status": status,
            "pages_processed": pages_processed,
            "pages_failed": pages_failed,
            "error": error,
            "updated_at": datetime.now().isoformat()
        }
        
        # é‡æ–°è®¡ç®—ç»Ÿè®¡
        self.completed_batches = sum(
            1 for d in self.batch_details.values() if d["status"] == "completed"
        )
        self.failed_batches = sum(
            1 for d in self.batch_details.values() if d["status"] == "failed"
        )
        self.in_progress_batches = sum(
            1 for d in self.batch_details.values() if d["status"] == "in_progress"
        )
        self.processed_pages = sum(
            d["pages_processed"] for d in self.batch_details.values()
        )
        self.failed_pages = sum(
            d["pages_failed"] for d in self.batch_details.values()
        )
        
        # è®¡ç®—ç™¾åˆ†æ¯”ï¼ˆæ‰¹æ”¹é˜¶æ®µå  15%-80%ï¼‰
        if self.total_batches > 0:
            batch_progress = self.completed_batches / self.total_batches
            self.percentage = 15.0 + batch_progress * 65.0
    
    def to_dict(self) -> Dict[str, Any]:
        """åºåˆ—åŒ–ä¸ºå­—å…¸"""
        return {
            "batch_id": self.batch_id,
            "total_batches": self.total_batches,
            "completed_batches": self.completed_batches,
            "failed_batches": self.failed_batches,
            "in_progress_batches": self.in_progress_batches,
            "total_pages": self.total_pages,
            "processed_pages": self.processed_pages,
            "failed_pages": self.failed_pages,
            "current_stage": self.current_stage,
            "percentage": self.percentage,
            "batch_details": self.batch_details,
            "timestamps": self.timestamps,
        }


# è¿›åº¦æŠ¥å‘Šå›è°ƒç±»å‹
ProgressCallback = Optional[callable]


# ==================== æ‰¹æ¬¡ä»»åŠ¡çŠ¶æ€ ====================


@dataclass
class BatchTaskState:
    """
    å•ä¸ªæ‰¹æ¬¡ä»»åŠ¡çš„çŠ¶æ€
    
    ç”¨äºè·Ÿè¸ªæ‰¹æ¬¡æ‰§è¡ŒçŠ¶æ€å’Œæ”¯æŒé‡è¯•ã€‚
    
    Requirements: 3.3, 9.3
    """
    batch_id: str
    batch_index: int
    total_batches: int
    page_indices: List[int]
    images: List[str]
    rubric: str
    parsed_rubric: Dict[str, Any]
    page_index_contexts: Dict[int, Dict[str, Any]] = field(default_factory=dict)
    api_key: str
    retry_count: int = 0
    max_retries: int = 2
    status: str = "pending"  # pending, in_progress, completed, failed
    error: Optional[str] = None
    results: List[Dict[str, Any]] = field(default_factory=list)


# ==================== èŠ‚ç‚¹å®ç° ====================

async def intake_node(state: BatchGradingGraphState) -> Dict[str, Any]:
    """
    æ¥æ”¶æ–‡ä»¶èŠ‚ç‚¹
    
    éªŒè¯è¾“å…¥æ–‡ä»¶ï¼Œå‡†å¤‡å¤„ç†ç¯å¢ƒã€‚
    """
    batch_id = state["batch_id"]
    
    logger.info(f"[intake] å¼€å§‹æ¥æ”¶æ–‡ä»¶: batch_id={batch_id}")
    
    # éªŒè¯å¿…è¦çš„è¾“å…¥
    answer_images = state.get("answer_images", [])
    rubric_images = state.get("rubric_images", [])
    
    if not answer_images:
        raise ValueError("æœªæä¾›ç­”é¢˜å›¾åƒ")
    
    logger.info(
        f"[intake] æ–‡ä»¶æ¥æ”¶å®Œæˆ: batch_id={batch_id}, "
        f"ç­”é¢˜é¡µæ•°={len(answer_images)}, è¯„åˆ†æ ‡å‡†é¡µæ•°={len(rubric_images)}"
    )
    
    return {
        "current_stage": "intake_completed",
        "percentage": 5.0,
        "timestamps": {
            **state.get("timestamps", {}),
            "intake_at": datetime.now().isoformat()
        }
    }


async def preprocess_node(state: BatchGradingGraphState) -> Dict[str, Any]:
    """
    å›¾åƒé¢„å¤„ç†èŠ‚ç‚¹
    
    å¯¹å›¾åƒè¿›è¡Œé¢„å¤„ç†ï¼ˆå»å™ªã€å¢å¼ºã€æ—‹è½¬æ ¡æ­£ç­‰ï¼‰ã€‚
    """
    batch_id = state["batch_id"]
    answer_images = state.get("answer_images", [])
    
    logger.info(f"[preprocess] å¼€å§‹å›¾åƒé¢„å¤„ç†: batch_id={batch_id}, é¡µæ•°={len(answer_images)}")
    
    # TODO: å®é™…çš„å›¾åƒé¢„å¤„ç†é€»è¾‘
    # ç›®å‰ç›´æ¥ä¼ é€’åŸå§‹å›¾åƒ
    processed_images = answer_images
    
    logger.info(f"[preprocess] å›¾åƒé¢„å¤„ç†å®Œæˆ: batch_id={batch_id}")
    
    return {
        "processed_images": processed_images,
        "current_stage": "preprocess_completed",
        "percentage": 10.0,
        "timestamps": {
            **state.get("timestamps", {}),
            "preprocess_at": datetime.now().isoformat()
        }
    }


async def index_node(state: BatchGradingGraphState) -> Dict[str, Any]:
    """
    ç´¢å¼•å±‚èŠ‚ç‚¹ï¼ˆæ‰¹æ”¹å‰ï¼‰

    ä½¿ç”¨ LLM ç”Ÿæˆæ¯é¡µé¢˜ç›®ä¿¡æ¯å¹¶è¯†åˆ«å­¦ç”Ÿï¼Œç”¨äºåç»­æ‰¹æ”¹ä¸Šä¸‹æ–‡å¯¹é½ã€‚
    """
    batch_id = state["batch_id"]
    processed_images = state.get("processed_images", [])
    api_key = state.get("api_key") or os.getenv("GEMINI_API_KEY")

    logger.info(
        f"[index] å¼€å§‹ç´¢å¼•: batch_id={batch_id}, é¡µæ•°={len(processed_images)}"
    )

    if not processed_images:
        logger.warning(f"[index] æ— å¾…ç´¢å¼•é¡µé¢: batch_id={batch_id}")
        return {
            "index_results": {
                "model": None,
                "total_pages": 0,
                "pages": [],
                "students": [],
                "unidentified_pages": [],
            },
            "page_index_contexts": {},
            "student_page_map": {},
            "indexed_students": [],
            "index_unidentified_pages": [],
            "student_boundaries": [],
            "current_stage": "index_completed",
            "percentage": 12.0,
            "timestamps": {
                **state.get("timestamps", {}),
                "index_at": datetime.now().isoformat(),
            },
        }

    if not api_key:
        logger.warning(f"[index] ç¼ºå°‘ API keyï¼Œè·³è¿‡ç´¢å¼•: batch_id={batch_id}")
        return {
            "index_results": {
                "model": None,
                "total_pages": len(processed_images),
                "pages": [],
                "students": [],
                "unidentified_pages": list(range(len(processed_images))),
            },
            "page_index_contexts": {},
            "student_page_map": {},
            "indexed_students": [],
            "index_unidentified_pages": list(range(len(processed_images))),
            "student_boundaries": [],
            "current_stage": "index_completed",
            "percentage": 12.0,
            "timestamps": {
                **state.get("timestamps", {}),
                "index_at": datetime.now().isoformat(),
            },
        }

    try:
        from src.config.models import get_index_model
        from src.services.student_identification import StudentIdentificationService

        model_name = get_index_model()
        id_service = StudentIdentificationService(api_key=api_key, model_name=model_name)

        max_concurrency = int(os.getenv("INDEX_MAX_CONCURRENCY", "5"))
        semaphore = asyncio.Semaphore(max_concurrency)

        async def analyze_page(image_data: bytes, page_index: int):
            async with semaphore:
                return await id_service.analyze_page(image_data, page_index)

        tasks = [
            analyze_page(image_data, page_index)
            for page_index, image_data in enumerate(processed_images)
        ]
        page_analyses = await asyncio.gather(*tasks)
        page_analyses.sort(key=lambda x: x.page_index)

        segmentation_result = id_service.segment_from_analyses(page_analyses)

        def student_info_to_dict(info):
            if not info:
                return None
            return {
                "name": info.name,
                "student_id": info.student_id,
                "class_name": info.class_name,
                "confidence": info.confidence,
                "is_placeholder": getattr(info, "is_placeholder", False),
            }

        # page_index -> student mapping
        page_student_map = {}
        for mapping in segmentation_result.page_mappings:
            student_info = mapping.student_info
            student_key = student_info.student_id or student_info.name or f"unknown_{mapping.page_index}"
            page_student_map[mapping.page_index] = {
                "student_key": student_key,
                "student_info": student_info,
                "is_first_page": mapping.is_first_page,
            }

        page_index_contexts = {}
        index_pages = []
        student_groups = {}
        last_question = None

        for analysis in page_analyses:
            index_notes = []
            continuation_of = None

            if analysis.is_cover_page:
                index_notes.append("cover_page")
            else:
                if analysis.question_numbers:
                    last_question = analysis.question_numbers[-1]
                elif last_question:
                    continuation_of = last_question
                    index_notes.append("continuation_assumed")
                else:
                    index_notes.append("no_question_numbers_detected")

            mapping = page_student_map.get(analysis.page_index)
            student_info = mapping["student_info"] if mapping else analysis.student_info
            student_key = None
            if mapping:
                student_key = mapping["student_key"]
            elif student_info and (student_info.student_id or student_info.name):
                student_key = student_info.student_id or student_info.name
            else:
                student_key = "UNKNOWN"

            context = {
                "page_index": analysis.page_index,
                "question_numbers": analysis.question_numbers,
                "first_question": analysis.first_question,
                "continuation_of": continuation_of,
                "student_key": student_key,
                "student_info": student_info_to_dict(student_info),
                "is_cover_page": analysis.is_cover_page,
                "index_notes": index_notes,
                "is_first_page": mapping["is_first_page"] if mapping else False,
            }

            page_index_contexts[analysis.page_index] = context
            index_pages.append(context)

            if not analysis.is_cover_page:
                group = student_groups.setdefault(
                    student_key,
                    {"student_key": student_key, "student_info": student_info, "pages": []}
                )
                group["pages"].append(analysis.page_index)

        indexed_students = []
        student_boundaries = []
        for student_key, group in student_groups.items():
            pages = sorted(group["pages"])
            if not pages:
                continue
            info = group.get("student_info")
            info_dict = student_info_to_dict(info)
            confidence = info.confidence if info else 0.0
            needs_confirmation = (
                info is None or
                getattr(info, "is_placeholder", False) or
                confidence < 0.7
            )
            start_page = pages[0]
            end_page = pages[-1]

            student_boundaries.append({
                "student_key": student_key,
                "start_page": start_page,
                "end_page": end_page,
                "confidence": confidence,
                "needs_confirmation": needs_confirmation,
                "detection_method": "index",
            })

            indexed_students.append({
                "student_key": student_key,
                "student_id": info.student_id if info else None,
                "student_name": info.name if info else None,
                "start_page": start_page,
                "end_page": end_page,
                "pages": pages,
                "confidence": confidence,
                "needs_confirmation": needs_confirmation,
            })

        index_results = {
            "model": model_name,
            "total_pages": len(processed_images),
            "pages": index_pages,
            "students": indexed_students,
            "unidentified_pages": segmentation_result.unidentified_pages,
        }

        logger.info(
            f"[index] ç´¢å¼•å®Œæˆ: batch_id={batch_id}, "
            f"è¯†åˆ«å­¦ç”Ÿæ•°={len(indexed_students)}, æœªè¯†åˆ«é¡µæ•°={len(segmentation_result.unidentified_pages)}"
        )

        return {
            "index_results": index_results,
            "page_index_contexts": page_index_contexts,
            "student_page_map": {
                page_index: context["student_key"]
                for page_index, context in page_index_contexts.items()
            },
            "indexed_students": indexed_students,
            "index_unidentified_pages": segmentation_result.unidentified_pages,
            "student_boundaries": student_boundaries,
            "current_stage": "index_completed",
            "percentage": 12.0,
            "timestamps": {
                **state.get("timestamps", {}),
                "index_at": datetime.now().isoformat(),
            },
        }
    except Exception as e:
        logger.error(f"[index] ç´¢å¼•å¤±è´¥: {e}", exc_info=True)
        return {
            "index_results": {
                "model": None,
                "total_pages": len(processed_images),
                "pages": [],
                "students": [],
                "unidentified_pages": list(range(len(processed_images))),
                "error": str(e),
            },
            "page_index_contexts": {},
            "student_page_map": {},
            "indexed_students": [],
            "index_unidentified_pages": list(range(len(processed_images))),
            "student_boundaries": [],
            "current_stage": "index_completed",
            "percentage": 12.0,
            "errors": state.get("errors", []) + [{
                "node": "index",
                "error": str(e),
                "timestamp": datetime.now().isoformat(),
            }],
        }


async def rubric_parse_node(state: BatchGradingGraphState) -> Dict[str, Any]:
    """
    è§£æè¯„åˆ†æ ‡å‡†èŠ‚ç‚¹
    
    ä½¿ç”¨ä¸“é—¨çš„ RubricParserService è§£æè¯„åˆ†æ ‡å‡†å›¾åƒï¼Œ
    æ”¯æŒåˆ†æ‰¹å¤„ç†å¤šé¡µè¯„åˆ†æ ‡å‡†ï¼Œæå–å®Œæ•´çš„é¢˜ç›®ç»“æ„å’Œè¯„åˆ†ç»†åˆ™ã€‚
    
    **å…³é”®**: è§£æåçš„è¯„åˆ†æ ‡å‡†ä¼šæ³¨å†Œåˆ° RubricRegistryï¼Œä¾›åç»­æ‰¹æ”¹æ—¶é€šè¿‡
    GradingSkills.get_rubric_for_question åŠ¨æ€è·å–æŒ‡å®šé¢˜ç›®çš„è¯„åˆ†æ ‡å‡†ã€‚
    """
    batch_id = state["batch_id"]
    rubric_images = state.get("rubric_images", [])
    rubric_text = state.get("rubric", "")
    api_key = state.get("api_key") or os.getenv("GEMINI_API_KEY")
    
    logger.info(f"[rubric_parse] å¼€å§‹è§£æè¯„åˆ†æ ‡å‡†: batch_id={batch_id}, è¯„åˆ†æ ‡å‡†é¡µæ•°={len(rubric_images)}")
    
    parsed_rubric = {
        "total_questions": 0,
        "total_score": 0,
        "questions": []
    }
    
    # åˆ›å»º RubricRegistry ç”¨äºå­˜å‚¨è§£æåçš„è¯„åˆ†æ ‡å‡†
    from src.services.rubric_registry import RubricRegistry
    from src.models.grading_models import QuestionRubric, ScoringPoint, AlternativeSolution
    
    rubric_registry = RubricRegistry(total_score=105.0)  # é¢„æœŸæ€»åˆ†
    
    try:
        if rubric_images and api_key:
            # ä½¿ç”¨ä¸“é—¨çš„ RubricParserService è¿›è¡Œåˆ†æ‰¹è§£æ
            from src.services.rubric_parser import RubricParserService
            
            parser = RubricParserService(api_key=api_key)
            
            # è§£æè¯„åˆ†æ ‡å‡†ï¼ˆå†…éƒ¨ä¼šåˆ†æ‰¹å¤„ç†ï¼Œæ¯æ‰¹æœ€å¤š4é¡µï¼‰
            result = await parser.parse_rubric(
                rubric_images=rubric_images,
                expected_total_score=105  # é¢„æœŸæ€»åˆ†ï¼Œç”¨äºéªŒè¯
            )
            
            # è½¬æ¢ä¸ºå­—å…¸æ ¼å¼
            parsed_rubric = {
                "total_questions": result.total_questions,
                "total_score": result.total_score,
                "rubric_format": result.rubric_format,
                "general_notes": result.general_notes,
                "questions": [
                    {
                        "id": q.question_id,
                        "max_score": q.max_score,
                        "question_text": q.question_text,
                        "standard_answer": q.standard_answer,
                        "criteria": [sp.description for sp in q.scoring_points],
                        "scoring_points": [
                            {
                                "description": sp.description,
                                "score": sp.score,
                                "is_required": sp.is_required
                            }
                            for sp in q.scoring_points
                        ],
                        "alternative_solutions": [
                            {
                                "description": alt.description,
                                "scoring_criteria": alt.scoring_criteria,
                                "note": alt.note
                            }
                            for alt in q.alternative_solutions
                        ],
                        "grading_notes": q.grading_notes
                    }
                    for q in result.questions
                ]
            }
            
            # ğŸ”¥ å…³é”®ï¼šå°†è§£æçš„è¯„åˆ†æ ‡å‡†æ³¨å†Œåˆ° RubricRegistry
            # è¿™æ ·åç»­æ‰¹æ”¹æ—¶å¯ä»¥é€šè¿‡ GradingSkills.get_rubric_for_question è·å–
            rubric_registry.register_rubrics(result.questions)
            logger.info(
                f"[rubric_parse] å·²æ³¨å†Œ {len(result.questions)} é“é¢˜ç›®åˆ° RubricRegistry"
            )
            
            # åŒæ—¶ç”Ÿæˆæ ¼å¼åŒ–çš„è¯„åˆ†æ ‡å‡†ä¸Šä¸‹æ–‡ï¼ˆä¾›æ‰¹æ”¹ä½¿ç”¨ï¼‰
            rubric_context = parser.format_rubric_context(result)
            parsed_rubric["rubric_context"] = rubric_context
            
            logger.info(
                f"[rubric_parse] è¯„åˆ†æ ‡å‡†è§£ææˆåŠŸ: "
                f"é¢˜ç›®æ•°={result.total_questions}, æ€»åˆ†={result.total_score}"
            )
        
        elif rubric_text:
            # å¦‚æœæœ‰æ–‡æœ¬å½¢å¼çš„è¯„åˆ†æ ‡å‡†ï¼Œç®€å•è§£æ
            parsed_rubric["raw_text"] = rubric_text
            
    except Exception as e:
        logger.error(f"[rubric_parse] è¯„åˆ†æ ‡å‡†è§£æå¤±è´¥: {e}", exc_info=True)
        # é™çº§å¤„ç†ï¼šè¿”å›ç©ºçš„è¯„åˆ†æ ‡å‡†
        parsed_rubric = {
            "total_questions": 0,
            "total_score": 0,
            "questions": [],
            "error": str(e)
        }
    
    logger.info(
        f"[rubric_parse] è¯„åˆ†æ ‡å‡†è§£æå®Œæˆ: batch_id={batch_id}, "
        f"é¢˜ç›®æ•°={parsed_rubric.get('total_questions', 0)}, "
        f"æ€»åˆ†={parsed_rubric.get('total_score', 0)}"
    )
    
    # æ³¨æ„ï¼šä¸åºåˆ—åŒ– RubricRegistryï¼Œå› ä¸º grade_batch_node ä¼šä» parsed_rubric é‡å»º
    # è¿™æ ·å¯ä»¥é¿å…ç±»å‹è½¬æ¢é—®é¢˜
    
    return {
        "parsed_rubric": parsed_rubric,
        "current_stage": "rubric_parse_completed",
        "percentage": 15.0,
        "timestamps": {
            **state.get("timestamps", {}),
            "rubric_parse_at": datetime.now().isoformat()
        }
    }


def grading_fanout_router(state: BatchGradingGraphState) -> List[Send]:
    """
    æ‰¹æ”¹æ‰‡å‡ºè·¯ç”±
    
    å°†æ‰€æœ‰é¡µé¢åˆ†æ‰¹ï¼Œæ¯æ‰¹å¹¶è¡Œæ‰¹æ”¹ã€‚
    ä¸é¢„å…ˆåˆ†å‰²å­¦ç”Ÿï¼Œè€Œæ˜¯æ‰¹æ”¹æ‰€æœ‰é¡µé¢ã€‚
    æ”¯æŒå¯é…ç½®çš„æ‰¹æ¬¡å¤§å°ã€‚
    
    **å…³é”®**: ä½¿ç”¨æ·±æ‹·è´ç¡®ä¿ Worker ä¹‹é—´ä¸å…±äº«å¯å˜çŠ¶æ€ (Requirement 3.2)
    
    Requirements: 3.1, 3.2, 10.1
    """
    import copy
    
    batch_id = state["batch_id"]
    processed_images = state.get("processed_images", [])
    rubric = state.get("rubric", "")
    parsed_rubric = state.get("parsed_rubric", {})
    page_index_contexts = state.get("page_index_contexts", {})
    api_key = state.get("api_key", "")
    
    if not processed_images:
        logger.warning(f"[grading_fanout] æ²¡æœ‰å¾…æ‰¹æ”¹çš„å›¾åƒ: batch_id={batch_id}")
        return [Send("index_merge", state)]
    
    # è·å–æ‰¹æ¬¡é…ç½® (Requirements: 3.1, 10.1)
    config = get_batch_config()
    batch_size = config.batch_size
    max_retries = config.max_retries
    
    total_pages = len(processed_images)
    num_batches = (total_pages + batch_size - 1) // batch_size
    
    logger.info(
        f"[grading_fanout] åˆ›å»ºæ‰¹æ”¹ä»»åŠ¡: batch_id={batch_id}, "
        f"æ€»é¡µæ•°={total_pages}, æ‰¹æ¬¡æ•°={num_batches}, "
        f"æ‰¹æ¬¡å¤§å°={batch_size}, æœ€å¤§é‡è¯•={max_retries}"
    )
    
    sends = []
    for batch_idx in range(num_batches):
        start_idx = batch_idx * batch_size
        end_idx = min(start_idx + batch_size, total_pages)
        batch_images = processed_images[start_idx:end_idx]
        
        # åˆ›å»ºæ‰¹æ¬¡ä»»åŠ¡çŠ¶æ€ï¼ŒåŒ…å«é‡è¯•é…ç½® (Requirements: 3.3)
        # ğŸ”¥ å…³é”®ï¼šæ·±æ‹·è´ parsed_rubric ç¡®ä¿ Worker ç‹¬ç«‹æ€§ (Requirement 3.2)
        batch_contexts = {
            idx: page_index_contexts.get(idx)
            for idx in range(start_idx, end_idx)
            if idx in page_index_contexts
        }

        task_state = {
            "batch_id": batch_id,
            "batch_index": batch_idx,
            "total_batches": num_batches,
            "page_indices": list(range(start_idx, end_idx)),
            "images": batch_images,
            "rubric": rubric,
            "parsed_rubric": copy.deepcopy(parsed_rubric),  # æ·±æ‹·è´ï¼
            "page_index_contexts": copy.deepcopy(batch_contexts),
            "api_key": api_key,
            "retry_count": 0,
            "max_retries": max_retries,
        }
        
        sends.append(Send("grade_batch", task_state))
    
    return sends


async def grade_batch_node(state: Dict[str, Any]) -> Dict[str, Any]:
    """
    æ‰¹é‡æ‰¹æ”¹èŠ‚ç‚¹
    
    æ‰¹æ”¹ä¸€æ‰¹é¡µé¢ï¼Œè¿”å›æ¯é¡µçš„æ‰¹æ”¹ç»“æœã€‚
    
    **æ ¸å¿ƒæµç¨‹**:
    1. ä» parsed_rubric é‡å»º RubricRegistry
    2. åˆ›å»º GradingSkills å®ä¾‹
    3. æ‰¹æ”¹æ—¶è¯†åˆ«é¢˜ç›®ç¼–å·
    4. ä½¿ç”¨ GradingSkills.get_rubric_for_question è·å–è¯¥é¢˜ç›®çš„è¯„åˆ†æ ‡å‡†
    5. åŸºäºæŒ‡å®šè¯„åˆ†æ ‡å‡†è¿›è¡Œæ‰¹æ”¹
    
    ç‰¹æ€§ï¼š
    - Worker ç‹¬ç«‹æ€§ï¼šæ¯ä¸ª Worker ç‹¬ç«‹è·å–è¯„åˆ†æ ‡å‡†ï¼Œä¸å…±äº«å¯å˜çŠ¶æ€ (Req 3.2)
    - Agent Skill é›†æˆï¼šä½¿ç”¨ GradingSkills åŠ¨æ€è·å–é¢˜ç›®è¯„åˆ†æ ‡å‡† (Req 5.1)
    - æ‰¹æ¬¡å¤±è´¥é‡è¯•ï¼šå•æ‰¹æ¬¡å¤±è´¥ä¸å½±å“å…¶ä»–æ‰¹æ¬¡ï¼Œæ”¯æŒé‡è¯• (Req 3.3, 9.3)
    - è¿›åº¦æŠ¥å‘Šï¼šå®æ—¶æŠ¥å‘Šæ‰¹æ¬¡å¤„ç†è¿›åº¦ (Req 3.4)
    - é”™è¯¯éš”ç¦»ï¼šå•é¡µå¤±è´¥ä¸å½±å“å…¶ä»–é¡µé¢ï¼Œè®°å½•é”™è¯¯å¹¶ç»§ç»­å¤„ç† (Req 9.2)
    
    Requirements: 3.2, 3.3, 3.4, 5.1, 9.2, 9.3
    """
    batch_id = state["batch_id"]
    batch_index = state["batch_index"]
    total_batches = state["total_batches"]
    page_indices = state["page_indices"]
    images = state["images"]
    rubric = state.get("rubric", "")
    page_index_contexts = state.get("page_index_contexts", {})
    api_key = state.get("api_key") or os.getenv("GEMINI_API_KEY")
    retry_count = state.get("retry_count", 0)
    max_retries = state.get("max_retries", 2)
    
    logger.info(
        f"[grade_batch] å¼€å§‹æ‰¹æ”¹æ‰¹æ¬¡ {batch_index + 1}/{total_batches}: "
        f"batch_id={batch_id}, é¡µé¢={page_indices}, é‡è¯•æ¬¡æ•°={retry_count}"
    )
    
    page_results = []
    batch_error = None
    
    try:
        if not api_key:
            raise ValueError("API key æœªé…ç½®")
        
        # Worker ç‹¬ç«‹æ€§ä¿è¯ (Requirement 3.2)
        # æ¯ä¸ª Worker ç‹¬ç«‹åˆ›å»ºå®ä¾‹ï¼Œä¸å…±äº«å¯å˜çŠ¶æ€
        from src.services.gemini_reasoning import GeminiReasoningClient
        from src.utils.error_handling import execute_with_isolation, get_error_manager
        from src.services.rubric_registry import RubricRegistry
        from src.skills.grading_skills import GradingSkills, create_grading_skills, get_skill_registry
        from src.models.grading_models import QuestionRubric, ScoringPoint
        
        # ç‹¬ç«‹è·å–è¯„åˆ†æ ‡å‡†å‰¯æœ¬ï¼ˆä¸å…±äº«å¯å˜çŠ¶æ€ï¼‰
        parsed_rubric = state.get("parsed_rubric", {})
        import copy
        local_parsed_rubric = copy.deepcopy(parsed_rubric)
        
        # ğŸ”¥ å…³é”®ï¼šä» parsed_rubric é‡å»º RubricRegistry (Requirement 5.1)
        rubric_registry = RubricRegistry(
            total_score=local_parsed_rubric.get("total_score", 100.0)
        )
        
        # å°†è§£æçš„é¢˜ç›®æ³¨å†Œåˆ° Registry
        questions_data = local_parsed_rubric.get("questions", [])
        if questions_data:
            question_rubrics = []
            for q in questions_data:
                # æ„å»º ScoringPoint åˆ—è¡¨
                scoring_points = [
                    ScoringPoint(
                        description=sp.get("description", ""),
                        score=sp.get("score", 0),
                        is_required=sp.get("is_required", True)
                    )
                    for sp in q.get("scoring_points", [])
                ]
                
                # æ„å»º QuestionRubric
                question_rubric = QuestionRubric(
                    question_id=str(q.get("id", "")),
                    question_text=q.get("question_text", ""),
                    max_score=q.get("max_score", 0),
                    scoring_points=scoring_points,
                    standard_answer=q.get("standard_answer", ""),
                    grading_notes=q.get("grading_notes", ""),
                    alternative_solutions=[]  # ç®€åŒ–å¤„ç†
                )
                question_rubrics.append(question_rubric)
            
            rubric_registry.register_rubrics(question_rubrics)
            logger.info(
                f"[grade_batch] å·²é‡å»º RubricRegistryï¼Œæ³¨å†Œ {len(question_rubrics)} é“é¢˜ç›®"
            )
        
        # ğŸ”¥ åˆ›å»º GradingSkills å®ä¾‹ (Requirement 5.1)
        grading_skills = create_grading_skills(rubric_registry=rubric_registry)
        if page_index_contexts:
            grading_skills.page_index_contexts = page_index_contexts
        
        # åˆ›å»º GeminiReasoningClient å¹¶é›†æˆ GradingSkills
        reasoning_client = GeminiReasoningClient(
            api_key=api_key,
            rubric_registry=rubric_registry,
            grading_skills=grading_skills
        )
        
        # é”™è¯¯éš”ç¦»ï¼šå•é¡µå¤±è´¥ä¸å½±å“å…¶ä»–é¡µé¢ (Requirement 9.2)
        error_manager = get_error_manager()
        
        async def grade_single_page(page_data):
            """æ‰¹æ”¹å•é¡µï¼ˆå¸¦é”™è¯¯éš”ç¦»å’Œ Agent Skill é›†æˆï¼‰"""
            page_idx, image = page_data
            
            try:
                # ç›´æ¥ä½¿ç”¨ grade_page æ–¹æ³•æ‰¹æ”¹å•é¡µ
                # grade_page å†…éƒ¨ä¼šï¼š
                # 1. è¯†åˆ«é¢˜ç›®ç¼–å·
                # 2. é€šè¿‡ GradingSkills.get_rubric_for_question è·å–è¯„åˆ†æ ‡å‡†
                # 3. åŸºäºæŒ‡å®šè¯„åˆ†æ ‡å‡†è¿›è¡Œæ‰¹æ”¹
                page_context = None
                if page_index_contexts:
                    skill_result = await grading_skills.get_index_context_for_page(
                        page_index=page_idx,
                        page_index_contexts=page_index_contexts
                    )
                    if skill_result.success:
                        page_context = skill_result.data

                result = await reasoning_client.grade_page(
                    image=image,
                    rubric=rubric,
                    max_score=10.0,
                    parsed_rubric=local_parsed_rubric,
                    page_context=page_context,
                )

                # ğŸ”¥ å¯¹è¯†åˆ«åˆ°çš„æ¯é“é¢˜ç›®ï¼Œä½¿ç”¨ Agent Skill è·å–è¯„åˆ†æ ‡å‡†å¹¶è®°å½•
                question_numbers = result.get("question_numbers", [])
                if page_context:
                    if not question_numbers and page_context.get("question_numbers"):
                        question_numbers = page_context.get("question_numbers", [])
                        result["question_numbers"] = question_numbers
                    if not question_numbers and page_context.get("continuation_of"):
                        question_numbers = [page_context["continuation_of"]]
                        result["question_numbers"] = question_numbers
                    if not result.get("student_info") and page_context.get("student_info"):
                        result["student_info"] = page_context.get("student_info")
                    if page_context.get("is_cover_page") and not result.get("is_blank_page", False):
                        result["is_blank_page"] = True
                        result["score"] = 0.0
                        result["max_score"] = 0.0

                skill_logs = []
                
                for q_num in question_numbers:
                    # ä½¿ç”¨ GradingSkills è·å–è¯¥é¢˜ç›®çš„è¯„åˆ†æ ‡å‡†
                    skill_result = await grading_skills.get_rubric_for_question(
                        question_id=str(q_num),
                        registry=rubric_registry
                    )
                    
                    if skill_result.success and skill_result.data:
                        rubric_data = skill_result.data
                        skill_logs.append({
                            "question_id": q_num,
                            "skill_used": "get_rubric_for_question",
                            "is_default": rubric_data.is_default,
                            "confidence": rubric_data.confidence,
                            "max_score": rubric_data.rubric.max_score if rubric_data.rubric else 0,
                        })
                        logger.info(
                            f"[grade_batch] Agent Skill è·å–é¢˜ç›® {q_num} è¯„åˆ†æ ‡å‡†: "
                            f"is_default={rubric_data.is_default}, "
                            f"confidence={rubric_data.confidence:.2f}"
                        )
                
                # æ„å»ºå®Œæ•´çš„é¡µé¢ç»“æœ
                page_result = {
                    "page_index": page_idx,
                    "status": "completed",
                    "score": result.get("score", 0.0),
                    "max_score": result.get("max_score", 10.0),
                    "confidence": result.get("confidence", 0.0),
                    "feedback": result.get("feedback", ""),
                    "question_id": f"Q{page_idx}",
                    "question_numbers": question_numbers,
                    "question_details": result.get("question_details", []),
                    "page_summary": result.get("page_summary", ""),
                    "student_info": result.get("student_info"),
                    "is_blank_page": result.get("is_blank_page", False),
                    "revision_count": 0,
                    "batch_index": batch_index,
                    "skill_logs": skill_logs,  # è®°å½• Agent Skill è°ƒç”¨æ—¥å¿—
                }
                
                # æ›´è¯¦ç»†çš„æ—¥å¿—
                is_blank = result.get("is_blank_page", False)
                
                if is_blank:
                    logger.info(f"[grade_batch] é¡µé¢ {page_idx} æ˜¯ç©ºç™½é¡µ/å°é¢é¡µ")
                else:
                    logger.info(
                        f"[grade_batch] é¡µé¢ {page_idx} æ‰¹æ”¹å®Œæˆ: "
                        f"score={result.get('score', 0)}/{result.get('max_score', 0)}, "
                        f"é¢˜ç›®={question_numbers}, confidence={result.get('confidence', 0):.2f}, "
                        f"Agent Skills è°ƒç”¨={len(skill_logs)}æ¬¡"
                    )
                
                return page_result
                
            except Exception as e:
                # è®°å½•é”™è¯¯åˆ°å…¨å±€é”™è¯¯ç®¡ç†å™¨ (Requirement 9.5)
                error_manager.add_error(
                    exc=e,
                    context={
                        "batch_id": batch_id,
                        "batch_index": batch_index,
                        "page_index": page_idx,
                        "function": "grade_single_page",
                    },
                    batch_id=batch_id,
                    page_index=page_idx,
                )
                
                logger.error(
                    f"[grade_batch] é¡µé¢ {page_idx} æ‰¹æ”¹å¤±è´¥: {e}. "
                    f"é”™è¯¯å·²éš”ç¦»ï¼Œç»§ç»­å¤„ç†å…¶ä»–é¡µé¢ã€‚"
                )
                
                # è¿”å›å¤±è´¥ç»“æœï¼ˆä¸ä¸­æ–­æ‰¹æ¬¡ï¼‰
                return {
                    "page_index": page_idx,
                    "status": "failed",
                    "error": str(e),
                    "score": 0,
                    "max_score": 0,
                    "batch_index": batch_index,
                }
        
        # ä½¿ç”¨é”™è¯¯éš”ç¦»æ‰¹é‡å¤„ç†æ‰€æœ‰é¡µé¢ (Requirement 9.2)
        page_data_list = list(zip(page_indices, images))
        
        # å¹¶å‘å¤„ç†æ‰€æœ‰é¡µé¢ï¼ˆå¸¦é”™è¯¯éš”ç¦»ï¼‰
        from src.utils.error_handling import execute_batch_with_isolation
        
        isolated_results = await execute_batch_with_isolation(
            func=grade_single_page,
            items=page_data_list,
            error_log_context={
                "batch_id": batch_id,
                "batch_index": batch_index,
            }
        )
        
        # æ”¶é›†ç»“æœ
        for isolated_result in isolated_results:
            if isolated_result.is_success():
                page_results.append(isolated_result.get_result())
            else:
                # å¤±è´¥çš„é¡µé¢ä¹Ÿæ·»åŠ åˆ°ç»“æœä¸­ï¼ˆæ ‡è®°ä¸ºå¤±è´¥ï¼‰
                page_idx = page_data_list[isolated_result.index][0]
                page_results.append({
                    "page_index": page_idx,
                    "status": "failed",
                    "error": str(isolated_result.get_error()),
                    "score": 0,
                    "max_score": 0,
                    "batch_index": batch_index,
                })
    
    except Exception as e:
        batch_error = str(e)
        logger.error(f"[grade_batch] æ‰¹æ¬¡ {batch_index} æ‰¹æ”¹å¤±è´¥: {e}", exc_info=True)
        
        # è®°å½•æ‰¹æ¬¡çº§é”™è¯¯
        from src.utils.error_handling import get_error_manager
        error_manager = get_error_manager()
        error_manager.add_error(
            exc=e,
            context={
                "batch_id": batch_id,
                "batch_index": batch_index,
                "function": "grade_batch_node",
                "retry_count": retry_count,
            },
            batch_id=batch_id,
            retry_count=retry_count,
        )
        
        # æ‰¹æ¬¡å¤±è´¥é‡è¯•é€»è¾‘ (Requirements: 3.3, 9.3)
        if retry_count < max_retries:
            logger.info(
                f"[grade_batch] æ‰¹æ¬¡ {batch_index} å°†è¿›è¡Œé‡è¯• "
                f"({retry_count + 1}/{max_retries})"
            )
            # è¿”å›é‡è¯•æ ‡è®°ï¼Œè®©è°ƒåº¦å™¨é‡æ–°è°ƒåº¦
            return {
                "grading_results": [],
                "batch_retry_needed": {
                    "batch_index": batch_index,
                    "retry_count": retry_count + 1,
                    "error": batch_error,
                }
            }
        
        # æ‰€æœ‰é¡µé¢æ ‡è®°ä¸ºå¤±è´¥
        for page_idx in page_indices:
            page_results.append({
                "page_index": page_idx,
                "status": "failed",
                "error": batch_error,
                "score": 0,
                "max_score": 0,
                "batch_index": batch_index,
            })
    
    success_count = sum(1 for r in page_results if r['status'] == 'completed')
    failed_count = sum(1 for r in page_results if r['status'] == 'failed')
    total_score = sum(r.get('score', 0) for r in page_results if r['status'] == 'completed')
    
    # è¿›åº¦æŠ¥å‘Š (Requirement 3.4)
    progress_info = {
        "batch_index": batch_index,
        "total_batches": total_batches,
        "pages_processed": success_count,
        "pages_failed": failed_count,
        "total_score": total_score,
        "status": "completed" if failed_count == 0 else "partial",
        "timestamp": datetime.now().isoformat(),
    }
    
    logger.info(
        f"[grade_batch] æ‰¹æ¬¡ {batch_index + 1}/{total_batches} å®Œæˆ: "
        f"æˆåŠŸ={success_count}/{len(page_results)}, å¤±è´¥={failed_count}, æ€»åˆ†={total_score}"
    )
    
    # è¿”å›ç»“æœï¼ˆä½¿ç”¨ add reducer èšåˆï¼‰
    return {
        "grading_results": page_results,
        "batch_progress": progress_info,
    }


async def cross_page_merge_node(state: BatchGradingGraphState) -> Dict[str, Any]:
    """
    è·¨é¡µé¢˜ç›®åˆå¹¶èŠ‚ç‚¹
    
    åœ¨ç´¢å¼•èšåˆä¹‹å‰æ‰§è¡Œï¼Œè´Ÿè´£ï¼š
    1. æ£€æµ‹è·¨é¡µé¢˜ç›®
    2. åˆå¹¶è·¨é¡µé¢˜ç›®çš„è¯„åˆ†ç»“æœ
    3. ç¡®ä¿æ»¡åˆ†ä¸é‡å¤è®¡ç®—
    
    Requirements: 2.1, 4.2, 4.3
    """
    batch_id = state["batch_id"]
    grading_results = state.get("grading_results", [])
    
    logger.info(f"[cross_page_merge] å¼€å§‹è·¨é¡µé¢˜ç›®åˆå¹¶: batch_id={batch_id}")
    
    # å»é‡ï¼šç”±äºå¹¶è¡Œèšåˆå¯èƒ½å¯¼è‡´é‡å¤ï¼ŒæŒ‰ page_index å»é‡
    seen_pages = set()
    unique_results = []
    for result in grading_results:
        page_idx = result.get("page_index")
        if page_idx is not None and page_idx not in seen_pages:
            seen_pages.add(page_idx)
            unique_results.append(result)
    
    # æŒ‰é¡µç æ’åº
    unique_results.sort(key=lambda x: x.get("page_index", 0))
    grading_results = unique_results
    
    try:
        # å°†å­—å…¸æ ¼å¼è½¬æ¢ä¸º PageGradingResult å¯¹è±¡
        from src.models.grading_models import PageGradingResult, QuestionResult, ScoringPoint, ScoringPointResult
        
        page_results = []
        for result in grading_results:
            # è½¬æ¢ question_details ä¸º QuestionResult å¯¹è±¡
            question_results = []
            for q in result.get("question_details", []):
                # æ„å»ºå¾—åˆ†ç‚¹ç»“æœ
                scoring_point_results = []
                for sp in q.get("scoring_point_results", []):
                    scoring_point = ScoringPoint(
                        description=sp.get("description", ""),
                        score=sp.get("score", 0.0),
                        is_required=sp.get("is_required", False)
                    )
                    scoring_point_result = ScoringPointResult(
                        scoring_point=scoring_point,
                        awarded=sp.get("awarded", 0.0),
                        evidence=sp.get("evidence", "")
                    )
                    scoring_point_results.append(scoring_point_result)
                
                question_result = QuestionResult(
                    question_id=q.get("question_id", ""),
                    score=q.get("score", 0.0),
                    max_score=q.get("max_score", 0.0),
                    confidence=q.get("confidence", 1.0),
                    feedback=q.get("feedback", ""),
                    scoring_point_results=scoring_point_results,
                    page_indices=[result.get("page_index", 0)],
                    is_cross_page=False,
                    merge_source=None,
                    student_answer=q.get("student_answer", "")
                )
                question_results.append(question_result)
            
            page_result = PageGradingResult(
                page_index=result.get("page_index", 0),
                question_results=question_results,
                student_info=result.get("student_info"),
                is_blank_page=result.get("is_blank_page", False),
                raw_response=result.get("page_summary", "")
            )
            page_results.append(page_result)
        
        # ä½¿ç”¨ ResultMerger è¿›è¡Œè·¨é¡µåˆå¹¶
        from src.services.result_merger import ResultMerger
        
        merger = ResultMerger()
        merged_questions, cross_page_questions = merger.merge_cross_page_questions(page_results)
        
        # å°†åˆå¹¶åçš„ç»“æœè½¬æ¢å›å­—å…¸æ ¼å¼
        merged_question_dicts = []
        for q in merged_questions:
            merged_question_dicts.append({
                "question_id": q.question_id,
                "score": q.score,
                "max_score": q.max_score,
                "confidence": q.confidence,
                "feedback": q.feedback,
                "student_answer": q.student_answer,
                "is_cross_page": q.is_cross_page,
                "page_indices": q.page_indices,
                "merge_source": q.merge_source,
                "scoring_point_results": [
                    {
                        "description": spr.scoring_point.description,
                        "score": spr.scoring_point.score,
                        "is_required": spr.scoring_point.is_required,
                        "awarded": spr.awarded,
                        "evidence": spr.evidence
                    }
                    for spr in q.scoring_point_results
                ]
            })
        
        # è½¬æ¢è·¨é¡µé¢˜ç›®ä¿¡æ¯
        cross_page_info = []
        for cpq in cross_page_questions:
            cross_page_info.append({
                "question_id": cpq.question_id,
                "page_indices": cpq.page_indices,
                "confidence": cpq.confidence,
                "merge_reason": cpq.merge_reason
            })
        
        logger.info(
            f"[cross_page_merge] è·¨é¡µåˆå¹¶å®Œæˆ: batch_id={batch_id}, "
            f"æ£€æµ‹åˆ° {len(cross_page_questions)} ä¸ªè·¨é¡µé¢˜ç›®, "
            f"åˆå¹¶åå…± {len(merged_questions)} é“é¢˜ç›®"
        )
        
        return {
            "merged_questions": merged_question_dicts,
            "cross_page_questions": cross_page_info,
            "current_stage": "cross_page_merge_completed",
            "percentage": 75.0,
            "timestamps": {
                **state.get("timestamps", {}),
                "cross_page_merge_at": datetime.now().isoformat()
            }
        }
        
    except Exception as e:
        logger.error(f"[cross_page_merge] è·¨é¡µåˆå¹¶å¤±è´¥: {e}", exc_info=True)
        
        # é™çº§å¤„ç†ï¼šä¸è¿›è¡Œè·¨é¡µåˆå¹¶ï¼Œç›´æ¥ä¼ é€’åŸå§‹ç»“æœ
        return {
            "merged_questions": [],
            "cross_page_questions": [],
            "current_stage": "cross_page_merge_completed",
            "percentage": 75.0,
            "errors": state.get("errors", []) + [{
                "node": "cross_page_merge",
                "error": str(e),
                "timestamp": datetime.now().isoformat()
            }]
        }


async def index_merge_node(state: BatchGradingGraphState) -> Dict[str, Any]:
    """
    ç´¢å¼•å¯¹é½èšåˆèŠ‚ç‚¹

    ä½¿ç”¨ç´¢å¼•é˜¶æ®µç”Ÿæˆçš„å­¦ç”Ÿè¾¹ç•Œèšåˆæ‰¹æ”¹ç»“æœï¼Œæ›¿ä»£æ‰¹æ”¹åå­¦ç”Ÿåˆ†å‰²ã€‚
    """
    batch_id = state["batch_id"]
    grading_results = state.get("grading_results", [])
    merged_questions = state.get("merged_questions", [])
    student_boundaries = state.get("student_boundaries", []) or []
    indexed_students = state.get("indexed_students", []) or []
    student_page_map = state.get("student_page_map", {}) or {}

    # å»é‡ï¼šç”±äºå¹¶è¡Œèšåˆå¯èƒ½å¯¼è‡´é‡å¤ï¼ŒæŒ‰ page_index å»é‡
    seen_pages = set()
    unique_results = []
    for result in grading_results:
        page_idx = result.get("page_index")
        if page_idx is not None and page_idx not in seen_pages:
            seen_pages.add(page_idx)
            unique_results.append(result)

    # æŒ‰é¡µç æ’åº
    unique_results.sort(key=lambda x: x.get("page_index", 0))
    grading_results = unique_results

    # è¿‡æ»¤ç©ºç™½é¡µ
    non_blank_results = [r for r in grading_results if not r.get("is_blank_page", False)]

    if not student_boundaries and indexed_students:
        student_boundaries = [
            {
                "student_key": s.get("student_key"),
                "start_page": s.get("start_page", 0),
                "end_page": s.get("end_page", 0),
                "confidence": s.get("confidence", 0.0),
                "needs_confirmation": s.get("needs_confirmation", False),
                "detection_method": "index",
            }
            for s in indexed_students
        ]

    if not student_boundaries and student_page_map:
        grouped = {}
        for page_index, student_key in student_page_map.items():
            grouped.setdefault(student_key, []).append(page_index)
        for student_key, pages in grouped.items():
            pages_sorted = sorted(pages)
            student_boundaries.append({
                "student_key": student_key,
                "start_page": pages_sorted[0],
                "end_page": pages_sorted[-1],
                "confidence": 0.0,
                "needs_confirmation": True,
                "detection_method": "index",
            })

    if not student_boundaries:
        # æ— ç´¢å¼•è¾¹ç•Œæ—¶é™çº§ä¸ºå•å­¦ç”Ÿ
        fallback_key = "å­¦ç”ŸA"
        fallback_end = max(0, len(grading_results) - 1)
        student_boundaries = [{
            "student_key": fallback_key,
            "start_page": 0,
            "end_page": fallback_end,
            "confidence": 0.0,
            "needs_confirmation": True,
            "detection_method": "fallback",
        }]

    logger.info(
        f"[index_merge] å¼€å§‹èšåˆ: batch_id={batch_id}, "
        f"æ‰¹æ”¹ç»“æœæ•°={len(grading_results)}ï¼ˆå»é‡åï¼‰ï¼Œéç©ºç™½é¡µ={len(non_blank_results)}, "
        f"è¾¹ç•Œæ•°={len(student_boundaries)}, åˆå¹¶åé¢˜ç›®æ•°={len(merged_questions)}"
    )

    try:
        student_info_by_key = {
            s.get("student_key"): s for s in indexed_students
        }

        student_results = []
        for boundary in student_boundaries:
            student_pages = [
                r for r in grading_results
                if boundary["start_page"] <= r.get("page_index", -1) <= boundary["end_page"]
            ]

            if merged_questions:
                student_questions = []
                for q in merged_questions:
                    q_pages = q.get("page_indices", [])
                    if any(boundary["start_page"] <= p <= boundary["end_page"] for p in q_pages):
                        student_questions.append(q)

                total_score = sum(q.get("score", 0) for q in student_questions)
                max_total_score = sum(q.get("max_score", 0) for q in student_questions)
                all_question_details = student_questions
            else:
                valid_pages = [
                    r for r in student_pages
                    if r.get("status") == "completed" and not r.get("is_blank_page", False)
                ]
                total_score = sum(r.get("score", 0) for r in valid_pages)
                max_total_score = sum(r.get("max_score", 0) for r in valid_pages)

                all_question_details = []
                for page in valid_pages:
                    for q in page.get("question_details", []):
                        all_question_details.append({
                            "question_id": q.get("question_id", ""),
                            "score": q.get("score", 0),
                            "max_score": q.get("max_score", 0),
                            "feedback": q.get("feedback", ""),
                            "student_answer": q.get("student_answer", ""),
                            "is_correct": q.get("is_correct", False)
                        })

            student_key = boundary["student_key"]
            info = student_info_by_key.get(student_key, {})

            student_results.append({
                "student_key": student_key,
                "student_id": info.get("student_id"),
                "student_name": info.get("student_name"),
                "start_page": boundary["start_page"],
                "end_page": boundary["end_page"],
                "total_score": total_score,
                "max_total_score": max_total_score,
                "page_results": student_pages,
                "question_details": all_question_details,
                "confidence": boundary.get("confidence", 0.0),
                "needs_confirmation": boundary.get("needs_confirmation", False),
            })

        logger.info(
            f"[index_merge] èšåˆå®Œæˆ: batch_id={batch_id}, å­¦ç”Ÿæ•°={len(student_boundaries)}"
        )

        return {
            "student_boundaries": student_boundaries,
            "student_results": student_results,
            "current_stage": "index_merge_completed",
            "percentage": 80.0,
            "timestamps": {
                **state.get("timestamps", {}),
                "index_merge_at": datetime.now().isoformat()
            }
        }

    except Exception as e:
        logger.error(f"[index_merge] èšåˆå¤±è´¥: {e}", exc_info=True)

        # é™çº§å¤„ç†ï¼šå°†æ‰€æœ‰é¡µé¢è§†ä¸ºä¸€ä¸ªå­¦ç”Ÿ
        if merged_questions:
            total_score = sum(q.get("score", 0) for q in merged_questions)
            max_total_score = sum(q.get("max_score", 0) for q in merged_questions)
            all_question_details = merged_questions
        else:
            valid_results = [
                r for r in grading_results
                if r.get("status") == "completed" and not r.get("is_blank_page", False)
            ]
            total_score = sum(r.get("score", 0) for r in valid_results)
            max_total_score = sum(r.get("max_score", 0) for r in valid_results)

            all_question_details = []
            for page in valid_results:
                for q in page.get("question_details", []):
                    all_question_details.append({
                        "question_id": q.get("question_id", ""),
                        "score": q.get("score", 0),
                        "max_score": q.get("max_score", 0),
                        "feedback": q.get("feedback", ""),
                        "student_answer": q.get("student_answer", ""),
                        "is_correct": q.get("is_correct", False)
                    })

        fallback_student_key = "å­¦ç”ŸA"
        fallback_student_id = "FALLBACK_001"

        fallback_end = max(0, len(grading_results) - 1)
        return {
            "student_boundaries": [{
                "student_key": fallback_student_key,
                "start_page": 0,
                "end_page": fallback_end,
                "confidence": 0.0,
                "needs_confirmation": True
            }],
            "student_results": [{
                "student_key": fallback_student_key,
                "student_id": fallback_student_id,
                "total_score": total_score,
                "max_total_score": max_total_score,
                "page_results": grading_results,
                "question_details": all_question_details,
                "confidence": 0.0,
                "needs_confirmation": True
            }],
            "current_stage": "index_merge_completed",
            "percentage": 80.0,
            "errors": state.get("errors", []) + [{
                "node": "index_merge",
                "error": str(e),
                "timestamp": datetime.now().isoformat()
            }]
        }


async def segment_node(state: BatchGradingGraphState) -> Dict[str, Any]:
    """
    å­¦ç”Ÿåˆ†å‰²èŠ‚ç‚¹
    
    åŸºäºæ‰¹æ”¹ç»“æœæ™ºèƒ½åˆ¤æ–­å­¦ç”Ÿè¾¹ç•Œã€‚
    è¿™æ˜¯åœ¨æ‰¹æ”¹å®Œæˆåè¿›è¡Œçš„ï¼Œåˆ©ç”¨æ‰¹æ”¹ç»“æœä¸­çš„é¢˜ç›®ä¿¡æ¯å’Œå­¦ç”Ÿæ ‡è¯†ã€‚
    ä½¿ç”¨åˆå¹¶åçš„é¢˜ç›®ç»“æœï¼ˆå¦‚æœæœ‰ï¼‰ã€‚
    
    Requirements: 4.1, 4.3
    """
    batch_id = state["batch_id"]
    grading_results = state.get("grading_results", [])
    merged_questions = state.get("merged_questions", [])
    
    # å»é‡ï¼šç”±äºå¹¶è¡Œèšåˆå¯èƒ½å¯¼è‡´é‡å¤ï¼ŒæŒ‰ page_index å»é‡
    seen_pages = set()
    unique_results = []
    for result in grading_results:
        page_idx = result.get("page_index")
        if page_idx is not None and page_idx not in seen_pages:
            seen_pages.add(page_idx)
            unique_results.append(result)
    
    # æŒ‰é¡µç æ’åº
    unique_results.sort(key=lambda x: x.get("page_index", 0))
    grading_results = unique_results
    
    # è¿‡æ»¤æ‰ç©ºç™½é¡µ
    non_blank_results = [r for r in grading_results if not r.get("is_blank_page", False)]
    
    logger.info(
        f"[segment] å¼€å§‹å­¦ç”Ÿåˆ†å‰²: batch_id={batch_id}, "
        f"æ‰¹æ”¹ç»“æœæ•°={len(grading_results)}ï¼ˆå»é‡åï¼‰ï¼Œéç©ºç™½é¡µ={len(non_blank_results)}, "
        f"åˆå¹¶åé¢˜ç›®æ•°={len(merged_questions)}"
    )
    
    try:
        from src.services.student_boundary_detector import StudentBoundaryDetector
        
        detector = StudentBoundaryDetector()
        
        # åŸºäºæ‰¹æ”¹ç»“æœæ£€æµ‹å­¦ç”Ÿè¾¹ç•Œ
        result = await detector.detect_boundaries(grading_results)
        
        # è½¬æ¢ä¸ºå­—å…¸æ ¼å¼
        boundaries = []
        for b in result.boundaries:
            boundaries.append({
                "student_key": b.student_key,
                "start_page": b.start_page,
                "end_page": b.end_page,
                "confidence": b.confidence,
                "needs_confirmation": b.needs_confirmation,
                "detection_method": b.detection_method
            })
        
        # æŒ‰å­¦ç”Ÿèšåˆæ‰¹æ”¹ç»“æœ
        student_results = []
        for boundary in boundaries:
            student_pages = [
                r for r in grading_results
                if boundary["start_page"] <= r.get("page_index", -1) <= boundary["end_page"]
            ]
            
            # å¦‚æœæœ‰åˆå¹¶åçš„é¢˜ç›®ç»“æœï¼Œä½¿ç”¨å®ƒä»¬
            if merged_questions:
                # ç­›é€‰å±äºè¯¥å­¦ç”Ÿçš„é¢˜ç›®ï¼ˆåŸºäºé¡µé¢èŒƒå›´ï¼‰
                student_questions = []
                for q in merged_questions:
                    # æ£€æŸ¥é¢˜ç›®çš„é¡µé¢ç´¢å¼•æ˜¯å¦åœ¨å­¦ç”ŸèŒƒå›´å†…
                    q_pages = q.get("page_indices", [])
                    if any(boundary["start_page"] <= p <= boundary["end_page"] for p in q_pages):
                        student_questions.append(q)
                
                # è®¡ç®—æ€»åˆ†ï¼ˆä½¿ç”¨åˆå¹¶åçš„é¢˜ç›®ï¼Œé¿å…é‡å¤è®¡ç®—ï¼‰
                total_score = sum(q.get("score", 0) for q in student_questions)
                max_total_score = sum(q.get("max_score", 0) for q in student_questions)
                
                all_question_details = student_questions
            else:
                # é™çº§ï¼šä½¿ç”¨åŸå§‹é¡µé¢ç»“æœ
                valid_pages = [r for r in student_pages if r.get("status") == "completed" and not r.get("is_blank_page", False)]
                
                total_score = sum(r.get("score", 0) for r in valid_pages)
                max_total_score = sum(r.get("max_score", 0) for r in valid_pages)
                
                # æ”¶é›†æ‰€æœ‰é¢˜ç›®è¯¦æƒ…
                all_question_details = []
                for page in valid_pages:
                    for q in page.get("question_details", []):
                        all_question_details.append({
                            "question_id": q.get("question_id", ""),
                            "score": q.get("score", 0),
                            "max_score": q.get("max_score", 0),
                            "feedback": q.get("feedback", ""),
                            "student_answer": q.get("student_answer", ""),
                            "is_correct": q.get("is_correct", False)
                        })
            
            student_results.append({
                "student_key": boundary["student_key"],
                "start_page": boundary["start_page"],
                "end_page": boundary["end_page"],
                "total_score": total_score,
                "max_total_score": max_total_score,
                "page_results": student_pages,
                "question_details": all_question_details,
                "confidence": boundary["confidence"],
                "needs_confirmation": boundary["needs_confirmation"]
            })
        
        logger.info(
            f"[segment] å­¦ç”Ÿåˆ†å‰²å®Œæˆ: batch_id={batch_id}, "
            f"æ£€æµ‹åˆ° {len(boundaries)} åå­¦ç”Ÿ"
        )
        
        return {
            "student_boundaries": boundaries,
            "student_results": student_results,
            "current_stage": "segment_completed",
            "percentage": 80.0,
            "timestamps": {
                **state.get("timestamps", {}),
                "segment_at": datetime.now().isoformat()
            }
        }
        
    except Exception as e:
        logger.error(f"[segment] å­¦ç”Ÿåˆ†å‰²å¤±è´¥: {e}", exc_info=True)
        
        # é™çº§å¤„ç†ï¼šå°†æ‰€æœ‰é¡µé¢è§†ä¸ºä¸€ä¸ªå­¦ç”Ÿ
        if merged_questions:
            # ä½¿ç”¨åˆå¹¶åçš„é¢˜ç›®
            total_score = sum(q.get("score", 0) for q in merged_questions)
            max_total_score = sum(q.get("max_score", 0) for q in merged_questions)
            all_question_details = merged_questions
        else:
            # ä½¿ç”¨åŸå§‹é¡µé¢ç»“æœ
            valid_results = [r for r in grading_results if r.get("status") == "completed" and not r.get("is_blank_page", False)]
            total_score = sum(r.get("score", 0) for r in valid_results)
            max_total_score = sum(r.get("max_score", 0) for r in valid_results)
            
            # æ”¶é›†æ‰€æœ‰é¢˜ç›®è¯¦æƒ…
            all_question_details = []
            for page in valid_results:
                for q in page.get("question_details", []):
                    all_question_details.append({
                        "question_id": q.get("question_id", ""),
                        "score": q.get("score", 0),
                        "max_score": q.get("max_score", 0),
                        "feedback": q.get("feedback", ""),
                        "student_answer": q.get("student_answer", ""),
                        "is_correct": q.get("is_correct", False)
                    })
        
        # ä½¿ç”¨å”¯ä¸€çš„å­¦ç”Ÿæ ‡è¯†
        fallback_student_key = "å­¦ç”ŸA"
        fallback_student_id = "FALLBACK_001"
        
        return {
            "student_boundaries": [{
                "student_key": fallback_student_key,
                "start_page": 0,
                "end_page": len(grading_results) - 1,
                "confidence": 0.0,
                "needs_confirmation": True
            }],
            "student_results": [{
                "student_key": fallback_student_key,
                "student_id": fallback_student_id,
                "total_score": total_score,
                "max_total_score": max_total_score,
                "page_results": grading_results,
                "question_details": all_question_details,
                "confidence": 0.0,
                "needs_confirmation": True
            }],
            "current_stage": "segment_completed",
            "percentage": 80.0,
            "errors": state.get("errors", []) + [{
                "node": "segment",
                "error": str(e),
                "timestamp": datetime.now().isoformat()
            }]
        }


async def review_node(state: BatchGradingGraphState) -> Dict[str, Any]:
    """
    ç»“æœå®¡æ ¸èŠ‚ç‚¹
    
    æ±‡æ€»å®¡æ ¸æ‰¹æ”¹ç»“æœï¼Œæ ‡è®°éœ€è¦äººå·¥ç¡®è®¤çš„é¡¹ç›®ã€‚
    """
    batch_id = state["batch_id"]
    student_results = state.get("student_results", [])
    student_boundaries = state.get("student_boundaries", [])
    
    logger.info(f"[review] å¼€å§‹ç»“æœå®¡æ ¸: batch_id={batch_id}")
    
    # ç»Ÿè®¡éœ€è¦ç¡®è®¤çš„è¾¹ç•Œ
    needs_confirmation = [b for b in student_boundaries if b.get("needs_confirmation")]
    
    # ç»Ÿè®¡ä½ç½®ä¿¡åº¦ç»“æœ
    low_confidence_results = []
    for student in student_results:
        for page_result in student.get("page_results", []):
            if page_result.get("confidence", 1.0) < 0.7:
                low_confidence_results.append({
                    "student_key": student["student_key"],
                    "page_index": page_result.get("page_index"),
                    "confidence": page_result.get("confidence")
                })
    
    review_summary = {
        "total_students": len(student_results),
        "boundaries_need_confirmation": len(needs_confirmation),
        "low_confidence_count": len(low_confidence_results),
        "low_confidence_results": low_confidence_results[:10]  # æœ€å¤šæ˜¾ç¤º10ä¸ª
    }
    
    logger.info(
        f"[review] å®¡æ ¸å®Œæˆ: batch_id={batch_id}, "
        f"å­¦ç”Ÿæ•°={review_summary['total_students']}, "
        f"å¾…ç¡®è®¤è¾¹ç•Œ={review_summary['boundaries_need_confirmation']}"
    )
    
    return {
        "review_summary": review_summary,
        "current_stage": "review_completed",
        "percentage": 90.0,
        "timestamps": {
            **state.get("timestamps", {}),
            "review_at": datetime.now().isoformat()
        }
    }


async def export_node(state: BatchGradingGraphState) -> Dict[str, Any]:
    """
    å¯¼å‡ºç»“æœèŠ‚ç‚¹
    
    æŒä¹…åŒ–ç»“æœå¹¶å‡†å¤‡å¯¼å‡ºæ•°æ®ã€‚
    æ”¯æŒæ— æ•°æ®åº“æ¨¡å¼ä¸‹å¯¼å‡ºç»“æœä¸º JSON æ–‡ä»¶ã€‚
    æ”¯æŒéƒ¨åˆ†ç»“æœä¿å­˜ï¼šä¸å¯æ¢å¤é”™è¯¯æ—¶ä¿å­˜å·²å®Œæˆç»“æœã€‚
    
    Requirements: 9.4, 11.4
    """
    batch_id = state["batch_id"]
    student_results = state.get("student_results", [])
    cross_page_questions = state.get("cross_page_questions", [])
    merged_questions = state.get("merged_questions", [])
    grading_results = state.get("grading_results", [])
    
    logger.info(f"[export] å¼€å§‹å¯¼å‡ºç»“æœ: batch_id={batch_id}, å­¦ç”Ÿæ•°={len(student_results)}")
    
    # æ£€æŸ¥æ˜¯å¦æœ‰å¤±è´¥çš„é¡µé¢
    failed_pages = [r for r in grading_results if r.get("status") == "failed"]
    has_failures = len(failed_pages) > 0
    
    if has_failures:
        logger.warning(
            f"[export] æ£€æµ‹åˆ° {len(failed_pages)} ä¸ªå¤±è´¥é¡µé¢ï¼Œ"
            f"å°†ä¿å­˜éƒ¨åˆ†ç»“æœ"
        )
    
    # å°è¯•æŒä¹…åŒ–åˆ°æ•°æ®åº“
    persisted = False
    try:
        from src.utils.database import get_db_pool
        
        db_pool = await get_db_pool()
        if db_pool:
            # TODO: å®é™…çš„æŒä¹…åŒ–é€»è¾‘
            persisted = True
            logger.info(f"[export] ç»“æœå·²æŒä¹…åŒ–åˆ°æ•°æ®åº“: batch_id={batch_id}")
    except Exception as e:
        logger.warning(f"[export] æ•°æ®åº“æŒä¹…åŒ–å¤±è´¥ï¼ˆç¦»çº¿æ¨¡å¼ï¼‰: {e}")
    
    # å‡†å¤‡å¯¼å‡ºæ•°æ®
    export_data = {
        "batch_id": batch_id,
        "export_time": datetime.now().isoformat(),
        "persisted": persisted,
        "has_failures": has_failures,
        "failed_pages_count": len(failed_pages),
        "cross_page_questions": cross_page_questions,
        "merged_questions": merged_questions,
        "students": []
    }
    
    # æ·»åŠ å¤±è´¥é¡µé¢ä¿¡æ¯ï¼ˆç”¨äºéƒ¨åˆ†ç»“æœä¿å­˜ï¼‰
    if has_failures:
        export_data["failed_pages"] = [
            {
                "page_index": p.get("page_index"),
                "error": p.get("error"),
                "batch_index": p.get("batch_index"),
            }
            for p in failed_pages
        ]
    
    for student in student_results:
        # è®¡ç®—ç™¾åˆ†æ¯”
        total_score = student.get("total_score", 0)
        max_score = student.get("max_total_score", 0)
        percentage = (total_score / max_score * 100) if max_score > 0 else 0
        
        # æ”¶é›†é¢˜ç›®ç»“æœ
        question_results = []
        
        # ä¼˜å…ˆä½¿ç”¨ question_details
        if student.get("question_details"):
            for q in student["question_details"]:
                question_results.append({
                    "question_id": q.get("question_id", ""),
                    "score": q.get("score", 0),
                    "max_score": q.get("max_score", 0),
                    "feedback": q.get("feedback", ""),
                    "student_answer": q.get("student_answer", ""),
                    "is_correct": q.get("is_correct", False),
                    "is_cross_page": q.get("is_cross_page", False),
                    "page_indices": q.get("page_indices", []),
                    "confidence": q.get("confidence", 1.0)
                })
        # å¦åˆ™ä» page_results æå–
        elif student.get("page_results"):
            for page in student["page_results"]:
                if page.get("status") == "completed" and not page.get("is_blank_page", False):
                    for q in page.get("question_details", []):
                        question_results.append({
                            "question_id": q.get("question_id", ""),
                            "score": q.get("score", 0),
                            "max_score": q.get("max_score", 0),
                            "feedback": q.get("feedback", ""),
                            "student_answer": q.get("student_answer", ""),
                            "is_correct": q.get("is_correct", False)
                        })
        
        export_data["students"].append({
            "student_name": student["student_key"],
            "student_id": student.get("student_id"),
            "score": total_score,
            "max_score": max_score,
            "percentage": round(percentage, 1),
            "question_results": question_results,
            "confidence": student.get("confidence", 0),
            "needs_confirmation": student.get("needs_confirmation", False),
            "start_page": student.get("start_page", 0),
            "end_page": student.get("end_page", 0)
        })
    
    # å¯¼å‡ºä¸º JSON æ–‡ä»¶ (Requirements: 9.4, 11.4)
    # æ— æ•°æ®åº“æ¨¡å¼æˆ–æœ‰å¤±è´¥æ—¶éƒ½å¯¼å‡º
    if not persisted or has_failures:
        try:
            import json
            import os
            
            # åˆ›å»ºå¯¼å‡ºç›®å½•
            export_dir = os.getenv("EXPORT_DIR", "./exports")
            os.makedirs(export_dir, exist_ok=True)
            
            # ç”Ÿæˆæ–‡ä»¶å
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            
            # å¦‚æœæœ‰å¤±è´¥ï¼Œæ ‡è®°ä¸ºéƒ¨åˆ†ç»“æœ (Requirement 9.4)
            if has_failures:
                filename = f"partial_result_{batch_id}_{timestamp}.json"
                logger.info(
                    f"[export] ä¿å­˜éƒ¨åˆ†ç»“æœï¼ˆ{len(failed_pages)} ä¸ªé¡µé¢å¤±è´¥ï¼‰: {filename}"
                )
            else:
                filename = f"grading_result_{batch_id}_{timestamp}.json"
            
            filepath = os.path.join(export_dir, filename)
            
            # å†™å…¥ JSON æ–‡ä»¶
            with open(filepath, 'w', encoding='utf-8') as f:
                json.dump(export_data, f, ensure_ascii=False, indent=2)
            
            export_data["json_file"] = filepath
            
            if has_failures:
                logger.warning(
                    f"[export] éƒ¨åˆ†ç»“æœå·²ä¿å­˜: {filepath}. "
                    f"å®Œæˆ={len(grading_results) - len(failed_pages)}/{len(grading_results)} é¡µ"
                )
            else:
                logger.info(f"[export] ç»“æœå·²å¯¼å‡ºä¸º JSON: {filepath}")
            
        except Exception as e:
            logger.error(f"[export] JSON å¯¼å‡ºå¤±è´¥: {e}", exc_info=True)
            export_data["json_export_error"] = str(e)
            
            # è®°å½•é”™è¯¯
            from src.utils.error_handling import get_error_manager
            error_manager = get_error_manager()
            error_manager.add_error(
                exc=e,
                context={
                    "batch_id": batch_id,
                    "function": "export_node",
                    "export_type": "json",
                },
                batch_id=batch_id,
            )
    
    # å¯¼å‡ºé”™è¯¯æ—¥å¿—ï¼ˆå¦‚æœæœ‰é”™è¯¯ï¼‰
    try:
        from src.utils.error_handling import get_error_manager
        error_manager = get_error_manager()
        
        batch_errors = error_manager.get_errors_by_batch(batch_id)
        if batch_errors:
            import os
            
            export_dir = os.getenv("EXPORT_DIR", "./exports")
            os.makedirs(export_dir, exist_ok=True)
            
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            error_log_file = os.path.join(
                export_dir,
                f"error_log_{batch_id}_{timestamp}.json"
            )
            
            error_manager.export_to_file(error_log_file)
            export_data["error_log_file"] = error_log_file
            
            logger.info(
                f"[export] é”™è¯¯æ—¥å¿—å·²å¯¼å‡º: {error_log_file} "
                f"({len(batch_errors)} ä¸ªé”™è¯¯)"
            )
    except Exception as e:
        logger.error(f"[export] é”™è¯¯æ—¥å¿—å¯¼å‡ºå¤±è´¥: {e}", exc_info=True)
    
    logger.info(
        f"[export] å¯¼å‡ºå®Œæˆ: batch_id={batch_id}, "
        f"å­¦ç”Ÿæ•°={len(export_data['students'])}, "
        f"è·¨é¡µé¢˜ç›®æ•°={len(cross_page_questions)}, "
        f"å¤±è´¥é¡µé¢æ•°={len(failed_pages)}"
    )
    
    return {
        "export_data": export_data,
        "current_stage": "completed",
        "percentage": 100.0,
        "timestamps": {
            **state.get("timestamps", {}),
            "export_at": datetime.now().isoformat(),
            "completed_at": datetime.now().isoformat()
        }
    }


# ==================== Graph ç¼–è¯‘ ====================

def create_batch_grading_graph(
    checkpointer: Optional[AsyncPostgresSaver] = None,
    batch_config: Optional[BatchConfig] = None,
) -> StateGraph:
    """åˆ›å»ºæ‰¹é‡æ‰¹æ”¹ Graph
    
    å·¥ä½œæµï¼š
    1. intake: æ¥æ”¶æ–‡ä»¶
    2. preprocess: å›¾åƒé¢„å¤„ç†
    3. index: æ‰¹æ”¹å‰ç´¢å¼•ï¼ˆé¢˜ç›®ä¿¡æ¯ + å­¦ç”Ÿè¯†åˆ«ï¼‰
    4. rubric_parse: è§£æè¯„åˆ†æ ‡å‡†
    5. grade_batch (å¹¶è¡Œ): å¯é…ç½®åˆ†æ‰¹æ‰¹æ”¹æ‰€æœ‰é¡µé¢
    6. cross_page_merge: è·¨é¡µé¢˜ç›®åˆå¹¶
    7. index_merge: åŸºäºç´¢å¼•èšåˆå­¦ç”Ÿç»“æœ
    8. review: ç»“æœå®¡æ ¸
    9. export: å¯¼å‡ºç»“æœ
    
    æµç¨‹å›¾ï¼š
    ```
    intake
      â†“
    preprocess
      â†“
    index
      â†“
    rubric_parse
      â†“
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚ grade_batch (N) â”‚  â† å¹¶è¡Œæ‰¹æ”¹ï¼ˆå¯é…ç½®æ‰¹æ¬¡å¤§å°ï¼‰
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
      â†“
    cross_page_merge  â† è·¨é¡µé¢˜ç›®åˆå¹¶
      â†“
    index_merge  â† åŸºäºç´¢å¼•èšåˆ
      â†“
    review
      â†“
    export
      â†“
    END
    ```
    
    ç‰¹æ€§ï¼š
    - å¯é…ç½®æ‰¹æ¬¡å¤§å° (Requirements: 3.1, 10.1)
    - Worker ç‹¬ç«‹æ€§ä¿è¯ (Requirements: 3.2)
    - æ‰¹æ¬¡å¤±è´¥é‡è¯• (Requirements: 3.3, 9.3)
    - å®æ—¶è¿›åº¦æŠ¥å‘Š (Requirements: 3.4)
    - è·¨é¡µé¢˜ç›®åˆå¹¶ (Requirements: 2.1, 4.2, 4.3)
    
    Args:
        checkpointer: PostgreSQL Checkpointerï¼ˆå¯é€‰ï¼‰
        batch_config: æ‰¹æ¬¡é…ç½®ï¼ˆå¯é€‰ï¼Œé»˜è®¤ä»ç¯å¢ƒå˜é‡åŠ è½½ï¼‰
        
    Returns:
        ç¼–è¯‘åçš„ Graph
    """
    # è®¾ç½®æ‰¹æ¬¡é…ç½®
    if batch_config:
        set_batch_config(batch_config)
    
    config = get_batch_config()
    logger.info(
        f"åˆ›å»ºæ‰¹é‡æ‰¹æ”¹ Graph: batch_size={config.batch_size}, "
        f"max_workers={config.max_concurrent_workers}, "
        f"max_retries={config.max_retries}"
    )
    
    graph = StateGraph(BatchGradingGraphState)
    
    # æ·»åŠ èŠ‚ç‚¹
    graph.add_node("intake", intake_node)
    graph.add_node("preprocess", preprocess_node)
    graph.add_node("index", index_node)
    graph.add_node("rubric_parse", rubric_parse_node)
    graph.add_node("grade_batch", grade_batch_node)
    graph.add_node("cross_page_merge", cross_page_merge_node)
    graph.add_node("index_merge", index_merge_node)
    graph.add_node("review", review_node)
    graph.add_node("export", export_node)
    
    # å…¥å£ç‚¹
    graph.set_entry_point("intake")
    
    # çº¿æ€§æµç¨‹ï¼šintake â†’ preprocess â†’ rubric_parse
    graph.add_edge("intake", "preprocess")
    graph.add_edge("preprocess", "index")
    graph.add_edge("index", "rubric_parse")
    
    # rubric_parse åæ‰‡å‡ºåˆ°å¹¶è¡Œæ‰¹æ”¹
    graph.add_conditional_edges(
        "rubric_parse",
        grading_fanout_router,
        ["grade_batch", "cross_page_merge", "index_merge"]
    )
    
    # å¹¶è¡Œæ‰¹æ”¹åèšåˆåˆ° cross_page_merge
    graph.add_edge("grade_batch", "cross_page_merge")
    
    # cross_page_merge â†’ index_merge â†’ review â†’ export â†’ END
    graph.add_edge("cross_page_merge", "index_merge")
    graph.add_edge("index_merge", "review")
    graph.add_edge("review", "export")
    graph.add_edge("export", END)
    
    # ç¼–è¯‘
    compile_kwargs = {}
    if checkpointer:
        compile_kwargs["checkpointer"] = checkpointer
    
    compiled_graph = graph.compile(**compile_kwargs)
    
    logger.info("æ‰¹é‡æ‰¹æ”¹ Graph å·²ç¼–è¯‘")
    
    return compiled_graph


# ==================== å¯¼å‡º ====================

__all__ = [
    # é…ç½®ç±»
    "BatchConfig",
    "get_batch_config",
    "set_batch_config",
    # è¿›åº¦ç±»
    "BatchProgress",
    "BatchTaskState",
    # èŠ‚ç‚¹å‡½æ•°
    "intake_node",
    "preprocess_node",
    "index_node",
    "rubric_parse_node",
    "grade_batch_node",
    "cross_page_merge_node",
    "index_merge_node",
    "review_node",
    "export_node",
    # è·¯ç”±å‡½æ•°
    "grading_fanout_router",
    # Graph åˆ›å»º
    "create_batch_grading_graph",
]
