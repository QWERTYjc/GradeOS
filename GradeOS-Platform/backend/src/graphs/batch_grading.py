import logging
import os
import asyncio
import json
import re
from functools import lru_cache
from typing import Optional, List, Dict, Any, Literal, Tuple
from datetime import datetime
from dataclasses import dataclass, field

from langgraph.graph import StateGraph, END
from langgraph.types import Send, interrupt
from langgraph.checkpoint.postgres.aio import AsyncPostgresSaver
from langchain_core.runnables import RunnableConfig, RunnableLambda

from src.graphs.state import BatchGradingGraphState
from src.utils.llm_thinking import split_thinking_content


logger = logging.getLogger(__name__)


@lru_cache(maxsize=1)
def _get_broadcast_progress():
    """å»¶è¿ŸåŠ è½½è¿›åº¦å¹¿æ’­å‡½æ•°ï¼Œé¿å…æµ‹è¯•åœºæ™¯è§¦å‘é‡ä¾èµ–å¯¼å…¥ã€‚"""
    if os.getenv("DISABLE_PROGRESS_BROADCAST", "false").strip().lower() in (
        "1",
        "true",
        "yes",
    ):

        async def _noop(*_args, **_kwargs) -> None:
            return None

        return _noop

    from src.api.routes.batch_langgraph import broadcast_progress

    return broadcast_progress


async def _broadcast_progress(batch_id: str, message: Dict[str, Any]) -> None:
    """åŒ…è£…è¿›åº¦å¹¿æ’­ï¼Œä¾¿äºŽæµ‹è¯•ä¸­ç¦ç”¨ã€‚"""
    await _get_broadcast_progress()(batch_id, message)


# ==================== æ‰¹æ¬¡é…ç½® ====================


@dataclass
class BatchConfig:
    """
    æ‰¹æ¬¡é…ç½®ç±»

    æ”¯æŒé…ç½®æ‰¹æ¬¡å¤§å°å’Œå¹¶å‘æ•°é‡ã€‚

    Requirements: 3.1, 10.1
    """

    batch_size: int = 1000  # æ¯æ‰¹å¤„ç†çš„é¡µé¢æ•°é‡ (è§£é™¤é™åˆ¶)
    max_concurrent_workers: int = 5  # æœ€å¤§å¹¶å‘ Worker æ•°é‡
    max_retries: int = 2  # æ‰¹æ¬¡å¤±è´¥æœ€å¤§é‡è¯•æ¬¡æ•°
    retry_delay: float = 1.0  # é‡è¯•å»¶è¿Ÿï¼ˆç§’ï¼‰

    @classmethod
    def from_env(cls) -> "BatchConfig":
        """ä»ŽçŽ¯å¢ƒå˜é‡åŠ è½½é…ç½®"""
        return cls(
            batch_size=int(os.getenv("GRADING_BATCH_SIZE", "1000")),
            max_concurrent_workers=int(os.getenv("GRADING_MAX_WORKERS", "5")),
            max_retries=int(os.getenv("GRADING_MAX_RETRIES", "2")),
            retry_delay=float(os.getenv("GRADING_RETRY_DELAY", "1.0")),
        )


# å…¨å±€æ‰¹æ¬¡é…ç½®
_batch_config: Optional[BatchConfig] = None


def get_batch_config() -> BatchConfig:
    """èŽ·å–æ‰¹æ¬¡é…ç½®"""
    global _batch_config
    if _batch_config is None:
        _batch_config = BatchConfig.from_env()
    return _batch_config


def set_batch_config(config: BatchConfig) -> None:
    """è®¾ç½®æ‰¹æ¬¡é…ç½®"""
    global _batch_config
    _batch_config = config
    logger.info(
        f"æ‰¹æ¬¡é…ç½®å·²æ›´æ–°: batch_size={config.batch_size}, "
        f"max_workers={config.max_concurrent_workers}, "
        f"max_retries={config.max_retries}"
    )


# ==================== è¿›åº¦æŠ¥å‘Š ====================


@dataclass
class BatchProgress:
    """
    æ‰¹æ¬¡è¿›åº¦ä¿¡æ¯

    Requirements: 3.4
    """

    batch_id: str
    total_batches: int
    completed_batches: int = 0
    failed_batches: int = 0
    in_progress_batches: int = 0
    total_pages: int = 0
    processed_pages: int = 0
    failed_pages: int = 0
    current_stage: str = "initialized"
    percentage: float = 0.0
    batch_details: Dict[int, Dict[str, Any]] = field(default_factory=dict)
    timestamps: Dict[str, str] = field(default_factory=dict)

    def update_batch_status(
        self,
        batch_index: int,
        status: str,
        pages_processed: int = 0,
        pages_failed: int = 0,
        error: Optional[str] = None,
    ) -> None:
        """æ›´æ–°å•ä¸ªæ‰¹æ¬¡çŠ¶æ€"""
        self.batch_details[batch_index] = {
            "status": status,
            "pages_processed": pages_processed,
            "pages_failed": pages_failed,
            "error": error,
            "updated_at": datetime.now().isoformat(),
        }

        # é‡æ–°è®¡ç®—ç»Ÿè®¡
        self.completed_batches = sum(
            1 for d in self.batch_details.values() if d["status"] == "completed"
        )
        self.failed_batches = sum(1 for d in self.batch_details.values() if d["status"] == "failed")
        self.in_progress_batches = sum(
            1 for d in self.batch_details.values() if d["status"] == "in_progress"
        )
        self.processed_pages = sum(d["pages_processed"] for d in self.batch_details.values())
        self.failed_pages = sum(d["pages_failed"] for d in self.batch_details.values())

        # è®¡ç®—ç™¾åˆ†æ¯”ï¼ˆæ‰¹æ”¹é˜¶æ®µå  15%-80%ï¼‰
        if self.total_batches > 0:
            batch_progress = self.completed_batches / self.total_batches
            self.percentage = 15.0 + batch_progress * 65.0

    def to_dict(self) -> Dict[str, Any]:
        """åºåˆ—åŒ–ä¸ºå­—å…¸"""
        return {
            "batch_id": self.batch_id,
            "total_batches": self.total_batches,
            "completed_batches": self.completed_batches,
            "failed_batches": self.failed_batches,
            "in_progress_batches": self.in_progress_batches,
            "total_pages": self.total_pages,
            "processed_pages": self.processed_pages,
            "failed_pages": self.failed_pages,
            "current_stage": self.current_stage,
            "percentage": self.percentage,
            "batch_details": self.batch_details,
            "timestamps": self.timestamps,
        }


# è¿›åº¦æŠ¥å‘Šå›žè°ƒç±»åž‹
ProgressCallback = Optional[callable]


# ==================== æ‰¹æ¬¡ä»»åŠ¡çŠ¶æ€ ====================


@dataclass
class BatchTaskState:
    """
    å•ä¸ªæ‰¹æ¬¡ä»»åŠ¡çš„çŠ¶æ€

    ç”¨äºŽè·Ÿè¸ªæ‰¹æ¬¡æ‰§è¡ŒçŠ¶æ€å’Œæ”¯æŒé‡è¯•ã€‚

    Requirements: 3.3, 9.3
    """

    batch_id: str
    batch_index: int
    total_batches: int
    page_indices: List[int]
    images: List[str]
    rubric: str
    parsed_rubric: Dict[str, Any]
    api_key: str
    page_index_contexts: Dict[int, Dict[str, Any]] = field(default_factory=dict)
    retry_count: int = 0
    max_retries: int = 2
    status: str = "pending"  # pending, in_progress, completed, failed
    error: Optional[str] = None
    results: List[Dict[str, Any]] = field(default_factory=list)


# ==================== èŠ‚ç‚¹å®žçŽ° ====================


async def intake_node(state: BatchGradingGraphState) -> Dict[str, Any]:
    """
    æŽ¥æ”¶æ–‡ä»¶èŠ‚ç‚¹

    éªŒè¯è¾“å…¥æ–‡ä»¶ï¼Œå‡†å¤‡å¤„ç†çŽ¯å¢ƒã€‚
    """
    batch_id = state["batch_id"]

    logger.info(f"[intake] å¼€å§‹æŽ¥æ”¶æ–‡ä»¶: batch_id={batch_id}")

    # éªŒè¯å¿…è¦çš„è¾“å…¥
    answer_images = state.get("answer_images", [])
    rubric_images = state.get("rubric_images", [])

    if not answer_images:
        raise ValueError("æœªæä¾›ç­”é¢˜å›¾åƒ")

    logger.info(
        f"[intake] æ–‡ä»¶æŽ¥æ”¶å®Œæˆ: batch_id={batch_id}, "
        f"ç­”é¢˜é¡µæ•°={len(answer_images)}, è¯„åˆ†æ ‡å‡†é¡µæ•°={len(rubric_images)}"
    )

    return {
        "current_stage": "intake_completed",
        "percentage": 5.0,
        "timestamps": {**state.get("timestamps", {}), "intake_at": datetime.now().isoformat()},
    }


async def preprocess_node(state: BatchGradingGraphState) -> Dict[str, Any]:
    """
    å›¾åƒé¢„å¤„ç†èŠ‚ç‚¹

    å¯¹å›¾åƒè¿›è¡Œé¢„å¤„ç†ï¼š
    1. è½¬æ¢ä¸º JPEG æ ¼å¼
    2. åŽ‹ç¼©è´¨é‡æŽ§åˆ¶
    3. åŽ»å™ªã€å¢žå¼ºã€æ—‹è½¬æ ¡æ­£ç­‰ï¼ˆTODOï¼‰
    """
    batch_id = state["batch_id"]
    answer_images = state.get("answer_images", [])

    logger.info(f"[preprocess] å¼€å§‹å›¾åƒé¢„å¤„ç†: batch_id={batch_id}, é¡µæ•°={len(answer_images)}")

    # è½¬æ¢ä¸º JPEG æ ¼å¼
    processed_images = []
    for idx, img_bytes in enumerate(answer_images):
        try:
            from PIL import Image
            import io

            # æ‰“å¼€å›¾åƒ
            img = Image.open(io.BytesIO(img_bytes))

            # è½¬æ¢ä¸º RGBï¼ˆJPEG ä¸æ”¯æŒ RGBA å’Œ P æ¨¡å¼ï¼‰
            if img.mode in ("RGBA", "P", "LA"):
                # åˆ›å»ºç™½è‰²èƒŒæ™¯
                background = Image.new("RGB", img.size, (255, 255, 255))
                if img.mode == "P":
                    img = img.convert("RGBA")
                if img.mode in ("RGBA", "LA"):
                    background.paste(img, mask=img.split()[-1])  # ä½¿ç”¨ alpha é€šé“ä½œä¸º mask
                    img = background
                else:
                    img = img.convert("RGB")
            elif img.mode != "RGB":
                img = img.convert("RGB")

            # ä¿å­˜ä¸º JPEG
            output = io.BytesIO()
            img.save(output, format="JPEG", quality=85, optimize=True)
            processed_images.append(output.getvalue())

            logger.debug(
                f"[preprocess] é¡µé¢ {idx} è½¬æ¢ä¸º JPEG: {len(img_bytes)} -> {len(output.getvalue())} bytes"
            )
        except Exception as e:
            logger.warning(f"[preprocess] é¡µé¢ {idx} JPEG è½¬æ¢å¤±è´¥: {e}ï¼Œä½¿ç”¨åŽŸå›¾")
            processed_images.append(img_bytes)

    logger.info(
        f"[preprocess] å›¾åƒé¢„å¤„ç†å®Œæˆ: batch_id={batch_id}, JPEGè½¬æ¢={len(processed_images)}/{len(answer_images)}"
    )

    student_boundaries = _build_student_boundaries(state, len(processed_images))

    return {
        "processed_images": processed_images,
        "student_boundaries": student_boundaries,
        "current_stage": "preprocess_completed",
        "percentage": 10.0,
        "timestamps": {**state.get("timestamps", {}), "preprocess_at": datetime.now().isoformat()},
    }


def _coerce_int(value: Any) -> Optional[int]:
    try:
        return int(value)
    except (TypeError, ValueError):
        return None


def _first_not_none(*values: Any) -> Any:
    for value in values:
        if value is not None:
            return value
    return None


def _sanitize_pages(raw_pages: Any, total_pages: int) -> List[int]:
    if not isinstance(raw_pages, (list, tuple)):
        return []
    cleaned = []
    for item in raw_pages:
        idx = _coerce_int(item)
        if idx is None:
            continue
        if 0 <= idx < total_pages:
            cleaned.append(idx)
    return sorted(set(cleaned))


def _normalize_manual_boundaries(raw: Any, total_pages: int) -> List[Dict[str, Any]]:
    if not raw:
        return []

    if isinstance(raw, str):
        try:
            raw = json.loads(raw)
        except Exception:
            return []

    if isinstance(raw, dict):
        for key in ("boundaries", "students", "start_pages", "start_indices"):
            if key in raw:
                raw = raw[key]
                break
        else:
            raw = []

    if isinstance(raw, list) and raw and all(not isinstance(x, (list, dict)) for x in raw):
        start_indices = _sanitize_pages(raw, total_pages)
        if 0 not in start_indices:
            start_indices.insert(0, 0)
        groups = []
        for idx, start in enumerate(start_indices):
            end = start_indices[idx + 1] - 1 if idx + 1 < len(start_indices) else total_pages - 1
            if end < start:
                continue
            groups.append(
                {
                    "pages": list(range(start, end + 1)),
                    "start_page": start,
                    "end_page": end,
                }
            )
        return groups

    if not isinstance(raw, list):
        return []

    groups = []
    for entry in raw:
        if isinstance(entry, list):
            pages = _sanitize_pages(entry, total_pages)
            if pages:
                groups.append({"pages": pages})
            continue
        if not isinstance(entry, dict):
            continue

        pages = entry.get("pages") or entry.get("page_indices") or entry.get("pageIndices")
        if pages is None:
            start = _first_not_none(
                entry.get("start_page"),
                entry.get("startPage"),
                entry.get("start"),
            )
            end = _first_not_none(
                entry.get("end_page"),
                entry.get("endPage"),
                entry.get("end"),
            )
            start_idx = _coerce_int(start) if start is not None else None
            end_idx = _coerce_int(end) if end is not None else None
            if start_idx is not None and end_idx is not None:
                pages = list(range(start_idx, end_idx + 1))

        pages = _sanitize_pages(pages, total_pages) if pages is not None else []
        if not pages:
            continue

        group = {"pages": pages}
        student_key = entry.get("student_key") or entry.get("studentKey")
        if student_key:
            group["student_key"] = str(student_key)
        student_id = entry.get("student_id") or entry.get("studentId")
        if student_id:
            group["student_id"] = str(student_id)
        student_name = entry.get("student_name") or entry.get("studentName") or entry.get("name")
        if student_name:
            group["student_name"] = str(student_name)
        class_name = entry.get("class_name") or entry.get("className")
        if class_name:
            group["class_name"] = str(class_name)
        groups.append(group)

    return groups


def _build_student_boundaries(
    state: BatchGradingGraphState, total_pages: int
) -> List[Dict[str, Any]]:
    inputs = state.get("inputs", {})
    manual_boundaries = _normalize_manual_boundaries(inputs.get("manual_boundaries"), total_pages)
    student_mapping = state.get("student_mapping") or inputs.get("student_mapping")
    student_boundaries: List[Dict[str, Any]] = []

    if student_mapping and isinstance(student_mapping, list):
        for idx, mapping in enumerate(student_mapping):
            pages = (
                mapping.get("pages") or mapping.get("page_indices") or mapping.get("pageIndices")
            )
            pages = _sanitize_pages(pages, total_pages) if pages is not None else []
            if not pages:
                start_idx = _first_not_none(
                    mapping.get("start_index"),
                    mapping.get("startIndex"),
                    mapping.get("start_page"),
                    mapping.get("startPage"),
                )
                end_idx = _first_not_none(
                    mapping.get("end_index"),
                    mapping.get("endIndex"),
                    mapping.get("end_page"),
                    mapping.get("endPage"),
                )
                start_page = _coerce_int(start_idx) if start_idx is not None else None
                end_page = _coerce_int(end_idx) if end_idx is not None else None
                if start_page is not None and end_page is not None:
                    pages = _sanitize_pages(list(range(start_page, end_page + 1)), total_pages)
            if not pages:
                continue

            student_name = mapping.get("student_name") or mapping.get("studentName")
            student_id = mapping.get("student_id") or mapping.get("studentId")
            student_key = (
                mapping.get("student_key")
                or mapping.get("studentKey")
                or student_name
                or student_id
                or f"å­¦ç”Ÿ{idx + 1}"
            )
            student_boundaries.append(
                {
                    "student_key": student_key,
                    "student_id": student_id,
                    "student_name": student_name,
                    "start_page": min(pages),
                    "end_page": max(pages),
                    "pages": sorted(pages),
                }
            )
    if not student_boundaries and manual_boundaries:
        for idx, boundary in enumerate(manual_boundaries):
            pages = boundary.get("pages") or boundary.get("page_indices") or boundary.get(
                "pageIndices"
            )
            pages = _sanitize_pages(pages, total_pages) if pages is not None else []
            if not pages:
                start_page = _first_not_none(
                    boundary.get("start_page"),
                    boundary.get("startPage"),
                    boundary.get("start"),
                )
                end_page = _first_not_none(
                    boundary.get("end_page"),
                    boundary.get("endPage"),
                    boundary.get("end"),
                )
                start_idx = _coerce_int(start_page) if start_page is not None else None
                end_idx = _coerce_int(end_page) if end_page is not None else None
                if start_idx is not None and end_idx is not None:
                    pages = _sanitize_pages(list(range(start_idx, end_idx + 1)), total_pages)
            if not pages:
                continue
            merged = dict(boundary)
            merged["pages"] = sorted(pages)
            merged.setdefault("start_page", pages[0])
            merged.setdefault("end_page", pages[-1])
            if "student_key" not in merged:
                merged["student_key"] = f"å­¦ç”Ÿ{idx + 1}"
            student_boundaries.append(merged)

    return student_boundaries


async def rubric_parse_node(state: BatchGradingGraphState) -> Dict[str, Any]:
    """
    è§£æžè¯„åˆ†æ ‡å‡†èŠ‚ç‚¹

    ä½¿ç”¨ä¸“é—¨çš„ RubricParserService è§£æžè¯„åˆ†æ ‡å‡†å›¾åƒï¼Œ
    æ”¯æŒåˆ†æ‰¹å¤„ç†å¤šé¡µè¯„åˆ†æ ‡å‡†ï¼Œæå–å®Œæ•´çš„é¢˜ç›®ç»“æž„å’Œè¯„åˆ†ç»†åˆ™ã€‚

    **å…³é”®**: è§£æžåŽçš„è¯„åˆ†æ ‡å‡†ä¼šæ³¨å†Œåˆ° RubricRegistryï¼Œä¾›åŽç»­æ‰¹æ”¹æ—¶é€šè¿‡
    GradingSkills.get_rubric_for_question åŠ¨æ€èŽ·å–æŒ‡å®šé¢˜ç›®çš„è¯„åˆ†æ ‡å‡†ã€‚
    """
    batch_id = state["batch_id"]
    rubric_images = state.get("rubric_images", [])
    rubric_text = state.get("rubric", "")
    api_key = state.get("api_key") or os.getenv("LLM_API_KEY") or os.getenv("OPENROUTER_API_KEY")

    logger.info(
        f"[rubric_parse] å¼€å§‹è§£æžè¯„åˆ†æ ‡å‡†: batch_id={batch_id}, è¯„åˆ†æ ‡å‡†é¡µæ•°={len(rubric_images)}"
    )

    # ðŸ” è¯Šæ–­æ—¥å¿—ï¼šæ£€æŸ¥ rubric_images æ˜¯å¦ä¼ å…¥
    if rubric_images:
        logger.info(f"[rubric_parse] ðŸ“¸ rubric_images è¯¦æƒ…: å…± {len(rubric_images)} é¡µ")
        for i, img in enumerate(rubric_images):
            if isinstance(img, bytes):
                logger.info(f"[rubric_parse]   - ç¬¬ {i+1} é¡µ: {len(img)} bytes")
            else:
                logger.warning(f"[rubric_parse]   - ç¬¬ {i+1} é¡µ: ç±»åž‹å¼‚å¸¸ {type(img)}")
    else:
        logger.warning(f"[rubric_parse] âš ï¸ rubric_images ä¸ºç©ºï¼è¯·æ£€æŸ¥å‰ç«¯æ˜¯å¦æ­£ç¡®ä¸Šä¼ äº†æ‰¹æ”¹æ ‡å‡†")

    parsed_rubric = {"total_questions": 0, "total_score": 0, "questions": []}

    # åˆ›å»º RubricRegistry ç”¨äºŽå­˜å‚¨è§£æžåŽçš„è¯„åˆ†æ ‡å‡†
    from src.services.rubric_registry import RubricRegistry
    from src.models.grading_models import QuestionRubric, ScoringPoint, AlternativeSolution

    rubric_registry = RubricRegistry()

    try:
        if rubric_images and api_key:
            # ä½¿ç”¨ä¸“é—¨çš„ RubricParserService è¿›è¡Œåˆ†æ‰¹è§£æž
            from src.services.rubric_parser import RubricParserService

            parser = RubricParserService(api_key=api_key)

            # æµå¼è¾“å‡ºå›žè°ƒ - å‘é€ llm_stream_chunk äº‹ä»¶åˆ°å‰ç«¯
            parse_agent_id = "rubric-parse"
            review_agent_id = "rubric-review"
            parse_agent_name = "Rubric Parse"
            review_agent_name = "Rubric Review"

            await _broadcast_progress(
                batch_id,
                {
                    "type": "agent_update",
                    "agentId": parse_agent_id,
                    "agentName": parse_agent_name,
                    "agentLabel": parse_agent_name,
                    "parentNodeId": "rubric_parse",
                    "status": "running",
                    "progress": 0,
                    "message": "Preparing rubric parse",
                },
            )

            async def stream_callback(stream_type: str, chunk: str) -> None:
                phase = "parse"
                real_type = stream_type

                parts = stream_type.split(":")
                if len(parts) >= 3:
                    phase = parts[1]
                    real_type = ":".join(parts[2:])
                elif len(parts) == 2:
                    real_type = parts[1]

                target_node = "rubric_parse"
                target_agent = parse_agent_id
                node_name = parse_agent_name

                if phase == "review":
                    target_node = "rubric_review"
                    target_agent = review_agent_id
                    node_name = review_agent_name

                await _broadcast_progress(
                    batch_id,
                    {
                        "type": "llm_stream_chunk",
                        "nodeId": target_node,
                        "agentId": target_agent,
                        "nodeName": node_name,
                        "streamType": real_type,
                        "chunk": chunk,
                    },
                )

            async def progress_callback(
                batch_index: int,
                total_batches: int,
                status: str,
                message: Optional[str],
            ) -> None:

                normalized_total = max(1, total_batches)
                batch_progress = int(((batch_index + 1) / normalized_total) * 100)
                is_last_batch = (batch_index + 1) >= normalized_total

                if status == "reviewing":
                    await _broadcast_progress(
                        batch_id,
                        {
                            "type": "agent_update",
                            "agentId": parse_agent_id,
                            "agentName": parse_agent_name,
                            "agentLabel": parse_agent_name,
                            "parentNodeId": "rubric_parse",
                            "status": "completed" if is_last_batch else "running",
                            "progress": 100 if is_last_batch else batch_progress,
                            "message": (
                                "Parsing completed"
                                if is_last_batch
                                else (message or f"Batch {batch_index + 1}/{total_batches}")
                            ),
                        },
                    )
                    await _broadcast_progress(
                        batch_id,
                        {
                            "type": "agent_update",
                            "agentId": review_agent_id,
                            "agentName": review_agent_name,
                            "agentLabel": review_agent_name,
                            "parentNodeId": "rubric_review",
                            "status": "running",
                            "progress": 0,
                            "message": message or "Reviewing...",
                        },
                    )
                    return

                if status == "completed":
                    await _broadcast_progress(
                        batch_id,
                        {
                            "type": "agent_update",
                            "agentId": parse_agent_id,
                            "agentName": parse_agent_name,
                            "agentLabel": parse_agent_name,
                            "parentNodeId": "rubric_parse",
                            "status": "completed",
                            "progress": 100,
                            "message": message or "Parsing completed",
                        },
                    )
                    return

                status_map = {
                    "parsing": "running",
                    "running": "running",
                    "failed": "failed",
                }
                progress = 100 if status == "failed" else batch_progress

                await _broadcast_progress(
                    batch_id,
                    {
                        "type": "agent_update",
                        "agentId": parse_agent_id,
                        "agentName": parse_agent_name,
                        "agentLabel": parse_agent_name,
                        "parentNodeId": "rubric_parse",
                        "status": status_map.get(status, "running"),
                        "progress": progress,
                        "message": message or f"Batch {batch_index + 1}/{total_batches}",
                    },
                )

            # æ·»åŠ è¶…æ—¶ä¿æŠ¤ï¼Œé»˜è®¤ 10 åˆ†é’Ÿï¼ˆè¯„åˆ†æ ‡å‡†å¯èƒ½å¾ˆé•¿ï¼‰
            rubric_parse_timeout = int(os.getenv("RUBRIC_PARSE_TIMEOUT", "600"))
            try:
                result = await asyncio.wait_for(
                    parser.parse_rubric(
                        rubric_images=rubric_images,
                        progress_callback=progress_callback,
                        stream_callback=stream_callback,
                    ),
                    timeout=rubric_parse_timeout,
                )
            except asyncio.TimeoutError:
                logger.error(f"[rubric_parse] è§£æžè¶…æ—¶ï¼ˆ{rubric_parse_timeout}ç§’ï¼‰ï¼Œbatch_id={batch_id}")
                await _broadcast_progress(
                    batch_id,
                    {
                        "type": "workflow_error",
                        "error": f"è¯„åˆ†æ ‡å‡†è§£æžè¶…æ—¶ï¼ˆ{rubric_parse_timeout}ç§’ï¼‰ï¼Œè¯·å°è¯•å‡å°‘è¯„åˆ†æ ‡å‡†é¡µæ•°æˆ–ç¨åŽé‡è¯•",
                        "stage": "rubric_parse",
                    },
                )
                raise Exception(f"Rubric parse timeout after {rubric_parse_timeout}s")

            # è½¬æ¢ä¸ºå­—å…¸æ ¼å¼
            parsed_rubric = {
                "total_questions": result.total_questions,
                "total_score": result.total_score,
                "rubric_format": result.rubric_format,
                "general_notes": result.general_notes,
                "questions": [
                    {
                        "id": q.question_id,
                        "question_id": q.question_id,
                        "max_score": q.max_score,
                        "question_text": q.question_text,
                        "standard_answer": q.standard_answer,
                        "source_pages": getattr(q, "source_pages", []),
                        "criteria": [sp.description for sp in q.scoring_points],
                        "scoring_points": [
                            {
                                "point_id": sp.point_id or f"{q.question_id}.{idx + 1}",
                                "description": sp.description,
                                "score": sp.score,
                                "is_required": sp.is_required,
                                "keywords": sp.keywords or [],
                                "expected_value": sp.expected_value,
                            }
                            for idx, sp in enumerate(q.scoring_points)
                        ],
                        "alternative_solutions": [
                            {
                                "description": alt.description,
                                "scoring_criteria": alt.scoring_criteria,
                                "note": alt.note,
                            }
                            for alt in q.alternative_solutions
                        ],
                        "deduction_rules": [
                            {
                                "rule_id": dr.rule_id or f"{q.question_id}.d{idx + 1}",
                                "description": dr.description,
                                "deduction": dr.deduction,
                                "conditions": dr.conditions,
                            }
                            for idx, dr in enumerate(getattr(q, "deduction_rules", []) or [])
                        ],
                        "grading_notes": q.grading_notes,
                    }
                    for q in result.questions
                ],
            }

            # ðŸ”¥ å…³é”®ï¼šå°†è§£æžçš„è¯„åˆ†æ ‡å‡†æ³¨å†Œåˆ° RubricRegistry
            # è¿™æ ·åŽç»­æ‰¹æ”¹æ—¶å¯ä»¥é€šè¿‡ GradingSkills.get_rubric_for_question èŽ·å–
            rubric_registry.register_rubrics(result.questions)
            logger.info(f"[rubric_parse] å·²æ³¨å†Œ {len(result.questions)} é“é¢˜ç›®åˆ° RubricRegistry")

            # åŒæ—¶ç”Ÿæˆæ ¼å¼åŒ–çš„è¯„åˆ†æ ‡å‡†ä¸Šä¸‹æ–‡ï¼ˆä¾›æ‰¹æ”¹ä½¿ç”¨ï¼‰
            rubric_context = parser.format_rubric_context(result)
            parsed_rubric["rubric_context"] = rubric_context

            # ç”Ÿæˆè‡ªç™½æŠ¥å‘Š
            inputs_dict = state.get("inputs", {}) or {}
            expected_question_count = inputs_dict.get("expected_question_count")
            expected_total_score = inputs_dict.get("expected_total_score")

            parse_self_report = parser._generate_parse_self_report(
                rubric=result,
                expected_question_count=expected_question_count,
                expected_total_score=expected_total_score,
            )

            # å°†è‡ªç™½æŠ¥å‘Šæ·»åŠ åˆ° parsed_rubric
            parsed_rubric["overall_parse_confidence"] = parse_self_report["overallConfidence"]
            parsed_rubric["parse_self_report"] = parse_self_report

            # åŒæ—¶æ›´æ–° ParsedRubric å¯¹è±¡ï¼ˆå¦‚æžœéœ€è¦é‡æ–°æ³¨å†Œï¼‰
            result.overall_parse_confidence = parse_self_report["overallConfidence"]
            result.parse_self_report = parse_self_report

            logger.info(
                f"[rubric_parse] è¯„åˆ†æ ‡å‡†è§£æžæˆåŠŸ: "
                f"é¢˜ç›®æ•°={result.total_questions}, æ€»åˆ†={result.total_score}, "
                f"ç½®ä¿¡åº¦={parse_self_report['overallConfidence']:.2f}, "
                f"çŠ¶æ€={parse_self_report['overallStatus']}"
            )
            
            # ðŸ” è¾“å‡ºå®Œæ•´çš„ AI è¿”å›žç»“æžœ JSON (ä»…åœ¨ DEBUG æ¨¡å¼)
            if logger.isEnabledFor(logging.DEBUG):
                logger.debug(f"[rubric_parse] ðŸ“‹ AI è¿”å›žçš„å®Œæ•´è¯„åˆ†æ ‡å‡† JSON:")
                logger.debug(f"[rubric_parse] {json.dumps(parsed_rubric, ensure_ascii=False, indent=2)}")
            else:
                # ç”Ÿäº§çŽ¯å¢ƒåªè¾“å‡ºé¢˜ç›®åˆ—è¡¨
                question_ids = [q.get('question_id', '?') for q in parsed_rubric.get('questions', [])]
                logger.info(f"[rubric_parse] é¢˜ç›®åˆ—è¡¨: {', '.join(question_ids)}")

        elif rubric_text:
            # å¦‚æžœæœ‰æ–‡æœ¬å½¢å¼çš„è¯„åˆ†æ ‡å‡†ï¼Œç®€å•è§£æž
            parsed_rubric["raw_text"] = rubric_text

    except Exception as e:
        logger.error(f"[rubric_parse] Rubric parse failed: {e}", exc_info=True)
        try:
            await _broadcast_progress(
                batch_id,
                {
                    "type": "rubric_parse_failed",
                    "message": "Rubric parse failed. Please re-upload a clear rubric.",
                    "error": str(e),
                },
            )
        except Exception:
            logger.debug("[rubric_parse] Failed to broadcast parse error")
        raise

    logger.info(
        f"[rubric_parse] è¯„åˆ†æ ‡å‡†è§£æžå®Œæˆ: batch_id={batch_id}, "
        f"é¢˜ç›®æ•°={parsed_rubric.get('total_questions', 0)}, "
        f"æ€»åˆ†={parsed_rubric.get('total_score', 0)}"
    )

    inputs_dict = state.get("inputs", {}) or {}
    expected_total_score = inputs_dict.get("expected_total_score")
    if expected_total_score is not None:
        try:
            expected_total_score = float(expected_total_score)
            parsed_total_score = float(parsed_rubric.get("total_score", 0) or 0)
            if parsed_total_score > 0 and parsed_total_score < expected_total_score:
                message = (
                    f"Parsed total score {parsed_total_score} is lower than "
                    f"expected {expected_total_score}."
                )
                await _broadcast_progress(
                    batch_id,
                    {
                        "type": "rubric_score_mismatch",
                        "expected_total_score": expected_total_score,
                        "parsed_total_score": parsed_total_score,
                        "message": message,
                    },
                )
                raise ValueError(message)
        except (TypeError, ValueError) as exc:
            logger.warning(f"[rubric_parse] Expected total score check skipped: {exc}")

    try:
        await _broadcast_progress(
            batch_id,
            {
                "type": "rubric_parsed",
                "totalQuestions": parsed_rubric.get("total_questions", 0),
                "totalScore": parsed_rubric.get("total_score", 0),
                "generalNotes": parsed_rubric.get("general_notes", ""),
                "rubricFormat": parsed_rubric.get("rubric_format", ""),
                "overallParseConfidence": parsed_rubric.get("overall_parse_confidence", 1.0),
                "parseSelfReport": parsed_rubric.get("parse_self_report"),
                "questions": [
                    {
                        "questionId": q.get("question_id", ""),
                        "maxScore": q.get("max_score", 0),
                        "questionText": q.get("question_text", ""),
                        "standardAnswer": q.get("standard_answer", ""),
                        "gradingNotes": q.get("grading_notes", ""),
                        "sourcePages": q.get("source_pages") or q.get("sourcePages") or [],
                        "parseConfidence": q.get("parse_confidence", 1.0),
                        "parseUncertainties": q.get("parse_uncertainties")
                        or q.get("parseUncertainties")
                        or [],
                        "parseQualityIssues": q.get("parse_quality_issues")
                        or q.get("parseQualityIssues")
                        or [],
                        "scoringPoints": [
                            {
                                "pointId": sp.get("point_id")
                                or sp.get("pointId")
                                or f"{q.get('question_id')}.{idx + 1}",
                                "description": sp.get("description", ""),
                                "expectedValue": sp.get("expected_value")
                                or sp.get("expectedValue", ""),
                                "keywords": sp.get("keywords") or [],
                                "score": sp.get("score", 0),
                                "isRequired": sp.get("is_required", True),
                            }
                            for idx, sp in enumerate(q.get("scoring_points", []))
                        ],
                        "deductionRules": [
                            {
                                "ruleId": dr.get("rule_id")
                                or dr.get("ruleId")
                                or f"{q.get('question_id')}.d{idx + 1}",
                                "description": dr.get("description", ""),
                                "deduction": dr.get("deduction", dr.get("score", 0)),
                                "conditions": dr.get("conditions") or dr.get("when") or "",
                            }
                            for idx, dr in enumerate(
                                q.get("deduction_rules") or q.get("deductionRules") or []
                            )
                        ],
                        "alternativeSolutions": [
                            {
                                "description": alt.get("description", ""),
                                "scoringCriteria": alt.get("scoring_criteria", ""),
                                "note": alt.get("note", ""),
                            }
                            for alt in q.get("alternative_solutions", [])
                        ],
                    }
                    for q in parsed_rubric.get("questions", [])
                ],
            },
        )
    except Exception as exc:
        logger.warning(f"[rubric_parse] failed to emit rubric_parsed: {exc}")

    # æ³¨æ„ï¼šä¸åºåˆ—åŒ– RubricRegistryï¼Œå› ä¸º grade_batch_node ä¼šä»Ž parsed_rubric é‡å»º
    # è¿™æ ·å¯ä»¥é¿å…ç±»åž‹è½¬æ¢é—®é¢˜

    return {
        "parsed_rubric": parsed_rubric,
        "current_stage": "rubric_parse_completed",
        "percentage": 15.0,
        "timestamps": {
            **state.get("timestamps", {}),
            "rubric_parse_at": datetime.now().isoformat(),
        },
    }


async def rubric_review_node(state: BatchGradingGraphState) -> Dict[str, Any]:
    """
    Rubric review node with interrupt.
    """
    batch_id = state["batch_id"]
    parsed_rubric = state.get("parsed_rubric", {})
    api_key = state.get("api_key") or os.getenv("LLM_API_KEY") or os.getenv("OPENROUTER_API_KEY")
    enable_review = state.get("inputs", {}).get("enable_review", True)
    grading_mode = _resolve_grading_mode(state.get("inputs", {}), parsed_rubric)

    if grading_mode.startswith("assist"):
        logger.info(f"[rubric_review] skip (assist mode): batch_id={batch_id}")
        return {
            "current_stage": "rubric_review_skipped",
            "percentage": 18.0,
            "timestamps": {
                **state.get("timestamps", {}),
                "rubric_review_at": datetime.now().isoformat(),
            },
        }

    if not parsed_rubric or not parsed_rubric.get("questions"):
        logger.info(f"[rubric_review] skip (no rubric): batch_id={batch_id}")
        return {
            "current_stage": "rubric_review_skipped",
            "percentage": 18.0,
            "timestamps": {
                **state.get("timestamps", {}),
                "rubric_review_at": datetime.now().isoformat(),
            },
        }

    if not enable_review:
        logger.info(f"[rubric_review] skip (review disabled): batch_id={batch_id}")
        return {
            "current_stage": "rubric_review_skipped",
            "percentage": 18.0,
            "timestamps": {
                **state.get("timestamps", {}),
                "rubric_review_at": datetime.now().isoformat(),
            },
        }

    review_request = {
        "type": "rubric_review_required",
        "batch_id": batch_id,
        "message": "Rubric review required",
        "requested_at": datetime.now().isoformat(),
        "parsed_rubric": parsed_rubric,
    }
    review_response = interrupt(review_request)

    action = (review_response or {}).get("action", "approve").lower()
    updated_rubric = parsed_rubric

    if action in ("update", "override"):
        updated_payload = (review_response or {}).get("parsed_rubric") or {}
        updated_rubric = _normalize_parsed_rubric_input(updated_payload, parsed_rubric)
    elif action == "reparse":
        selected_ids = (review_response or {}).get("selected_question_ids") or []
        notes = (review_response or {}).get("notes") or ""
        if selected_ids and api_key:
            try:
                from src.services.rubric_parser import RubricParserService

                parser = RubricParserService(api_key=api_key)
                selected_questions = [
                    q
                    for q in parsed_rubric.get("questions", [])
                    if q.get("question_id") in selected_ids or q.get("id") in selected_ids
                ]
                revised = await parser.revise_questions(selected_questions, notes=notes)
                revised_map = {
                    (q.get("question_id") or q.get("id")): q for q in revised if isinstance(q, dict)
                }
                updated_questions = []
                for q in parsed_rubric.get("questions", []):
                    qid = q.get("question_id") or q.get("id")
                    if qid in revised_map:
                        normalized = _normalize_parsed_rubric_input(
                            {
                                "questions": [revised_map[qid]],
                            },
                            parsed_rubric,
                        )
                        if normalized.get("questions"):
                            updated_questions.append(normalized["questions"][0])
                            continue
                    updated_questions.append(q)
                updated_rubric = {
                    **parsed_rubric,
                    "questions": updated_questions,
                }
            except Exception as exc:
                logger.warning(f"[rubric_review] reparse failed: {exc}", exc_info=True)

    if updated_rubric.get("questions"):
        updated_rubric["total_questions"] = len(updated_rubric["questions"])
        updated_rubric["total_score"] = sum(
            q.get("max_score", 0) for q in updated_rubric["questions"]
        )
        updated_rubric["rubric_context"] = _format_rubric_context_from_dict(updated_rubric)

    return {
        "parsed_rubric": updated_rubric,
        "rubric_review_result": review_response,
        "current_stage": "rubric_review_completed",
        "percentage": 20.0,
        "timestamps": {
            **state.get("timestamps", {}),
            "rubric_review_at": datetime.now().isoformat(),
        },
    }


def grading_fanout_router(state: BatchGradingGraphState) -> List[Send]:
    """
    æ‰¹æ”¹æ‰‡å‡ºè·¯ç”±

    å°†æ‰€æœ‰é¡µé¢åˆ†æ‰¹ï¼Œæ¯æ‰¹å¹¶è¡Œæ‰¹æ”¹ã€‚
    ä¸é¢„å…ˆåˆ†å‰²å­¦ç”Ÿï¼Œè€Œæ˜¯æ‰¹æ”¹æ‰€æœ‰é¡µé¢ã€‚
    æ”¯æŒå¯é…ç½®çš„æ‰¹æ¬¡å¤§å°ã€‚

    **å…³é”®**: ä½¿ç”¨æ·±æ‹·è´ç¡®ä¿ Worker ä¹‹é—´ä¸å…±äº«å¯å˜çŠ¶æ€ (Requirement 3.2)

    Requirements: 3.1, 3.2, 10.1
    """
    import copy

    batch_id = state["batch_id"]
    inputs = state.get("inputs", {})
    processed_images = state.get("processed_images") or state.get("answer_images") or []
    rubric = state.get("rubric", "")
    parsed_rubric = state.get("parsed_rubric", {})
    api_key = state.get("api_key", "")
    student_boundaries = state.get("student_boundaries")
    if not student_boundaries:
        student_boundaries = _build_student_boundaries(state, len(processed_images))
        if student_boundaries:
            logger.info(f"[grading_fanout] ç”Ÿæˆ {len(student_boundaries)} ä¸ªå­¦ç”Ÿè¾¹ç•Œ")

    if not processed_images:
        logger.warning(f"[grading_fanout] âš ï¸ æ²¡æœ‰å¾…æ‰¹æ”¹çš„å›¾åƒ: batch_id={batch_id}")
        logger.warning(f"[grading_fanout] ðŸ” è°ƒè¯•: state keys={list(state.keys())}")
        logger.warning(f"[grading_fanout] ðŸ” answer_images count={len(state.get('answer_images', []))}")
        logger.warning(f"[grading_fanout] ðŸ” processed_images count={len(state.get('processed_images', []))}")
        return [Send("self_report", state)]

    # ä¸å†ä»Ž page_index_contexts æŽ¨å¯¼ student_boundaries
    # å¦‚æžœå‰ç«¯æ²¡æœ‰æä¾› student_mappingï¼Œåˆ™æŒ‰æ‰¹æ¬¡å¤§å°åˆ†é…

    # èŽ·å–æ‰¹æ¬¡é…ç½® (Requirements: 3.1, 10.1)
    config = get_batch_config()
    max_retries = config.max_retries
    total_pages = len(processed_images)

    # ðŸ”¥ ä¼˜å…ˆæŒ‰å­¦ç”Ÿè¾¹ç•ŒåŠ¨æ€åˆ†é…æ‰¹æ¬¡
    if student_boundaries and len(student_boundaries) > 0:
        num_batches = len(student_boundaries)
        logger.info(
            f"[grading_fanout] æŒ‰å­¦ç”Ÿè¾¹ç•Œåˆ›å»ºæ‰¹æ”¹ä»»åŠ¡: batch_id={batch_id}, "
            f"å­¦ç”Ÿæ•°={num_batches}, æ€»é¡µæ•°={total_pages}"
        )

        sends = []
        for batch_idx, boundary in enumerate(student_boundaries):
            student_key = boundary.get("student_key", f"student_{batch_idx}")
            student_name = boundary.get("student_name")
            student_id = boundary.get("student_id")
            pages = boundary.get("pages")
            if pages:
                page_indices = sorted(list(pages))
            else:
                start_page = boundary.get("start_page", 0)
                end_page = boundary.get("end_page", total_pages - 1)
                page_indices = list(range(start_page, end_page + 1))
            if page_indices:
                start_page = page_indices[0]
                end_page = page_indices[-1]
            else:
                start_page = 0
                end_page = 0

            batch_images = [processed_images[i] for i in page_indices if i < len(processed_images)]

            if not batch_images:
                logger.warning(f"[grading_fanout] å­¦ç”Ÿ {student_key} æ²¡æœ‰å›¾åƒï¼Œè·³è¿‡")
                continue

            task_state = {
                "batch_id": batch_id,
                "batch_index": batch_idx,
                "total_batches": num_batches,
                "student_key": student_key,
                "student_name": student_name,
                "student_id": student_id,
                "page_indices": page_indices,
                "images": batch_images,
                "rubric": rubric,
                "parsed_rubric": copy.deepcopy(parsed_rubric),
                "api_key": api_key,
                "retry_count": 0,
                "max_retries": max_retries,
                "inputs": copy.deepcopy(inputs),
            }

            sends.append(Send("grade_batch", task_state))
            logger.info(
                f"[grading_fanout] åˆ›å»ºå­¦ç”Ÿæ‰¹æ¬¡: student={student_key}, pages={start_page}-{end_page}"
            )

        if sends:
            logger.info(f"[grading_fanout] âœ… æˆåŠŸåˆ›å»º {len(sends)} ä¸ªå­¦ç”Ÿæ‰¹æ”¹ä»»åŠ¡")
            return sends
        logger.warning(f"[grading_fanout] âš ï¸ æ²¡æœ‰æœ‰æ•ˆçš„å­¦ç”Ÿæ‰¹æ¬¡")
        logger.warning(f"[grading_fanout] ðŸ” student_boundaries={student_boundaries}")

    # å›žé€€ï¼šæŒ‰å›ºå®šæ‰¹æ¬¡å¤§å°åˆ†é…
    batch_size = config.batch_size
    if batch_size <= 0:
        batch_size = max(1, total_pages)
    num_batches = (total_pages + batch_size - 1) // batch_size

    logger.info(
        f"[grading_fanout] åˆ›å»ºæ‰¹æ”¹ä»»åŠ¡: batch_id={batch_id}, "
        f"æ€»é¡µæ•°={total_pages}, æ‰¹æ¬¡æ•°={num_batches}, "
        f"æ‰¹æ¬¡å¤§å°={batch_size}, æœ€å¤§é‡è¯•={max_retries}"
    )

    sends = []
    for batch_idx in range(num_batches):
        start_idx = batch_idx * batch_size
        end_idx = min(start_idx + batch_size, total_pages)
        batch_images = processed_images[start_idx:end_idx]

        # ðŸ”§ ä¿®å¤ï¼šä¸ºå›žé€€é€»è¾‘æ·»åŠ é»˜è®¤ student_keyï¼ˆä¿®å¤ total_students=0 é—®é¢˜ï¼‰
        # å½“æ²¡æœ‰ student_mapping æ—¶ï¼Œåˆ›å»ºä¸€ä¸ªé»˜è®¤å­¦ç”Ÿè¦†ç›–æ‰€æœ‰é¡µé¢
        if num_batches == 1:
            # åªæœ‰ä¸€ä¸ªæ‰¹æ¬¡ï¼Œè§†ä¸ºå•ä¸ªå­¦ç”Ÿ
            default_student_key = "å­¦ç”Ÿ1"
        else:
            # å¤šä¸ªæ‰¹æ¬¡ï¼Œä¸ºæ¯ä¸ªæ‰¹æ¬¡åˆ†é…ä¸€ä¸ªå­¦ç”Ÿç¼–å·
            default_student_key = f"å­¦ç”Ÿ{batch_idx + 1}"

        task_state = {
            "batch_id": batch_id,
            "batch_index": batch_idx,
            "total_batches": num_batches,
            "student_key": default_student_key,  # âœ… æ·»åŠ  student_key
            "page_indices": list(range(start_idx, end_idx)),
            "images": batch_images,
            "rubric": rubric,
            "parsed_rubric": copy.deepcopy(parsed_rubric),
            "api_key": api_key,
            "retry_count": 0,
            "max_retries": max_retries,
            "inputs": copy.deepcopy(inputs),
        }

        logger.info(
            f"[grading_fanout] å›žé€€æ‰¹æ¬¡: batch={batch_idx+1}/{num_batches}, "
            f"student_key={default_student_key}, pages={start_idx}-{end_idx-1}"
        )

        sends.append(Send("grade_batch", task_state))

    return sends


def _normalize_question_id(value: Any) -> str:
    if value is None:
        return ""
    text = str(value).strip()
    if not text:
        return ""
    for token in ["ç¬¬", "é¢˜ç›®", "é¢˜", "Q", "q"]:
        text = text.replace(token, "")
    return text.strip().rstrip(".:ï¼š")


def _estimate_page_max_score(
    parsed_rubric: Optional[Dict[str, Any]],
    page_context: Optional[Dict[str, Any]],
) -> float:
    if not parsed_rubric or not page_context:
        return 0.0
    if page_context.get("is_cover_page"):
        return 0.0
    question_numbers = page_context.get("question_numbers") or []
    if not question_numbers:
        return 0.0
    normalized = {_normalize_question_id(qnum) for qnum in question_numbers if qnum is not None}
    normalized = {qid for qid in normalized if qid}
    if not normalized:
        return 0.0
    total = 0.0
    for question in parsed_rubric.get("questions", []):
        qid = _normalize_question_id(question.get("question_id") or question.get("id"))
        if not qid or qid not in normalized:
            continue
        try:
            total += float(question.get("max_score", 0) or 0)
        except (TypeError, ValueError):
            continue
    return total


def _is_placeholder_evidence(text: Optional[str]) -> bool:
    if not text:
        return True
    content = text.strip()
    if not content:
        return True
    placeholders = [
        "æœªæ‰¾åˆ°",
        "æœªè¯†åˆ«",
        "ä¸æ¸…æ™°",
        "æ— æ³•è¾¨è®¤",
        "N/A",
        "null",
        "None",
        "ã€åŽŸæ–‡å¼•ç”¨ã€‘æœªæ‰¾åˆ°",
    ]
    return any(p in content for p in placeholders)


def _trim_text(value: Any, max_len: int) -> str:
    if value is None:
        return ""
    text = str(value)
    if max_len <= 0:
        return ""
    if len(text) <= max_len:
        return text
    if max_len <= 3:
        return text[:max_len]
    return text[: max_len - 3].rstrip() + "..."


def _normalize_scoring_point_results(
    raw_points: Any,
    question_id: str,
) -> List[Dict[str, Any]]:
    if not isinstance(raw_points, list):
        return []
    qid = _normalize_question_id(question_id) or str(question_id or "").strip()
    normalized = []
    for idx, spr in enumerate(raw_points, 1):
        if not isinstance(spr, dict):
            continue
        scoring_point = spr.get("scoring_point") or spr.get("scoringPoint") or {}
        point_id = (
            spr.get("point_id")
            or spr.get("pointId")
            or scoring_point.get("point_id")
            or f"{qid}.{idx}"
        )
        description = scoring_point.get("description") or spr.get("description") or ""
        rubric_reference = spr.get("rubric_reference") or spr.get("rubricReference") or ""
        rubric_reference_source = spr.get("rubric_reference_source") or spr.get(
            "rubricReferenceSource"
        )
        if not rubric_reference:
            rubric_reference = f"[{point_id}] {description}".strip()
            rubric_reference_source = "system"
        max_points = (
            spr.get("max_points")
            or spr.get("maxPoints")
            or spr.get("max_score")
            or spr.get("maxScore")
            or scoring_point.get("score")
            or 0
        )
        normalized.append(
            {
                **spr,
                "point_id": point_id,
                "rubric_reference": rubric_reference,
                "rubric_reference_source": rubric_reference_source,
                "max_points": max_points,
            }
        )
    return normalized


def _trim_list(items: Any, max_items: int) -> List[Any]:
    if items is None:
        return []
    if isinstance(items, list):
        values = items
    elif isinstance(items, tuple):
        values = list(items)
    else:
        values = [items]
    if max_items <= 0:
        return []
    return values[:max_items]


def _compact_evidence(evidence: Dict[str, Any], limits: Dict[str, int]) -> Dict[str, Any]:
    if not isinstance(evidence, dict):
        return evidence
    max_qnums = limits.get("max_question_numbers", 6)
    qnums = evidence.get("question_numbers")
    if isinstance(qnums, list):
        evidence["question_numbers"] = qnums[:max_qnums]
    evidence["page_summary"] = _trim_text(
        evidence.get("page_summary", ""),
        limits.get("max_page_summary_chars", 100),
    )
    answers = evidence.get("answers")
    if isinstance(answers, list):
        for answer in answers:
            if not isinstance(answer, dict):
                continue
            answer["answer_text"] = _trim_text(
                answer.get("answer_text", ""),
                limits.get("max_answer_chars", 160),
            )
            snippets = answer.get("evidence_snippets", [])
            snippets = _trim_list(snippets, limits.get("max_snippets", 1))
            answer["evidence_snippets"] = [
                _trim_text(snippet, limits.get("max_snippet_chars", 90))
                for snippet in snippets
                if snippet
            ]
            flags = answer.get("uncertainty_flags", [])
            answer["uncertainty_flags"] = _trim_list(
                flags,
                limits.get("max_uncertainty_flags", 3),
            )
    return evidence


def _compact_score_result(result: Dict[str, Any], limits: Dict[str, int]) -> Dict[str, Any]:
    if not isinstance(result, dict):
        return result
    result["page_summary"] = _trim_text(
        result.get("page_summary", ""),
        limits.get("max_page_summary_chars", 100),
    )
    q_details = result.get("question_details")
    if isinstance(q_details, list):
        for q in q_details:
            if not isinstance(q, dict):
                continue
            q["feedback"] = _trim_text(
                q.get("feedback", ""),
                limits.get("max_feedback_chars", 120),
            )
            q["student_answer"] = _trim_text(
                q.get("student_answer", ""),
                limits.get("max_student_answer_chars", 120),
            )
            typo_notes = q.get("typo_notes") or q.get("typoNotes") or []
            typo_notes = _trim_list(typo_notes, limits.get("max_typo_notes", 3))
            q["typo_notes"] = [
                _trim_text(note, limits.get("max_typo_chars", 24)) for note in typo_notes if note
            ]
            sprs = q.get("scoring_point_results") or q.get("scoring_results") or []
            if isinstance(sprs, list):
                for spr in sprs:
                    if not isinstance(spr, dict):
                        continue
                    spr["evidence"] = _trim_text(
                        spr.get("evidence", ""),
                        limits.get("max_evidence_chars", 90),
                    )
                    spr["reason"] = _trim_text(
                        spr.get("reason", ""),
                        limits.get("max_reason_chars", 120),
                    )
                    spr["decision"] = _trim_text(
                        spr.get("decision", ""),
                        limits.get("max_decision_chars", 24),
                    )
    return result


def _normalize_text(value: Any) -> str:
    if value is None:
        return ""
    return str(value).strip()


def _is_choice_question(question_text: str, standard_answer: str) -> bool:
    text = _normalize_text(question_text)
    answer = _normalize_text(standard_answer)
    if not text and not answer:
        return False
    text_no_space = re.sub(r"\s+", "", text)
    if re.search(r"[A-D][\\.ã€ï¼Ž]", text_no_space):
        return True
    if any(token in text for token in ["é€‰æ‹©é¢˜", "å•é€‰", "å¤šé€‰", "é€‰é¡¹", "è¯·é€‰æ‹©", "ä¸‹åˆ—"]):
        return True
    if answer:
        answer_clean = re.sub(r"\s+", "", answer.upper())
        if re.fullmatch(r"[A-D](?:[ã€,/ï¼Œ ]*[A-D]){0,3}", answer_clean):
            return True
    return False


def _infer_question_type(question: Dict[str, Any]) -> str:
    raw_type = question.get("question_type") or question.get("questionType") or ""
    raw_type = str(raw_type).strip().lower()
    if raw_type:
        return raw_type

    question_text = _normalize_text(
        question.get("question_text") or question.get("questionText") or ""
    )
    grading_notes = _normalize_text(
        question.get("grading_notes") or question.get("gradingNotes") or ""
    )
    standard_answer = _normalize_text(
        question.get("standard_answer") or question.get("standardAnswer") or ""
    )
    alternative_solutions = (
        question.get("alternative_solutions") or question.get("alternativeSolutions") or []
    )

    if _is_choice_question(question_text, standard_answer):
        return "choice"

    text_blob = f"{question_text} {grading_notes}".lower()
    subjective_keywords = [
        "ç®€ç­”",
        "è®ºè¿°",
        "è¯æ˜Ž",
        "æŽ¨å¯¼",
        "è§£é‡Š",
        "åˆ†æž",
        "è®¨è®º",
        "è®¾è®¡",
        "è¯´æ˜Ž",
        "è¿‡ç¨‹",
        "æ­¥éª¤",
        "åº”ç”¨",
        "å®žéªŒ",
    ]
    objective_keywords = [
        "åˆ¤æ–­",
        "å¡«ç©º",
        "å¯¹é”™",
        "æ˜¯éž",
        "true",
        "false",
        "âˆš",
        "Ã—",
    ]

    if alternative_solutions:
        return "subjective"
    if any(token.lower() in text_blob for token in subjective_keywords):
        return "subjective"
    if any(token.lower() in text_blob for token in objective_keywords):
        return "objective"

    if standard_answer:
        answer_clean = re.sub(r"\s+", "", standard_answer)
        if len(answer_clean) <= 4 and re.fullmatch(r"[0-9A-Za-z\\-+.=()ï¼ˆï¼‰/]+", answer_clean):
            return "objective"
        if len(standard_answer) > 30 or "\n" in standard_answer:
            return "subjective"

    return "objective"


def _resolve_grading_mode(
    inputs: Optional[Dict[str, Any]],
    parsed_rubric: Optional[Dict[str, Any]],
) -> str:
    raw_mode = (inputs or {}).get("grading_mode") or (inputs or {}).get("gradingMode") or ""
    mode = str(raw_mode).strip().lower()
    mode_map = {
        "standard": "standard",
        "auto": "auto",
        "assist_teacher": "assist_teacher",
        "teacher_assist": "assist_teacher",
        "assistant_teacher": "assist_teacher",
        "assist_student": "assist_student",
        "student_assist": "assist_student",
        "assistant_student": "assist_student",
        "teacher": "assist_teacher",
        "student": "assist_student",
    }
    resolved = mode_map.get(mode, "auto" if not mode else "standard")
    has_rubric = bool((parsed_rubric or {}).get("questions"))
    if resolved == "auto":
        return "standard" if has_rubric else "assist_teacher"
    if resolved.startswith("assist"):
        return resolved
    return "standard"


def _build_rubric_question_map(parsed_rubric: Dict[str, Any]) -> Dict[str, Dict[str, Any]]:
    question_map: Dict[str, Dict[str, Any]] = {}
    for q in parsed_rubric.get("questions", []):
        qid = _normalize_question_id(q.get("question_id") or q.get("id"))
        if not qid:
            continue
        question_type = _infer_question_type(q)
        scoring_points = []
        for idx, sp in enumerate(q.get("scoring_points", [])):
            point_id = sp.get("point_id") or sp.get("pointId") or f"{qid}.{idx + 1}"
            scoring_points.append(
                {
                    "point_id": point_id,
                    "description": sp.get("description", ""),
                    "score": sp.get("score", 0),
                    "is_required": sp.get("is_required", True),
                    "expected_value": sp.get("expected_value") or sp.get("expectedValue") or "",
                    "keywords": sp.get("keywords") or [],
                }
            )
        alternative_solutions = []
        for alt in q.get("alternative_solutions") or q.get("alternativeSolutions") or []:
            if not isinstance(alt, dict):
                continue
            alternative_solutions.append(
                {
                    "description": alt.get("description", ""),
                    "scoring_criteria": alt.get("scoring_criteria")
                    or alt.get("scoringCriteria")
                    or alt.get("scoring_conditions")
                    or alt.get("scoringConditions")
                    or "",
                    "max_score": alt.get("max_score", alt.get("maxScore", q.get("max_score", 0))),
                }
            )
        question_map[qid] = {
            "question_id": qid,
            "max_score": q.get("max_score", 0),
            "question_text": q.get("question_text", ""),
            "question_type": question_type,
            "is_choice": question_type == "choice",
            "standard_answer": q.get("standard_answer", ""),
            "grading_notes": q.get("grading_notes", ""),
            "scoring_points": scoring_points,
            "deduction_rules": q.get("deduction_rules") or q.get("deductionRules") or [],
            "alternative_solutions": alternative_solutions,
        }
    return question_map


def _normalize_parsed_rubric_input(
    raw_rubric: Dict[str, Any],
    fallback: Optional[Dict[str, Any]] = None,
) -> Dict[str, Any]:
    fallback = fallback or {}
    raw_questions = raw_rubric.get("questions") or []
    normalized_questions = []

    for q in raw_questions:
        qid = q.get("question_id") or q.get("questionId") or q.get("id") or ""
        max_score = q.get("max_score", q.get("maxScore"))
        question_text = q.get("question_text") or q.get("questionText") or ""
        standard_answer = q.get("standard_answer") or q.get("standardAnswer") or ""
        question_type = q.get("question_type") or q.get("questionType") or ""
        grading_notes = q.get("grading_notes") or q.get("gradingNotes") or ""
        source_pages = q.get("source_pages") or q.get("sourcePages") or []
        if not isinstance(source_pages, list):
            source_pages = []

        scoring_points_raw = q.get("scoring_points") or q.get("scoringPoints") or []
        scoring_points = []
        for idx, sp in enumerate(scoring_points_raw):
            if isinstance(sp, dict):
                point_id = sp.get("point_id") or sp.get("pointId") or f"{qid}.{idx + 1}"
                keywords = sp.get("keywords") or []
                if isinstance(keywords, str):
                    keywords = [keywords]
                scoring_points.append(
                    {
                        "point_id": point_id,
                        "description": sp.get("description", ""),
                        "score": float(sp.get("score", sp.get("maxScore", 0)) or 0),
                        "is_required": sp.get("is_required", sp.get("isRequired", True)),
                        "keywords": keywords,
                        "expected_value": sp.get("expected_value") or sp.get("expectedValue") or "",
                    }
                )
            elif isinstance(sp, str):
                scoring_points.append(
                    {
                        "point_id": f"{qid}.{idx + 1}",
                        "description": sp,
                        "score": 0,
                        "is_required": True,
                        "keywords": [],
                        "expected_value": "",
                    }
                )

        if max_score is None:
            max_score = sum(sp.get("score", 0) for sp in scoring_points)
        max_score = float(max_score or 0)

        alternative_solutions_raw = (
            q.get("alternative_solutions") or q.get("alternativeSolutions") or []
        )
        alternative_solutions = []
        for alt in alternative_solutions_raw:
            if isinstance(alt, dict):
                alternative_solutions.append(
                    {
                        "description": alt.get("description", ""),
                        "scoring_criteria": alt.get("scoring_criteria")
                        or alt.get("scoringCriteria")
                        or "",
                        "note": alt.get("note", ""),
                    }
                )
            elif isinstance(alt, str):
                alternative_solutions.append(
                    {
                        "description": alt,
                        "scoring_criteria": "",
                        "note": "",
                    }
                )

        deduction_rules_raw = q.get("deduction_rules") or q.get("deductionRules") or []
        deduction_rules = []
        for idx, dr in enumerate(deduction_rules_raw):
            if isinstance(dr, dict):
                deduction_rules.append(
                    {
                        "rule_id": dr.get("rule_id") or dr.get("ruleId") or f"{qid}.d{idx + 1}",
                        "description": dr.get("description", ""),
                        "deduction": float(dr.get("deduction", dr.get("score", 0)) or 0),
                        "conditions": dr.get("conditions") or dr.get("when") or "",
                    }
                )
            elif isinstance(dr, str):
                deduction_rules.append(
                    {
                        "rule_id": f"{qid}.d{idx + 1}",
                        "description": dr,
                        "deduction": 0.0,
                        "conditions": "",
                    }
                )

        criteria = q.get("criteria")
        if not criteria:
            criteria = [sp.get("description", "") for sp in scoring_points]

        normalized_questions.append(
            {
                "id": qid,
                "question_id": qid,
                "max_score": max_score,
                "question_text": question_text,
                "question_type": question_type,
                "standard_answer": standard_answer,
                "criteria": criteria,
                "scoring_points": scoring_points,
                "alternative_solutions": alternative_solutions,
                "deduction_rules": deduction_rules,
                "grading_notes": grading_notes,
                "source_pages": source_pages,
            }
        )

    total_score = raw_rubric.get("total_score") or raw_rubric.get("totalScore")
    if total_score is None:
        total_score = sum(q.get("max_score", 0) for q in normalized_questions)

    return {
        "total_questions": int(
            raw_rubric.get("total_questions")
            or raw_rubric.get("totalQuestions")
            or len(normalized_questions)
        ),
        "total_score": float(total_score or 0),
        "rubric_format": raw_rubric.get("rubric_format")
        or raw_rubric.get("rubricFormat")
        or fallback.get("rubric_format", "standard"),
        "general_notes": raw_rubric.get("general_notes")
        or raw_rubric.get("generalNotes")
        or fallback.get("general_notes", ""),
        "questions": normalized_questions,
        "rubric_context": raw_rubric.get("rubric_context") or fallback.get("rubric_context"),
        "raw_text": raw_rubric.get("raw_text") or fallback.get("raw_text"),
    }


def _format_rubric_context_from_dict(parsed_rubric: Dict[str, Any]) -> str:
    def ensure_str(value: Any) -> str:
        if value is None:
            return ""
        if isinstance(value, list):
            return " ".join(str(item) for item in value)
        return str(value)

    lines = [
        "=" * 60,
        "RUBRIC SUMMARY",
        "=" * 60,
        f"Questions: {parsed_rubric.get('total_questions', 0)}",
        f"Total Score: {parsed_rubric.get('total_score', 0)}",
        f"Format: {ensure_str(parsed_rubric.get('rubric_format', 'standard'))}",
        "",
    ]

    general_notes = ensure_str(parsed_rubric.get("general_notes", ""))
    if general_notes:
        lines.append(f"General Notes: {general_notes}")
        lines.append("")

    for q in parsed_rubric.get("questions", []):
        lines.append("-" * 40)
        question_id = ensure_str(q.get("question_id", ""))
        lines.append(f"Question {question_id} max_score: {q.get('max_score', 0)}")

        question_text = ensure_str(q.get("question_text", ""))
        if question_text:
            preview = question_text[:100] if len(question_text) > 100 else question_text
            lines.append(f"Question text: {preview}")

        standard_answer = ensure_str(q.get("standard_answer", ""))
        if standard_answer:
            preview = standard_answer[:200] if len(standard_answer) > 200 else standard_answer
            lines.append(f"Standard answer: {preview}")

        scoring_points = q.get("scoring_points", [])
        if scoring_points:
            lines.append("Scoring points:")
            for idx, sp in enumerate(scoring_points, 1):
                required = "required" if sp.get("is_required", True) else "optional"
                keywords = sp.get("keywords") or []
                keywords_str = f" keywords:{keywords}" if keywords else ""
                expected_value = ensure_str(sp.get("expected_value", ""))
                expected_value_str = f" expected:{expected_value}" if expected_value else ""
                point_id = sp.get("point_id") or sp.get("pointId") or f"{question_id}.{idx}"
                lines.append(
                    f"  [{point_id}] {sp.get('score', 0)}?/{required} - "
                    f"{ensure_str(sp.get('description', ''))}{keywords_str}{expected_value_str}"
                )

        deduction_rules = q.get("deduction_rules") or q.get("deductionRules") or []
        if deduction_rules:
            lines.append("Deduction rules:")
            for idx, dr in enumerate(deduction_rules, 1):
                rule_id = dr.get("rule_id") or dr.get("ruleId") or f"{question_id}.d{idx}"
                deduction = dr.get("deduction", dr.get("score", 0))
                conditions = ensure_str(dr.get("conditions") or dr.get("when") or "")
                condition_text = f" conditions:{conditions}" if conditions else ""
                lines.append(
                    f"  [{rule_id}] -{deduction} {ensure_str(dr.get('description', ''))}{condition_text}"
                )

        alternative_solutions = q.get("alternative_solutions", [])
        if alternative_solutions:
            lines.append("Alternative solutions:")
            for alt in alternative_solutions:
                lines.append(f"  - {ensure_str(alt.get('description', ''))}")
                lines.append(f"    criteria: {ensure_str(alt.get('scoring_criteria', ''))}")

        grading_notes = ensure_str(q.get("grading_notes", ""))
        if grading_notes:
            lines.append(f"Notes: {grading_notes}")

        lines.append("")

    return "\n".join(lines)


def _finalize_scoring_result(
    raw_result: Dict[str, Any],
    evidence: Dict[str, Any],
    rubric_map: Dict[str, Dict[str, Any]],
    page_index: int,
) -> Dict[str, Any]:
    raw_questions = raw_result.get("question_details") or []
    raw_by_id = {}
    for q in raw_questions:
        qid = _normalize_question_id(q.get("question_id"))
        if qid:
            raw_by_id[qid] = q

    answer_map = {}
    for answer in evidence.get("answers", []):
        qid = _normalize_question_id(answer.get("question_id"))
        if qid:
            answer_map[qid] = answer

    question_ids = list(answer_map.keys())
    if not question_ids:
        question_ids = [
            _normalize_question_id(q.get("question_id"))
            for q in raw_questions
            if q.get("question_id")
        ]
    if not question_ids:
        question_ids = list(rubric_map.keys())
    seen = set()
    question_ids = [qid for qid in question_ids if qid and not (qid in seen or seen.add(qid))]

    question_details = []
    for qid in question_ids:
        rubric = rubric_map.get(qid, {})
        expected_points = rubric.get("scoring_points", [])
        raw_question = raw_by_id.get(qid, {})
        question_type = rubric.get("question_type") or (
            _infer_question_type(rubric) if rubric else ""
        )
        if not question_type:
            question_type = (
                raw_question.get("question_type") or raw_question.get("questionType") or ""
            )
        is_choice = bool(rubric.get("is_choice") or question_type == "choice")
        raw_scoring = (
            raw_question.get("scoring_point_results") or raw_question.get("scoring_results") or []
        )
        answer_info = answer_map.get(qid, {}) if isinstance(answer_map, dict) else {}
        evidence_snippets = answer_info.get("evidence_snippets") or []
        fallback_snippet = ""
        if isinstance(evidence_snippets, list) and evidence_snippets:
            fallback_snippet = _trim_text(evidence_snippets[0], 90)
        raw_scoring_by_id = {
            _normalize_question_id(spr.get("point_id") or spr.get("pointId")): spr
            for spr in raw_scoring
            if spr.get("point_id") or spr.get("pointId")
        }

        scoring_point_results = []
        review_corrections = []
        missing_points = 0
        missing_evidence = 0
        for idx, sp in enumerate(expected_points):
            point_id = _normalize_question_id(sp.get("point_id")) or f"{qid}.{idx + 1}"
            existing = raw_scoring_by_id.get(point_id, {})
            awarded = existing.get("awarded", existing.get("score", 0))
            max_points = sp.get("score", existing.get("max_points", 0))
            if awarded is None:
                awarded = 0
            if max_points is None:
                max_points = 0
            if awarded > max_points:
                review_corrections.append(
                    {
                        "point_id": point_id,
                        "review_reason": "Score exceeds max; capped to max.",
                    }
                )
                awarded = max_points
            if awarded < 0:
                review_corrections.append(
                    {
                        "point_id": point_id,
                        "review_reason": "Score below zero; clamped to 0.",
                    }
                )
                awarded = 0

            evidence_text = existing.get("evidence")
            if _is_placeholder_evidence(evidence_text):
                missing_evidence += 1
                if fallback_snippet:
                    evidence_text = f"ã€åŽŸæ–‡å¼•ç”¨ã€‘{fallback_snippet}"
                elif not evidence_text:
                    evidence_text = "ã€åŽŸæ–‡å¼•ç”¨ã€‘æœªæ‰¾åˆ°"
            if not existing:
                missing_points += 1
                review_corrections.append(
                    {
                        "point_id": point_id,
                        "review_reason": "Missing scoring point; added with 0 score.",
                    }
                )

            description = sp.get("description", "")
            expected_value = sp.get("expected_value") or sp.get("expectedValue") or ""
            rubric_reference = f"[{point_id}] {description}".strip()
            if expected_value:
                rubric_reference = f"{rubric_reference}ï¼ˆæ ‡å‡†å€¼:{expected_value}ï¼‰"

            scoring_point_results.append(
                {
                    "point_id": point_id,
                    "rubric_reference": rubric_reference,
                    "rubric_reference_source": "system",
                    "decision": "å¾—åˆ†" if awarded > 0 else "æœªå¾—åˆ†",
                    "awarded": awarded,
                    "max_points": max_points,
                    "evidence": evidence_text,
                    "reason": existing.get("reason", ""),
                    "scoring_point": {
                        "description": sp.get("description", ""),
                        "score": max_points,
                        "is_required": sp.get("is_required", True),
                    },
                }
            )

        if not scoring_point_results and raw_scoring:
            for idx, spr in enumerate(raw_scoring, 1):
                point_id = spr.get("point_id") or spr.get("pointId") or f"{qid}.{idx}"
                scoring_point = spr.get("scoring_point") or spr.get("scoringPoint") or {}
                description = scoring_point.get("description") or spr.get("description") or ""
                rubric_reference = spr.get("rubric_reference") or spr.get("rubricReference") or ""
                rubric_reference_source = spr.get("rubric_reference_source") or spr.get(
                    "rubricReferenceSource"
                )
                if not rubric_reference:
                    rubric_reference = f"[{point_id}] {description}".strip()
                    rubric_reference_source = "system"
                max_points = spr.get("max_points", spr.get("maxScore"))
                if max_points is None:
                    max_points = scoring_point.get("score", 0)
                scoring_point_results.append(
                    {
                        "point_id": point_id,
                        "rubric_reference": rubric_reference,
                        "rubric_reference_source": rubric_reference_source,
                        "decision": spr.get("decision") or spr.get("result") or "",
                        "awarded": spr.get("awarded", spr.get("score", 0)),
                        "max_points": max_points or 0,
                        "evidence": spr.get("evidence", ""),
                        "reason": spr.get("reason", ""),
                        "scoring_point": scoring_point if scoring_point else None,
                    }
                )

        sum_awarded = sum(r.get("awarded", 0) for r in scoring_point_results)
        max_score = rubric.get("max_score", raw_question.get("max_score", 0))
        if not max_score:
            max_score = sum(r.get("max_points", 0) for r in scoring_point_results)
        score = raw_question.get("score")
        score_adjusted = False
        if score is None:
            score = sum_awarded
        if abs(sum_awarded - score) > 0.25:
            score = sum_awarded
            score_adjusted = True
        if score > max_score:
            score = max_score
            score_adjusted = True
        if score_adjusted:
            review_corrections.append(
                {
                    "point_id": qid,
                    "review_reason": "Total mismatch; recalculated from point scores.",
                }
            )

        typo_notes = raw_question.get("typo_notes") or raw_question.get("typoNotes") or []
        if isinstance(typo_notes, str):
            typo_notes = [typo_notes]
        if not isinstance(typo_notes, list):
            typo_notes = []

        total_points = (
            max(1, len(expected_points)) if expected_points else max(1, len(scoring_point_results))
        )
        coverage = min(1.0, len(scoring_point_results) / total_points)
        evidence_ok = min(1.0, (total_points - missing_evidence) / total_points)
        consistency = 1.0 if not score_adjusted else 0.6
        confidence = 0.2 + coverage * 0.5 + evidence_ok * 0.2 + consistency * 0.1
        answer_confidence = answer_map.get(qid, {}).get("confidence")
        if isinstance(answer_confidence, (int, float)):
            confidence = max(0.0, min(1.0, confidence * max(0.4, min(1.0, answer_confidence))))
        used_alt = bool(
            raw_question.get("used_alternative_solution")
            or raw_question.get("usedAlternativeSolution")
            or raw_question.get("alternative_solution_ref")
            or raw_question.get("alternativeSolutionRef")
        )
        confidence_multiplier = 1.0
        if question_type in ("subjective", "essay", "stepwise"):
            confidence_multiplier *= 0.85
        if used_alt or rubric.get("alternative_solutions"):
            confidence_multiplier *= 0.9
        confidence = max(0.0, min(1.0, confidence * confidence_multiplier))

        rubric_ref_coverage = 1.0
        if scoring_point_results:
            rubric_ref_coverage = sum(
                1 for spr in scoring_point_results if spr.get("rubric_reference")
            ) / max(1, len(scoring_point_results))
            if rubric_ref_coverage < 1.0:
                confidence = max(0.0, min(1.0, confidence * (0.6 + 0.4 * rubric_ref_coverage)))

        issues = []
        if missing_points:
            issues.append(f"Scoring coverage incomplete (missing {missing_points} points)")
        if missing_evidence:
            issues.append("Insufficient evidence for some points")
        if score_adjusted:
            issues.append("Point sum mismatched; adjusted total")

        missing_rubric_ref = any(not spr.get("rubric_reference") for spr in scoring_point_results)
        missing_point_id = any(not spr.get("point_id") for spr in scoring_point_results)
        if missing_rubric_ref:
            issues.append("Missing rubric reference for some points")
        if missing_point_id:
            issues.append("Missing point_id for some points")

        audit_flags = []
        if missing_points:
            audit_flags.append("missing_scoring_points")
        if missing_evidence:
            audit_flags.append("missing_evidence")
        if score_adjusted:
            audit_flags.append("score_adjusted")
        if missing_rubric_ref:
            audit_flags.append("missing_rubric_reference")
        if missing_point_id:
            audit_flags.append("missing_point_id")

        review_summary = "; ".join(issues) if issues else "Logic consistent; no obvious issues"

        confidence_reason = (
            f"coverage={coverage:.2f}, evidence={evidence_ok:.2f}, consistency={consistency:.2f}"
        )
        if question_type:
            confidence_reason = f"{confidence_reason}, type={question_type}"
        if used_alt or rubric.get("alternative_solutions"):
            confidence_reason = f"{confidence_reason}, alt_solution=1"
        confidence_reason = f"{confidence_reason}, rubric_refs={rubric_ref_coverage:.2f}"

        feedback = raw_question.get("feedback", "")
        self_critique = raw_question.get("self_critique") or review_summary
        if is_choice:
            feedback = ""
            self_critique = ""

        question_details.append(
            {
                "question_id": qid,
                "score": score,
                "max_score": max_score,
                "confidence": confidence,
                "confidence_reason": confidence_reason,
                "feedback": feedback,
                "student_answer": raw_question.get("student_answer")
                or answer_map.get(qid, {}).get("answer_text", ""),
                "self_critique": self_critique,
                "self_critique_confidence": raw_question.get(
                    "self_critique_confidence", confidence
                ),
                "typo_notes": typo_notes,
                "rubric_refs": [
                    spr.get("rubric_reference")
                    for spr in scoring_point_results
                    if spr.get("rubric_reference")
                ],
                "scoring_point_results": scoring_point_results,
                "review_summary": review_summary,
                "review_corrections": review_corrections,
                "audit_flags": audit_flags,
                "page_indices": [page_index],
                "is_correct": max_score > 0 and score >= max_score,
                "question_type": question_type,
                "used_alternative_solution": used_alt,
                "alternative_solution_ref": raw_question.get("alternative_solution_ref")
                or raw_question.get("alternativeSolutionRef")
                or "",
            }
        )

    page_confidence = (
        sum(q.get("confidence", 0) for q in question_details) / len(question_details)
        if question_details
        else 0.0
    )
    return {
        "question_details": question_details,
        "score": sum(q.get("score", 0) for q in question_details),
        "max_score": sum(q.get("max_score", 0) for q in question_details),
        "page_confidence": page_confidence,
    }


def _finalize_assist_result(
    raw_result: Dict[str, Any],
    evidence: Dict[str, Any],
    page_index: int,
    grading_mode: str,
) -> Dict[str, Any]:
    raw_questions = raw_result.get("question_details") or []
    raw_by_id = {}
    for q in raw_questions:
        qid = _normalize_question_id(q.get("question_id"))
        if qid:
            raw_by_id[qid] = q

    answer_map = {}
    for answer in evidence.get("answers", []):
        qid = _normalize_question_id(answer.get("question_id"))
        if qid:
            answer_map[qid] = answer

    question_ids = list(answer_map.keys())
    if not question_ids:
        question_ids = [
            _normalize_question_id(q.get("question_id"))
            for q in raw_questions
            if q.get("question_id")
        ]
    seen = set()
    question_ids = [qid for qid in question_ids if qid and not (qid in seen or seen.add(qid))]

    question_details = []
    for qid in question_ids:
        raw_question = raw_by_id.get(qid, {})
        answer_info = answer_map.get(qid, {}) if isinstance(answer_map, dict) else {}
        feedback = raw_question.get("feedback", "")
        if not feedback:
            feedback = raw_question.get("explanation") or raw_question.get("analysis") or ""
        if not feedback:
            hints = raw_question.get("error_hints") or raw_question.get("errorHints") or []
            if isinstance(hints, list) and hints:
                feedback = "ï¼›".join([str(h).strip() for h in hints if h])
        confidence = raw_question.get("confidence", 0.4)
        if not isinstance(confidence, (int, float)):
            confidence = 0.4
        question_type = (
            raw_question.get("question_type") or raw_question.get("questionType") or "unknown"
        )

        question_details.append(
            {
                "question_id": qid,
                "score": 0.0,
                "max_score": 0.0,
                "confidence": float(confidence),
                "feedback": feedback,
                "student_answer": raw_question.get("student_answer")
                or answer_info.get("answer_text", ""),
                "self_critique": raw_question.get("self_critique") or "",
                "self_critique_confidence": raw_question.get(
                    "self_critique_confidence", confidence
                ),
                "typo_notes": raw_question.get("typo_notes") or raw_question.get("typoNotes") or [],
                "rubric_refs": [],
                "scoring_point_results": [],
                "review_summary": "",
                "review_corrections": [],
                "audit_flags": ["assist_mode", grading_mode],
                "page_indices": [page_index],
                "is_correct": False,
                "question_type": question_type,
                "grading_mode": grading_mode,
            }
        )

    page_confidence = (
        sum(q.get("confidence", 0) for q in question_details) / len(question_details)
        if question_details
        else 0.0
    )
    return {
        "question_details": question_details,
        "score": 0.0,
        "max_score": 0.0,
        "page_confidence": page_confidence,
    }


async def grade_batch_node(state: Dict[str, Any]) -> Dict[str, Any]:
    return await _grade_batch_node_impl(state)


async def _grade_batch_node_impl(state: Dict[str, Any]) -> Dict[str, Any]:
    """
    æ‰¹é‡æ‰¹æ”¹èŠ‚ç‚¹

    æ‰¹æ”¹ä¸€æ‰¹é¡µé¢ï¼Œè¿”å›žæ¯é¡µçš„æ‰¹æ”¹ç»“æžœã€‚

    **æ ¸å¿ƒæµç¨‹**:
    1. ä»Ž parsed_rubric é‡å»º RubricRegistry
    2. åˆ›å»º GradingSkills å®žä¾‹
    3. æ‰¹æ”¹æ—¶è¯†åˆ«é¢˜ç›®ç¼–å·
    4. ä½¿ç”¨ GradingSkills.get_rubric_for_question èŽ·å–è¯¥é¢˜ç›®çš„è¯„åˆ†æ ‡å‡†
    5. åŸºäºŽæŒ‡å®šè¯„åˆ†æ ‡å‡†è¿›è¡Œæ‰¹æ”¹

    ç‰¹æ€§ï¼š
    - Worker ç‹¬ç«‹æ€§ï¼šæ¯ä¸ª Worker ç‹¬ç«‹èŽ·å–è¯„åˆ†æ ‡å‡†ï¼Œä¸å…±äº«å¯å˜çŠ¶æ€ (Req 3.2)
    - Agent Skill é›†æˆï¼šä½¿ç”¨ GradingSkills åŠ¨æ€èŽ·å–é¢˜ç›®è¯„åˆ†æ ‡å‡† (Req 5.1)
    - æ‰¹æ¬¡å¤±è´¥é‡è¯•ï¼šå•æ‰¹æ¬¡å¤±è´¥ä¸å½±å“å…¶ä»–æ‰¹æ¬¡ï¼Œæ”¯æŒé‡è¯• (Req 3.3, 9.3)
    - è¿›åº¦æŠ¥å‘Šï¼šå®žæ—¶æŠ¥å‘Šæ‰¹æ¬¡å¤„ç†è¿›åº¦ (Req 3.4)
    - é”™è¯¯éš”ç¦»ï¼šå•é¡µå¤±è´¥ä¸å½±å“å…¶ä»–é¡µé¢ï¼Œè®°å½•é”™è¯¯å¹¶ç»§ç»­å¤„ç† (Req 9.2)

    Requirements: 3.2, 3.3, 3.4, 5.1, 9.2, 9.3
    """
    batch_id = state["batch_id"]
    batch_index = state["batch_index"]
    total_batches = state["total_batches"]
    page_indices = state["page_indices"]
    images = state["images"]
    rubric = state.get("rubric", "")
    page_index_contexts = state.get("page_index_contexts", {})
    api_key = state.get("api_key") or os.getenv("LLM_API_KEY") or os.getenv("OPENROUTER_API_KEY")
    retry_count = state.get("retry_count", 0)
    max_retries = state.get("max_retries", 2)
    batch_student_key = state.get("student_key") or f"Student {batch_index + 1}"
    batch_student_name = state.get("student_name")
    batch_student_id = state.get("student_id")

    logger.info(
        f"[grade_batch] å¼€å§‹æ‰¹æ”¹æ‰¹æ¬¡ {batch_index + 1}/{total_batches}: "
        f"batch_id={batch_id}, é¡µé¢={page_indices}, é‡è¯•æ¬¡æ•°={retry_count}"
    )

    page_results = []
    batch_error = None
    output_limits = {
        "max_answer_chars": int(os.getenv("GRADING_MAX_ANSWER_CHARS", "160")),
        "max_student_answer_chars": int(os.getenv("GRADING_MAX_STUDENT_ANSWER_CHARS", "4000")),
        "max_snippet_chars": int(os.getenv("GRADING_MAX_SNIPPET_CHARS", "90")),
        "max_snippets": int(os.getenv("GRADING_MAX_SNIPPETS", "1")),
        "max_page_summary_chars": int(os.getenv("GRADING_MAX_PAGE_SUMMARY_CHARS", "100")),
        "max_feedback_chars": int(os.getenv("GRADING_MAX_FEEDBACK_CHARS", "120")),
        "max_evidence_chars": int(os.getenv("GRADING_MAX_EVIDENCE_CHARS", "90")),
        "max_reason_chars": int(os.getenv("GRADING_MAX_REASON_CHARS", "120")),
        "max_decision_chars": int(os.getenv("GRADING_MAX_DECISION_CHARS", "24")),
        "max_typo_notes": int(os.getenv("GRADING_MAX_TYPO_NOTES", "3")),
        "max_typo_chars": int(os.getenv("GRADING_MAX_TYPO_CHARS", "24")),
        "max_question_numbers": int(os.getenv("GRADING_MAX_QUESTION_NUMBERS", "6")),
        "max_uncertainty_flags": int(os.getenv("GRADING_MAX_UNCERTAINTY_FLAGS", "3")),
    }
    second_pass_threshold = float(os.getenv("GRADING_SECOND_PASS_CONFIDENCE", "0.65"))
    second_pass_max_ratio = float(os.getenv("GRADING_SECOND_PASS_MAX_RATIO", "0.2"))
    second_pass_budget_fraction = float(os.getenv("GRADING_SECOND_PASS_BUDGET_FRACTION", "0.25"))
    budget_per_page = float(os.getenv("GRADING_BUDGET_PER_PAGE_USD", "0.01"))
    cost_per_m_input = float(os.getenv("GRADING_COST_PER_M_INPUT_TOKENS", "0.5"))
    cost_per_m_output = float(os.getenv("GRADING_COST_PER_M_OUTPUT_TOKENS", "3.0"))
    strict_est_input_tokens = int(os.getenv("GRADING_STRICT_EST_INPUT_TOKENS", "1200"))
    strict_est_output_tokens = int(os.getenv("GRADING_STRICT_EST_OUTPUT_TOKENS", "600"))
    fast_pass_only = os.getenv("GRADING_FAST_PASS_ONLY", "false").strip().lower() in (
        "1",
        "true",
        "yes",
    )
    max_second_passes = int(len(page_indices) * max(0.0, second_pass_max_ratio))
    if second_pass_max_ratio > 0 and max_second_passes == 0:
        max_second_passes = 1
    second_pass_used = 0
    second_pass_lock = asyncio.Lock()
    est_second_pass_cost = (strict_est_input_tokens / 1_000_000.0) * cost_per_m_input + (
        strict_est_output_tokens / 1_000_000.0
    ) * cost_per_m_output
    budget_allows_second_pass = (
        budget_per_page > 0
        and est_second_pass_cost <= budget_per_page * second_pass_budget_fraction
    )
    grading_mode = _resolve_grading_mode(state.get("inputs", {}), state.get("parsed_rubric", {}))

    try:
        if not api_key:
            raise ValueError("API key æœªé…ç½®")

        # Worker ç‹¬ç«‹æ€§ä¿è¯ (Requirement 3.2)
        # æ¯ä¸ª Worker ç‹¬ç«‹åˆ›å»ºå®žä¾‹ï¼Œä¸å…±äº«å¯å˜çŠ¶æ€
        from src.services.llm_reasoning import LLMReasoningClient
        from src.utils.error_handling import execute_with_isolation, get_error_manager
        from src.services.rubric_registry import RubricRegistry

        # æ³¨æ„ï¼šå·²ç§»é™¤ Agent Skillï¼Œç›´æŽ¥ä½¿ç”¨ rubric_registry
        from src.models.grading_models import QuestionRubric, ScoringPoint

        # ç‹¬ç«‹èŽ·å–è¯„åˆ†æ ‡å‡†å‰¯æœ¬ï¼ˆä¸å…±äº«å¯å˜çŠ¶æ€ï¼‰
        parsed_rubric = state.get("parsed_rubric", {})
        import copy

        local_parsed_rubric = copy.deepcopy(parsed_rubric)
        rubric_map = _build_rubric_question_map(local_parsed_rubric)
        grading_mode = _resolve_grading_mode(state.get("inputs", {}), local_parsed_rubric)
        if grading_mode == "assist_student":
            output_limits["max_feedback_chars"] = int(
                os.getenv("GRADING_ASSIST_FEEDBACK_CHARS", "600")
            )
            output_limits["max_page_summary_chars"] = int(
                os.getenv("GRADING_ASSIST_SUMMARY_CHARS", "180")
            )
            output_limits["max_student_answer_chars"] = int(
                os.getenv("GRADING_ASSIST_ANSWER_CHARS", "220")
            )
        logger.info(f"[grade_batch] grading_mode={grading_mode}")

        # ðŸ” è°ƒè¯•æ—¥å¿—ï¼šç¡®è®¤ parsed_rubric å†…å®¹
        logger.info(
            f"[grade_batch] æŽ¥æ”¶åˆ° parsed_rubric: "
            f"total_questions={local_parsed_rubric.get('total_questions', 0)}, "
            f"total_score={local_parsed_rubric.get('total_score', 0)}, "
            f"questions_count={len(local_parsed_rubric.get('questions', []))}"
        )

        # ðŸ”¥ å…³é”®ï¼šä»Ž parsed_rubric é‡å»º RubricRegistry (Requirement 5.1)
        rubric_registry = RubricRegistry(total_score=local_parsed_rubric.get("total_score", 100.0))

        # å°†è§£æžçš„é¢˜ç›®æ³¨å†Œåˆ° Registry
        questions_data = local_parsed_rubric.get("questions", [])
        if questions_data:
            question_rubrics = []
            for q in questions_data:
                # æž„å»º ScoringPoint åˆ—è¡¨
                qid = q.get("question_id") or q.get("id") or ""
                scoring_points = [
                    ScoringPoint(
                        description=sp.get("description", ""),
                        score=sp.get("score", 0),
                        is_required=sp.get("is_required", True),
                        point_id=sp.get("point_id") or sp.get("pointId") or f"{qid}.{idx + 1}",
                    )
                    for idx, sp in enumerate(q.get("scoring_points", []))
                ]

                # æž„å»º QuestionRubric
                question_rubric = QuestionRubric(
                    question_id=str(qid),
                    question_text=q.get("question_text", ""),
                    max_score=q.get("max_score", 0),
                    scoring_points=scoring_points,
                    standard_answer=q.get("standard_answer", ""),
                    grading_notes=q.get("grading_notes", ""),
                    alternative_solutions=[],  # ç®€åŒ–å¤„ç†
                )
                question_rubrics.append(question_rubric)

            rubric_registry.register_rubrics(question_rubrics, log=False)
            logger.info(f"[grade_batch] å·²é‡å»º RubricRegistryï¼Œæ³¨å†Œ {len(question_rubrics)} é“é¢˜ç›®")

        # åˆ›å»º LLMReasoningClientï¼ˆå·²ç§»é™¤ Agent Skillï¼‰
        reasoning_client = LLMReasoningClient(
            api_key=api_key,
            rubric_registry=rubric_registry,
        )
        # é”™è¯¯éš”ç¦»ï¼šå•é¡µå¤±è´¥ä¸å½±å“å…¶ä»–é¡µé¢ (Requirement 9.2)
        error_manager = get_error_manager()

        batch_agent_id = f"batch_{batch_index}"
        batch_student_key = state.get("student_key")
        batch_agent_label = batch_student_key or f"Student Batch {batch_index + 1}"
        total_pages_in_batch = len(page_indices)
        pages_done = 0
        pages_lock = asyncio.Lock()

        async def emit_agent_update(
            status: str,
            message: str = "",
            progress: Optional[int] = None,
        ) -> None:
            payload = {
                "type": "agent_update",
                "parentNodeId": "grade_batch",
                "agentId": batch_agent_id,
                "agentName": batch_agent_label,
                "agentLabel": batch_agent_label,
                "status": status,
                "message": message,
            }
            if progress is not None:
                payload["progress"] = progress
            await _broadcast_progress(batch_id, payload)

        async def emit_stage(message: str) -> None:
            await emit_agent_update("running", message)

        async def mark_page_done(page_idx: int, detail: str) -> None:
            nonlocal pages_done
            async with pages_lock:
                pages_done += 1
                progress = int((pages_done / max(1, total_pages_in_batch)) * 100)
            await emit_agent_update("running", detail, progress=progress)

        await emit_agent_update(
            "running",
            f"Start grading {total_pages_in_batch} pages",
            progress=0,
        )

        async def allow_second_pass() -> bool:
            nonlocal second_pass_used
            async with second_pass_lock:
                if second_pass_used >= max_second_passes:
                    return False
                second_pass_used += 1
                return True

        max_pages_per_student = int(os.getenv("GRADING_MAX_PAGES_PER_STUDENT", "12"))
        use_student_grading = len(images) <= max_pages_per_student
        student_error = None

        # ðŸš€ ä½¿ç”¨ grade_student ä¸€æ¬¡ LLM call æ‰¹æ”¹æ•´ä¸ªå­¦ç”Ÿ
        async def stream_callback(stream_type: str, chunk: str) -> None:
            await _broadcast_progress(
                batch_id,
                {
                    "type": "llm_stream_chunk",
                    "nodeId": "grade_batch",
                    "nodeName": "Batch Grading",
                    "agentId": f"batch_{batch_index}",
                    "agentLabel": batch_student_key,
                    "streamType": stream_type,
                    "chunk": chunk,
                },
            )

        await _broadcast_progress(
            batch_id,
            {
                "type": "agent_update",
                "parentNodeId": "grade_batch",
                "agentId": f"batch_{batch_index}",
                "agentName": batch_student_key,
                "agentLabel": batch_student_key,
                "status": "running",
                "message": f"Grading {len(images)} pages...",
                "progress": 10,
            },
        )

        if use_student_grading:
            logger.info(f"[grade_batch] grade_student for {batch_student_key} pages={len(images)}")

            # grade_student
            student_result = await reasoning_client.grade_student(
                images=images,
                student_key=batch_student_key,
                parsed_rubric=local_parsed_rubric,
                page_indices=page_indices,
                page_contexts=page_index_contexts,
                stream_callback=stream_callback,
            )

            # Convert to legacy page result format.
            if student_result.get("status") == "completed":
                total_score = student_result.get("total_score", 0)
                max_score = student_result.get("max_score", 0)
                question_details = student_result.get("question_details", [])

                page_results.append(
                    {
                        "page_index": page_indices[0] if page_indices else 0,
                        "page_indices": page_indices,
                        "status": "completed",
                        "score": total_score,
                        "max_score": max_score,
                        "confidence": student_result.get("confidence", 0.8),
                        "feedback": student_result.get("overall_feedback", ""),
                        "question_details": question_details,
                        "student_key": batch_student_key,
                        "student_name": batch_student_name,
                        "student_id": batch_student_id,
                        "batch_index": batch_index,
                    }
                )
            else:
                student_error = student_result.get("error", "Unknown error")
                logger.warning(
                    f"[grade_batch] grade_student failed for {batch_student_key}: {student_error}"
                )
                use_student_grading = False

        if not use_student_grading:
            if student_error:
                logger.warning(f"[grade_batch] fallback to per-page grading: {student_error}")
            else:
                logger.info(
                    "[grade_batch] page count exceeds limit; "
                    f"fallback to per-page: pages={len(images)} limit={max_pages_per_student}"
                )

            await emit_agent_update(
                "running",
                "Fallback to per-page grading",
                progress=10,
            )

            for idx, image in enumerate(images):
                page_index = page_indices[idx] if idx < len(page_indices) else idx
                page_context = page_index_contexts.get(page_index)
                page_max_score = _estimate_page_max_score(local_parsed_rubric, page_context)
                try:
                    page_result = await reasoning_client.grade_page(
                        image=image,
                        rubric=rubric,
                        max_score=page_max_score,
                        parsed_rubric=local_parsed_rubric,
                        page_context=page_context,
                        stream_callback=stream_callback,
                    )
                except Exception as exc:
                    logger.warning(f"[grade_batch] page {page_index} grading failed: {exc}")
                    page_results.append(
                        {
                            "page_index": page_index,
                            "page_indices": [page_index],
                            "status": "failed",
                            "error": str(exc),
                            "score": 0,
                            "max_score": page_max_score,
                            "confidence": 0,
                            "feedback": "",
                            "question_details": [],
                            "question_numbers": [],
                            "student_key": batch_student_key,
                            "student_name": batch_student_name,
                            "student_id": batch_student_id,
                            "batch_index": batch_index,
                            "is_blank_page": bool(
                                page_context and page_context.get("is_cover_page")
                            ),
                        }
                    )
                    await mark_page_done(page_index, f"Graded page {idx + 1}/{len(images)}")
                    continue

                question_numbers = page_result.get("question_numbers")
                if not question_numbers and page_context:
                    question_numbers = page_context.get("question_numbers", [])

                status = "completed"
                if page_result.get("confidence", 0) <= 0 and not page_result.get(
                    "question_details"
                ):
                    status = "failed"

                page_results.append(
                    {
                        "page_index": page_index,
                        "page_indices": [page_index],
                        "status": status,
                        "score": page_result.get("score", 0),
                        "max_score": page_result.get("max_score", page_max_score),
                        "confidence": page_result.get("confidence", 0),
                        "feedback": page_result.get("feedback", ""),
                        "question_details": page_result.get("question_details", []),
                        "question_numbers": question_numbers or [],
                        "student_key": batch_student_key,
                        "student_name": batch_student_name,
                        "student_id": batch_student_id,
                        "batch_index": batch_index,
                        "is_blank_page": bool(page_context and page_context.get("is_cover_page")),
                    }
                )

                await mark_page_done(page_index, f"Graded page {idx + 1}/{len(images)}")
    except Exception as e:
        batch_error = str(e)
        logger.error(f"[grade_batch] æ‰¹æ¬¡ {batch_index} æ‰¹æ”¹å¤±è´¥: {e}", exc_info=True)
        try:
            await emit_agent_update("failed", "Batch failed", progress=100)
        except Exception:
            pass

        # è®°å½•æ‰¹æ¬¡çº§é”™è¯¯
        from src.utils.error_handling import get_error_manager

        error_manager = get_error_manager()
        error_manager.add_error(
            exc=e,
            context={
                "batch_id": batch_id,
                "batch_index": batch_index,
                "function": "grade_batch_node",
                "retry_count": retry_count,
            },
            batch_id=batch_id,
            retry_count=retry_count,
        )

        # æ‰¹æ¬¡å¤±è´¥é‡è¯•é€»è¾‘ (Requirements: 3.3, 9.3)
        if retry_count < max_retries:
            logger.info(
                f"[grade_batch] æ‰¹æ¬¡ {batch_index} å°†è¿›è¡Œé‡è¯• " f"({retry_count + 1}/{max_retries})"
            )
            # è¿”å›žé‡è¯•æ ‡è®°ï¼Œè®©è°ƒåº¦å™¨é‡æ–°è°ƒåº¦
            return {
                "grading_results": [],
                "batch_retry_needed": {
                    "batch_index": batch_index,
                    "retry_count": retry_count + 1,
                    "error": batch_error,
                },
            }

        # æ‰€æœ‰é¡µé¢æ ‡è®°ä¸ºå¤±è´¥
        for page_idx in page_indices:
            page_results.append(
                {
                    "page_index": page_idx,
                    "status": "failed",
                    "error": batch_error,
                    "score": 0,
                    "max_score": 0,
                    "batch_index": batch_index,
                    "grading_mode": grading_mode,
                }
            )

    success_count = sum(1 for r in page_results if r["status"] == "completed")
    failed_count = sum(1 for r in page_results if r["status"] == "failed")
    total_score = sum(r.get("score", 0) for r in page_results if r["status"] == "completed")

    # è¿›åº¦æŠ¥å‘Š (Requirement 3.4)
    progress_info = {
        "batch_index": batch_index,
        "total_batches": total_batches,
        "pages_processed": success_count,
        "pages_failed": failed_count,
        "total_score": total_score,
        "status": "completed" if failed_count == 0 else "partial",
        "timestamp": datetime.now().isoformat(),
    }

    logger.info(
        f"[grade_batch] æ‰¹æ¬¡ {batch_index + 1}/{total_batches} å®Œæˆ: "
        f"æˆåŠŸ={success_count}/{len(page_results)}, å¤±è´¥={failed_count}, æ€»åˆ†={total_score}"
    )

    final_status = "completed" if success_count > 0 else "failed"
    await _broadcast_progress(
        batch_id,
        {
            "type": "agent_update",
            "parentNodeId": "grade_batch",
            "agentId": f"batch_{batch_index}",
            "agentName": batch_student_key,
            "agentLabel": batch_student_key,
            "status": final_status,
            "message": f"Completed {success_count}/{len(page_results)} students",
            "progress": 100,
        },
    )

    # ===== ç›´æŽ¥æž„å»º student_results æ ¼å¼ï¼ˆç§»é™¤ simple_aggregate_node çš„éœ€è¦ï¼‰=====
    student_results = _build_student_results_from_page_results(
        page_results,
        default_student_key=batch_student_key,
        grading_mode=grading_mode,
    )

    logger.debug(
        f"[grade_batch] Page results summary: total={len(page_results)}, "
        f"success={success_count}, failed={failed_count}"
    )
    logger.debug(f"[grade_batch] Student results count: {len(student_results)}")

    # ðŸ” è¾“å‡ºå®Œæ•´çš„æ‰¹æ”¹ç»“æžœ JSON

    # è¿”å›žç»“æžœï¼ˆä½¿ç”¨ add reducer èšåˆï¼Œç›´æŽ¥è¾“å‡º student_resultsï¼‰
    return {
        "student_results": student_results,
        "grading_results": page_results,  # ä¿ç•™ç”¨äºŽè°ƒè¯•/æ—¥å¿—
        "batch_progress": progress_info,
    }


def _apply_student_result_overrides(
    student_results: List[Dict[str, Any]],
    overrides: List[Dict[str, Any]],
) -> List[Dict[str, Any]]:
    if not overrides:
        return student_results

    overrides_by_key = {}
    for override in overrides:
        key = (
            override.get("studentKey")
            or override.get("student_key")
            or override.get("studentName")
            or override.get("student_name")
        )
        if key:
            overrides_by_key[key] = override

    for student in student_results:
        student_key = (
            student.get("student_key") or student.get("student_id") or student.get("student_name")
        )
        override = overrides_by_key.get(student_key)
        if not override:
            continue

        question_overrides = {}
        for q in override.get("questionResults", []) or override.get("question_results", []):
            qid = _normalize_question_id(q.get("questionId") or q.get("question_id"))
            if qid:
                question_overrides[qid] = q

        if student.get("question_details"):
            for q in student.get("question_details", []):
                qid = _normalize_question_id(q.get("question_id"))
                if not qid or qid not in question_overrides:
                    continue
                update = question_overrides[qid]
                if update.get("score") is not None:
                    q["score"] = float(update.get("score", q.get("score", 0)))
                if update.get("feedback") is not None:
                    q["feedback"] = update.get("feedback", q.get("feedback", ""))

        if student.get("page_results"):
            for page in student.get("page_results", []):
                if not page.get("question_details"):
                    continue
                for q in page.get("question_details", []):
                    qid = _normalize_question_id(q.get("question_id"))
                    if not qid or qid not in question_overrides:
                        continue
                    update = question_overrides[qid]
                    if update.get("score") is not None:
                        q["score"] = float(update.get("score", q.get("score", 0)))
                    if update.get("feedback") is not None:
                        q["feedback"] = update.get("feedback", q.get("feedback", ""))
                page["score"] = sum(q.get("score", 0) for q in page.get("question_details", []))

        if student.get("question_details"):
            student["total_score"] = sum(
                q.get("score", 0) for q in student.get("question_details", [])
            )
        elif student.get("page_results"):
            student["total_score"] = sum(p.get("score", 0) for p in student.get("page_results", []))

    return student_results


def _safe_float(value: Any, default: float = 0.0) -> float:
    try:
        if value is None:
            return default
        return float(value)
    except (TypeError, ValueError):
        return default


def _build_student_results_from_page_results(
    page_results: List[Dict[str, Any]],
    *,
    default_student_key: Optional[str] = None,
    grading_mode: Optional[str] = None,
) -> List[Dict[str, Any]]:
    if not page_results:
        return []

    grouped: Dict[str, Dict[str, Any]] = {}
    for result in page_results:
        student_key = result.get("student_key") or default_student_key or "Student"
        entry = grouped.get(student_key)
        if not entry:
            entry = {
                "student_key": student_key,
                "student_id": None,
                "student_name": None,
                "start_page": None,
                "end_page": None,
                "total_score": 0.0,
                "max_total_score": 0.0,
                "question_details": [],
                "page_results": [],
                "grading_mode": grading_mode,
                "feedback": "",
                "confidence": 0.0,
                "_confidence_sum": 0.0,
                "_confidence_count": 0,
                "_has_completed": False,
                "_has_failed": False,
                "_errors": [],
            }
            grouped[student_key] = entry

        entry["page_results"].append(result)
        if result.get("grading_mode"):
            entry["grading_mode"] = result.get("grading_mode")

        if result.get("student_id") and not entry.get("student_id"):
            entry["student_id"] = result.get("student_id")
        if result.get("student_name") and not entry.get("student_name"):
            entry["student_name"] = result.get("student_name")

        if not entry["feedback"] and result.get("feedback"):
            entry["feedback"] = result.get("feedback")

        if result.get("question_details"):
            entry["question_details"].extend(result.get("question_details", []))

        page_indices = result.get("page_indices")
        if not page_indices:
            page_index = result.get("page_index")
            if page_index is not None:
                page_indices = [page_index]

        if page_indices:
            start_page = min(page_indices)
            end_page = max(page_indices)
            entry["start_page"] = (
                start_page if entry["start_page"] is None else min(entry["start_page"], start_page)
            )
            entry["end_page"] = (
                end_page if entry["end_page"] is None else max(entry["end_page"], end_page)
            )

        entry["total_score"] += _safe_float(result.get("score", 0))
        entry["max_total_score"] += _safe_float(result.get("max_score", 0))

        if result.get("status") == "completed":
            entry["_has_completed"] = True
        if result.get("status") == "failed":
            entry["_has_failed"] = True
            if result.get("error"):
                entry["_errors"].append(result.get("error"))

        confidence = result.get("confidence")
        if confidence is not None:
            entry["_confidence_sum"] += _safe_float(confidence)
            entry["_confidence_count"] += 1

    student_results: List[Dict[str, Any]] = []
    for entry in grouped.values():
        if entry["start_page"] is None:
            entry["start_page"] = 0
        if entry["end_page"] is None:
            entry["end_page"] = entry["start_page"]
        if entry["_confidence_count"]:
            entry["confidence"] = entry["_confidence_sum"] / entry["_confidence_count"]
        if entry["_has_failed"] and not entry["_has_completed"]:
            entry["status"] = "failed"
        elif entry["_has_failed"] and entry["_has_completed"]:
            entry["status"] = "partial"
        if entry["_errors"]:
            entry["error"] = entry["_errors"][0]
        entry.pop("_confidence_sum", None)
        entry.pop("_confidence_count", None)
        entry.pop("_has_completed", None)
        entry.pop("_has_failed", None)
        entry.pop("_errors", None)
        student_results.append(entry)

    return student_results


def _recompute_student_totals(student: Dict[str, Any]) -> None:
    total_score = _safe_float(student.get("total_score", 0))
    max_total_score = _safe_float(student.get("max_total_score", 0))

    question_details = student.get("question_details") or []
    if question_details:
        computed_score = sum(_safe_float(q.get("score", 0)) for q in question_details)
        computed_max = sum(
            _safe_float(q.get("max_score", q.get("maxScore", 0))) for q in question_details
        )
        if total_score <= 0 and computed_score > 0:
            student["total_score"] = computed_score
        if max_total_score <= 0 and computed_max > 0:
            student["max_total_score"] = computed_max
        return

    page_results = student.get("page_results") or []
    if page_results:
        computed_score = sum(_safe_float(p.get("score", 0)) for p in page_results)
        computed_max = sum(_safe_float(p.get("max_score", 0)) for p in page_results)
        if total_score <= 0 and computed_score > 0:
            student["total_score"] = computed_score
        if max_total_score <= 0 and computed_max > 0:
            student["max_total_score"] = computed_max


def _resolve_student_key_for_page(
    student_results: List[Dict[str, Any]],
    page_index: int,
) -> str:
    for student in student_results:
        start_page = student.get("start_page")
        end_page = student.get("end_page")
        if start_page is None or end_page is None:
            continue
        if start_page <= page_index <= end_page:
            return (
                student.get("student_key")
                or student.get("student_id")
                or student.get("student_name")
                or ""
            )
    return ""


def _find_question_pages(
    student_results: List[Dict[str, Any]],
    student_key: str,
    question_id: str,
    total_pages: int,
) -> List[int]:
    normalized_qid = _normalize_question_id(question_id)
    for student in student_results:
        key = (
            student.get("student_key")
            or student.get("student_id")
            or student.get("student_name")
            or ""
        )
        if student_key and key != student_key:
            continue
        for question in student.get("question_details", []) or []:
            qid = _normalize_question_id(question.get("question_id") or question.get("questionId"))
            if qid != normalized_qid:
                continue
            pages = _sanitize_pages(
                question.get("page_indices") or question.get("pageIndices") or [],
                total_pages,
            )
            if pages:
                return pages
        for page in student.get("page_results", []) or []:
            page_index = page.get("page_index")
            if page_index is None:
                continue
            for question in page.get("question_details", []) or []:
                qid = _normalize_question_id(
                    question.get("question_id") or question.get("questionId")
                )
                if qid == normalized_qid:
                    return [page_index]
    return []


def _select_best_question_result(
    current: Optional[Dict[str, Any]],
    candidate: Optional[Dict[str, Any]],
) -> Optional[Dict[str, Any]]:
    if not candidate:
        return current
    if not current:
        return candidate
    current_conf = _safe_float(current.get("confidence", 0))
    candidate_conf = _safe_float(candidate.get("confidence", 0))
    if candidate_conf > current_conf + 1e-6:
        return candidate
    if candidate_conf < current_conf - 1e-6:
        return current
    current_score = _safe_float(current.get("score", 0))
    candidate_score = _safe_float(candidate.get("score", 0))
    if candidate_score > current_score:
        return candidate
    if candidate_score < current_score:
        return current
    current_feedback = current.get("feedback", "") or ""
    candidate_feedback = candidate.get("feedback", "") or ""
    if len(candidate_feedback) > len(current_feedback):
        return candidate
    return current


def _apply_question_result_update(
    question: Dict[str, Any],
    update: Dict[str, Any],
) -> None:
    if update.get("score") is not None:
        question["score"] = _safe_float(update.get("score", question.get("score", 0)))
    if update.get("max_score") is not None:
        question["max_score"] = _safe_float(update.get("max_score", question.get("max_score", 0)))
    if update.get("feedback") is not None:
        question["feedback"] = update.get("feedback", question.get("feedback", ""))
    if update.get("confidence") is not None:
        question["confidence"] = _safe_float(
            update.get("confidence", question.get("confidence", 0))
        )
    scoring_points = update.get("scoring_point_results") or update.get("scoring_results")
    if scoring_points is not None:
        question_id = question.get("question_id") or question.get("questionId") or ""
        question["scoring_point_results"] = _normalize_scoring_point_results(
            scoring_points, question_id
        )
    if update.get("student_answer"):
        question["student_answer"] = update.get(
            "student_answer", question.get("student_answer", "")
        )
    if update.get("page_indices"):
        question["page_indices"] = update.get("page_indices", question.get("page_indices", []))


def _apply_regrade_updates(
    student_results: List[Dict[str, Any]],
    updates_by_student: Dict[str, Dict[str, Dict[str, Any]]],
) -> List[Dict[str, Any]]:
    if not updates_by_student:
        return student_results

    for student in student_results:
        student_key = (
            student.get("student_key")
            or student.get("student_id")
            or student.get("student_name")
            or ""
        )
        question_updates = updates_by_student.get(student_key)
        if not question_updates:
            continue

        if student.get("question_details"):
            for q in student.get("question_details", []):
                qid = _normalize_question_id(q.get("question_id") or q.get("questionId"))
                update = question_updates.get(qid)
                if update:
                    _apply_question_result_update(q, update)

        if student.get("page_results"):
            for page in student.get("page_results", []):
                page_questions = page.get("question_details") or []
                updated = False
                for q in page_questions:
                    qid = _normalize_question_id(q.get("question_id") or q.get("questionId"))
                    update = question_updates.get(qid)
                    if update:
                        _apply_question_result_update(q, update)
                        updated = True
                if updated:
                    page["score"] = sum(q.get("score", 0) for q in page_questions)

        if student.get("question_details"):
            student["total_score"] = sum(
                _safe_float(q.get("score", 0)) for q in student.get("question_details", [])
            )
        elif student.get("page_results"):
            student["total_score"] = sum(
                _safe_float(p.get("score", 0)) for p in student.get("page_results", [])
            )

    return student_results


async def _regrade_selected_questions(
    state: BatchGradingGraphState,
    student_results: List[Dict[str, Any]],
    regrade_items: List[Dict[str, Any]],
) -> List[Dict[str, Any]]:
    if not regrade_items:
        return student_results

    api_key = state.get("api_key") or os.getenv("LLM_API_KEY") or os.getenv("OPENROUTER_API_KEY")
    if not api_key:
        logger.warning("[review] regrade skipped: missing API key")
        return student_results

    processed_images = state.get("processed_images") or state.get("answer_images") or []
    total_pages = len(processed_images)
    if total_pages == 0:
        logger.warning("[review] regrade skipped: missing images")
        return student_results

    parsed_rubric = state.get("parsed_rubric", {})
    questions_data = parsed_rubric.get("questions", []) if isinstance(parsed_rubric, dict) else []

    try:
        from src.services.llm_reasoning import LLMReasoningClient
        from src.services.rubric_registry import RubricRegistry
        from src.models.grading_models import QuestionRubric, ScoringPoint
    except Exception as exc:
        logger.warning(f"[review] regrade skipped: {exc}")
        return student_results

    rubric_registry = RubricRegistry(total_score=parsed_rubric.get("total_score", 100.0))
    question_rubrics = []
    for q in questions_data:
        qid = q.get("question_id") or q.get("id") or ""
        if not qid:
            continue
        scoring_points = [
            ScoringPoint(
                description=sp.get("description", ""),
                score=sp.get("score", 0),
                is_required=sp.get("is_required", True),
                point_id=sp.get("point_id") or sp.get("pointId") or f"{qid}.{idx + 1}",
            )
            for idx, sp in enumerate(q.get("scoring_points", []))
        ]
        question_rubrics.append(
            QuestionRubric(
                question_id=str(qid),
                question_text=q.get("question_text", ""),
                max_score=q.get("max_score", 0),
                scoring_points=scoring_points,
                standard_answer=q.get("standard_answer", ""),
                grading_notes=q.get("grading_notes", ""),
                alternative_solutions=[],
            )
        )
    if question_rubrics:
        rubric_registry.register_rubrics(question_rubrics, log=False)

    reasoning_client = LLMReasoningClient(
        api_key=api_key,
        rubric_registry=rubric_registry,
    )

    student_page_map = state.get("student_page_map") or {}
    resolved_items: List[Dict[str, Any]] = []

    for item in regrade_items:
        if not isinstance(item, dict):
            continue
        question_id = _normalize_question_id(item.get("question_id") or item.get("questionId"))
        if not question_id:
            continue
        student_key = (
            item.get("student_key")
            or item.get("studentKey")
            or item.get("studentName")
            or item.get("student_name")
            or ""
        )
        raw_pages = (
            item.get("page_indices")
            or item.get("pageIndices")
            or item.get("page_index")
            or item.get("pageIndex")
        )
        if raw_pages is not None and not isinstance(raw_pages, (list, tuple)):
            raw_pages = [raw_pages]
        pages = _sanitize_pages(raw_pages, total_pages)
        if not pages:
            pages = _find_question_pages(student_results, student_key, question_id, total_pages)
        if not student_key and pages:
            student_key = student_page_map.get(pages[0]) or _resolve_student_key_for_page(
                student_results, pages[0]
            )
        if not student_key or not pages:
            logger.warning(
                f"[review] regrade skipped item: question={question_id}, student={student_key or 'unknown'}"
            )
            continue
        for page_index in pages:
            resolved_items.append(
                {
                    "student_key": student_key,
                    "question_id": question_id,
                    "page_index": page_index,
                    "notes": item.get("notes") or item.get("note") or "",
                }
            )

    if not resolved_items:
        return student_results

    updates_by_student: Dict[str, Dict[str, Dict[str, Any]]] = {}
    for item in resolved_items:
        page_index = item["page_index"]
        if not (0 <= page_index < total_pages):
            continue
        image = processed_images[page_index]
        try:
            result = await reasoning_client.grade_with_detailed_scoring_points(
                image=image,
                question_id=item["question_id"],
                page_index=page_index,
                reviewer_notes=item.get("notes") or "",
            )
            result_dict = result.to_dict()
            student_key = item["student_key"]
            qid = _normalize_question_id(item["question_id"])
            bucket = updates_by_student.setdefault(student_key, {})
            bucket[qid] = _select_best_question_result(bucket.get(qid), result_dict)
        except Exception as exc:
            logger.warning(
                f"[review] regrade failed: question={item['question_id']} page={page_index} error={exc}"
            )

    return _apply_regrade_updates(student_results, updates_by_student)


def _extract_scoring_points(question: Dict[str, Any]) -> List[Dict[str, Any]]:
    qid = _normalize_question_id(question.get("question_id") or question.get("questionId"))
    raw_points = question.get("scoring_point_results") or question.get("scoring_results") or []
    points: List[Dict[str, Any]] = []
    for spr in raw_points:
        if not isinstance(spr, dict):
            continue
        scoring_point = spr.get("scoring_point") or spr.get("scoringPoint") or {}
        description = (
            scoring_point.get("description")
            or spr.get("description")
            or spr.get("rubric_reference")
            or spr.get("rubricReference")
            or ""
        )
        point_id = spr.get("point_id") or spr.get("pointId") or scoring_point.get("point_id") or ""
        awarded = _safe_float(spr.get("awarded", spr.get("score", 0)))
        max_points = _safe_float(
            spr.get("max_points") or spr.get("maxPoints") or scoring_point.get("score") or 0
        )
        points.append(
            {
                "question_id": qid,
                "point_id": str(point_id) if point_id is not None else "",
                "description": description,
                "score": awarded,
                "max_score": max_points,
                "evidence": spr.get("evidence") or "",
                "rubric_reference": spr.get("rubric_reference") or spr.get("rubricReference") or "",
            }
        )
    return points


def _build_student_summary(student: Dict[str, Any]) -> Dict[str, Any]:
    total_score = _safe_float(student.get("total_score", 0))
    max_total_score = _safe_float(student.get("max_total_score", 0))
    percentage = (total_score / max_total_score * 100) if max_total_score > 0 else 0.0

    knowledge_points: List[Dict[str, Any]] = []
    weak_points: List[Dict[str, Any]] = []

    question_details = student.get("question_details") or []
    for question in question_details:
        for point in _extract_scoring_points(question):
            max_score = point.get("max_score", 0) or 0
            ratio = (point.get("score", 0) / max_score) if max_score > 0 else 0.0
            if ratio >= 0.85:
                mastery = "mastered"
            elif ratio >= 0.6:
                mastery = "partial"
            else:
                mastery = "weak"
            enriched = {
                **point,
                "mastery_level": mastery,
                "ratio": ratio,
            }
            knowledge_points.append(enriched)
            if mastery == "weak":
                weak_points.append(enriched)

    if not knowledge_points:
        for question in question_details:
            qid = _normalize_question_id(question.get("question_id") or question.get("questionId"))
            score = _safe_float(question.get("score", 0))
            max_score = _safe_float(question.get("max_score", 0))
            ratio = (score / max_score) if max_score > 0 else 0.0
            mastery = "partial" if ratio >= 0.6 else "weak"
            knowledge_points.append(
                {
                    "question_id": qid,
                    "point_id": "",
                    "description": question.get("feedback", "") or f"Question {qid}",
                    "score": score,
                    "max_score": max_score,
                    "mastery_level": mastery,
                    "ratio": ratio,
                    "evidence": "",
                    "rubric_reference": "",
                }
            )
            if mastery == "weak":
                weak_points.append(knowledge_points[-1])

    suggestion_candidates = []
    for point in weak_points:
        label = point.get("description") or f"Question {point.get('question_id', '')}"
        if label:
            suggestion_candidates.append(f"å»ºè®®å¤ä¹ ï¼š{label}")
    if not suggestion_candidates:
        for point in knowledge_points:
            if point.get("ratio", 0) < 0.7:
                label = point.get("description") or f"Question {point.get('question_id', '')}"
                if label:
                    suggestion_candidates.append(f"å»ºè®®å¤ä¹ ï¼š{label}")

    improvement_suggestions = []
    seen = set()
    for item in suggestion_candidates:
        if item not in seen:
            improvement_suggestions.append(item)
            seen.add(item)
        if len(improvement_suggestions) >= 5:
            break

    overall_parts = [f"æ•´ä½“å¾—åˆ† {total_score}/{max_total_score}ï¼ˆ{percentage:.1f}%ï¼‰ã€‚"]
    if percentage >= 85:
        overall_parts.append("æ•´ä½“è¡¨çŽ°ä¼˜ç§€ã€‚")
    elif percentage >= 70:
        overall_parts.append("æ•´ä½“è¡¨çŽ°è‰¯å¥½ã€‚")
    elif percentage >= 60:
        overall_parts.append("æ•´ä½“è¾¾åˆ°åŠæ ¼æ°´å¹³ã€‚")
    else:
        overall_parts.append("æ•´ä½“è¡¨çŽ°éœ€é‡ç‚¹æå‡ã€‚")

    if weak_points:
        weak_labels = []
        for point in weak_points[:3]:
            label = point.get("description") or f"Question {point.get('question_id', '')}"
            if label:
                weak_labels.append(label)
        if weak_labels:
            overall_parts.append(f"è–„å¼±ç‚¹é›†ä¸­åœ¨ï¼š{'ï¼Œ'.join(weak_labels)}ã€‚")
    else:
        overall_parts.append("æš‚æ— æ˜Žæ˜¾è–„å¼±ç‚¹ã€‚")

    return {
        "overall": " ".join(overall_parts),
        "percentage": percentage,
        "knowledge_points": knowledge_points,
        "improvement_suggestions": improvement_suggestions,
        "generated_at": datetime.now().isoformat(),
    }


def _build_self_audit(student: Dict[str, Any]) -> Dict[str, Any]:
    question_details = student.get("question_details") or []
    issues: List[Dict[str, Any]] = []
    confidence_values: List[float] = []

    for question in question_details:
        qid = _normalize_question_id(question.get("question_id") or question.get("questionId"))
        confidence = _safe_float(question.get("confidence", 0), 0.0)
        if confidence:
            confidence_values.append(confidence)

        if confidence and confidence < 0.7:
            issues.append(
                {
                    "issue_type": "low_confidence",
                    "message": f"é¢˜ç›® {qid} è¯„åˆ†ç½®ä¿¡åº¦è¾ƒä½Ž",
                    "question_id": qid,
                }
            )

        review_corrections = question.get("review_corrections") or []
        if review_corrections:
            issues.append(
                {
                    "issue_type": "logic_review_adjusted",
                    "message": f"é¢˜ç›® {qid} å­˜åœ¨é€»è¾‘å¤æ ¸ä¿®æ­£è®°å½•",
                    "question_id": qid,
                }
            )

        if not question.get("self_critique"):
            issues.append(
                {
                    "issue_type": "missing_self_critique",
                    "message": f"é¢˜ç›® {qid} ç¼ºå°‘è‡ªç™½è¯´æ˜Ž",
                    "question_id": qid,
                }
            )

        scoring_points = (
            question.get("scoring_point_results") or question.get("scoring_results") or []
        )
        if not scoring_points:
            issues.append(
                {
                    "issue_type": "missing_scoring_points",
                    "message": f"é¢˜ç›® {qid} ç¼ºå°‘è¯„åˆ†ç‚¹æ˜Žç»†",
                    "question_id": qid,
                }
            )
        else:
            missing_evidence = False
            missing_rubric_ref = False
            for spr in scoring_points:
                if not isinstance(spr, dict):
                    continue
                evidence = spr.get("evidence")
                if _is_placeholder_evidence(evidence):
                    missing_evidence = True
                rubric_ref = spr.get("rubric_reference") or spr.get("rubricReference")
                if not rubric_ref:
                    missing_rubric_ref = True
            if missing_evidence:
                issues.append(
                    {
                        "issue_type": "missing_evidence",
                        "message": f"é¢˜ç›® {qid} éƒ¨åˆ†è¯„åˆ†ç‚¹è¯æ®ä¸è¶³",
                        "question_id": qid,
                    }
                )
            if missing_rubric_ref and not question.get("rubric_refs"):
                issues.append(
                    {
                        "issue_type": "missing_rubric_ref",
                        "message": f"é¢˜ç›® {qid} éƒ¨åˆ†è¯„åˆ†ç‚¹ç¼ºå°‘æ ‡å‡†å¼•ç”¨",
                        "question_id": qid,
                    }
                )

        typo_notes = question.get("typo_notes") or question.get("typoNotes") or []
        if typo_notes:
            issues.append(
                {
                    "issue_type": "typo_detected",
                    "message": f"é¢˜ç›® {qid} å‘çŽ°é”™åˆ«å­—æ ‡æ³¨",
                    "question_id": qid,
                }
            )

    issue_types = {issue.get("issue_type") for issue in issues}
    low_confidence_questions = [
        issue.get("question_id")
        for issue in issues
        if issue.get("issue_type") == "low_confidence" and issue.get("question_id")
    ]

    compliance_analysis = [
        {
            "goal": "ä¸¥æ ¼æŒ‰è¯„åˆ†æ ‡å‡†ç»™åˆ†",
            "tag": (
                "unsure_not_reported" if "missing_rubric_ref" in issue_types else "fully_complied"
            ),
            "notes": (
                "éƒ¨åˆ†è¯„åˆ†ç‚¹ç¼ºå°‘æ ‡å‡†å¼•ç”¨"
                if "missing_rubric_ref" in issue_types
                else "æœªå‘çŽ°æ˜Žæ˜¾åç¦»è¯„åˆ†æ ‡å‡†"
            ),
        },
        {
            "goal": "æ‰£åˆ†ç‚¹éœ€æœ‰ç­”æ¡ˆè¯æ®",
            "tag": "failed_not_reported" if "missing_evidence" in issue_types else "fully_complied",
            "notes": (
                "å­˜åœ¨è¯æ®ä¸è¶³çš„è¯„åˆ†ç‚¹" if "missing_evidence" in issue_types else "è¯„åˆ†ç‚¹è¯æ®å……è¶³"
            ),
        },
        {
            "goal": "ä¸ç¡®å®šæ€§éœ€æ˜Žç¡®æŠ«éœ²",
            "tag": "unsure_not_reported" if "low_confidence" in issue_types else "fully_complied",
            "notes": (
                "å­˜åœ¨ä½Žç½®ä¿¡åº¦é¢˜ç›®" if "low_confidence" in issue_types else "æœªå‘çŽ°æ˜Žæ˜¾ä¸ç¡®å®šæ€§"
            ),
        },
    ]

    uncertainties_and_conflicts = []
    if low_confidence_questions:
        uncertainties_and_conflicts.append(
            {
                "issue": "éƒ¨åˆ†é¢˜ç›®è¯„åˆ†ç½®ä¿¡åº¦ä¸è¶³",
                "impact": "å¯èƒ½å¯¼è‡´è¯„åˆ†åå·®",
                "question_ids": low_confidence_questions,
                "reported_to_user": False,
            }
        )

    avg_confidence = sum(confidence_values) / len(confidence_values) if confidence_values else 0.7
    penalty = min(0.4, 0.05 * len(issues))
    audit_confidence = max(0.1, min(1.0, avg_confidence - penalty))
    base_grade = 7
    if "missing_evidence" in issue_types:
        base_grade -= 2
    if "missing_rubric_ref" in issue_types:
        base_grade -= 1
    if "low_confidence" in issue_types:
        base_grade -= 1
    if "missing_self_critique" in issue_types:
        base_grade -= 1
    overall_compliance_grade = max(1, min(7, base_grade))

    if issues:
        issue_labels = [issue.get("message", "") for issue in issues[:3] if issue.get("message")]
        summary = f"å‘çŽ° {len(issues)} é¡¹å¯ç–‘ç‚¹ï¼Œå»ºè®®å¤æ ¸ï¼š{'ï¼›'.join(issue_labels)}ã€‚"
    else:
        summary = "æœªå‘çŽ°æ˜Žæ˜¾å¯ç–‘ç‚¹ï¼Œç»“æžœä¸€è‡´æ€§è‰¯å¥½ã€‚"

    return {
        "summary": summary,
        "confidence": audit_confidence,
        "issues": issues,
        "compliance_analysis": compliance_analysis,
        "uncertainties_and_conflicts": uncertainties_and_conflicts,
        "overall_compliance_grade": overall_compliance_grade,
        "generated_at": datetime.now().isoformat(),
    }


def _collect_review_reasons(
    question: Dict[str, Any],
    confidence_threshold: float,
) -> List[str]:
    reasons: List[str] = []
    confidence = _safe_float(question.get("confidence", 0))
    if confidence < confidence_threshold:
        reasons.append("low_confidence")

    audit_flags = question.get("audit_flags") or []
    for flag in audit_flags:
        if flag not in reasons:
            reasons.append(flag)

    if question.get("review_corrections"):
        reasons.append("logic_review_adjusted")

    return reasons


def _apply_review_flags_and_queue(
    student_results: List[Dict[str, Any]],
    confidence_threshold: float,
) -> Tuple[List[Dict[str, Any]], List[Dict[str, Any]]]:
    queue_map: Dict[str, Dict[str, Any]] = {}
    low_confidence_questions: List[Dict[str, Any]] = []

    for student in student_results:
        student_key = (
            student.get("student_key")
            or student.get("student_id")
            or student.get("student_name")
            or ""
        )

        if student.get("needs_confirmation"):
            key = f"boundary:{student_key}"
            queue_map.setdefault(
                key,
                {
                    "type": "boundary",
                    "student_key": student_key,
                    "start_page": student.get("start_page"),
                    "end_page": student.get("end_page"),
                    "confidence": _safe_float(student.get("confidence", 0)),
                    "reasons": ["boundary_needs_confirmation"],
                },
            )

        self_audit = student.get("self_audit") or {}
        compliance_grade = _safe_float(self_audit.get("overall_compliance_grade"))
        if compliance_grade and compliance_grade <= 3:
            key = f"confession:{student_key}"
            queue_map.setdefault(
                key,
                {
                    "type": "confession",
                    "student_key": student_key,
                    "confidence": _safe_float(self_audit.get("confidence", 0)),
                    "compliance_grade": compliance_grade,
                    "reasons": ["confession_low_grade"],
                },
            )

        for question in student.get("question_details", []) or []:
            qid = _normalize_question_id(question.get("question_id") or question.get("questionId"))
            if not qid:
                continue
            reasons = _collect_review_reasons(question, confidence_threshold)
            if not reasons:
                continue
            question["needs_review"] = True
            question["review_reasons"] = reasons

            if "low_confidence" in reasons:
                low_confidence_questions.append(
                    {
                        "student_key": student_key,
                        "question_id": qid,
                        "confidence": _safe_float(question.get("confidence", 0)),
                    }
                )

            page_indices = question.get("page_indices") or question.get("pageIndices") or []
            key = f"question:{student_key}:{qid}"
            existing = queue_map.get(key)
            if existing:
                merged_reasons = set(existing.get("reasons") or [])
                merged_reasons.update(reasons)
                existing["reasons"] = list(merged_reasons)
                if page_indices:
                    existing_pages = set(existing.get("page_indices") or [])
                    existing_pages.update(page_indices)
                    existing["page_indices"] = sorted(existing_pages)
                continue

            queue_map[key] = {
                "type": "question",
                "student_key": student_key,
                "question_id": qid,
                "page_indices": page_indices,
                "confidence": _safe_float(question.get("confidence", 0)),
                "reasons": reasons,
            }

        for page in student.get("page_results", []) or []:
            page_confidence = _safe_float(page.get("confidence", 1.0))
            if page_confidence < confidence_threshold:
                page["needs_review"] = True
                page["review_reasons"] = ["low_confidence"]

    return list(queue_map.values()), low_confidence_questions


def _build_class_report(student_results: List[Dict[str, Any]]) -> Dict[str, Any]:
    total_students = len(student_results)
    if total_students == 0:
        return {
            "total_students": 0,
            "generated_at": datetime.now().isoformat(),
        }

    total_scores = []
    total_percentages = []
    distribution = {"A": 0, "B": 0, "C": 0, "D": 0, "E": 0}
    knowledge_aggregate: Dict[str, Dict[str, Any]] = {}

    for student in student_results:
        total_score = _safe_float(student.get("total_score", 0))
        max_score = _safe_float(student.get("max_total_score", 0))
        percentage = (total_score / max_score * 100) if max_score > 0 else 0.0
        total_scores.append(total_score)
        total_percentages.append(percentage)

        if percentage >= 85:
            distribution["A"] += 1
        elif percentage >= 70:
            distribution["B"] += 1
        elif percentage >= 60:
            distribution["C"] += 1
        elif percentage >= 50:
            distribution["D"] += 1
        else:
            distribution["E"] += 1

        summary = student.get("student_summary") or _build_student_summary(student)
        for point in summary.get("knowledge_points", []):
            key = point.get("point_id") or point.get("description") or ""
            if not key:
                continue
            entry = knowledge_aggregate.setdefault(
                key,
                {
                    "point_id": point.get("point_id") or "",
                    "description": point.get("description") or "",
                    "total_score": 0.0,
                    "total_max_score": 0.0,
                },
            )
            entry["total_score"] += _safe_float(point.get("score", 0))
            entry["total_max_score"] += _safe_float(point.get("max_score", 0))

    average_score = sum(total_scores) / total_students if total_students else 0.0
    average_percentage = sum(total_percentages) / total_students if total_students else 0.0
    pass_rate = (
        sum(1 for pct in total_percentages if pct >= 60) / total_students if total_students else 0.0
    )

    weak_points = []
    strong_points = []
    for entry in knowledge_aggregate.values():
        max_score = entry.get("total_max_score", 0) or 0
        ratio = (entry.get("total_score", 0) / max_score) if max_score > 0 else 0.0
        record = {
            "point_id": entry.get("point_id"),
            "description": entry.get("description"),
            "mastery_ratio": ratio,
        }
        if ratio < 0.6:
            weak_points.append(record)
        elif ratio >= 0.85:
            strong_points.append(record)

    weak_points.sort(key=lambda x: x.get("mastery_ratio", 0))
    strong_points.sort(key=lambda x: x.get("mastery_ratio", 0), reverse=True)

    summary_parts = [
        f"ç­çº§å¹³å‡åˆ† {average_score:.1f}ï¼Œå¹³å‡å¾—åˆ†çŽ‡ {average_percentage:.1f}%ã€‚",
        f"åŠæ ¼çŽ‡ {pass_rate * 100:.1f}%ã€‚",
    ]
    if weak_points:
        weak_labels = [p.get("description", "") for p in weak_points[:3] if p.get("description")]
        if weak_labels:
            summary_parts.append(f"ä¸»è¦è–„å¼±çŸ¥è¯†ç‚¹ï¼š{'ï¼Œ'.join(weak_labels)}ã€‚")
    if strong_points:
        strong_labels = [
            p.get("description", "") for p in strong_points[:3] if p.get("description")
        ]
        if strong_labels:
            summary_parts.append(f"ä¼˜åŠ¿çŸ¥è¯†ç‚¹ï¼š{'ï¼Œ'.join(strong_labels)}ã€‚")

    return {
        "total_students": total_students,
        "average_score": average_score,
        "average_percentage": average_percentage,
        "pass_rate": pass_rate,
        "score_distribution": distribution,
        "weak_points": weak_points[:10],
        "strong_points": strong_points[:10],
        "summary": " ".join(summary_parts),
        "generated_at": datetime.now().isoformat(),
    }


def _apply_student_result_overrides(
    student_results: List[Dict[str, Any]],
    overrides: List[Dict[str, Any]],
) -> List[Dict[str, Any]]:
    """åº”ç”¨å­¦ç”Ÿç»“æžœè¦†ç›–"""
    if not overrides:
        return student_results

    # æž„å»ºè¦†ç›–æ˜ å°„
    override_map = {}
    for item in overrides:
        key = item.get("student_key") or item.get("studentKey")
        if key:
            override_map[key] = item

    updated_results = []
    for student in student_results:
        student_key = student.get("student_key")
        if student_key not in override_map:
            updated_results.append(student)
            continue

        override = override_map[student_key]
        updated_student = student.copy()

        # æž„å»ºé¢˜ç›®è¦†ç›–æ˜ å°„
        q_override_map = {}
        for q in override.get("questionResults") or override.get("question_results") or []:
            qid = _normalize_question_id(q.get("questionId") or q.get("question_id"))
            if qid:
                q_override_map[qid] = q

        # æ›´æ–° question_details
        current_details = student.get("question_details") or []
        updated_details = []
        for q in current_details:
            qid = _normalize_question_id(q.get("question_id"))
            if qid in q_override_map:
                logger.info(f"[review] applying override for student={student_key} question={qid}")
                q_override = q_override_map[qid]
                updated_q = q.copy()

                # æ›´æ–°åˆ†æ•°
                if "score" in q_override:
                    updated_q["score"] = float(q_override["score"])

                # æ›´æ–°åé¦ˆ
                if "feedback" in q_override:
                    updated_q["feedback"] = q_override["feedback"]

                updated_details.append(updated_q)
            else:
                updated_details.append(q)

        updated_student["question_details"] = updated_details

        # é‡æ–°è®¡ç®—æ€»åˆ†
        updated_student["total_score"] = sum(float(q.get("score", 0)) for q in updated_details)

        updated_results.append(updated_student)

    return updated_results


def _extract_logic_review_questions(student: Dict[str, Any]) -> List[Dict[str, Any]]:
    """
    æå–éœ€è¦é€»è¾‘å¤æ ¸çš„é¢˜ç›®

    åŸºäºŽè‡ªç™½ï¼ˆself_reportï¼‰ç­›é€‰éœ€è¦å¤æ ¸çš„é¢˜ç›®ï¼š
    - åªå¤æ ¸è‡ªç™½ä¸­æ ‡è®°æœ‰é—®é¢˜/ä½Žç½®ä¿¡åº¦çš„é¢˜ç›®
    - å¦‚æžœæ²¡æœ‰è‡ªç™½æˆ–è‡ªç™½ä¸ºç©ºï¼Œåˆ™ä¸å¤æ ¸ä»»ä½•é¢˜ç›®

    è¿™æ ·å¯ä»¥é¿å…å¯¹æ‰€æœ‰é¢˜ç›®è¿›è¡Œé‡å¤å¤æ ¸ï¼Œæé«˜æ•ˆçŽ‡å¹¶å‡å°‘ token æ¶ˆè€—ã€‚
    """
    details = student.get("question_details") or []
    if not isinstance(details, list) or not details:
        # å°è¯•ä»Ž page_results ä¸­æå–
        fallback: List[Dict[str, Any]] = []
        for page in student.get("page_results", []) or []:
            for q in page.get("question_details", []) or []:
                merged = dict(q)
                if not merged.get("page_indices") and page.get("page_index") is not None:
                    merged["page_indices"] = [page.get("page_index")]
                fallback.append(merged)
        details = fallback

    if not details:
        return []

    # èŽ·å–è‡ªç™½æŠ¥å‘Š
    self_report = student.get("self_report") or student.get("confession") or {}
    if not self_report:
        # æ²¡æœ‰è‡ªç™½ï¼Œä¸éœ€è¦å¤æ ¸
        logger.debug("[_extract_logic_review_questions] æ²¡æœ‰è‡ªç™½æŠ¥å‘Šï¼Œè·³è¿‡é€»è¾‘å¤æ ¸")
        return []

    # æ”¶é›†è‡ªç™½ä¸­æ ‡è®°æœ‰é—®é¢˜çš„é¢˜ç›® ID
    flagged_question_ids: set = set()

    # 1. ä»Ž high_risk_questions ä¸­æå–
    high_risk = self_report.get("high_risk_questions") or []
    if isinstance(high_risk, list):
        for item in high_risk:
            if isinstance(item, dict):
                qid = item.get("question_id") or item.get("questionId")
                if qid:
                    flagged_question_ids.add(_normalize_question_id(str(qid)))
            elif isinstance(item, (str, int)):
                flagged_question_ids.add(_normalize_question_id(str(item)))

    # 2. ä»Ž issues ä¸­æå–
    issues = self_report.get("issues") or []
    if isinstance(issues, list):
        for issue in issues:
            if isinstance(issue, dict):
                qid = issue.get("question_id") or issue.get("questionId")
                if qid:
                    flagged_question_ids.add(_normalize_question_id(str(qid)))

    # 3. ä»Ž potential_errors ä¸­æå–
    potential_errors = self_report.get("potential_errors") or []
    if isinstance(potential_errors, list):
        for err in potential_errors:
            if isinstance(err, dict):
                qid = err.get("question_id") or err.get("questionId")
                if qid:
                    flagged_question_ids.add(_normalize_question_id(str(qid)))

    # 4. ä»Ž warnings ä¸­æå–
    warnings = self_report.get("warnings") or []
    if isinstance(warnings, list):
        for warn in warnings:
            if isinstance(warn, dict):
                qid = warn.get("question_id") or warn.get("questionId")
                if qid:
                    flagged_question_ids.add(_normalize_question_id(str(qid)))

    # 5. æ£€æŸ¥æ¯é“é¢˜çš„ self_critique_confidenceï¼Œä½ŽäºŽé˜ˆå€¼çš„ä¹Ÿéœ€è¦å¤æ ¸
    confidence_threshold = float(os.getenv("LOGIC_REVIEW_CONFIDENCE_THRESHOLD", "0.7"))
    for q in details:
        qid = _normalize_question_id(q.get("question_id") or q.get("questionId") or "")
        self_critique_conf = q.get("self_critique_confidence")
        if self_critique_conf is not None:
            try:
                if float(self_critique_conf) < confidence_threshold:
                    flagged_question_ids.add(qid)
            except (ValueError, TypeError):
                pass

        # æ£€æŸ¥ self_critique æ˜¯å¦åŒ…å«ä¸ç¡®å®š/éœ€è¦å¤æ ¸çš„å…³é”®è¯
        self_critique = q.get("self_critique") or ""
        if isinstance(self_critique, str):
            uncertainty_keywords = [
                "ä¸ç¡®å®š",
                "å¯èƒ½",
                "å»ºè®®å¤æ ¸",
                "éœ€è¦ç¡®è®¤",
                "è¯æ®ä¸è¶³",
                "uncertain",
                "may",
                "might",
                "review",
                "unclear",
            ]
            if any(kw in self_critique.lower() for kw in uncertainty_keywords):
                flagged_question_ids.add(qid)

    # å¦‚æžœæ²¡æœ‰ä»»ä½•é¢˜ç›®è¢«æ ‡è®°ï¼Œè¿”å›žç©ºåˆ—è¡¨
    if not flagged_question_ids:
        logger.info("[_extract_logic_review_questions] è‡ªç™½ä¸­æ²¡æœ‰æ ‡è®°éœ€è¦å¤æ ¸çš„é¢˜ç›®")
        return []

    # åªè¿”å›žè¢«æ ‡è®°çš„é¢˜ç›®
    flagged_questions = []
    for q in details:
        qid = _normalize_question_id(q.get("question_id") or q.get("questionId") or "")
        if qid in flagged_question_ids:
            flagged_questions.append(q)

    logger.info(
        f"[_extract_logic_review_questions] ä»Ž {len(details)} é“é¢˜ä¸­ç­›é€‰å‡º "
        f"{len(flagged_questions)} é“éœ€è¦å¤æ ¸çš„é¢˜ç›®: {list(flagged_question_ids)}"
    )

    return flagged_questions


def _normalize_logic_review_items(raw: Any) -> List[Dict[str, Any]]:
    if not raw:
        return []
    if isinstance(raw, list):
        return [item for item in raw if isinstance(item, dict)]
    if isinstance(raw, dict):
        return [raw]
    return []


def _normalize_logic_review_issues(raw: Any) -> List[Dict[str, Any]]:
    issues: List[Dict[str, Any]] = []
    for item in _normalize_logic_review_items(raw):
        if "message" in item:
            issues.append(item)
            continue
        summary = item.get("summary") if isinstance(item, dict) else None
        if summary:
            issues.append({"issue_type": "logic_review_note", "message": summary})
    if isinstance(raw, list):
        for item in raw:
            if isinstance(item, str):
                issues.append({"issue_type": "logic_review_note", "message": item})
    if isinstance(raw, str):
        issues.append({"issue_type": "logic_review_note", "message": raw})
    return issues


def _normalize_logic_review_self_audit(raw: Any) -> Optional[Dict[str, Any]]:
    if not isinstance(raw, dict):
        return None
    issues = _normalize_logic_review_issues(raw.get("issues"))
    compliance_analysis = raw.get("compliance_analysis") or raw.get("complianceAnalysis") or []
    uncertainties = (
        raw.get("uncertainties_and_conflicts") or raw.get("uncertaintiesAndConflicts") or []
    )
    overall_grade = raw.get("overall_compliance_grade") or raw.get("overallComplianceGrade")
    return {
        "summary": raw.get("summary") or "",
        "confidence": _safe_float(raw.get("confidence", 0.0)),
        "issues": issues,
        "compliance_analysis": compliance_analysis if isinstance(compliance_analysis, list) else [],
        "uncertainties_and_conflicts": uncertainties if isinstance(uncertainties, list) else [],
        "overall_compliance_grade": _safe_float(overall_grade, 0.0),
        "generated_at": datetime.now().isoformat(),
        "honesty_note": raw.get("honesty_note") or raw.get("honestyNote") or "",
    }


def _build_logic_review_summary(question_details: List[Dict[str, Any]]) -> Dict[str, Any]:
    confidences = [
        _safe_float(q.get("confidence"))
        for q in question_details
        if q.get("confidence") is not None
    ]
    avg_confidence = sum(confidences) / len(confidences) if confidences else None
    low_confidence_count = sum(1 for c in confidences if c < 0.7)
    return {
        "totalQuestions": len(question_details),
        "averageConfidence": avg_confidence,
        "lowConfidenceCount": low_confidence_count,
    }


def _extract_logic_review_awarded(item: Dict[str, Any]) -> Optional[float]:
    keys = [
        "correct_awarded",
        "correctAwarded",
        "correct_score",
        "correctScore",
        "adjusted_awarded",
        "adjustedAwarded",
        "corrected_awarded",
        "correctedAwarded",
    ]
    for key in keys:
        if key in item:
            value = item.get(key)
            if value is None:
                continue
            return _safe_float(value)
    return None


def _apply_logic_review_corrections(
    question: Dict[str, Any],
    review: Dict[str, Any],
) -> Dict[str, Any]:
    corrections = _normalize_logic_review_items(
        review.get("review_corrections") or review.get("reviewCorrections")
    )
    if not corrections:
        return question
    scoring_points = question.get("scoring_point_results") or question.get("scoring_results") or []
    if not isinstance(scoring_points, list) or not scoring_points:
        return question

    correction_map: Dict[str, Dict[str, Any]] = {}
    for item in corrections:
        point_id = _normalize_question_id(item.get("point_id") or item.get("pointId"))
        if point_id:
            correction_map[point_id] = item

    if not correction_map:
        return question

    updated_points = []
    adjusted = False
    for sp in scoring_points:
        if not isinstance(sp, dict):
            updated_points.append(sp)
            continue
        sp_copy = dict(sp)
        point_id = _normalize_question_id(
            sp.get("point_id")
            or sp.get("pointId")
            or (sp.get("scoring_point") or {}).get("point_id")
        )
        correction = correction_map.get(point_id or "")
        if correction:
            proposed = _extract_logic_review_awarded(correction)
            if proposed is not None:
                max_points = (
                    sp_copy.get("max_points")
                    or sp_copy.get("maxPoints")
                    or (sp_copy.get("scoring_point") or {}).get("score")
                    or 0
                )
                max_points = max(0.0, _safe_float(max_points))
                current_awarded = _safe_float(sp_copy.get("awarded", sp_copy.get("score", 0)))
                proposed = min(max(proposed, 0.0), max_points)
                delta = proposed - current_awarded
                if abs(delta) <= 1.01:
                    sp_copy["review_adjusted"] = True
                    sp_copy["review_before"] = {
                        "awarded": current_awarded,
                        "decision": sp_copy.get("decision"),
                    }
                    sp_copy["review_reason"] = (
                        correction.get("review_reason")
                        or correction.get("reviewReason")
                        or "Logic review adjustment"
                    )
                    sp_copy["review_by"] = "logic_review"
                    sp_copy["awarded"] = proposed
                    corrected_decision = correction.get("correct_decision") or correction.get(
                        "correctDecision"
                    )
                    if corrected_decision:
                        sp_copy["decision"] = corrected_decision
                    adjusted = True
        updated_points.append(sp_copy)

    if not adjusted:
        return question

    updated = dict(question)
    updated["scoring_point_results"] = updated_points
    new_score = sum(_safe_float(sp.get("awarded", sp.get("score", 0))) for sp in updated_points)
    max_score = _safe_float(updated.get("max_score") or updated.get("maxScore") or new_score)
    if max_score:
        new_score = min(new_score, max_score)
    updated["score"] = new_score
    return updated


def _merge_logic_review_fields(
    question: Dict[str, Any],
    review: Dict[str, Any],
) -> Dict[str, Any]:
    updated = dict(question)
    updated = _apply_logic_review_corrections(updated, review)
    confidence = review.get("confidence")
    if confidence is not None:
        updated["confidence"] = max(0.0, min(1.0, _safe_float(confidence)))
    confidence_reason = review.get("confidence_reason") or review.get("confidenceReason")
    if confidence_reason:
        updated["confidence_reason"] = confidence_reason
    self_critique = review.get("self_critique") or review.get("selfCritique")
    if self_critique:
        updated["self_critique"] = self_critique
    self_conf = review.get("self_critique_confidence") or review.get("selfCritiqueConfidence")
    if self_conf is not None:
        updated["self_critique_confidence"] = max(0.0, min(1.0, _safe_float(self_conf)))
    review_summary = review.get("review_summary") or review.get("reviewSummary")
    if review_summary:
        updated["review_summary"] = review_summary
    review_corrections = review.get("review_corrections") or review.get("reviewCorrections") or []
    existing_corrections = updated.get("review_corrections") or []
    merged_corrections = (
        list(existing_corrections) if isinstance(existing_corrections, list) else []
    )
    for item in _normalize_logic_review_items(review_corrections):
        if item not in merged_corrections:
            merged_corrections.append(item)
    if merged_corrections:
        updated["review_corrections"] = merged_corrections
    honesty = review.get("honesty_note") or review.get("honestyNote")
    if honesty:
        updated["honesty_note"] = honesty
    return updated


# ==================== è‡ªç™½èŠ‚ç‚¹ ====================


def _build_self_report_prompt(
    student: Dict[str, Any],
    question_details: List[Dict[str, Any]],
    rubric_map: Dict[str, Dict[str, Any]],
    memory_context: Optional[Dict[str, Any]] = None,
) -> str:
    """
    æž„å»ºè‡ªç™½ (Confession) LLM æç¤ºè¯

    è‡ªç™½çš„æ ¸å¿ƒåŠŸèƒ½ï¼šé£Žé™©æŠ«éœ² / é€æ˜Žåº¦æŠ¥å‘Š
    - ä¸å…·å¤‡æ‰¹æ”¹ç»“æžœçš„æ›´æ­£èƒ½åŠ›
    - åªå…è®¸è¯´å‡è®¾ã€ä¿¡æ¯ç¼ºå£ã€ä¸ç¡®å®šç‚¹ã€å¯èƒ½å‡ºé”™ç‚¹
    - ç¦æ­¢ç¼–é€ "æˆ‘æŸ¥è¿‡/æˆ‘çœ‹åˆ°äº†"ä¹‹ç±»è¯æ®
    - **é›†æˆè®°å¿†ç³»ç»Ÿ**ï¼šåŸºäºŽåŽ†å²ç»éªŒè¿›è¡Œé£Žé™©åˆ†æž
    """
    student_key = student.get("student_key") or student.get("student_name") or "Unknown"

    lines = [
        "# è§’è‰²ï¼šèµ„æ·±æ‰¹æ”¹è´¨é‡å®¡è®¡å¸ˆ (Confession / Risk Disclosure)",
        "",
        "## æ ¸å¿ƒä»»åŠ¡",
        "ä½ æ˜¯ä¸€ä½ç»éªŒä¸°å¯Œçš„è´¨é‡å®¡è®¡å¸ˆï¼Œæ‹¥æœ‰**å…±äº«è®°å¿†**èƒ½åŠ›ï¼Œèƒ½å¤ŸåŸºäºŽåŽ†å²æ‰¹æ”¹ç»éªŒè¿›è¡Œé£Žé™©åˆ†æžã€‚",
        "ä½ çš„ä»»åŠ¡æ˜¯**æŠ«éœ²æ‰¹æ”¹é£Žé™©**ï¼Œå¹¶ç»“åˆåŽ†å²æ•™è®­æå‡ºè­¦ç¤ºã€‚",
        "äº¤ä»£ï¼š'æˆ‘å‡­ä»€ä¹ˆè¿™ä¹ˆè¯´ / æˆ‘å“ªé‡Œä¸ç¡®å®š / æˆ‘å¯èƒ½åœ¨çžŽçŒœ / åŽ†å²ä¸Šç±»ä¼¼æƒ…å†µå¦‚ä½•'",
        "",
        "## ä¸¥æ ¼çº¦æŸ",
        "1. **ç¦æ­¢æ›´æ­£**ï¼šä½ ä¸èƒ½ä¿®æ”¹ä»»ä½•è¯„åˆ†ç»“æžœï¼Œåªèƒ½æŠ«éœ²é£Žé™©",
        '2. **ç¦æ­¢ç¼–é€ è¯æ®**ï¼šç¦æ­¢è¯´"æˆ‘æŸ¥è¿‡/æˆ‘çœ‹åˆ°äº†/æ ¹æ®å›¾ç‰‡..."ç­‰',
        "3. **åªå…è®¸æŠ«éœ²**ï¼šå‡è®¾ã€ä¿¡æ¯ç¼ºå£ã€ä¸ç¡®å®šç‚¹ã€å¯èƒ½å‡ºé”™ç‚¹ã€ç½®ä¿¡åº¦",
        "4. **è¯šå®žæ ¡å‡†**ï¼šå¦‚æžœä¸ç¡®å®šï¼Œå¿…é¡»æ˜Žç¡®è¯´æ˜Žä¸ç¡®å®šç¨‹åº¦",
        "",
        "## éœ€è¦æŠ«éœ²çš„å†…å®¹ç±»åž‹",
        "- **å‡è®¾æ¸…å• (assumptions)**ï¼šæ‰¹æ”¹æ—¶åšäº†å“ªäº›éšå«å‡è®¾ï¼Ÿ",
        "- **ä¿¡æ¯ç¼ºå£ (information_gaps)**ï¼šç¼ºå°‘ä»€ä¹ˆä¿¡æ¯å¯¼è‡´åˆ¤æ–­å›°éš¾ï¼Ÿ",
        "- **è¯æ®ç¼ºå£ (evidence_gaps)**ï¼šå“ªäº›è¯„åˆ†ç‚¹ç¼ºä¹å……åˆ†è¯æ®ï¼Ÿ",
        "- **ä¸ç¡®å®šç‚¹ (uncertainties)**ï¼šå“ªäº›åˆ¤æ–­ç½®ä¿¡åº¦ä½Žï¼Ÿä¸ºä»€ä¹ˆï¼Ÿ",
        "- **å¯èƒ½å‡ºé”™ç‚¹ (potential_errors)**ï¼šå“ªäº›åœ°æ–¹æœ€å¯èƒ½æ˜¯å¹»è§‰æˆ–è¯¯åˆ¤ï¼Ÿ",
        "- **æ•´ä½“ç½®ä¿¡åº¦ (overall_confidence)**ï¼š0.0-1.0ï¼Œæ ¡å‡†åŽçš„ç½®ä¿¡åº¦",
        "",
        "## é«˜é£Žé™©ä¿¡å·è¯†åˆ«ï¼ˆé‡ç‚¹å…³æ³¨ï¼‰",
        "",
        "### 1. è¯„åˆ†å¼‚å¸¸ä¿¡å·",
        "- **æ»¡åˆ†/é›¶åˆ†é£Žé™©**ï¼šæ»¡åˆ†æˆ–é›¶åˆ†çš„è¯„åˆ†éœ€è¦é¢å¤–å®¡è§†",
        "- **å¾—åˆ†ä¸Žè¯æ®ä¸åŒ¹é…**ï¼šç»™é«˜åˆ†ä½†è¯æ®æ¨¡ç³Šï¼Œæˆ–æ‰£åˆ†ä½†ç†ç”±ä¸å……åˆ†",
        "- **è¾¹ç•Œåˆ†æ•°**ï¼šåˆšå¥½è¸©çº¿çš„åˆ†æ•°ï¼ˆå¦‚ 59/60 åˆ†ï¼‰éœ€è¦ç‰¹åˆ«è¯´æ˜Ž",
        "- **ç½®ä¿¡åº¦ä¸Žå¾—åˆ†çŸ›ç›¾**ï¼šä½Žç½®ä¿¡åº¦ä½†ç»™äº†æ˜Žç¡®åˆ†æ•°",
        "",
        "### 2. è¯æ®è´¨é‡ä¿¡å·",
        "- **ç©ºè¯æ®/æ¨¡ç³Šè¯æ®**ï¼ševidence å­—æ®µä¸ºç©ºæˆ–è¿‡äºŽç¬¼ç»Ÿ",
        "- **ä½ç½®ä¿¡æ¯ç¼ºå¤±**ï¼šæ²¡æœ‰æ˜Žç¡®æŒ‡å‡ºè¯æ®åœ¨å›¾ç‰‡ä¸­çš„ä½ç½®",
        "- **è‡ªç›¸çŸ›ç›¾**ï¼šfeedback å’Œ evidence å†…å®¹å†²çª",
        "- **å…¬å¼è¯†åˆ«é£Žé™©**ï¼šæ•°å­¦å…¬å¼å¯èƒ½è¢«è¯¯è¯»",
        "",
        "### 3. é¢˜åž‹ç‰¹å®šé£Žé™©",
        "- **å¼€æ”¾é¢˜é£Žé™©**ï¼šä¸»è§‚è¯„åˆ†ä¾èµ–åˆ¤æ–­ï¼Œå®¹æ˜“æœ‰åå·®",
        "- **å¤šæ­¥éª¤é¢˜é£Žé™©**ï¼šé”™è¯¯ä¼ é€’å¯èƒ½å¯¼è‡´é‡å¤æ‰£åˆ†",
        "- **å¦ç±»è§£æ³•é£Žé™©**ï¼šå­¦ç”Ÿå¯èƒ½ä½¿ç”¨éžæ ‡å‡†ä½†æ­£ç¡®çš„è§£æ³•",
        "- **è·¨é¡µé¢˜é£Žé™©**ï¼šä¿¡æ¯åˆ†æ•£åœ¨å¤šé¡µï¼Œå¯èƒ½é—æ¼å†…å®¹",
        "",
        "### 4. ç³»ç»Ÿæ€§é£Žé™©",
        "- **å­—è¿¹è¯†åˆ«**ï¼šå­—è¿¹æ½¦è‰å¯èƒ½å¯¼è‡´è¯¯è¯»",
        "- **å›¾ç‰‡è´¨é‡**ï¼šæ¨¡ç³Šã€å€¾æ–œã€å…‰çº¿é—®é¢˜",
        "- **è¯„åˆ†æ ‡å‡†ç†è§£**ï¼šå¯èƒ½è¯¯è§£äº†æŸä¸ªå¾—åˆ†ç‚¹çš„å«ä¹‰",
        "",
        f"## å­¦ç”Ÿæ ‡è¯†: {student_key}",
        "",
        "## æ‰¹æ”¹æ‘˜è¦ï¼ˆä¾›ä½ åšé£Žé™©åˆ†æžï¼‰",
    ]

    # ç»Ÿè®¡é£Žé™©æŒ‡æ ‡
    total_questions = len(question_details)
    high_score_count = 0  # æ»¡åˆ†é¢˜æ•°
    zero_score_count = 0  # é›¶åˆ†é¢˜æ•°
    low_confidence_count = 0  # ä½Žç½®ä¿¡åº¦é¢˜æ•°
    empty_evidence_count = 0  # ç©ºè¯æ®é¢˜æ•°

    for idx, question in enumerate(question_details[:20]):
        qid = _normalize_question_id(
            question.get("question_id") or question.get("questionId")
        ) or str(idx + 1)
        rubric = rubric_map.get(qid, {})
        score = question.get("score", 0)
        max_score = question.get("max_score", rubric.get("max_score", 0))
        confidence = question.get("confidence", 0.0)
        student_answer = _trim_text(question.get("student_answer", ""), 300)
        feedback = _trim_text(question.get("feedback", ""), 200)

        # ç»Ÿè®¡é£Žé™©æŒ‡æ ‡
        if max_score > 0 and score >= max_score:
            high_score_count += 1
        if score == 0 and max_score > 0:
            zero_score_count += 1
        if confidence < 0.7:
            low_confidence_count += 1

        # æ ‡è®°å¯èƒ½çš„é£Žé™©
        risk_flags = []
        if max_score > 0 and score >= max_score:
            risk_flags.append("âš ï¸æ»¡åˆ†")
        if score == 0 and max_score > 0:
            risk_flags.append("âš ï¸é›¶åˆ†")
        if confidence < 0.7:
            risk_flags.append(f"âš ï¸ä½Žç½®ä¿¡åº¦({confidence:.2f})")

        risk_str = " ".join(risk_flags) if risk_flags else ""
        lines.append(f"- Q{qid}: {score}/{max_score} (ç½®ä¿¡åº¦: {confidence:.2f}) {risk_str}")
        if student_answer:
            lines.append(f"  å­¦ç”Ÿç­”æ¡ˆ: {student_answer}")
        if feedback:
            lines.append(f"  åé¦ˆ: {feedback}")

        scoring_points = (
            question.get("scoring_point_results") or question.get("scoring_results") or []
        )
        if scoring_points:
            for sp in scoring_points[:4]:
                if not isinstance(sp, dict):
                    continue
                point_id = sp.get("point_id") or sp.get("pointId") or ""
                awarded = sp.get("awarded", sp.get("score", 0))
                evidence = _trim_text(sp.get("evidence", ""), 100)

                # æ£€æŸ¥è¯æ®è´¨é‡
                evidence_flag = ""
                if not evidence or evidence.strip() in ["", "æ— ", "N/A", "null", "None"]:
                    evidence_flag = " âš ï¸ç©ºè¯æ®"
                    empty_evidence_count += 1

                lines.append(
                    f"    - {point_id}: {awarded}åˆ†, è¯æ®: {evidence or 'æ— '}{evidence_flag}"
                )
        lines.append("")

    # æ·»åŠ é£Žé™©æ‘˜è¦
    lines.append("## æ‰¹æ”¹é£Žé™©æ‘˜è¦")
    lines.append(f"- æ€»é¢˜æ•°: {total_questions}")
    lines.append(
        f"- æ»¡åˆ†é¢˜æ•°: {high_score_count} {'(éœ€è¦å®¡è§†)' if high_score_count > total_questions * 0.5 else ''}"
    )
    lines.append(
        f"- é›¶åˆ†é¢˜æ•°: {zero_score_count} {'(éœ€è¦å®¡è§†)' if zero_score_count > total_questions * 0.3 else ''}"
    )
    lines.append(
        f"- ä½Žç½®ä¿¡åº¦é¢˜æ•°: {low_confidence_count} {'(é‡ç‚¹å…³æ³¨)' if low_confidence_count > 0 else ''}"
    )
    lines.append(
        f"- ç©ºè¯æ®é¢˜æ•°: {empty_evidence_count} {'(å¿…é¡»æŠ«éœ²)' if empty_evidence_count > 0 else ''}"
    )
    lines.append("")

    # æ·»åŠ è®°å¿†ä¸Šä¸‹æ–‡ï¼ˆå¦‚æžœæœ‰ï¼‰
    if memory_context:
        lines.append("## å…±äº«è®°å¿†ï¼šåŽ†å²æ‰¹æ”¹ç»éªŒ")
        lines.append("ä»¥ä¸‹ä¿¡æ¯æ¥è‡ªåŽ†å²æ‰¹æ”¹æ•°æ®çš„ç§¯ç´¯ï¼Œè¯·æ®æ­¤è¿›è¡Œæ›´å‡†ç¡®çš„é£Žé™©åˆ†æžï¼š")
        lines.append("")

        # åŽ†å²é”™è¯¯æ¨¡å¼
        error_patterns = memory_context.get("historical_error_patterns", [])
        if error_patterns:
            lines.append("### åŽ†å²å¸¸è§é”™è¯¯æ¨¡å¼")
            for i, pattern in enumerate(error_patterns[:5], 1):
                lines.append(
                    f"{i}. **{pattern['pattern']}** (å‡ºçŽ° {pattern['occurrence_count']} æ¬¡, "
                    f"å¯ä¿¡åº¦ {pattern['confidence']:.0%})"
                )
                lines.append(f"   æ•™è®­: {pattern['lesson']}")
            lines.append("")

        # ä¿®æ­£åŽ†å²
        corrections = memory_context.get("correction_history", [])
        if corrections:
            lines.append("### åŽ†å²é‡å¤§ä¿®æ­£æ¡ˆä¾‹")
            lines.append("ä»¥ä¸‹æ˜¯åŽ†å²ä¸Šè¢«å¤§å¹…ä¿®æ­£çš„è¯„åˆ†æ¡ˆä¾‹ï¼Œ**åŠ¡å¿…å¼•ä»¥ä¸ºæˆ’**ï¼š")
            for i, corr in enumerate(corrections[:3], 1):
                lines.append(f"{i}. {corr['pattern']}")
                if corr.get("context") and corr["context"].get("difference"):
                    lines.append(f"   åˆ†æ•°å·®è·: {abs(corr['context']['difference'])} åˆ†")
            lines.append("")

        # ç½®ä¿¡åº¦æ ¡å‡†
        calibrations = memory_context.get("calibration_suggestions", {})
        if calibrations:
            lines.append("### ç½®ä¿¡åº¦æ ¡å‡†å»ºè®®")
            lines.append("åŸºäºŽåŽ†å²æ•°æ®ï¼Œä½ çš„ç½®ä¿¡åº¦é¢„æµ‹å¯èƒ½éœ€è¦è°ƒæ•´ï¼š")
            for qt, suggestion in calibrations.items():
                if suggestion.get("has_data"):
                    adj = suggestion["adjustment"]
                    lines.append(
                        f"- **{qt}**: åŽ†å²æ•°æ®æ˜¾ç¤ºç½®ä¿¡åº¦{'åé«˜' if adj < 0 else 'åä½Ž'} "
                        f"{abs(adj):.2f}ï¼Œå»ºè®®{'ä¸‹è°ƒ' if adj < 0 else 'ä¸Šè°ƒ'}"
                    )
            lines.append("")

        # å½“å‰æ‰¹æ¬¡æ¨¡å¼
        batch_patterns = memory_context.get("batch_patterns", {})
        if batch_patterns.get("error_patterns"):
            lines.append("### å½“å‰æ‰¹æ¬¡å·²å‘çŽ°çš„æ¨¡å¼")
            lines.append("åœ¨å½“å‰æ‰¹æ¬¡ä¸­å·²ç»å‘çŽ°ä»¥ä¸‹æ¨¡å¼ï¼Œè¯·æ³¨æ„æ˜¯å¦å­˜åœ¨ç›¸åŒé—®é¢˜ï¼š")
            for pattern, count in list(batch_patterns["error_patterns"].items())[:5]:
                lines.append(f"- {pattern}: å·²å‡ºçŽ° {count} æ¬¡")
            lines.append("")

        # è®°å¿†ç³»ç»Ÿç»Ÿè®¡
        stats = memory_context.get("memory_stats", {})
        if stats:
            lines.append(f"### è®°å¿†ç³»ç»ŸçŠ¶æ€")
            lines.append(f"- æ€»è®°å¿†æ¡ç›®: {stats.get('total_memories', 0)}")
            lines.append(f"- é”™è¯¯æ¨¡å¼è®°å½•: {stats.get('error_pattern_count', 0)}")
            lines.append(f"- ä¿®æ­£åŽ†å²è®°å½•: {stats.get('correction_count', 0)}")
            lines.append("")

    schema_hint = {
        "student_key": student_key,
        "confession": {
            "assumptions": [
                {"question_id": "1", "assumption": "å‡è®¾æè¿°", "impact": "å¦‚æžœå‡è®¾é”™è¯¯çš„å½±å“"}
            ],
            "information_gaps": [
                {"question_id": "1", "gap": "ç¼ºå°‘çš„ä¿¡æ¯", "needed_for": "è¿™ä¸ªä¿¡æ¯ç”¨äºŽä»€ä¹ˆåˆ¤æ–­"}
            ],
            "evidence_gaps": [
                {
                    "question_id": "1",
                    "point_id": "1.1",
                    "gap": "è¯æ®ä¸è¶³æè¿°",
                    "severity": "high | medium | low",
                }
            ],
            "uncertainties": [
                {
                    "question_id": "1",
                    "uncertainty": "ä¸ç¡®å®šç‚¹æè¿°",
                    "confidence": 0.0,
                    "reason": "ä¸ºä»€ä¹ˆä¸ç¡®å®š",
                }
            ],
            "potential_errors": [
                {
                    "question_id": "1",
                    "error_type": "hallucination | misread | logic_leap | evidence_mismatch | formula_error | alternative_solution",
                    "description": "å¯èƒ½é”™è¯¯æè¿°",
                    "likelihood": "low | medium | high",
                }
            ],
            "score_anomalies": [
                {
                    "question_id": "1",
                    "anomaly_type": "full_marks | zero_marks | boundary_score | confidence_mismatch",
                    "explanation": "ä¸ºä»€ä¹ˆè¿™ä¸ªåˆ†æ•°å¯èƒ½æœ‰é—®é¢˜",
                }
            ],
            "overall_confidence": 0.0,
            "calibration_note": "ç½®ä¿¡åº¦æ ¡å‡†è¯´æ˜Žï¼šè€ƒè™‘äº†å“ªäº›å› ç´ ",
            "high_risk_questions": ["1", "2"],
            "summary": "è‡ªç™½æ€»ç»“ï¼šæˆ‘å¯èƒ½åœ¨å“ªé‡Œå‡ºé”™äº†...",
        },
    }

    lines.append("")
    lines.append("## è¾“å‡ºè¦æ±‚")
    lines.append("è¯·ä»…è¾“å‡º JSONï¼Œä¸è¦æ·»åŠ é¢å¤–è¯´æ˜Žæˆ– markdownã€‚")
    lines.append("è®°ä½ï¼šä½ æ˜¯åœ¨åš'é£Žé™©è¯´æ˜Žä¹¦'ï¼Œä¸æ˜¯å®¡è®¡æŠ¥å‘Šã€‚")
    lines.append("**é‡ç‚¹**ï¼šåŠ¡å¿…æŠ«éœ²æ‰€æœ‰æ»¡åˆ†ã€é›¶åˆ†ã€ä½Žç½®ä¿¡åº¦ã€ç©ºè¯æ®çš„é¢˜ç›®ï¼")
    lines.append("")
    lines.append("è¾“å‡º JSON æ¨¡æ¿ï¼š")
    lines.append(json.dumps(schema_hint, ensure_ascii=False, indent=2))
    return "\n".join(lines)


async def self_report_node(state: BatchGradingGraphState) -> Dict[str, Any]:
    """
    è‡ªç™½èŠ‚ç‚¹ (Text LLM) - é›†æˆå…±äº«è®°å¿†ç³»ç»Ÿ

    æ¯ä¸ªå­¦ç”Ÿè¿›è¡Œä¸€æ¬¡ LLM è‡ªç™½ï¼Œå®¡æŸ¥æ‰¹æ”¹ç»“æžœï¼š
    - ä½Žç½®ä¿¡åº¦è¯„åˆ†ç‚¹
    - è¯æ®ä¸è¶³çš„è¯„åˆ†
    - å¯èƒ½çš„è¯†åˆ«é”™è¯¯
    - éœ€è¦äººå·¥å¤æ ¸çš„é¢˜ç›®
    - **æ–°å¢ž**ï¼šåŸºäºŽåŽ†å²è®°å¿†è¿›è¡Œé£Žé™©åˆ†æž
    - **æ–°å¢ž**ï¼šå°†å‘çŽ°çš„æ¨¡å¼è®°å½•åˆ°è®°å¿†ç³»ç»Ÿ

    å·¥ä½œæµä½ç½®ï¼šindex_merge â†’ self_report â†’ logic_review
    """
    batch_id = state["batch_id"]
    student_results = state.get("student_results", []) or []
    parsed_rubric = state.get("parsed_rubric", {}) or {}
    api_key = state.get("api_key") or os.getenv("LLM_API_KEY") or os.getenv("OPENROUTER_API_KEY")
    grading_mode = _resolve_grading_mode(state.get("inputs", {}), parsed_rubric)

    # èŽ·å–ç§‘ç›®ï¼ˆç”¨äºŽè®°å¿†éš”ç¦»ï¼‰
    # ç§‘ç›®æ¥æºä¼˜å…ˆçº§ï¼šstate["subject"] > inputs["subject"] > "general"
    subject = state.get("subject") or state.get("inputs", {}).get("subject", "general")

    # åˆå§‹åŒ–è®°å¿†æœåŠ¡
    from src.services.grading_memory import get_memory_service, MemoryType, MemoryImportance

    memory_service = get_memory_service()

    # åˆ›å»ºæ‰¹æ¬¡è®°å¿†ï¼ˆæŒ‰ç§‘ç›®éš”ç¦»ï¼‰
    memory_service.create_batch_memory(batch_id, subject=subject)

    logger.info(f"[self_report] æ‰¹æ¬¡è®°å¿†å·²åˆ›å»º: batch_id={batch_id}, subject={subject}")

    # è¾…åŠ©æ¨¡å¼è·³è¿‡è‡ªç™½
    if grading_mode.startswith("assist"):
        logger.info(f"[self_report] skip (assist mode): batch_id={batch_id}")
        return {
            "current_stage": "self_report_completed",
            "percentage": 80.0,
            "timestamps": {
                **state.get("timestamps", {}),
                "self_report_at": datetime.now().isoformat(),
            },
        }

    if not student_results:
        return {
            "current_stage": "self_report_completed",
            "percentage": 80.0,
            "timestamps": {
                **state.get("timestamps", {}),
                "self_report_at": datetime.now().isoformat(),
            },
        }

    rubric_map = _build_rubric_question_map(parsed_rubric)

    if not api_key:
        logger.warning(f"[self_report] no API key, using rule-based report: batch_id={batch_id}")
        # ä½¿ç”¨åŸºäºŽè§„åˆ™çš„è‡ªç™½
        from src.services.grading_self_report import generate_self_report

        updated_results = []
        for student in student_results:
            updated = dict(student)
            question_details = _extract_logic_review_questions(student)
            if question_details:
                # ä¸ºæ¯ä¸ªé¢˜ç›®ç”Ÿæˆè§„åˆ™è‡ªç™½
                for q in question_details:
                    rule_report = generate_self_report(
                        evidence={},
                        score_result={"question_details": [q]},
                        page_index=0,
                    )
                    if not updated.get("self_report"):
                        updated["self_report"] = rule_report
                    else:
                        updated["self_report"]["issues"].extend(rule_report.get("issues", []))
                        updated["self_report"]["warnings"].extend(rule_report.get("warnings", []))
            updated_results.append(updated)
        return {
            "student_results": updated_results,
            "current_stage": "self_report_completed",
            "percentage": 80.0,
            "timestamps": {
                **state.get("timestamps", {}),
                "self_report_at": datetime.now().isoformat(),
            },
        }

    from src.services.llm_reasoning import LLMReasoningClient

    reasoning_client = LLMReasoningClient(api_key=api_key, rubric_registry=None)
    max_workers = int(os.getenv("SELF_REPORT_MAX_WORKERS", "3"))

    updated_results: List[Optional[Dict[str, Any]]] = [None] * len(student_results)

    async def report_student(payload: Dict[str, Any]) -> Dict[str, Any]:
        index = payload["index"]
        student = payload["student"]
        student_key = (
            student.get("student_key") or student.get("student_name") or f"Student {index + 1}"
        )
        agent_id = f"report-worker-{index}"

        try:
            await _broadcast_progress(
                batch_id,
                {
                    "type": "agent_update",
                    "agentId": agent_id,
                    "agentName": student_key,
                    "parentNodeId": "self_report",
                    "status": "running",
                    "progress": 0,
                    "message": "Generating self-report...",
                },
            )

            question_details = _extract_logic_review_questions(student)
            if not question_details:
                updated_student = dict(student)
                updated_student["self_report"] = {
                    "overall_status": "ok",
                    "issues": [],
                    "warnings": [],
                    "summary": "æ— é¢˜ç›®éœ€è¦å®¡æŸ¥",
                    "generated_at": datetime.now().isoformat(),
                }
                await _broadcast_progress(
                    batch_id,
                    {
                        "type": "agent_update",
                        "agentId": agent_id,
                        "parentNodeId": "self_report",
                        "status": "completed",
                        "progress": 100,
                        "message": "Self-report skipped (no questions)",
                    },
                )
                return {"index": index, "result": updated_student}

            # èŽ·å–è®°å¿†ä¸Šä¸‹æ–‡ï¼ˆæŒ‰ç§‘ç›®éš”ç¦»ï¼‰
            memory_context = memory_service.generate_confession_context(
                question_details=question_details,
                batch_id=batch_id,
                subject=subject,  # ç§‘ç›®éš”ç¦»ï¼šç¡®ä¿ä¸åŒç§‘ç›®çš„æ‰¹æ”¹ç»éªŒä¸ä¼šæ··ç”¨
            )

            # è®°å½•æ‰¹æ¬¡å†…çš„ç½®ä¿¡åº¦åˆ†å¸ƒ
            for q in question_details:
                qt = q.get("question_type") or "unknown"
                conf = q.get("confidence", 0.7)
                memory_service.record_batch_confidence(batch_id, qt, conf)

                # æ£€æµ‹å¹¶è®°å½•é£Žé™©ä¿¡å·
                score = q.get("score", 0)
                max_score = q.get("max_score", 0)
                if max_score > 0 and score >= max_score:
                    memory_service.record_batch_risk_signal(
                        batch_id, "æ»¡åˆ†é£Žé™©", q.get("question_id", "?"), "medium"
                    )
                if score == 0 and max_score > 0:
                    memory_service.record_batch_risk_signal(
                        batch_id, "é›¶åˆ†é£Žé™©", q.get("question_id", "?"), "medium"
                    )
                if conf < 0.5:
                    memory_service.record_batch_risk_signal(
                        batch_id, "æžä½Žç½®ä¿¡åº¦", q.get("question_id", "?"), "high"
                    )

            prompt = _build_self_report_prompt(
                student, question_details, rubric_map, memory_context
            )

            response_text = ""
            try:
                async for chunk in reasoning_client._call_text_api_stream(prompt):
                    output_text, thinking_text = split_thinking_content(chunk)
                    if output_text:
                        response_text += output_text
                        await _broadcast_progress(
                            batch_id,
                            {
                                "type": "stream_delta",
                                "nodeId": "self_report",
                                "agentId": agent_id,
                                "deltaType": "output",
                                "content": output_text,
                            },
                        )
            except Exception as exc:
                logger.warning(f"[self_report] LLM failed student={student_key}: {exc}")
                response_text = ""

            self_report = None
            if response_text:
                try:
                    json_text = _extract_json_from_response(response_text)
                    payload = json.loads(json_text)
                    # LLM å¯èƒ½è¿”å›ž confession æˆ– self_report å­—æ®µ
                    self_report = payload.get("confession") or payload.get("self_report") or payload
                    # æ ‡å‡†åŒ–å­—æ®µåï¼ˆsnake_case -> camelCase å…¼å®¹ï¼‰
                    if isinstance(self_report, dict):
                        # ç¡®ä¿ overallStatus å­˜åœ¨
                        if (
                            "overall_confidence" in self_report
                            and "overallStatus" not in self_report
                        ):
                            conf = self_report.get("overall_confidence", 0)
                            if conf >= 0.8:
                                self_report["overallStatus"] = "ok"
                            elif conf >= 0.5:
                                self_report["overallStatus"] = "caution"
                            else:
                                self_report["overallStatus"] = "needs_review"
                        # ç¡®ä¿ highRiskQuestions æ ¼å¼æ­£ç¡®
                        if "high_risk_questions" in self_report:
                            hrq = self_report["high_risk_questions"]
                            if isinstance(hrq, list) and hrq and isinstance(hrq[0], str):
                                # è½¬æ¢ä¸ºå‰ç«¯æœŸæœ›çš„æ ¼å¼
                                self_report["highRiskQuestions"] = [
                                    {"questionId": q, "description": ""} for q in hrq
                                ]
                            else:
                                self_report["highRiskQuestions"] = hrq
                        # å¤åˆ¶ overall_confidence åˆ° overallConfidence
                        if "overall_confidence" in self_report:
                            self_report["overallConfidence"] = self_report["overall_confidence"]
                except Exception as exc:
                    logger.warning(f"[self_report] parse failed student={student_key}: {exc}")

            updated_student = dict(student)
            if self_report:
                self_report["generated_at"] = datetime.now().isoformat()
                self_report["source"] = "llm"
                self_report["memory_context_used"] = bool(memory_context)
                updated_student["self_report"] = self_report

                # å°†è‡ªç™½å‘çŽ°çš„æ¨¡å¼è®°å½•åˆ°è®°å¿†ç³»ç»Ÿ
                try:
                    # è®°å½•æ½œåœ¨é”™è¯¯æ¨¡å¼
                    potential_errors = self_report.get("potential_errors", [])
                    for err in potential_errors:
                        if isinstance(err, dict):
                            error_type = err.get("error_type", "unknown")
                            likelihood = err.get("likelihood", "medium")
                            if likelihood in ["high", "medium"]:
                                memory_service.record_batch_error_pattern(
                                    batch_id=batch_id,
                                    pattern=f"{error_type}: {err.get('description', '')}",
                                    question_id=err.get("question_id", "?"),
                                )

                    # è®°å½•è¯æ®ç¼ºå£
                    evidence_gaps = self_report.get("evidence_gaps", [])
                    for gap in evidence_gaps:
                        if isinstance(gap, dict):
                            severity = gap.get("severity", "medium")
                            if severity in ["high", "critical"]:
                                memory_service.record_batch_error_pattern(
                                    batch_id=batch_id,
                                    pattern=f"è¯æ®ç¼ºå£: {gap.get('gap', '')}",
                                    question_id=gap.get("question_id", "?"),
                                )

                    # è®°å½•é«˜é£Žé™©é¢˜ç›®
                    high_risk = self_report.get("high_risk_questions", [])
                    for hrq in high_risk:
                        qid = hrq if isinstance(hrq, str) else hrq.get("questionId", "?")
                        memory_service.record_batch_risk_signal(
                            batch_id=batch_id,
                            signal="è‡ªç™½æ ‡è®°é«˜é£Žé™©",
                            question_id=qid,
                            severity="high",
                        )
                except Exception as mem_exc:
                    logger.warning(f"[self_report] è®°å¿†è®°å½•å¤±è´¥: {mem_exc}")
            else:
                # å›žé€€åˆ°è§„åˆ™è‡ªç™½
                from src.services.grading_self_report import generate_self_report

                fallback_report = generate_self_report(
                    evidence={},
                    score_result={"question_details": question_details},
                    page_index=0,
                )
                fallback_report["source"] = "rule_fallback"
                updated_student["self_report"] = fallback_report

            await _broadcast_progress(
                batch_id,
                {
                    "type": "agent_update",
                    "agentId": agent_id,
                    "parentNodeId": "self_report",
                    "status": "completed",
                    "progress": 100,
                    "message": "Self-report generated",
                    "output": {
                        "selfReport": updated_student.get("self_report"),
                    },
                },
            )
            return {"index": index, "result": updated_student}
        except Exception as exc:
            logger.warning(f"[self_report] worker failed student={student_key}: {exc}")
            return {"index": index, "result": dict(student)}

    report_runner = RunnableLambda(report_student)
    inputs = [{"index": idx, "student": student} for idx, student in enumerate(student_results)]
    config = RunnableConfig(max_concurrency=max_workers) if max_workers > 0 else RunnableConfig()
    results = await report_runner.abatch(inputs, config=config)
    for result in results:
        if not result:
            continue
        updated_results[result["index"]] = result["result"]

    final_results = [r if r else student_results[i] for i, r in enumerate(updated_results)]

    logger.info(f"[self_report] completed for {len(final_results)} students: batch_id={batch_id}")

    # ä¿å­˜è®°å¿†åˆ°æŒä¹…åŒ–å­˜å‚¨
    try:
        memory_service.save_to_storage()
        logger.info(f"[self_report] è®°å¿†ç³»ç»Ÿå·²ä¿å­˜: batch_id={batch_id}")
    except Exception as e:
        logger.warning(f"[self_report] è®°å¿†ä¿å­˜å¤±è´¥: {e}")

    return {
        "student_results": final_results,
        "current_stage": "self_report_completed",
        "percentage": 80.0,
        "timestamps": {
            **state.get("timestamps", {}),
            "self_report_at": datetime.now().isoformat(),
        },
    }


def _build_logic_review_prompt(
    student: Dict[str, Any],
    question_details: List[Dict[str, Any]],
    rubric_map: Dict[str, Dict[str, Any]],
    limits: Dict[str, int],
    confession: Optional[Dict[str, Any]] = None,
) -> str:
    """
    æž„å»ºé€»è¾‘å¤æ ¸ (Logic Review) LLM æç¤ºè¯

    é€»è¾‘å¤æ ¸çš„æ ¸å¿ƒåŠŸèƒ½ï¼šéªŒè¯/å®¡è®¡ + ä¸€è‡´æ€§ä¿®å¤
    - åªèƒ½åŸºäºŽæ‰¹æ”¹ç»“æžœã€è¯„åˆ†æ ‡å‡†è§£æžç»“æžœå’Œè‡ªç™½ç»“æžœ
    - ä¸å…è®¸å¼•å…¥æ–°äº‹å®ž/æ–°æŽ¨ç†
    - è¦æœ‰æ‰¹åˆ¤æ€§æ€ç»´ï¼ŒæŸ¥æ¼è¡¥ç¼º
    - å…·å¤‡æœ‰é™çš„ä¿®æ­£èƒ½åŠ›ï¼ˆæ˜Žæ˜¾é”™è¯¯ï¼‰

    âš ï¸ é‡è¦ï¼šé€»è¾‘å¤æ ¸ç‹¬ç«‹æ€§åŽŸåˆ™ (P3)
    =========================================
    æ­¤å‡½æ•°æž„å»ºçš„ prompt ä¸èƒ½åŒ…å«ä»»ä½•è®°å¿†ç³»ç»Ÿçš„æ•°æ®ï¼

    é€»è¾‘å¤æ ¸å¿…é¡»æ˜¯"æ— çŠ¶æ€"çš„ï¼š
    1. ä¸èƒ½å¼•ç”¨åŽ†å²æ‰¹æ”¹ç»éªŒæˆ–è®°å¿†
    2. ä¸èƒ½ä½¿ç”¨ generate_confession_context() çš„è¾“å‡º
    3. è¯„åˆ†å†³ç­–å®Œå…¨åŸºäºŽå½“å‰è¯„åˆ†æ ‡å‡†å’Œå­¦ç”Ÿç­”æ¡ˆ

    å…è®¸çš„è¾“å…¥ï¼š
    - student: å½“å‰å­¦ç”Ÿçš„æ‰¹æ”¹ç»“æžœ
    - question_details: å½“å‰æ‰¹æ”¹çš„é¢˜ç›®è¯¦æƒ…
    - rubric_map: è¯„åˆ†æ ‡å‡†ï¼ˆä»Ž parsed_rubric æž„å»ºï¼‰
    - confession: è‡ªç™½æŠ¥å‘Šï¼ˆä»…ç”¨äºŽäº¤å‰éªŒè¯ï¼Œä¸å½±å“è¯„åˆ†ï¼‰

    ç¦æ­¢çš„è¾“å…¥ï¼š
    - ä»»ä½•æ¥è‡ª GradingMemoryService çš„æ•°æ®
    - åŽ†å²æ‰¹æ”¹æ¨¡å¼æˆ–ç»éªŒ
    - æ ¡å‡†å»ºè®®æˆ–ç½®ä¿¡åº¦è°ƒæ•´
    =========================================
    """
    student_key = student.get("student_key") or student.get("student_name") or "Unknown"
    max_questions = limits.get("max_questions", 20)
    if max_questions <= 0:
        max_questions = len(question_details)
    max_answer_chars = limits.get("max_answer_chars", 400)
    max_feedback_chars = limits.get("max_feedback_chars", 200)
    max_rubric_chars = limits.get("max_rubric_chars", 240)
    max_points = limits.get("max_scoring_points", 4)
    max_evidence_chars = limits.get("max_evidence_chars", 120)

    lines = [
        "# è§’è‰²ï¼šèµ„æ·±é€»è¾‘å¤æ ¸ä¸“å®¶ (Logic Review / Audit & Correction Expert)",
        "",
        "ä½ æ˜¯ä¸€ä½æ‹¥æœ‰ä¸°å¯Œé˜…å·ç»éªŒçš„é€»è¾‘å¤æ ¸ä¸“å®¶ï¼Œä¸“é—¨è´Ÿè´£å®¡è®¡æ‰¹æ”¹ç»“æžœçš„ä¸€è‡´æ€§å’Œå‡†ç¡®æ€§ã€‚",
        "",
        "## æ ¸å¿ƒä»»åŠ¡",
        "åˆ¤å®šï¼š'æ‰¹æ”¹å¯¹ä¸å¯¹ / æœ‰æ²¡æœ‰æ¼æ´ž'ï¼Œå¹¶**ä¿®æ­£å‘çŽ°çš„é”™è¯¯**",
        "ä½ æ˜¯éªŒè¯å®¡è®¡è€…+ä¿®æ­£è€…ï¼Œæ£€æŸ¥æ‰¹æ”¹æ˜¯å¦è‡ªæ´½ï¼Œå‘çŽ°é”™è¯¯æ—¶å¿…é¡»æ›´æ­£ã€‚",
        "",
        "## æ£€æŸ¥ç»´åº¦",
        "1. **ä¸€è‡´æ€§**ï¼šè¯„åˆ†ä¸Žè¯æ®æ˜¯å¦ä¸€è‡´ï¼Ÿä¸Žåé¦ˆæ˜¯å¦ä¸€è‡´ï¼Ÿ",
        "2. **å®Œå¤‡æ€§**ï¼šæ˜¯å¦é—æ¼å…³é”®è¯„åˆ†ç‚¹ï¼Ÿæ˜¯å¦æœ‰æœªè¦†ç›–çš„é¢˜ç›®ï¼Ÿ",
        "3. **å¯æŽ¨å¯¼æ€§**ï¼šç»“è®ºæ˜¯å¦ç”±è¯æ®æŽ¨å‡ºï¼Ÿæ˜¯å¦å­˜åœ¨è·³æ­¥ (non sequitur)ï¼Ÿ",
        "4. **å¯éªŒè¯æ€§**ï¼šç»™å‡ºçš„è¯æ®æ˜¯å¦å¯ä»¥éªŒè¯è¯„åˆ†å†³å®šï¼Ÿ",
        "5. **æ•°å­¦æ­£ç¡®æ€§**ï¼šå¾—åˆ†ç´¯åŠ æ˜¯å¦æ­£ç¡®ï¼Ÿæ˜¯å¦è¶…å‡ºæ»¡åˆ†ï¼Ÿ",
        "",
        "## æ‰¹æ”¹ç»éªŒï¼šå¸¸è§é€»è¾‘é”™è¯¯æ¨¡å¼",
        "",
        "### 1. è¯„åˆ†é€»è¾‘é”™è¯¯",
        "- **é‡å¤æ‰£åˆ†**ï¼šå‰æ­¥é”™è¯¯å·²æ‰£åˆ†ï¼ŒåŽæ­¥å› æ­¤äº§ç”Ÿçš„é”™è¯¯ä¸åº”é‡å¤æ‰£åˆ†",
        "- **é”™è¯¯ä¼ é€’å¿½è§†**ï¼šå‰æ­¥è®¡ç®—é”™è¯¯ä½†æ–¹æ³•æ­£ç¡®ï¼ŒåŽæ­¥åº”æŒ‰å‰æ­¥ç»“æžœç»§ç»­ç»™åˆ†",
        "- **å¾—åˆ†ç‚¹é—æ¼**ï¼šè¯„åˆ†æ ‡å‡†ä¸­çš„æŸäº›å¾—åˆ†ç‚¹æœªè¢«è¯„åˆ¤",
        "- **åˆ†æ•°æº¢å‡º**ï¼šæŸé¢˜å¾—åˆ†è¶…è¿‡æ»¡åˆ†æˆ–ä¸ºè´Ÿæ•°",
        "- **ç´¯åŠ é”™è¯¯**ï¼šå¾—åˆ†ç‚¹åˆ†æ•°ä¹‹å’Œä¸Žé¢˜ç›®æ€»åˆ†ä¸ä¸€è‡´",
        "",
        "### 2. è¯æ®ä¸Žè¯„åˆ†çŸ›ç›¾",
        "- **è¯æ®è¯´å¯¹ä½†æ‰£åˆ†**ï¼ševidence è¯´å­¦ç”Ÿæ­£ç¡®ï¼Œä½† awarded = 0",
        "- **è¯æ®è¯´é”™ä½†ç»™åˆ†**ï¼ševidence è¯´å­¦ç”Ÿé”™è¯¯ï¼Œä½† awarded > 0",
        "- **è¯æ®ç©ºä½†æœ‰åˆ†**ï¼šæ²¡æœ‰è¯æ®æ”¯æŒçš„ç»™åˆ†å†³å®š",
        "- **è¯æ®ä¸Žåé¦ˆå†²çª**ï¼ševidence å’Œ feedback å†…å®¹çŸ›ç›¾",
        "",
        "### 3. æ ‡å‡†åº”ç”¨é”™è¯¯",
        "- **æ ‡å‡†ç†è§£åå·®**ï¼šè¯„åˆ†æ ‡å‡†è¢«é”™è¯¯ç†è§£æˆ–åº”ç”¨",
        "- **å¦ç±»è§£æ³•è¯¯åˆ¤**ï¼šæ­£ç¡®çš„éžæ ‡å‡†è§£æ³•è¢«é”™è¯¯æ‰£åˆ†",
        "- **éƒ¨åˆ†åˆ†ç»™äºˆä¸å½“**ï¼šåº”ç»™éƒ¨åˆ†åˆ†çš„æƒ…å†µç»™äº†é›¶åˆ†æˆ–æ»¡åˆ†",
        "",
        "## çº¦æŸä¸Žä¿®æ­£æƒé™",
        '1. **ç¦æ­¢å¼•å…¥æ–°äº‹å®ž**ï¼šä¸èƒ½è¯´"æˆ‘çœ‹åˆ°äº†..."ï¼Œåªèƒ½åŸºäºŽå·²æœ‰ä¿¡æ¯åˆ¤æ–­',
        "2. **å¿…é¡»ä¿®æ­£é”™è¯¯**ï¼šå‘çŽ°é€»è¾‘é”™è¯¯ã€è¯æ®å†²çªã€è¯„åˆ†ä¸ä¸€è‡´æ—¶ï¼Œå¿…é¡»åœ¨ review_corrections ä¸­æå‡ºä¿®æ­£",
        "3. **ä¿®æ­£éœ€æœ‰ä¾æ®**ï¼šæ¯ä¸ªä¿®æ­£å¿…é¡»å¼•ç”¨å·²æœ‰è¯æ®æˆ–è¯„åˆ†æ ‡å‡†ä½œä¸ºä¾æ®",
        "4. **æ‰¹åˆ¤æ€§æ€ç»´**ï¼šå¯¹è‡ªç™½ä¸­æŠ«éœ²çš„é£Žé™©ç‚¹è¿›è¡Œäº¤å‰éªŒè¯",
        "5. **ä¿å®ˆä¿®æ­£åŽŸåˆ™**ï¼šåªä¿®æ­£æœ‰æ˜Žç¡®ä¾æ®çš„é”™è¯¯ï¼Œä¸ç¡®å®šæ—¶ä¿ç•™åŽŸåˆ¤",
        "",
        "## ä¿®æ­£å†³ç­–æ ‘",
        "```",
        "if è¯æ®æ˜Žç¡®è¯´æ­£ç¡® and awarded == 0:",
        "    â†’ ä¿®æ­£ä¸ºå¾—åˆ†ï¼ˆç»™å‡º review_reasonï¼‰",
        "elif è¯æ®æ˜Žç¡®è¯´é”™è¯¯ and awarded > 0:",
        "    â†’ ä¿®æ­£ä¸ºæ‰£åˆ†ï¼ˆç»™å‡º review_reasonï¼‰",
        "elif è¯æ®ä¸Žåé¦ˆçŸ›ç›¾:",
        "    â†’ ä»¥è¯æ®ä¸ºå‡†è¿›è¡Œä¿®æ­£",
        "elif å¾—åˆ†è¶…å‡ºæ»¡åˆ† or å¾—åˆ†ä¸ºè´Ÿ:",
        "    â†’ ä¿®æ­£ä¸ºåˆç†åˆ†æ•°",
        "elif å‰æ­¥é”™è¯¯å¯¼è‡´åŽæ­¥é”™è¯¯ and åŽæ­¥å·²æ‰£åˆ†:",
        "    â†’ æ¢å¤åŽæ­¥å¾—åˆ†ï¼ˆé”™è¯¯ä¸é‡å¤æ‰£ï¼‰",
        "else:",
        "    â†’ ä¿ç•™åŽŸåˆ¤ï¼Œåœ¨ honesty_note ä¸­è¯´æ˜Žä¸ç¡®å®š",
        "```",
        "",
        "## å¯ç”¨ä¿¡æ¯æºï¼ˆä»…é™è¿™äº›ï¼‰",
        "- æ‰¹æ”¹ç»“æžœï¼ˆè¯„åˆ†ã€è¯æ®ã€åé¦ˆï¼‰",
        "- è¯„åˆ†æ ‡å‡†ï¼ˆrubricï¼‰",
        "- è‡ªç™½æŠ¥å‘Šï¼ˆå¦‚æœ‰ï¼‰",
        "",
        "## è¾“å‡ºå†…å®¹",
        "- **errors**ï¼šé€»è¾‘é”™è¯¯ã€ä¸Žè¯æ®çŸ›ç›¾",
        "- **contradictions**ï¼šå†…éƒ¨ä¸ä¸€è‡´",
        "- **missing**ï¼šé—æ¼çš„è¯„åˆ†ç‚¹æˆ–éªŒè¯",
        "- **review_corrections**ï¼š**å¿…é¡»å¡«å†™**çš„è¯„åˆ†ä¿®æ­£ï¼ŒåŒ…å« point_idã€correct_awardedã€correct_decisionã€review_reason",
        "",
        f"## å­¦ç”Ÿæ ‡è¯†: {student_key}",
        "",
    ]

    # æ·»åŠ è‡ªç™½ä¿¡æ¯ï¼ˆå¦‚æžœæœ‰ï¼‰
    if confession:
        lines.append("## è‡ªç™½æŠ¥å‘Šæ‘˜è¦ï¼ˆä¾›ä½ äº¤å‰éªŒè¯ï¼‰")
        if confession.get("high_risk_questions"):
            lines.append(f"- é«˜é£Žé™©é¢˜ç›®: {confession.get('high_risk_questions')}")
        if confession.get("overall_confidence"):
            lines.append(f"- æ•´ä½“ç½®ä¿¡åº¦: {confession.get('overall_confidence')}")
        if confession.get("potential_errors"):
            lines.append("- æŠ«éœ²çš„å¯èƒ½é”™è¯¯ç‚¹:")
            for err in confession.get("potential_errors", [])[:5]:
                if isinstance(err, dict):
                    lines.append(
                        f"  - Q{err.get('question_id', '?')}: {err.get('description', '')}"
                    )
        lines.append("")

    lines.append("## é¢˜ç›®æ‘˜è¦ï¼ˆä¾›ä½ åšä¸€è‡´æ€§æ£€æŸ¥ï¼‰")

    for idx, question in enumerate(question_details[:max_questions]):
        qid = _normalize_question_id(
            question.get("question_id") or question.get("questionId")
        ) or str(idx + 1)
        rubric = rubric_map.get(qid, {})
        score = question.get("score", 0)
        max_score = question.get("max_score", rubric.get("max_score", 0))
        question_text = _trim_text(rubric.get("question_text", ""), max_rubric_chars)
        standard_answer = _trim_text(rubric.get("standard_answer", ""), max_rubric_chars)
        student_answer = _trim_text(question.get("student_answer", ""), max_answer_chars)
        feedback = _trim_text(question.get("feedback", ""), max_feedback_chars)

        lines.append(f"- Q{qid}: score {score}/{max_score}")
        if question_text:
            lines.append(f"  prompt: {question_text}")
        if standard_answer:
            lines.append(f"  standard_answer: {standard_answer}")
        if student_answer:
            lines.append(f"  student_answer: {student_answer}")
        if feedback:
            lines.append(f"  feedback: {feedback}")

        scoring_points = (
            question.get("scoring_point_results") or question.get("scoring_results") or []
        )
        if scoring_points:
            lines.append("  scoring_points:")
            for sp in scoring_points[:max_points]:
                if not isinstance(sp, dict):
                    continue
                point_id = (
                    sp.get("point_id")
                    or sp.get("pointId")
                    or (sp.get("scoring_point") or {}).get("point_id")
                    or ""
                )
                awarded = sp.get("awarded", sp.get("score", 0))
                max_points_val = (
                    sp.get("max_points")
                    or sp.get("maxPoints")
                    or (sp.get("scoring_point") or {}).get("score")
                    or 0
                )
                evidence = _trim_text(sp.get("evidence", ""), max_evidence_chars)
                rubric_ref = _trim_text(
                    sp.get("rubric_reference") or sp.get("rubricReference") or "",
                    max_rubric_chars,
                )
                decision = sp.get("decision") or sp.get("result") or ""
                lines.append(
                    f"    - {point_id}: {awarded}/{max_points_val} decision: {decision} "
                    f"evidence: {evidence} rubric_ref: {rubric_ref}"
                )
        lines.append("")

    schema_hint = {
        "student_key": student_key,
        "question_reviews": [
            {
                "question_id": "1",
                "confidence": 0.0,
                "confidence_reason": "string",
                "self_critique": "string",
                "self_critique_confidence": 0.0,
                "review_summary": "string",
                "review_corrections": [
                    {
                        "point_id": "1.1",
                        "correct_awarded": 1,
                        "correct_decision": "å¾—åˆ†",
                        "review_reason": "string",
                    }
                ],
                "honesty_note": "string",
            }
        ],
        "self_audit": {
            "summary": "string",
            "confidence": 0.0,
            "issues": [{"issue_type": "string", "message": "string", "question_id": "1"}],
            "compliance_analysis": [{"goal": "string", "tag": "fully_complied", "notes": "string"}],
            "uncertainties_and_conflicts": [
                {
                    "issue": "string",
                    "impact": "string",
                    "question_ids": ["1"],
                    "reported_to_user": True,
                }
            ],
            "overall_compliance_grade": 4,
            "honesty_note": "string",
        },
    }

    lines.append("è¾“å‡º JSON æ¨¡æ¿ï¼š")
    lines.append(json.dumps(schema_hint, ensure_ascii=False, indent=2))
    return "\n".join(lines)


async def logic_review_node(state: BatchGradingGraphState) -> Dict[str, Any]:
    """
    é€»è¾‘å¤æ ¸èŠ‚ç‚¹ï¼ˆæ–‡æœ¬è¾“å…¥ï¼‰

    æ¯ä¸ªå­¦ç”Ÿè¿›è¡Œä¸€æ¬¡çº¯æ–‡æœ¬ LLM å¤æ ¸ï¼Œè¾“å‡ºé¢˜ç›®ç½®ä¿¡åº¦ä¸Žè‡ªç™½è¯´æ˜Žã€‚

    âš ï¸ é‡è¦ï¼šé€»è¾‘å¤æ ¸ç‹¬ç«‹æ€§åŽŸåˆ™ (P3)
    =========================================
    é€»è¾‘å¤æ ¸å¿…é¡»æ˜¯"æ— çŠ¶æ€"çš„ï¼Œå³ï¼š
    1. è¯„åˆ†å†³ç­–ä¸èƒ½ä¾èµ–è®°å¿†ç³»ç»Ÿä¸­çš„ä»»ä½•æ•°æ®
    2. LLM prompt ä¸èƒ½åŒ…å«åŽ†å²è®°å¿†ä¸Šä¸‹æ–‡
    3. å¤æ ¸ç»“æžœå®Œå…¨åŸºäºŽå½“å‰è¯„åˆ†æ ‡å‡†å’Œå­¦ç”Ÿç­”æ¡ˆ

    è®°å¿†ç³»ç»Ÿåœ¨æ­¤èŠ‚ç‚¹çš„ä½¿ç”¨ä»…é™äºŽï¼š
    - è®°å½•ä¿®æ­£åŽ†å²ï¼ˆç”¨äºŽæœªæ¥çš„æ‰¹æ”¹æ”¹è¿›ï¼‰
    - æ•´åˆæ‰¹æ¬¡è®°å¿†åˆ°é•¿æœŸè®°å¿†

    è¿™äº›æ“ä½œå‘ç”Ÿåœ¨è¯„åˆ†å†³ç­–ä¹‹åŽï¼Œä¸å½±å“è¯„åˆ†ç»“æžœã€‚
    =========================================
    """
    batch_id = state["batch_id"]
    student_results = state.get("student_results", []) or []
    parsed_rubric = state.get("parsed_rubric", {}) or {}
    api_key = state.get("api_key") or os.getenv("LLM_API_KEY") or os.getenv("OPENROUTER_API_KEY")
    grading_mode = _resolve_grading_mode(state.get("inputs", {}), parsed_rubric)

    # èŽ·å–è®°å¿†æœåŠ¡
    from src.services.grading_memory import get_memory_service, MemoryType, MemoryImportance

    memory_service = get_memory_service()

    if grading_mode.startswith("assist"):
        logger.info(f"[logic_review] skip (assist mode): batch_id={batch_id}")
        return {
            "logic_review_results": [],
            "current_stage": "logic_review_completed",
            "percentage": 85.0,
            "timestamps": {
                **state.get("timestamps", {}),
                "logic_review_at": datetime.now().isoformat(),
            },
        }

    if not student_results:
        return {
            "logic_review_results": [],
            "current_stage": "logic_review_completed",
            "percentage": 85.0,
            "timestamps": {
                **state.get("timestamps", {}),
                "logic_review_at": datetime.now().isoformat(),
            },
        }

    rubric_map = _build_rubric_question_map(parsed_rubric)
    limits = {
        "max_questions": int(os.getenv("LOGIC_REVIEW_MAX_QUESTIONS", "0")),
        "max_answer_chars": int(os.getenv("LOGIC_REVIEW_MAX_ANSWER_CHARS", "4000")),
        "max_feedback_chars": int(os.getenv("LOGIC_REVIEW_MAX_FEEDBACK_CHARS", "200")),
        "max_rubric_chars": int(os.getenv("LOGIC_REVIEW_MAX_RUBRIC_CHARS", "240")),
        "max_scoring_points": int(os.getenv("LOGIC_REVIEW_MAX_SCORING_POINTS", "4")),
        "max_evidence_chars": int(os.getenv("LOGIC_REVIEW_MAX_EVIDENCE_CHARS", "120")),
    }

    if not api_key:
        updated_results = []
        for student in student_results:
            updated = dict(student)
            updated.setdefault("self_audit", _build_self_audit(updated))
            updated_results.append(updated)
        return {
            "student_results": updated_results,
            "logic_review_results": [],
            "current_stage": "logic_review_completed",
            "percentage": 85.0,
            "timestamps": {
                **state.get("timestamps", {}),
                "logic_review_at": datetime.now().isoformat(),
            },
        }

    from src.services.llm_reasoning import LLMReasoningClient

    reasoning_client = LLMReasoningClient(api_key=api_key, rubric_registry=None)
    max_workers = int(os.getenv("LOGIC_REVIEW_MAX_WORKERS", "3"))

    logic_review_results: List[Dict[str, Any]] = []
    updated_results: List[Optional[Dict[str, Any]]] = [None] * len(student_results)

    async def review_student(payload: Dict[str, Any]) -> Dict[str, Any]:
        index = payload["index"]
        student = payload["student"]
        student_key = (
            student.get("student_key") or student.get("student_name") or f"Student {index + 1}"
        )
        agent_id = f"review-worker-{index}"

        try:
            await _broadcast_progress(
                batch_id,
                {
                    "type": "agent_update",
                    "agentId": agent_id,
                    "agentName": student_key,
                    "parentNodeId": "logic_review",
                    "status": "running",
                    "progress": 0,
                    "message": "Logic review running...",
                },
            )

            question_details = _extract_logic_review_questions(student)
            if not question_details:
                updated_student = dict(student)
                _recompute_student_totals(updated_student)
                updated_student["self_audit"] = _build_self_audit(updated_student)
                updated_student["logic_reviewed_at"] = datetime.now().isoformat()
                review_summary = _build_logic_review_summary(question_details)
                await _broadcast_progress(
                    batch_id,
                    {
                        "type": "agent_update",
                        "agentId": agent_id,
                        "parentNodeId": "logic_review",
                        "status": "completed",
                        "progress": 100,
                        "message": "Logic review skipped (no questions)",
                        "output": {
                            "reviewSummary": review_summary,
                            "selfAudit": updated_student.get("self_audit"),
                        },
                    },
                )
                return {"index": index, "result": updated_student, "review": None}
            prompt = _build_logic_review_prompt(
                student,
                question_details,
                rubric_map,
                limits,
                confession=student.get("self_report") or student.get("confession"),
            )

            response_text = ""
            try:
                async for chunk in reasoning_client._call_text_api_stream(prompt):
                    output_text, thinking_text = split_thinking_content(chunk)
                    if thinking_text:
                        await _broadcast_progress(
                            batch_id,
                            {
                                "type": "llm_stream_chunk",
                                "nodeId": "logic_review",
                                "nodeName": "Logic Review",
                                "agentId": agent_id,
                                "agentLabel": student_key,
                                "streamType": "thinking",
                                "chunk": thinking_text,
                            },
                        )
                    if output_text:
                        await _broadcast_progress(
                            batch_id,
                            {
                                "type": "llm_stream_chunk",
                                "nodeId": "logic_review",
                                "nodeName": "Logic Review",
                                "agentId": agent_id,
                                "agentLabel": student_key,
                                "streamType": "output",
                                "chunk": output_text,
                            },
                        )
                        response_text += output_text
                    elif thinking_text:
                        response_text += thinking_text
            except Exception as exc:
                logger.warning(f"[logic_review] LLM failed student={student_key}: {exc}")

            payload_data: Dict[str, Any] = {}
            if response_text:
                try:
                    json_text = reasoning_client._extract_json_from_text(response_text)
                    payload_data = json.loads(json_text)
                except Exception as exc:
                    logger.warning(f"[logic_review] parse failed student={student_key}: {exc}")

            question_reviews = (
                payload_data.get("question_reviews")
                or payload_data.get("questionReviews")
                or payload_data.get("questions")
                or payload_data.get("reviews")
                or []
            )
            review_map: Dict[str, Dict[str, Any]] = {}
            for item in _normalize_logic_review_items(question_reviews):
                qid = _normalize_question_id(item.get("question_id") or item.get("questionId"))
                if not qid:
                    continue
                review_map[qid] = item

            updated_student = dict(student)
            import copy

            updated_student["draft_question_details"] = copy.deepcopy(question_details)
            updated_student["draft_total_score"] = sum(
                _safe_float(q.get("score", 0)) for q in question_details
            )
            updated_student["draft_max_score"] = sum(
                _safe_float(q.get("max_score", 0)) for q in question_details
            )
            updated_details = []
            for q in question_details:
                qid = _normalize_question_id(q.get("question_id") or q.get("questionId"))
                if qid and qid in review_map:
                    merged = _merge_logic_review_fields(q, review_map[qid])
                    updated_details.append(merged)

                    # è®°å½•ä¿®æ­£åˆ°è®°å¿†ç³»ç»Ÿ
                    try:
                        original_score = _safe_float(q.get("score", 0))
                        new_score = _safe_float(merged.get("score", 0))
                        if abs(new_score - original_score) >= 0.5:
                            review_reason = review_map[qid].get("review_reason", "")
                            for corr in review_map[qid].get("review_corrections", []):
                                if isinstance(corr, dict):
                                    review_reason = corr.get("review_reason", review_reason)
                                    break
                            memory_service.record_correction(
                                batch_id=batch_id,
                                question_id=qid,
                                original_score=original_score,
                                corrected_score=new_score,
                                reason=review_reason or "é€»è¾‘å¤æ ¸ä¿®æ­£",
                                source="logic_review",
                            )
                    except Exception as mem_exc:
                        logger.debug(f"[logic_review] è®°å½•ä¿®æ­£å¤±è´¥: {mem_exc}")
                else:
                    updated_details.append(dict(q))
            updated_student["question_details"] = updated_details
            _recompute_student_totals(updated_student)

            self_audit = _normalize_logic_review_self_audit(
                payload_data.get("self_audit") or payload_data.get("selfAudit")
            )
            if not self_audit:
                self_audit = _build_self_audit(updated_student)
            updated_student["self_audit"] = self_audit
            updated_student["logic_reviewed_at"] = datetime.now().isoformat()

            review_payload = None
            if payload_data:
                review_payload = {
                    "student_key": student_key,
                    "student_id": updated_student.get("student_id"),
                    "reviewed_at": updated_student["logic_reviewed_at"],
                    "question_reviews": list(review_map.values()),
                    "self_audit": self_audit,
                }

            review_summary = _build_logic_review_summary(updated_details)
            await _broadcast_progress(
                batch_id,
                {
                    "type": "agent_update",
                    "agentId": agent_id,
                    "parentNodeId": "logic_review",
                    "status": "completed",
                    "progress": 100,
                    "message": "Logic review completed",
                    "output": {
                        "reviewSummary": review_summary,
                        "selfAudit": self_audit,
                    },
                },
            )
            return {"index": index, "result": updated_student, "review": review_payload}
        except Exception as exc:
            logger.warning(f"[logic_review] worker failed student={student_key}: {exc}")
            return {"index": index, "result": dict(student), "review": None}

    review_runner = RunnableLambda(review_student)
    inputs = [{"index": idx, "student": student} for idx, student in enumerate(student_results)]
    config = RunnableConfig(max_concurrency=max_workers) if max_workers > 0 else RunnableConfig()
    results = await review_runner.abatch(inputs, config=config)
    for result in results:
        if not result:
            continue
        updated_results[result["index"]] = result["result"]
        review_payload = result.get("review")
        if review_payload:
            logic_review_results.append(review_payload)

    final_results = [r for r in updated_results if r is not None]

    # æ•´åˆæ‰¹æ¬¡è®°å¿†åˆ°é•¿æœŸè®°å¿†
    try:
        new_memories = memory_service.consolidate_batch_memory(batch_id)
        memory_service.save_to_storage()
        logger.info(
            f"[logic_review] è®°å¿†æ•´åˆå®Œæˆ: batch_id={batch_id}, æ–°å¢ž {new_memories} æ¡é•¿æœŸè®°å¿†"
        )
    except Exception as e:
        logger.warning(f"[logic_review] è®°å¿†æ•´åˆå¤±è´¥: {e}")

    return {
        "student_results": final_results,
        "logic_review_results": logic_review_results,
        "current_stage": "logic_review_completed",
        "percentage": 85.0,
        "timestamps": {
            **state.get("timestamps", {}),
            "logic_review_at": datetime.now().isoformat(),
        },
    }


async def annotation_generation_node(state: BatchGradingGraphState) -> Dict[str, Any]:
    """
    æ‰¹æ³¨ç”ŸæˆèŠ‚ç‚¹

    åœ¨é€»è¾‘å¤æ ¸å®ŒæˆåŽï¼ŒåŸºäºŽæœ€ç»ˆçš„æ‰¹æ”¹ç»“æžœç”Ÿæˆè§†è§‰æ‰¹æ³¨ã€‚
    æ‰¹æ³¨ç”¨äºŽåœ¨å­¦ç”Ÿç­”å·å›¾ç‰‡ä¸Šæ ‡æ³¨å¾—åˆ†/é”™è¯¯ä½ç½®ã€‚

    å·¥ä½œæµä½ç½®ï¼šlogic_review â†’ annotation_generation â†’ review
    """
    from src.services.post_grading_annotator import (
        PostGradingAnnotator,
        AnnotatorConfig,
        AnnotationMode,
    )

    batch_id = state["batch_id"]
    student_results = state.get("student_results", []) or []
    grading_mode = _resolve_grading_mode(state.get("inputs", {}), state.get("parsed_rubric", {}))

    logger.info(f"[annotation_generation] å¼€å§‹ç”Ÿæˆæ‰¹æ³¨: batch_id={batch_id}")

    # èŽ·å–æ‰¹æ³¨æ¨¡å¼é…ç½®
    annotation_mode_str = state.get("inputs", {}).get("annotation_mode", "standard")
    try:
        annotation_mode = AnnotationMode(annotation_mode_str)
    except ValueError:
        annotation_mode = AnnotationMode.STANDARD

    # è¾…åŠ©æ¨¡å¼ä½¿ç”¨ç®€æ´æ‰¹æ³¨
    if grading_mode.startswith("assist"):
        annotation_mode = AnnotationMode.SIMPLE

    # åˆ›å»ºæ‰¹æ³¨ç”Ÿæˆå™¨
    config = AnnotatorConfig(mode=annotation_mode)
    annotator = PostGradingAnnotator(config)

    updated_results = []
    total_annotations = 0

    for student in student_results:
        student_key = student.get("student_key") or "unknown"

        try:
            # ç”Ÿæˆè¯¥å­¦ç”Ÿçš„æ‰¹æ³¨
            annotation_result = annotator.generate_annotations_for_student(student)

            # å°†æ‰¹æ³¨ç»“æžœé™„åŠ åˆ°å­¦ç”Ÿæ•°æ®
            updated_student = dict(student)
            updated_student["annotations"] = annotation_result.to_dict()

            # ç»Ÿè®¡æ‰¹æ³¨æ•°é‡
            for page in annotation_result.pages:
                total_annotations += len(page.annotations)

            updated_results.append(updated_student)

            logger.debug(
                f"[annotation_generation] å­¦ç”Ÿ {student_key}: "
                f"ç”Ÿæˆ {sum(len(p.annotations) for p in annotation_result.pages)} ä¸ªæ‰¹æ³¨"
            )
        except Exception as e:
            logger.warning(f"[annotation_generation] å­¦ç”Ÿ {student_key} æ‰¹æ³¨ç”Ÿæˆå¤±è´¥: {e}")
            updated_results.append(dict(student))

    logger.info(
        f"[annotation_generation] å®Œæˆ: batch_id={batch_id}, "
        f"å­¦ç”Ÿæ•°={len(updated_results)}, æ€»æ‰¹æ³¨æ•°={total_annotations}"
    )

    return {
        "student_results": updated_results,
        "current_stage": "annotation_generation_completed",
        "percentage": 88.0,
        "timestamps": {
            **state.get("timestamps", {}),
            "annotation_generation_at": datetime.now().isoformat(),
        },
    }


async def review_node(state: BatchGradingGraphState) -> Dict[str, Any]:
    """
    ç»“æžœå®¡æ ¸èŠ‚ç‚¹

    æ±‡æ€»å®¡æ ¸æ‰¹æ”¹ç»“æžœï¼Œæ ‡è®°éœ€è¦äººå·¥ç¡®è®¤çš„é¡¹ç›®ã€‚
    """
    batch_id = state["batch_id"]
    student_results = state.get("student_results", [])
    student_boundaries = state.get("student_boundaries", [])
    enable_review = state.get("inputs", {}).get("enable_review", True)
    grading_mode = _resolve_grading_mode(state.get("inputs", {}), state.get("parsed_rubric", {}))

    logger.info(f"[review] å¼€å§‹ç»“æžœå®¡æ ¸: batch_id={batch_id}")

    review_threshold = float(os.getenv("GRADING_REVIEW_CONFIDENCE_THRESHOLD", "0.7"))
    max_queue_items = int(os.getenv("GRADING_REVIEW_QUEUE_MAX_ITEMS", "200"))

    # ç»Ÿè®¡éœ€è¦ç¡®è®¤çš„è¾¹ç•Œ
    needs_confirmation = [b for b in student_boundaries if b.get("needs_confirmation")]

    review_queue, low_confidence_questions = _apply_review_flags_and_queue(
        student_results, review_threshold
    )

    # ç»Ÿè®¡ä½Žç½®ä¿¡åº¦ç»“æžœï¼ˆæŒ‰é¡µï¼‰
    low_confidence_results = []
    for student in student_results:
        for page_result in student.get("page_results", []):
            if page_result.get("confidence", 1.0) < review_threshold:
                low_confidence_results.append(
                    {
                        "student_key": student["student_key"],
                        "page_index": page_result.get("page_index"),
                        "confidence": page_result.get("confidence"),
                    }
                )

    review_summary = {
        "total_students": len(student_results),
        "boundaries_need_confirmation": len(needs_confirmation),
        "low_confidence_count": len(low_confidence_results),
        "low_confidence_results": low_confidence_results[:10],  # æœ€å¤šæ˜¾ç¤º10ä¸ª
        "low_confidence_question_count": len(low_confidence_questions),
        "low_confidence_questions": low_confidence_questions[:10],
        "review_threshold": review_threshold,
        "review_queue_count": len(review_queue),
        "review_queue": review_queue[:max_queue_items],
    }

    logger.info(
        f"[review] å®¡æ ¸å®Œæˆ: batch_id={batch_id}, "
        f"å­¦ç”Ÿæ•°={review_summary['total_students']}, "
        f"å¾…ç¡®è®¤è¾¹ç•Œ={review_summary['boundaries_need_confirmation']}"
    )

    if grading_mode.startswith("assist"):
        logger.info(f"[review] skip (assist mode): batch_id={batch_id}")
        return {
            "review_summary": review_summary,
            "review_result": {"action": "skip", "reason": "assist_mode"},
            "student_results": student_results,
            "current_stage": "review_completed",
            "percentage": 90.0,
            "timestamps": {**state.get("timestamps", {}), "review_at": datetime.now().isoformat()},
        }

    if not enable_review:
        logger.info(f"[review] skip (review disabled): batch_id={batch_id}")
        return {
            "review_summary": review_summary,
            "review_result": {"action": "skip"},
            "student_results": student_results,
            "current_stage": "review_completed",
            "percentage": 90.0,
            "timestamps": {**state.get("timestamps", {}), "review_at": datetime.now().isoformat()},
        }

    review_request = {
        "type": "results_review_required",
        "batch_id": batch_id,
        "summary": review_summary,
        "review_queue": review_queue[:max_queue_items],
        "message": "Results review required",
        "requested_at": datetime.now().isoformat(),
    }
    review_response = interrupt(review_request)

    action = (review_response or {}).get("action", "approve").lower()
    regrade_items = (
        (review_response or {}).get("regrade_items")
        or (review_response or {}).get("regradeItems")
        or []
    )

    updated_results = student_results
    if action == "regrade" and regrade_items:
        updated_results = await _regrade_selected_questions(state, updated_results, regrade_items)

    overrides = (
        (review_response or {}).get("results")
        or (review_response or {}).get("student_results")
        or []
    )
    updated_results = _apply_student_result_overrides(updated_results, overrides)

    return {
        "review_summary": review_summary,
        "review_result": review_response,
        "student_results": updated_results,
        "current_stage": "review_completed",
        "percentage": 90.0,
        "timestamps": {**state.get("timestamps", {}), "review_at": datetime.now().isoformat()},
    }


async def export_node(state: BatchGradingGraphState) -> Dict[str, Any]:
    """
    å¯¼å‡ºç»“æžœèŠ‚ç‚¹

    æŒä¹…åŒ–ç»“æžœå¹¶å‡†å¤‡å¯¼å‡ºæ•°æ®ã€‚
    æ”¯æŒæ— æ•°æ®åº“æ¨¡å¼ä¸‹å¯¼å‡ºç»“æžœä¸º JSON æ–‡ä»¶ã€‚
    æ”¯æŒéƒ¨åˆ†ç»“æžœä¿å­˜ï¼šä¸å¯æ¢å¤é”™è¯¯æ—¶ä¿å­˜å·²å®Œæˆç»“æžœã€‚

    Requirements: 9.4, 11.4
    """
    batch_id = state["batch_id"]
    student_results = state.get("student_results", [])
    cross_page_questions = state.get("cross_page_questions", [])
    merged_questions = state.get("merged_questions", [])
    grading_results = state.get("grading_results", [])

    logger.info(f"[export] å¼€å§‹å¯¼å‡ºç»“æžœ: batch_id={batch_id}, å­¦ç”Ÿæ•°={len(student_results)}")

    # æ£€æŸ¥æ˜¯å¦æœ‰å¤±è´¥çš„é¡µé¢
    failed_pages = [r for r in grading_results if r.get("status") == "failed"]
    has_failures = len(failed_pages) > 0

    if has_failures:
        logger.warning(f"[export] æ£€æµ‹åˆ° {len(failed_pages)} ä¸ªå¤±è´¥é¡µé¢ï¼Œ" f"å°†ä¿å­˜éƒ¨åˆ†ç»“æžœ")

    # æ£€æŸ¥æ•°æ®åº“å¯ç”¨æ€§å¹¶å®žçŽ°æŒä¹…åŒ–é€»è¾‘
    persisted = False
    try:
        from src.utils.database import db

        # ä½¿ç”¨ db.is_available æ£€æŸ¥æ•°æ®åº“å¯ç”¨æ€§
        if db.is_available:
            logger.info("[export] æ•°æ®åº“è¿žæŽ¥å¯ç”¨ï¼Œå¼€å§‹æŒä¹…åŒ–æ‰¹æ”¹ç»“æžœ...")
            
            try:
                from src.db.postgres_grading import (
                    GradingHistory,
                    StudentGradingResult,
                    save_grading_history,
                    save_student_result,
                )
                import uuid
                
                # 1. ä¿å­˜æ‰¹æ”¹åŽ†å²
                history_id = str(uuid.uuid4())
                total_students = len(student_results)
                
                # è®¡ç®—å¹³å‡åˆ†
                total_scores = [s.get("total_score", 0) for s in student_results]
                average_score = sum(total_scores) / total_students if total_students > 0 else 0
                
                grading_history = GradingHistory(
                    id=history_id,
                    batch_id=batch_id,
                    status="completed" if not has_failures else "partial",
                    class_ids=None,  # å¯ä»¥ä»Ž state ä¸­èŽ·å– class_ids
                    created_at=datetime.now().isoformat(),
                    completed_at=datetime.now().isoformat(),
                    total_students=total_students,
                    average_score=average_score,
                    result_data={
                        "has_failures": has_failures,
                        "failed_pages_count": len(failed_pages),
                        "cross_page_questions": cross_page_questions,
                        "merged_questions": merged_questions,
                    }
                )
                
                await save_grading_history(grading_history)
                logger.info(f"[export] æ‰¹æ”¹åŽ†å²å·²ä¿å­˜åˆ°æ•°æ®åº“: history_id={history_id}")
                
                # 2. ä¿å­˜æ¯ä¸ªå­¦ç”Ÿçš„æ‰¹æ”¹ç»“æžœå’Œé¡µé¢å›¾åƒ
                saved_students = 0
                saved_images = 0
                
                from src.db.postgres_grading import GradingPageImage, save_page_image
                
                for student in student_results:
                    try:
                        # èŽ·å–å­¦ç”Ÿæ ‡è¯†ï¼Œä¼˜å…ˆä½¿ç”¨ student_keyï¼Œç„¶åŽæ˜¯ student_name
                        student_key = (
                            student.get("student_key") 
                            or student.get("student_name") 
                            or f"student_{saved_students + 1}"
                        )
                        
                        student_result = StudentGradingResult(
                            id=str(uuid.uuid4()),
                            grading_history_id=history_id,
                            student_key=student_key,
                            score=student.get("total_score"),
                            max_score=student.get("max_total_score"),
                            class_id=None,  # å¯ä»¥ä»Ž state ä¸­èŽ·å–
                            student_id=student.get("student_id"),
                            summary=student.get("student_summary"),
                            self_report=student.get("self_audit"),
                            result_data={
                                "question_results": student.get("question_details", []),
                                "percentage": student.get("percentage", 0),
                            },
                            imported_at=datetime.now().isoformat(),
                        )
                        
                        await save_student_result(student_result)
                        saved_students += 1
                        
                        # 3. ä¿å­˜è¯¥å­¦ç”Ÿçš„é¡µé¢å›¾åƒ
                        page_results = student.get("page_results", [])
                        
                        for page_result in page_results:
                            page_index = page_result.get("page_index", 0)
                            image_bytes = page_result.get("image")
                            
                            if image_bytes and isinstance(image_bytes, bytes):
                                try:
                                    page_image = GradingPageImage(
                                        id=str(uuid.uuid4()),
                                        grading_history_id=history_id,
                                        student_key=student_key,
                                        page_index=page_index,
                                        image_data=image_bytes,
                                        image_format="png",
                                        created_at=datetime.now().isoformat(),
                                    )
                                    
                                    await save_page_image(page_image)
                                    saved_images += 1
                                except Exception as e:
                                    logger.error(f"[export] ä¿å­˜é¡µé¢å›¾åƒå¤±è´¥ (student={student_key}, page={page_index}): {e}")
                        
                    except Exception as e:
                        logger.error(f"[export] ä¿å­˜å­¦ç”Ÿç»“æžœå¤±è´¥: {e}")
                
                logger.info(f"[export] å·²ä¿å­˜ {saved_students}/{total_students} ä¸ªå­¦ç”Ÿç»“æžœåˆ°æ•°æ®åº“")
                logger.info(f"[export] å·²ä¿å­˜ {saved_images} å¼ é¡µé¢å›¾åƒåˆ°æ•°æ®åº“")
                persisted = True
                
            except Exception as e:
                logger.error(f"[export] æ•°æ®åº“æŒä¹…åŒ–å¤±è´¥: {e}", exc_info=True)
                persisted = False
        else:
            logger.info("[export] æ•°æ®åº“ä¸å¯ç”¨ï¼Œè·³è¿‡æŒä¹…åŒ–")
    except Exception as e:
        logger.warning(f"[export] æ•°æ®åº“è¿žæŽ¥æ£€æŸ¥å¤±è´¥ï¼ˆç¦»çº¿æ¨¡å¼ï¼‰: {e}")

    # å‡†å¤‡å¯¼å‡ºæ•°æ®
    export_data = {
        "batch_id": batch_id,
        "export_time": datetime.now().isoformat(),
        "persisted": persisted,
        "has_failures": has_failures,
        "failed_pages_count": len(failed_pages),
        "cross_page_questions": cross_page_questions,
        "merged_questions": merged_questions,
        "students": [],
    }

    # æ·»åŠ å¤±è´¥é¡µé¢ä¿¡æ¯ï¼ˆç”¨äºŽéƒ¨åˆ†ç»“æžœä¿å­˜ï¼‰
    if has_failures:
        export_data["failed_pages"] = [
            {
                "page_index": p.get("page_index"),
                "error": p.get("error"),
                "batch_index": p.get("batch_index"),
            }
            for p in failed_pages
        ]

    for student in student_results:
        _recompute_student_totals(student)
        # è®¡ç®—ç™¾åˆ†æ¯”
        total_score = student.get("total_score", 0)
        max_score = student.get("max_total_score", 0)
        percentage = (total_score / max_score * 100) if max_score > 0 else 0

        summary = student.get("student_summary") or _build_student_summary(student)
        audit = student.get("self_audit") or _build_self_audit(student)
        student["student_summary"] = summary
        student["self_audit"] = audit

        # æ”¶é›†é¢˜ç›®ç»“æžœ
        question_results = []

        # ä¼˜å…ˆä½¿ç”¨ question_details
        if student.get("question_details"):
            for q in student["question_details"]:
                question_results.append(
                    {
                        "question_id": q.get("question_id", ""),
                        "score": q.get("score", 0),
                        "max_score": q.get("max_score", 0),
                        "feedback": q.get("feedback", ""),
                        "student_answer": q.get("student_answer", ""),
                        "is_correct": q.get("is_correct", False),
                        "is_cross_page": q.get("is_cross_page", False),
                        "page_indices": q.get("page_indices", []),
                        "confidence": q.get("confidence", 1.0),
                        "confidence_reason": q.get("confidence_reason")
                        or q.get("confidenceReason"),
                        "self_critique": q.get("self_critique") or q.get("selfCritique"),
                        "self_critique_confidence": (
                            q.get("self_critique_confidence") or q.get("selfCritiqueConfidence")
                        ),
                        "review_summary": q.get("review_summary") or q.get("reviewSummary"),
                        "review_corrections": q.get("review_corrections")
                        or q.get("reviewCorrections")
                        or [],
                        "review_reasons": q.get("review_reasons") or q.get("reviewReasons") or [],
                        "needs_review": (
                            q.get("needs_review")
                            if q.get("needs_review") is not None
                            else q.get("needsReview")
                        ),
                        "audit_flags": q.get("audit_flags") or q.get("auditFlags") or [],
                        "typo_notes": q.get("typo_notes") or q.get("typoNotes") or [],
                        "rubric_refs": q.get("rubric_refs") or q.get("rubricRefs") or [],
                        "honesty_note": q.get("honesty_note") or q.get("honestyNote"),
                        "question_type": q.get("question_type") or q.get("questionType"),
                        "merge_source": q.get("merge_source") or q.get("mergeSource"),
                        "scoring_point_results": (
                            q.get("scoring_point_results") or q.get("scoring_results") or []
                        ),
                    }
                )
        # å¦åˆ™ä»Ž page_results æå–
        elif student.get("page_results"):
            for page in student["page_results"]:
                if page.get("status") == "completed" and not page.get("is_blank_page", False):
                    for q in page.get("question_details", []):
                        question_results.append(
                            {
                                "question_id": q.get("question_id", ""),
                                "score": q.get("score", 0),
                                "max_score": q.get("max_score", 0),
                                "feedback": q.get("feedback", ""),
                                "student_answer": q.get("student_answer", ""),
                                "is_correct": q.get("is_correct", False),
                                "confidence": q.get("confidence", 1.0),
                                "confidence_reason": q.get("confidence_reason")
                                or q.get("confidenceReason"),
                                "self_critique": q.get("self_critique") or q.get("selfCritique"),
                                "self_critique_confidence": (
                                    q.get("self_critique_confidence")
                                    or q.get("selfCritiqueConfidence")
                                ),
                                "review_summary": q.get("review_summary") or q.get("reviewSummary"),
                                "review_corrections": q.get("review_corrections")
                                or q.get("reviewCorrections")
                                or [],
                                "review_reasons": q.get("review_reasons")
                                or q.get("reviewReasons")
                                or [],
                                "needs_review": (
                                    q.get("needs_review")
                                    if q.get("needs_review") is not None
                                    else q.get("needsReview")
                                ),
                                "audit_flags": q.get("audit_flags") or q.get("auditFlags") or [],
                                "typo_notes": q.get("typo_notes") or q.get("typoNotes") or [],
                                "rubric_refs": q.get("rubric_refs") or q.get("rubricRefs") or [],
                                "honesty_note": q.get("honesty_note") or q.get("honestyNote"),
                                "question_type": q.get("question_type") or q.get("questionType"),
                                "is_cross_page": q.get("is_cross_page", False),
                                "page_indices": q.get("page_indices") or [page.get("page_index")],
                                "merge_source": q.get("merge_source") or q.get("mergeSource"),
                                "scoring_point_results": (
                                    q.get("scoring_point_results") or q.get("scoring_results") or []
                                ),
                            }
                        )

        export_data["students"].append(
            {
                "student_name": student["student_key"],
                "student_id": student.get("student_id"),
                "score": total_score,
                "max_score": max_score,
                "percentage": round(percentage, 1),
                "question_results": question_results,
                "confidence": student.get("confidence", 0),
                "needs_confirmation": student.get("needs_confirmation", False),
                "start_page": student.get("start_page", 0),
                "end_page": student.get("end_page", 0),
                "student_summary": summary,
                "self_audit": audit,
                "draft_question_details": student.get("draft_question_details"),
                "draft_total_score": student.get("draft_total_score"),
                "draft_max_score": student.get("draft_max_score"),
                "missing_question_ids": student.get("missing_question_ids"),
            }
        )

    class_report = _build_class_report(student_results)
    export_data["class_report"] = class_report

    # å¯¼å‡ºä¸º JSON æ–‡ä»¶ (Requirements: 9.4, 11.4)
    # æ— æ•°æ®åº“æ¨¡å¼æˆ–æœ‰å¤±è´¥æ—¶éƒ½å¯¼å‡º
    if not persisted or has_failures:
        try:
            import os

            # åˆ›å»ºå¯¼å‡ºç›®å½•
            export_dir = os.getenv("EXPORT_DIR", "./exports")
            os.makedirs(export_dir, exist_ok=True)

            # ç”Ÿæˆæ–‡ä»¶å
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")

            # å¦‚æžœæœ‰å¤±è´¥ï¼Œæ ‡è®°ä¸ºéƒ¨åˆ†ç»“æžœ (Requirement 9.4)
            if has_failures:
                filename = f"partial_result_{batch_id}_{timestamp}.json"
                logger.info(f"[export] ä¿å­˜éƒ¨åˆ†ç»“æžœï¼ˆ{len(failed_pages)} ä¸ªé¡µé¢å¤±è´¥ï¼‰: {filename}")
            else:
                filename = f"grading_result_{batch_id}_{timestamp}.json"

            filepath = os.path.join(export_dir, filename)

            # å†™å…¥ JSON æ–‡ä»¶
            with open(filepath, "w", encoding="utf-8") as f:
                json.dump(export_data, f, ensure_ascii=False, indent=2)

            export_data["json_file"] = filepath

            if has_failures:
                logger.warning(
                    f"[export] éƒ¨åˆ†ç»“æžœå·²ä¿å­˜: {filepath}. "
                    f"å®Œæˆ={len(grading_results) - len(failed_pages)}/{len(grading_results)} é¡µ"
                )
            else:
                logger.info(f"[export] ç»“æžœå·²å¯¼å‡ºä¸º JSON: {filepath}")

        except Exception as e:
            logger.error(f"[export] JSON å¯¼å‡ºå¤±è´¥: {e}", exc_info=True)
            export_data["json_export_error"] = str(e)

            # è®°å½•é”™è¯¯
            from src.utils.error_handling import get_error_manager

            error_manager = get_error_manager()
            error_manager.add_error(
                exc=e,
                context={
                    "batch_id": batch_id,
                    "function": "export_node",
                    "export_type": "json",
                },
                batch_id=batch_id,
            )

    # å¯¼å‡ºé”™è¯¯æ—¥å¿—ï¼ˆå¦‚æžœæœ‰é”™è¯¯ï¼‰
    try:
        from src.utils.error_handling import get_error_manager

        error_manager = get_error_manager()

        batch_errors = error_manager.get_errors_by_batch(batch_id)
        if batch_errors:
            import os

            export_dir = os.getenv("EXPORT_DIR", "./exports")
            os.makedirs(export_dir, exist_ok=True)

            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            error_log_file = os.path.join(export_dir, f"error_log_{batch_id}_{timestamp}.json")

            error_manager.export_to_file(error_log_file)
            export_data["error_log_file"] = error_log_file

            logger.info(
                f"[export] é”™è¯¯æ—¥å¿—å·²å¯¼å‡º: {error_log_file} " f"({len(batch_errors)} ä¸ªé”™è¯¯)"
            )
    except Exception as e:
        logger.error(f"[export] é”™è¯¯æ—¥å¿—å¯¼å‡ºå¤±è´¥: {e}", exc_info=True)

    logger.info(
        f"[export] å¯¼å‡ºå®Œæˆ: batch_id={batch_id}, "
        f"å­¦ç”Ÿæ•°={len(export_data['students'])}, "
        f"è·¨é¡µé¢˜ç›®æ•°={len(cross_page_questions)}, "
        f"å¤±è´¥é¡µé¢æ•°={len(failed_pages)}"
    )

    return {
        "export_data": export_data,
        "student_results": student_results,
        "class_report": class_report,
        "current_stage": "completed",
        "percentage": 100.0,
        "timestamps": {
            **state.get("timestamps", {}),
            "export_at": datetime.now().isoformat(),
            "completed_at": datetime.now().isoformat(),
        },
    }


# ==================== Graph ç¼–è¯‘ ====================


def create_batch_grading_graph(
    checkpointer: Optional[AsyncPostgresSaver] = None,
    batch_config: Optional[BatchConfig] = None,
) -> StateGraph:
    """åˆ›å»ºæ‰¹é‡æ‰¹æ”¹ Graphï¼ˆç®€åŒ–ç‰ˆï¼‰

    å·¥ä½œæµï¼š
    1. intake: æŽ¥æ”¶æ–‡ä»¶
    2. preprocess: å›¾åƒé¢„å¤„ç†
    3. rubric_parse: è§£æžè¯„åˆ†æ ‡å‡†
    4. rubric_review: äººå·¥å®¡æ ¸ï¼ˆå¯è·³è¿‡ï¼‰
    5. grade_batch (å¹¶è¡Œ): æŒ‰å­¦ç”Ÿæˆ–æ‰¹æ¬¡å¤§å°å¹¶è¡Œæ‰¹æ”¹
    6. simple_aggregate: ç®€å•èšåˆä¸ºå­¦ç”Ÿç»“æžœ
    7. self_report: è‡ªç™½èŠ‚ç‚¹ï¼ˆé£Žé™©åˆ†æžï¼‰
    8. logic_review: é€»è¾‘å¤æ ¸
    9. annotation_generation: ç”Ÿæˆæ‰¹æ³¨
    10. review: ç»“æžœå®¡æ ¸
    11. export: å¯¼å‡ºç»“æžœ

    æµç¨‹å›¾ï¼š
    ```
    intake
      â†“
    preprocess
      â†“
    rubric_parse
      â†“
    rubric_review (å¯è·³è¿‡)
      â†“
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚ grade_batch (N) â”‚  â† å¹¶è¡Œæ‰¹æ”¹ï¼ˆæŒ‰å­¦ç”Ÿåˆ†æ‰¹ï¼‰
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
      â†“
    simple_aggregate  â† ç®€å•èšåˆ
      â†“
    self_report  â† è‡ªç™½ï¼ˆè®°å¿†ç³»ç»Ÿï¼‰
      â†“
    logic_review  â† é€»è¾‘å¤æ ¸
      â†“
    annotation_generation  â† æ‰¹æ³¨ç”Ÿæˆ
      â†“
    review
      â†“
    export
      â†“
    END
    ```

    ç‰¹æ€§ï¼š
    - æŒ‰å­¦ç”Ÿåˆ†æ‰¹æ‰¹æ”¹ï¼ˆå‰ç«¯æä¾› student_mappingï¼‰
    - Worker ç‹¬ç«‹æ€§ä¿è¯ (Requirements: 3.2)
    - æ‰¹æ¬¡å¤±è´¥é‡è¯• (Requirements: 3.3, 9.3)
    - å®žæ—¶è¿›åº¦æŠ¥å‘Š (Requirements: 3.4)
    - è®°å¿†ç³»ç»Ÿé›†æˆï¼ˆç§‘ç›®éš”ç¦»ï¼‰

    å·²ç§»é™¤ï¼š
    - index èŠ‚ç‚¹ï¼ˆä¸å†éœ€è¦ç´¢å¼•å±‚ï¼‰
    - cross_page_merge èŠ‚ç‚¹ï¼ˆä¸å†éœ€è¦è·¨é¡µåˆå¹¶ï¼‰
    - index_merge èŠ‚ç‚¹ï¼ˆä¸å†éœ€è¦ç´¢å¼•èšåˆï¼‰

    Args:
        checkpointer: PostgreSQL Checkpointerï¼ˆå¯é€‰ï¼‰
        batch_config: æ‰¹æ¬¡é…ç½®ï¼ˆå¯é€‰ï¼Œé»˜è®¤ä»ŽçŽ¯å¢ƒå˜é‡åŠ è½½ï¼‰

    Returns:
        ç¼–è¯‘åŽçš„ Graph
    """
    # è®¾ç½®æ‰¹æ¬¡é…ç½®
    if batch_config:
        set_batch_config(batch_config)

    config = get_batch_config()
    logger.info(
        f"åˆ›å»ºæ‰¹é‡æ‰¹æ”¹ Graph: batch_size={config.batch_size}, "
        f"max_workers={config.max_concurrent_workers}, "
        f"max_retries={config.max_retries}"
    )

    graph = StateGraph(BatchGradingGraphState)

    # æ·»åŠ èŠ‚ç‚¹
    graph.add_node("intake", intake_node)
    graph.add_node("preprocess", preprocess_node)
    # graph.add_node("index", index_node)  # å·²ç§»é™¤ï¼šä¸å†éœ€è¦ç´¢å¼•å±‚
    graph.add_node("rubric_parse", rubric_parse_node)
    graph.add_node("rubric_review", rubric_review_node)
    graph.add_node("grade_batch", grade_batch_node)
    # graph.add_node("simple_aggregate", simple_aggregate_node)  # å·²ç§»é™¤ï¼šgrade_batch ç›´æŽ¥è¾“å‡º student_results
    # graph.add_node("cross_page_merge", cross_page_merge_node)  # å·²ç§»é™¤ï¼šä¸å†éœ€è¦è·¨é¡µåˆå¹¶
    # graph.add_node("index_merge", index_merge_node)  # å·²ç§»é™¤ï¼šä¸å†éœ€è¦ç´¢å¼•èšåˆ
    graph.add_node("self_report", self_report_node)
    graph.add_node("logic_review", logic_review_node)
    graph.add_node("annotation_generation", annotation_generation_node)
    graph.add_node("review", review_node)
    graph.add_node("export", export_node)

    # å…¥å£ç‚¹
    graph.set_entry_point("intake")

    # ç®€åŒ–æµç¨‹ï¼šintake â†’ preprocess â†’ rubric_parse â†’ rubric_review
    graph.add_edge("intake", "preprocess")
    graph.add_edge("preprocess", "rubric_parse")  # è·³è¿‡ index
    graph.add_edge("rubric_parse", "rubric_review")

    # rubric_review åŽæ‰‡å‡ºåˆ°å¹¶è¡Œæ‰¹æ”¹
    graph.add_conditional_edges(
        "rubric_review",
        grading_fanout_router,
        [
            "grade_batch",
            "self_report",
        ],  # ç®€åŒ–ï¼šgrade_batch ç›´æŽ¥è¾“å‡º student_resultsï¼Œå®ŒæˆåŽè¿›å…¥ self_report
    )

    # å¹¶è¡Œæ‰¹æ”¹åŽç›´æŽ¥è¿›å…¥ self_reportï¼ˆgrade_batch é€šè¿‡ add reducer èšåˆ student_resultsï¼‰
    graph.add_edge("grade_batch", "self_report")

    # ç®€åŒ–æµç¨‹ï¼šself_report â†’ logic_review â†’ annotation_generation â†’ review â†’ export â†’ END
    graph.add_edge("self_report", "logic_review")
    graph.add_edge("logic_review", "annotation_generation")
    graph.add_edge("annotation_generation", "review")
    graph.add_edge("review", "export")
    graph.add_edge("export", END)

    # ç¼–è¯‘
    compile_kwargs = {}
    if checkpointer:
        compile_kwargs["checkpointer"] = checkpointer

    compiled_graph = graph.compile(**compile_kwargs)

    logger.info("æ‰¹é‡æ‰¹æ”¹ Graph å·²ç¼–è¯‘")

    return compiled_graph


# ==================== å¯¼å‡º ====================

__all__ = [
    # é…ç½®ç±»
    "BatchConfig",
    "get_batch_config",
    "set_batch_config",
    # è¿›åº¦ç±»
    "BatchProgress",
    "BatchTaskState",
    # èŠ‚ç‚¹å‡½æ•°
    "intake_node",
    "preprocess_node",
    "rubric_parse_node",
    "grade_batch_node",
    "self_report_node",
    "logic_review_node",
    "annotation_generation_node",  # æ–°å¢žæ‰¹æ³¨ç”ŸæˆèŠ‚ç‚¹
    "review_node",
    "export_node",
    # è·¯ç”±å‡½æ•°
    "grading_fanout_router",
    # Graph åˆ›å»º
    "create_batch_grading_graph",
]
