import logging
import os
import asyncio
import json
import re
import time
import copy
from functools import lru_cache
from typing import Optional, List, Dict, Any, Literal, Tuple
from datetime import datetime
from dataclasses import dataclass, field

from langgraph.graph import StateGraph, END
from langgraph.types import Send, interrupt
from langgraph.checkpoint.postgres.aio import AsyncPostgresSaver
from langchain_core.runnables import RunnableConfig, RunnableLambda

from src.graphs.state import BatchGradingGraphState
from src.utils.llm_thinking import split_thinking_content


logger = logging.getLogger(__name__)


# PostgreSQL å›¾ç‰‡å­˜å‚¨ï¼ˆå»¶è¿Ÿå¯¼å…¥ä»¥é¿å…å¾ªç¯ä¾èµ–ï¼‰
def _get_pg_image_reader():
    """è·å– PostgreSQL å›¾ç‰‡è¯»å–å‡½æ•°"""
    try:
        from src.db.postgres_images import get_batch_images_as_bytes_list
        return get_batch_images_as_bytes_list
    except ImportError:
        return None

# Stdout-visible workflow markers for Railway verification.
workflow_logger = logging.getLogger("gradeos.workflow")


@lru_cache(maxsize=1)
def _get_broadcast_progress():
    """å»¶è¿ŸåŠ è½½è¿›åº¦å¹¿æ’­å‡½æ•°ï¼Œé¿å…æµ‹è¯•åœºæ™¯è§¦å‘é‡ä¾èµ–å¯¼å…¥ã€‚"""
    if os.getenv("DISABLE_PROGRESS_BROADCAST", "false").strip().lower() in (
        "1",
        "true",
        "yes",
    ):

        async def _noop(*_args, **_kwargs) -> None:
            return None

        return _noop

    from src.api.routes.batch_langgraph import broadcast_progress

    return broadcast_progress


async def _broadcast_progress(batch_id: str, message: Dict[str, Any]) -> None:
    """åŒ…è£…è¿›åº¦å¹¿æ’­ï¼Œä¾¿äºæµ‹è¯•ä¸­ç¦ç”¨ã€‚"""
    await _get_broadcast_progress()(batch_id, message)


# ==================== æ‰¹æ¬¡é…ç½® ====================


@dataclass
class BatchConfig:
    """
    æ‰¹æ¬¡é…ç½®ç±»

    æ”¯æŒé…ç½®æ‰¹æ¬¡å¤§å°å’Œå¹¶å‘æ•°é‡ã€‚

    Requirements: 3.1, 10.1
    """

    batch_size: int = 1000  # æ¯æ‰¹å¤„ç†çš„é¡µé¢æ•°é‡ (è§£é™¤é™åˆ¶)
    max_concurrent_workers: int = 5  # æœ€å¤§å¹¶å‘ Worker æ•°é‡
    max_retries: int = 2  # æ‰¹æ¬¡å¤±è´¥æœ€å¤§é‡è¯•æ¬¡æ•°
    retry_delay: float = 1.0  # é‡è¯•å»¶è¿Ÿï¼ˆç§’ï¼‰

    @classmethod
    def from_env(cls) -> "BatchConfig":
        """ä»ç¯å¢ƒå˜é‡åŠ è½½é…ç½®"""
        return cls(
            batch_size=int(os.getenv("GRADING_BATCH_SIZE", "1000")),
            max_concurrent_workers=int(os.getenv("GRADING_MAX_WORKERS", "5")),
            max_retries=int(os.getenv("GRADING_MAX_RETRIES", "2")),
            retry_delay=float(os.getenv("GRADING_RETRY_DELAY", "1.0")),
        )


# å…¨å±€æ‰¹æ¬¡é…ç½®
_batch_config: Optional[BatchConfig] = None


def get_batch_config() -> BatchConfig:
    """è·å–æ‰¹æ¬¡é…ç½®"""
    global _batch_config
    if _batch_config is None:
        _batch_config = BatchConfig.from_env()
    return _batch_config


def set_batch_config(config: BatchConfig) -> None:
    """è®¾ç½®æ‰¹æ¬¡é…ç½®"""
    global _batch_config
    _batch_config = config
    logger.info(
        f"æ‰¹æ¬¡é…ç½®å·²æ›´æ–°: batch_size={config.batch_size}, "
        f"max_workers={config.max_concurrent_workers}, "
        f"max_retries={config.max_retries}"
    )


# ==================== è¿›åº¦æŠ¥å‘Š ====================


@dataclass
class BatchProgress:
    """
    æ‰¹æ¬¡è¿›åº¦ä¿¡æ¯

    Requirements: 3.4
    """

    batch_id: str
    total_batches: int
    completed_batches: int = 0
    failed_batches: int = 0
    in_progress_batches: int = 0
    total_pages: int = 0
    processed_pages: int = 0
    failed_pages: int = 0
    current_stage: str = "initialized"
    percentage: float = 0.0
    batch_details: Dict[int, Dict[str, Any]] = field(default_factory=dict)
    timestamps: Dict[str, str] = field(default_factory=dict)

    def update_batch_status(
        self,
        batch_index: int,
        status: str,
        pages_processed: int = 0,
        pages_failed: int = 0,
        error: Optional[str] = None,
    ) -> None:
        """æ›´æ–°å•ä¸ªæ‰¹æ¬¡çŠ¶æ€"""
        self.batch_details[batch_index] = {
            "status": status,
            "pages_processed": pages_processed,
            "pages_failed": pages_failed,
            "error": error,
            "updated_at": datetime.now().isoformat(),
        }

        # é‡æ–°è®¡ç®—ç»Ÿè®¡
        self.completed_batches = sum(
            1 for d in self.batch_details.values() if d["status"] == "completed"
        )
        self.failed_batches = sum(1 for d in self.batch_details.values() if d["status"] == "failed")
        self.in_progress_batches = sum(
            1 for d in self.batch_details.values() if d["status"] == "in_progress"
        )
        self.processed_pages = sum(d["pages_processed"] for d in self.batch_details.values())
        self.failed_pages = sum(d["pages_failed"] for d in self.batch_details.values())

        # è®¡ç®—ç™¾åˆ†æ¯”ï¼ˆæ‰¹æ”¹é˜¶æ®µå  15%-80%ï¼‰
        if self.total_batches > 0:
            batch_progress = self.completed_batches / self.total_batches
            self.percentage = 15.0 + batch_progress * 65.0

    def to_dict(self) -> Dict[str, Any]:
        """åºåˆ—åŒ–ä¸ºå­—å…¸"""
        return {
            "batch_id": self.batch_id,
            "total_batches": self.total_batches,
            "completed_batches": self.completed_batches,
            "failed_batches": self.failed_batches,
            "in_progress_batches": self.in_progress_batches,
            "total_pages": self.total_pages,
            "processed_pages": self.processed_pages,
            "failed_pages": self.failed_pages,
            "current_stage": self.current_stage,
            "percentage": self.percentage,
            "batch_details": self.batch_details,
            "timestamps": self.timestamps,
        }


# è¿›åº¦æŠ¥å‘Šå›è°ƒç±»å‹
ProgressCallback = Optional[callable]


# ==================== æ‰¹æ¬¡ä»»åŠ¡çŠ¶æ€ ====================


@dataclass
class BatchTaskState:
    """
    å•ä¸ªæ‰¹æ¬¡ä»»åŠ¡çš„çŠ¶æ€

    ç”¨äºè·Ÿè¸ªæ‰¹æ¬¡æ‰§è¡ŒçŠ¶æ€å’Œæ”¯æŒé‡è¯•ã€‚

    Requirements: 3.3, 9.3
    """

    batch_id: str
    batch_index: int
    total_batches: int
    page_indices: List[int]
    images: List[str]
    rubric: str
    parsed_rubric: Dict[str, Any]
    api_key: str
    page_index_contexts: Dict[int, Dict[str, Any]] = field(default_factory=dict)
    retry_count: int = 0
    max_retries: int = 2
    status: str = "pending"  # pending, in_progress, completed, failed
    error: Optional[str] = None
    results: List[Dict[str, Any]] = field(default_factory=list)


# ==================== èŠ‚ç‚¹å®ç° ====================


async def intake_node(state: BatchGradingGraphState) -> Dict[str, Any]:
    """
    æ¥æ”¶æ–‡ä»¶èŠ‚ç‚¹

    éªŒè¯è¾“å…¥æ–‡ä»¶ï¼Œå‡†å¤‡å¤„ç†ç¯å¢ƒã€‚
    """
    batch_id = state["batch_id"]
    public_base = (
        os.getenv("BACKEND_PUBLIC_URL")
        or os.getenv("PUBLIC_BACKEND_URL")
        or os.getenv("PUBLIC_API_BASE_URL")
        or ""
    )

    def _build_file_url(file_id: str) -> str:
        if public_base:
            return public_base.rstrip("/") + f"/api/batch/files/{file_id}/download"
        return f"/api/batch/files/{file_id}/download"

    logger.info(f"[intake] å¼€å§‹æ¥æ”¶æ–‡ä»¶: batch_id={batch_id}")

    # éªŒè¯å¿…è¦çš„è¾“å…¥
    answer_images = state.get("answer_images", [])
    rubric_images = state.get("rubric_images", [])

    if not answer_images:
        raise ValueError("æœªæä¾›ç­”é¢˜å›¾åƒ")

    logger.info(
        f"[intake] æ–‡ä»¶æ¥æ”¶å®Œæˆ: batch_id={batch_id}, "
        f"ç­”é¢˜é¡µæ•°={len(answer_images)}, è¯„åˆ†æ ‡å‡†é¡µæ•°={len(rubric_images)}"
    )

    return {
        "current_stage": "intake_completed",
        "percentage": 5.0,
        "timestamps": {**state.get("timestamps", {}), "intake_at": datetime.now().isoformat()},
    }


async def preprocess_node(state: BatchGradingGraphState) -> Dict[str, Any]:
    """
    å›¾åƒé¢„å¤„ç†èŠ‚ç‚¹

    å¯¹å›¾åƒè¿›è¡Œé¢„å¤„ç†ï¼š
    1. è½¬æ¢ä¸º JPEG æ ¼å¼
    2. å‹ç¼©è´¨é‡æ§åˆ¶
    3. å»å™ªã€å¢å¼ºã€æ—‹è½¬æ ¡æ­£ç­‰ï¼ˆTODOï¼‰
    """
    batch_id = state["batch_id"]
    answer_images = state.get("answer_images", [])

    logger.info(f"[preprocess] å¼€å§‹å›¾åƒé¢„å¤„ç†: batch_id={batch_id}, é¡µæ•°={len(answer_images)}")

    # è½¬æ¢ä¸º JPEG æ ¼å¼
    processed_images = []
    for idx, img_bytes in enumerate(answer_images):
        try:
            from PIL import Image
            import io

            # æ‰“å¼€å›¾åƒ
            img = Image.open(io.BytesIO(img_bytes))

            # è½¬æ¢ä¸º RGBï¼ˆJPEG ä¸æ”¯æŒ RGBA å’Œ P æ¨¡å¼ï¼‰
            if img.mode in ("RGBA", "P", "LA"):
                # åˆ›å»ºç™½è‰²èƒŒæ™¯
                background = Image.new("RGB", img.size, (255, 255, 255))
                if img.mode == "P":
                    img = img.convert("RGBA")
                if img.mode in ("RGBA", "LA"):
                    background.paste(img, mask=img.split()[-1])  # ä½¿ç”¨ alpha é€šé“ä½œä¸º mask
                    img = background
                else:
                    img = img.convert("RGB")
            elif img.mode != "RGB":
                img = img.convert("RGB")

            # ä¿å­˜ä¸º JPEG
            output = io.BytesIO()
            img.save(output, format="JPEG", quality=85, optimize=True)
            processed_images.append(output.getvalue())

            logger.debug(
                f"[preprocess] é¡µé¢ {idx} è½¬æ¢ä¸º JPEG: {len(img_bytes)} -> {len(output.getvalue())} bytes"
            )
        except Exception as e:
            logger.warning(f"[preprocess] é¡µé¢ {idx} JPEG è½¬æ¢å¤±è´¥: {e}ï¼Œä½¿ç”¨åŸå›¾")
            processed_images.append(img_bytes)

    logger.info(
        f"[preprocess] å›¾åƒé¢„å¤„ç†å®Œæˆ: batch_id={batch_id}, JPEGè½¬æ¢={len(processed_images)}/{len(answer_images)}"
    )

    student_boundaries = _build_student_boundaries(state, len(processed_images))

    return {
        "processed_images": processed_images,
        "student_boundaries": student_boundaries,
        "current_stage": "preprocess_completed",
        "percentage": 10.0,
        "timestamps": {**state.get("timestamps", {}), "preprocess_at": datetime.now().isoformat()},
    }


def _coerce_int(value: Any) -> Optional[int]:
    try:
        return int(value)
    except (TypeError, ValueError):
        return None


def _first_not_none(*values: Any) -> Any:
    for value in values:
        if value is not None:
            return value
    return None


def _sanitize_pages(raw_pages: Any, total_pages: int) -> List[int]:
    if not isinstance(raw_pages, (list, tuple)):
        return []
    cleaned = []
    for item in raw_pages:
        idx = _coerce_int(item)
        if idx is None:
            continue
        if 0 <= idx < total_pages:
            cleaned.append(idx)
    return sorted(set(cleaned))


def _normalize_manual_boundaries(raw: Any, total_pages: int) -> List[Dict[str, Any]]:
    if not raw:
        return []

    if isinstance(raw, str):
        try:
            raw = json.loads(raw)
        except Exception:
            return []

    if isinstance(raw, dict):
        for key in ("boundaries", "students", "start_pages", "start_indices"):
            if key in raw:
                raw = raw[key]
                break
        else:
            raw = []

    if isinstance(raw, list) and raw and all(not isinstance(x, (list, dict)) for x in raw):
        start_indices = _sanitize_pages(raw, total_pages)
        if 0 not in start_indices:
            start_indices.insert(0, 0)
        groups = []
        for idx, start in enumerate(start_indices):
            end = start_indices[idx + 1] - 1 if idx + 1 < len(start_indices) else total_pages - 1
            if end < start:
                continue
            groups.append(
                {
                    "pages": list(range(start, end + 1)),
                    "start_page": start,
                    "end_page": end,
                }
            )
        return groups

    if not isinstance(raw, list):
        return []

    groups = []
    for entry in raw:
        if isinstance(entry, list):
            pages = _sanitize_pages(entry, total_pages)
            if pages:
                groups.append({"pages": pages})
            continue
        if not isinstance(entry, dict):
            continue

        pages = entry.get("pages") or entry.get("page_indices") or entry.get("pageIndices")
        if pages is None:
            start = _first_not_none(
                entry.get("start_page"),
                entry.get("startPage"),
                entry.get("start"),
            )
            end = _first_not_none(
                entry.get("end_page"),
                entry.get("endPage"),
                entry.get("end"),
            )
            start_idx = _coerce_int(start) if start is not None else None
            end_idx = _coerce_int(end) if end is not None else None
            if start_idx is not None and end_idx is not None:
                pages = list(range(start_idx, end_idx + 1))

        pages = _sanitize_pages(pages, total_pages) if pages is not None else []
        if not pages:
            continue

        group = {"pages": pages}
        student_key = entry.get("student_key") or entry.get("studentKey")
        if student_key:
            group["student_key"] = str(student_key)
        student_id = entry.get("student_id") or entry.get("studentId")
        if student_id:
            group["student_id"] = str(student_id)
        student_name = entry.get("student_name") or entry.get("studentName") or entry.get("name")
        if student_name:
            group["student_name"] = str(student_name)
        class_name = entry.get("class_name") or entry.get("className")
        if class_name:
            group["class_name"] = str(class_name)
        groups.append(group)

    return groups


def _build_student_boundaries(
    state: BatchGradingGraphState, total_pages: int
) -> List[Dict[str, Any]]:
    inputs = state.get("inputs", {})
    manual_boundaries = _normalize_manual_boundaries(inputs.get("manual_boundaries"), total_pages)
    student_mapping = state.get("student_mapping") or inputs.get("student_mapping")
    student_boundaries: List[Dict[str, Any]] = []

    if student_mapping and isinstance(student_mapping, list):
        for idx, mapping in enumerate(student_mapping):
            pages = (
                mapping.get("pages") or mapping.get("page_indices") or mapping.get("pageIndices")
            )
            pages = _sanitize_pages(pages, total_pages) if pages is not None else []
            if not pages:
                start_idx = _first_not_none(
                    mapping.get("start_index"),
                    mapping.get("startIndex"),
                    mapping.get("start_page"),
                    mapping.get("startPage"),
                )
                end_idx = _first_not_none(
                    mapping.get("end_index"),
                    mapping.get("endIndex"),
                    mapping.get("end_page"),
                    mapping.get("endPage"),
                )
                start_page = _coerce_int(start_idx) if start_idx is not None else None
                end_page = _coerce_int(end_idx) if end_idx is not None else None
                if start_page is not None and end_page is not None:
                    pages = _sanitize_pages(list(range(start_page, end_page + 1)), total_pages)
            if not pages:
                continue

            student_name = mapping.get("student_name") or mapping.get("studentName")
            student_id = mapping.get("student_id") or mapping.get("studentId")
            student_key = (
                mapping.get("student_key")
                or mapping.get("studentKey")
                or student_name
                or student_id
                or f"å­¦ç”Ÿ{idx + 1}"
            )
            student_boundaries.append(
                {
                    "student_key": student_key,
                    "student_id": student_id,
                    "student_name": student_name,
                    "start_page": min(pages),
                    "end_page": max(pages),
                    "pages": sorted(pages),
                }
            )
    if not student_boundaries and manual_boundaries:
        for idx, boundary in enumerate(manual_boundaries):
            pages = boundary.get("pages") or boundary.get("page_indices") or boundary.get(
                "pageIndices"
            )
            pages = _sanitize_pages(pages, total_pages) if pages is not None else []
            if not pages:
                start_page = _first_not_none(
                    boundary.get("start_page"),
                    boundary.get("startPage"),
                    boundary.get("start"),
                )
                end_page = _first_not_none(
                    boundary.get("end_page"),
                    boundary.get("endPage"),
                    boundary.get("end"),
                )
                start_idx = _coerce_int(start_page) if start_page is not None else None
                end_idx = _coerce_int(end_page) if end_page is not None else None
                if start_idx is not None and end_idx is not None:
                    pages = _sanitize_pages(list(range(start_idx, end_idx + 1)), total_pages)
            if not pages:
                continue
            merged = dict(boundary)
            merged["pages"] = sorted(pages)
            merged.setdefault("start_page", pages[0])
            merged.setdefault("end_page", pages[-1])
            if "student_key" not in merged:
                merged["student_key"] = f"å­¦ç”Ÿ{idx + 1}"
            student_boundaries.append(merged)

    return student_boundaries


async def rubric_parse_node(state: BatchGradingGraphState) -> Dict[str, Any]:
    """
    è§£æè¯„åˆ†æ ‡å‡†èŠ‚ç‚¹

    ä½¿ç”¨ä¸“é—¨çš„ RubricParserService è§£æè¯„åˆ†æ ‡å‡†å›¾åƒï¼Œ
    æ”¯æŒåˆ†æ‰¹å¤„ç†å¤šé¡µè¯„åˆ†æ ‡å‡†ï¼Œæå–å®Œæ•´çš„é¢˜ç›®ç»“æ„å’Œè¯„åˆ†ç»†åˆ™ã€‚

    **å…³é”®**: è§£æåçš„è¯„åˆ†æ ‡å‡†ä¼šæ³¨å†Œåˆ° RubricRegistryï¼Œä¾›åç»­æ‰¹æ”¹æ—¶é€šè¿‡
    GradingSkills.get_rubric_for_question åŠ¨æ€è·å–æŒ‡å®šé¢˜ç›®çš„è¯„åˆ†æ ‡å‡†ã€‚
    """
    batch_id = state["batch_id"]
    rubric_images = state.get("rubric_images", [])
    rubric_text = state.get("rubric", "")
    api_key = state.get("api_key") or os.getenv("LLM_API_KEY") or os.getenv("OPENROUTER_API_KEY")

    logger.info(
        f"[rubric_parse] å¼€å§‹è§£æè¯„åˆ†æ ‡å‡†: batch_id={batch_id}, è¯„åˆ†æ ‡å‡†é¡µæ•°={len(rubric_images)}"
    )

    # ğŸ” è¯Šæ–­æ—¥å¿—ï¼šæ£€æŸ¥ rubric_images æ˜¯å¦ä¼ å…¥
    if rubric_images:
        logger.info(f"[rubric_parse] ğŸ“¸ rubric_images è¯¦æƒ…: å…± {len(rubric_images)} é¡µ")
        for i, img in enumerate(rubric_images):
            if isinstance(img, bytes):
                logger.info(f"[rubric_parse]   - ç¬¬ {i+1} é¡µ: {len(img)} bytes")
            else:
                logger.warning(f"[rubric_parse]   - ç¬¬ {i+1} é¡µ: ç±»å‹å¼‚å¸¸ {type(img)}")
    else:
        logger.warning(f"[rubric_parse] âš ï¸ rubric_images ä¸ºç©ºï¼è¯·æ£€æŸ¥å‰ç«¯æ˜¯å¦æ­£ç¡®ä¸Šä¼ äº†æ‰¹æ”¹æ ‡å‡†")

    parsed_rubric = {"total_questions": 0, "total_score": 0, "questions": []}

    # åˆ›å»º RubricRegistry ç”¨äºå­˜å‚¨è§£æåçš„è¯„åˆ†æ ‡å‡†
    from src.services.rubric_registry import RubricRegistry
    from src.models.grading_models import QuestionRubric, ScoringPoint, AlternativeSolution

    rubric_registry = RubricRegistry()

    try:
        if rubric_images and api_key:
            # ä½¿ç”¨ä¸“é—¨çš„ RubricParserService è¿›è¡Œåˆ†æ‰¹è§£æ
            from src.services.rubric_parser import RubricParserService

            parser = RubricParserService(api_key=api_key)

            # æµå¼è¾“å‡ºå›è°ƒ - å‘é€ llm_stream_chunk äº‹ä»¶åˆ°å‰ç«¯
            parse_agent_id = "rubric-parse"
            review_agent_id = "rubric-review"
            parse_agent_name = "Rubric Parse"
            review_agent_name = "Rubric Review"

            await _broadcast_progress(
                batch_id,
                {
                    "type": "agent_update",
                    "agentId": parse_agent_id,
                    "agentName": parse_agent_name,
                    "agentLabel": parse_agent_name,
                    "parentNodeId": "rubric_parse",
                    "status": "running",
                    "progress": 0,
                    "message": "Preparing rubric parse",
                },
            )

            async def stream_callback(stream_type: str, chunk: str) -> None:
                phase = "parse"
                real_type = stream_type

                parts = stream_type.split(":")
                if len(parts) >= 3:
                    phase = parts[1]
                    real_type = ":".join(parts[2:])
                elif len(parts) == 2:
                    real_type = parts[1]

                target_node = "rubric_parse"
                target_agent = parse_agent_id
                node_name = parse_agent_name

                if phase == "review":
                    target_node = "rubric_review"
                    target_agent = review_agent_id
                    node_name = review_agent_name

                await _broadcast_progress(
                    batch_id,
                    {
                        "type": "llm_stream_chunk",
                        "nodeId": target_node,
                        "agentId": target_agent,
                        "nodeName": node_name,
                        "streamType": real_type,
                        "chunk": chunk,
                    },
                )

            async def progress_callback(
                batch_index: int,
                total_batches: int,
                status: str,
                message: Optional[str],
            ) -> None:

                normalized_total = max(1, total_batches)
                batch_progress = int(((batch_index + 1) / normalized_total) * 100)
                is_last_batch = (batch_index + 1) >= normalized_total

                if status == "reviewing":
                    await _broadcast_progress(
                        batch_id,
                        {
                            "type": "agent_update",
                            "agentId": parse_agent_id,
                            "agentName": parse_agent_name,
                            "agentLabel": parse_agent_name,
                            "parentNodeId": "rubric_parse",
                            "status": "completed" if is_last_batch else "running",
                            "progress": 100 if is_last_batch else batch_progress,
                            "message": (
                                "Parsing completed"
                                if is_last_batch
                                else (message or f"Batch {batch_index + 1}/{total_batches}")
                            ),
                        },
                    )
                    await _broadcast_progress(
                        batch_id,
                        {
                            "type": "agent_update",
                            "agentId": review_agent_id,
                            "agentName": review_agent_name,
                            "agentLabel": review_agent_name,
                            "parentNodeId": "rubric_review",
                            "status": "running",
                            "progress": 0,
                            "message": message or "Reviewing...",
                        },
                    )
                    return

                if status == "completed":
                    await _broadcast_progress(
                        batch_id,
                        {
                            "type": "agent_update",
                            "agentId": parse_agent_id,
                            "agentName": parse_agent_name,
                            "agentLabel": parse_agent_name,
                            "parentNodeId": "rubric_parse",
                            "status": "completed",
                            "progress": 100,
                            "message": message or "Parsing completed",
                        },
                    )
                    return

                status_map = {
                    "parsing": "running",
                    "running": "running",
                    "failed": "failed",
                }
                progress = 100 if status == "failed" else batch_progress

                await _broadcast_progress(
                    batch_id,
                    {
                        "type": "agent_update",
                        "agentId": parse_agent_id,
                        "agentName": parse_agent_name,
                        "agentLabel": parse_agent_name,
                        "parentNodeId": "rubric_parse",
                        "status": status_map.get(status, "running"),
                        "progress": progress,
                        "message": message or f"Batch {batch_index + 1}/{total_batches}",
                    },
                )

            # æ·»åŠ è¶…æ—¶ä¿æŠ¤ï¼Œé»˜è®¤ 10 åˆ†é’Ÿï¼ˆè¯„åˆ†æ ‡å‡†å¯èƒ½å¾ˆé•¿ï¼‰
            rubric_parse_timeout = int(os.getenv("RUBRIC_PARSE_TIMEOUT", "600"))
            try:
                result = await asyncio.wait_for(
                    parser.parse_rubric(
                        rubric_images=rubric_images,
                        progress_callback=progress_callback,
                        stream_callback=stream_callback,
                    ),
                    timeout=rubric_parse_timeout,
                )
            except asyncio.TimeoutError:
                logger.error(f"[rubric_parse] è§£æè¶…æ—¶ï¼ˆ{rubric_parse_timeout}ç§’ï¼‰ï¼Œbatch_id={batch_id}")
                await _broadcast_progress(
                    batch_id,
                    {
                        "type": "workflow_error",
                        "error": f"è¯„åˆ†æ ‡å‡†è§£æè¶…æ—¶ï¼ˆ{rubric_parse_timeout}ç§’ï¼‰ï¼Œè¯·å°è¯•å‡å°‘è¯„åˆ†æ ‡å‡†é¡µæ•°æˆ–ç¨åé‡è¯•",
                        "stage": "rubric_parse",
                    },
                )
                raise Exception(f"Rubric parse timeout after {rubric_parse_timeout}s")

            # è½¬æ¢ä¸ºå­—å…¸æ ¼å¼
            parsed_rubric = {
                "total_questions": result.total_questions,
                "total_score": result.total_score,
                "rubric_format": result.rubric_format,
                "general_notes": result.general_notes,
                # LLM ç›´æ¥ç”Ÿæˆçš„è‡ªç™½ï¼ˆæçŸ­ï¼‰
                "confession": result.confession.to_dict() if hasattr(result.confession, 'to_dict') else {
                    "risks": getattr(result.confession, 'risks', []),
                    "uncertainties": getattr(result.confession, 'uncertainties', []),
                    "blindSpots": getattr(result.confession, 'blind_spots', []),
                    "needsReview": getattr(result.confession, 'needs_review', []),
                    "confidence": getattr(result.confession, 'confidence', 1.0),
                },
                "questions": [
                    {
                        "id": q.question_id,
                        "question_id": q.question_id,
                        "max_score": q.max_score,
                        "question_text": q.question_text,
                        "standard_answer": q.standard_answer,
                        "source_pages": getattr(q, "source_pages", []),
                        "criteria": [sp.description for sp in q.scoring_points],
                        # LLM ç›´æ¥ç”Ÿæˆçš„é¢˜ç›®çº§è‡ªç™½ï¼ˆæçŸ­ï¼‰
                        "confession": q.confession.to_dict() if hasattr(q.confession, 'to_dict') else {
                            "risk": getattr(q.confession, 'risk', ''),
                            "uncertainty": getattr(q.confession, 'uncertainty', ''),
                        },
                        "scoring_points": [
                            {
                                "point_id": sp.point_id or f"{q.question_id}.{idx + 1}",
                                "description": sp.description,
                                "score": sp.score,
                                "is_required": sp.is_required,
                                "keywords": sp.keywords or [],
                                "expected_value": sp.expected_value,
                            }
                            for idx, sp in enumerate(q.scoring_points)
                        ],
                        "alternative_solutions": [
                            {
                                "description": alt.description,
                                "scoring_criteria": alt.scoring_criteria,
                                "note": alt.note,
                            }
                            for alt in q.alternative_solutions
                        ],
                        "deduction_rules": [
                            {
                                "rule_id": dr.rule_id or f"{q.question_id}.d{idx + 1}",
                                "description": dr.description,
                                "deduction": dr.deduction,
                                "conditions": dr.conditions,
                            }
                            for idx, dr in enumerate(getattr(q, "deduction_rules", []) or [])
                        ],
                        "grading_notes": q.grading_notes,
                    }
                    for q in result.questions
                ],
            }

            # ğŸ”¥ å…³é”®ï¼šå°†è§£æçš„è¯„åˆ†æ ‡å‡†æ³¨å†Œåˆ° RubricRegistry
            # è¿™æ ·åç»­æ‰¹æ”¹æ—¶å¯ä»¥é€šè¿‡ GradingSkills.get_rubric_for_question è·å–
            rubric_registry.register_rubrics(result.questions)
            logger.info(f"[rubric_parse] å·²æ³¨å†Œ {len(result.questions)} é“é¢˜ç›®åˆ° RubricRegistry")

            # åŒæ—¶ç”Ÿæˆæ ¼å¼åŒ–çš„è¯„åˆ†æ ‡å‡†ä¸Šä¸‹æ–‡ï¼ˆä¾›æ‰¹æ”¹ä½¿ç”¨ï¼‰
            rubric_context = parser.format_rubric_context(result)
            parsed_rubric["rubric_context"] = rubric_context

            # ç”Ÿæˆè‡ªç™½æŠ¥å‘Š
            inputs_dict = state.get("inputs", {}) or {}
            expected_question_count = inputs_dict.get("expected_question_count")
            expected_total_score = inputs_dict.get("expected_total_score")

            parse_confession = parser._generate_parse_confession(
                rubric=result,
                expected_question_count=expected_question_count,
                expected_total_score=expected_total_score,
            )

            # å°†è‡ªç™½æŠ¥å‘Šæ·»åŠ åˆ° parsed_rubric
            parsed_rubric["overall_parse_confidence"] = parse_confession["overallConfidence"]
            parsed_rubric["parse_confession"] = parse_confession
            
            # ğŸ”§ é‡è¦ï¼šç”¨è®¡ç®—å‡ºçš„ç½®ä¿¡åº¦è¦†ç›– LLM è¿”å›çš„ç½®ä¿¡åº¦
            parsed_rubric["confession"]["confidence"] = parse_confession["overallConfidence"]

            # åŒæ—¶æ›´æ–° ParsedRubric å¯¹è±¡ï¼ˆå¦‚æœéœ€è¦é‡æ–°æ³¨å†Œï¼‰
            result.overall_parse_confidence = parse_confession["overallConfidence"]
            result.parse_confession = parse_confession

            logger.info(
                f"[rubric_parse] è¯„åˆ†æ ‡å‡†è§£ææˆåŠŸ: "
                f"é¢˜ç›®æ•°={result.total_questions}, æ€»åˆ†={result.total_score}, "
                f"ç½®ä¿¡åº¦={parse_confession['overallConfidence']:.2f}, "
                f"çŠ¶æ€={parse_confession['overallStatus']}"
            )
            
            # ğŸ” è¾“å‡ºå®Œæ•´çš„ AI è¿”å›ç»“æœ JSON (ä»…åœ¨ DEBUG æ¨¡å¼)
            if logger.isEnabledFor(logging.DEBUG):
                logger.debug(f"[rubric_parse] ğŸ“‹ AI è¿”å›çš„å®Œæ•´è¯„åˆ†æ ‡å‡† JSON:")
                logger.debug(f"[rubric_parse] {json.dumps(parsed_rubric, ensure_ascii=False, indent=2)}")
            else:
                # ç”Ÿäº§ç¯å¢ƒåªè¾“å‡ºé¢˜ç›®åˆ—è¡¨
                question_ids = [q.get('question_id', '?') for q in parsed_rubric.get('questions', [])]
                logger.info(f"[rubric_parse] é¢˜ç›®åˆ—è¡¨: {', '.join(question_ids)}")

        elif rubric_text:
            # å¦‚æœæœ‰æ–‡æœ¬å½¢å¼çš„è¯„åˆ†æ ‡å‡†ï¼Œç®€å•è§£æ
            parsed_rubric["raw_text"] = rubric_text

    except Exception as e:
        logger.error(f"[rubric_parse] Rubric parse failed: {e}", exc_info=True)
        try:
            await _broadcast_progress(
                batch_id,
                {
                    "type": "rubric_parse_failed",
                    "message": "Rubric parse failed. Please re-upload a clear rubric.",
                    "error": str(e),
                },
            )
        except Exception:
            logger.debug("[rubric_parse] Failed to broadcast parse error")
        raise

    logger.info(
        f"[rubric_parse] è¯„åˆ†æ ‡å‡†è§£æå®Œæˆ: batch_id={batch_id}, "
        f"é¢˜ç›®æ•°={parsed_rubric.get('total_questions', 0)}, "
        f"æ€»åˆ†={parsed_rubric.get('total_score', 0)}"
    )

    inputs_dict = state.get("inputs", {}) or {}
    expected_total_score = inputs_dict.get("expected_total_score")
    if expected_total_score is not None:
        try:
            expected_total_score = float(expected_total_score)
            parsed_total_score = float(parsed_rubric.get("total_score", 0) or 0)
            if parsed_total_score > 0 and parsed_total_score < expected_total_score:
                message = (
                    f"Parsed total score {parsed_total_score} is lower than "
                    f"expected {expected_total_score}."
                )
                await _broadcast_progress(
                    batch_id,
                    {
                        "type": "rubric_score_mismatch",
                        "expected_total_score": expected_total_score,
                        "parsed_total_score": parsed_total_score,
                        "message": message,
                    },
                )
                raise ValueError(message)
        except (TypeError, ValueError) as exc:
            logger.warning(f"[rubric_parse] Expected total score check skipped: {exc}")

    try:
        await _broadcast_progress(
            batch_id,
            {
                "type": "rubric_parsed",
                "totalQuestions": parsed_rubric.get("total_questions", 0),
                "totalScore": parsed_rubric.get("total_score", 0),
                "generalNotes": parsed_rubric.get("general_notes", ""),
                "rubricFormat": parsed_rubric.get("rubric_format", ""),
                "overallParseConfidence": parsed_rubric.get("overall_parse_confidence", 1.0),
                "parseConfession": parsed_rubric.get("parse_confession"),
                "questions": [
                    {
                        "questionId": q.get("question_id", ""),
                        "maxScore": q.get("max_score", 0),
                        "questionText": q.get("question_text", ""),
                        "standardAnswer": q.get("standard_answer", ""),
                        "gradingNotes": q.get("grading_notes", ""),
                        "sourcePages": q.get("source_pages") or q.get("sourcePages") or [],
                        "parseConfidence": q.get("parse_confidence", 1.0),
                        "parseUncertainties": q.get("parse_uncertainties")
                        or q.get("parseUncertainties")
                        or [],
                        "parseQualityIssues": q.get("parse_quality_issues")
                        or q.get("parseQualityIssues")
                        or [],
                        "scoringPoints": [
                            {
                                "pointId": sp.get("point_id")
                                or sp.get("pointId")
                                or f"{q.get('question_id')}.{idx + 1}",
                                "description": sp.get("description", ""),
                                "expectedValue": sp.get("expected_value")
                                or sp.get("expectedValue", ""),
                                "keywords": sp.get("keywords") or [],
                                "score": sp.get("score", 0),
                                "isRequired": sp.get("is_required", True),
                            }
                            for idx, sp in enumerate(q.get("scoring_points", []))
                        ],
                        "deductionRules": [
                            {
                                "ruleId": dr.get("rule_id")
                                or dr.get("ruleId")
                                or f"{q.get('question_id')}.d{idx + 1}",
                                "description": dr.get("description", ""),
                                "deduction": dr.get("deduction", dr.get("score", 0)),
                                "conditions": dr.get("conditions") or dr.get("when") or "",
                            }
                            for idx, dr in enumerate(
                                q.get("deduction_rules") or q.get("deductionRules") or []
                            )
                        ],
                        "alternativeSolutions": [
                            {
                                "description": alt.get("description", ""),
                                "scoringCriteria": alt.get("scoring_criteria", ""),
                                "note": alt.get("note", ""),
                            }
                            for alt in q.get("alternative_solutions", [])
                        ],
                    }
                    for q in parsed_rubric.get("questions", [])
                ],
            },
        )
    except Exception as exc:
        logger.warning(f"[rubric_parse] failed to emit rubric_parsed: {exc}")

    # æ³¨æ„ï¼šä¸åºåˆ—åŒ– RubricRegistryï¼Œå› ä¸º grade_batch_node ä¼šä» parsed_rubric é‡å»º
    # è¿™æ ·å¯ä»¥é¿å…ç±»å‹è½¬æ¢é—®é¢˜

    # ğŸ”§ ä¿®å¤ï¼šæ˜¾å¼ä¼ é€’å›¾ç‰‡æ•°æ®ï¼Œé˜²æ­¢åœ¨ state ä¼ é€’ä¸­ä¸¢å¤±ï¼ˆå¤§æ‰¹é‡å›¾ç‰‡åœºæ™¯ï¼‰
    result = {
        "parsed_rubric": parsed_rubric,
        "current_stage": "rubric_parse_completed",
        "percentage": 15.0,
        "timestamps": {
            **state.get("timestamps", {}),
            "rubric_parse_at": datetime.now().isoformat(),
        },
    }
    
    # ç¡®ä¿å›¾ç‰‡æ•°æ®ä¸ä¸¢å¤±
    if state.get("processed_images"):
        result["processed_images"] = state.get("processed_images")
    if state.get("answer_images"):
        result["answer_images"] = state.get("answer_images")
    if state.get("student_boundaries"):
        result["student_boundaries"] = state.get("student_boundaries")
    
    return result


def _preserve_images_in_result(state: BatchGradingGraphState, result: Dict[str, Any]) -> Dict[str, Any]:
    """ç¡®ä¿å›¾ç‰‡æ•°æ®åœ¨èŠ‚ç‚¹è¿”å›æ—¶ä¸ä¸¢å¤±ï¼ˆä¿®å¤å¤§æ‰¹é‡å›¾ç‰‡åœºæ™¯ï¼‰"""
    if state.get("processed_images"):
        result["processed_images"] = state.get("processed_images")
    if state.get("answer_images"):
        result["answer_images"] = state.get("answer_images")
    if state.get("student_boundaries"):
        result["student_boundaries"] = state.get("student_boundaries")
    return result


async def rubric_self_review_node(state: BatchGradingGraphState) -> Dict[str, Any]:
    """
    è¯„åˆ†æ ‡å‡†è‡ªåŠ¨å¤æ ¸èŠ‚ç‚¹ï¼ˆåŸºäºè‡ªç™½çš„ LLM å¤æ ¸ï¼‰
    
    åœ¨äººå·¥å¤æ ¸ä¹‹å‰ï¼ŒåŸºäº LLM ç”Ÿæˆçš„ confessionï¼ˆè‡ªç™½ï¼‰å’ŒåŸå›¾ï¼Œ
    è‡ªåŠ¨è°ƒç”¨ LLM å¤æ ¸å¹¶ä¿®æ­£è§£æç»“æœä¸­çš„é£é™©ç‚¹å’Œä¸ç¡®å®šé¡¹ã€‚
    
    è§¦å‘æ¡ä»¶ï¼š
    - confession ä¸­æœ‰ needs_review é¡¹
    - confession ä¸­æœ‰ risks æˆ– uncertainties
    - æ•´ä½“ç½®ä¿¡åº¦ < 0.9
    """
    batch_id = state["batch_id"]
    parsed_rubric = state.get("parsed_rubric", {})
    rubric_images = state.get("rubric_images", [])
    api_key = state.get("api_key") or os.getenv("LLM_API_KEY") or os.getenv("OPENROUTER_API_KEY")
    
    # è·å– confession
    confession = parsed_rubric.get("confession", {})
    overall_confidence = confession.get("confidence", 1.0)
    needs_review = confession.get("needsReview") or confession.get("needs_review") or []
    risks = confession.get("risks", [])
    uncertainties = confession.get("uncertainties", [])
    blind_spots = confession.get("blindSpots") or confession.get("blind_spots") or []
    
    # åˆ¤æ–­æ˜¯å¦éœ€è¦è‡ªåŠ¨å¤æ ¸
    should_self_review = (
        needs_review or 
        (risks and len(risks) > 0) or 
        (uncertainties and len(uncertainties) > 0) or
        overall_confidence < 0.9
    )
    
    if not should_self_review:
        logger.info(f"[rubric_self_review] skip (no issues): batch_id={batch_id}, confidence={overall_confidence}")
        return _preserve_images_in_result(state, {
            "current_stage": "rubric_self_review_skipped",
            "percentage": 16.0,
            "timestamps": {
                **state.get("timestamps", {}),
                "rubric_self_review_at": datetime.now().isoformat(),
            },
        })
    
    if not api_key or not rubric_images:
        logger.info(f"[rubric_self_review] skip (no api_key or images): batch_id={batch_id}")
        return _preserve_images_in_result(state, {
            "current_stage": "rubric_self_review_skipped",
            "percentage": 16.0,
            "timestamps": {
                **state.get("timestamps", {}),
                "rubric_self_review_at": datetime.now().isoformat(),
            },
        })
    
    logger.info(
        f"[rubric_self_review] å¼€å§‹è‡ªåŠ¨å¤æ ¸: batch_id={batch_id}, "
        f"risks={len(risks)}, uncertainties={len(uncertainties)}, "
        f"needs_review={len(needs_review)}, confidence={overall_confidence}"
    )
    
    # å¹¿æ’­è¿›åº¦
    await _broadcast_progress(
        batch_id,
        {
            "type": "agent_update",
            "agentId": "rubric-self-review",
            "agentName": "Rubric Self Review",
            "agentLabel": "Rubric Self Review",
            "parentNodeId": "rubric_self_review",
            "status": "running",
            "progress": 0,
            "message": "å¼€å§‹è‡ªåŠ¨å¤æ ¸è§£æç»“æœ...",
        },
    )
    
    try:
        from src.services.llm_reasoning import LLMReasoningClient
        
        client = LLMReasoningClient(api_key=api_key)
        
        # æ„å»ºå¤æ ¸æç¤ºè¯
        review_prompt = _build_self_review_prompt(parsed_rubric, confession)
        
        # æµå¼å›è°ƒ
        async def stream_callback(stream_type: str, chunk: str) -> None:
            await _broadcast_progress(
                batch_id,
                {
                    "type": "llm_stream_chunk",
                    "nodeId": "rubric_self_review",
                    "agentId": "rubric-self-review",
                    "nodeName": "Rubric Self Review",
                    "streamType": stream_type,
                    "chunk": chunk,
                },
            )
        
        # è°ƒç”¨ LLM è¿›è¡Œå¤æ ¸
        response = await client.analyze_with_vision(
            images=rubric_images,
            prompt=review_prompt,
            stream_callback=stream_callback,
        )
        
        result_text = response.get("response", "")
        
        # è§£æå¤æ ¸ç»“æœ
        updated_rubric = _parse_self_review_result(result_text, parsed_rubric)
        
        # æ›´æ–° confession çŠ¶æ€
        if updated_rubric.get("confession"):
            updated_rubric["confession"]["self_reviewed"] = True
            updated_rubric["confession"]["self_review_applied"] = True
        
        logger.info(f"[rubric_self_review] è‡ªåŠ¨å¤æ ¸å®Œæˆ: batch_id={batch_id}")
        
        await _broadcast_progress(
            batch_id,
            {
                "type": "agent_update",
                "agentId": "rubric-self-review",
                "agentName": "Rubric Self Review",
                "agentLabel": "Rubric Self Review",
                "parentNodeId": "rubric_self_review",
                "status": "completed",
                "progress": 100,
                "message": "è‡ªåŠ¨å¤æ ¸å®Œæˆ",
            },
        )
        
        # å¹¿æ’­å¤æ ¸ç»“æœ
        await _broadcast_progress(
            batch_id,
            {
                "type": "rubric_self_reviewed",
                "batch_id": batch_id,
                "changes_made": updated_rubric.get("self_review_changes", []),
                "confidence_before": overall_confidence,
                "confidence_after": updated_rubric.get("confession", {}).get("confidence", overall_confidence),
            },
        )
        
        return _preserve_images_in_result(state, {
            "parsed_rubric": updated_rubric,
            "current_stage": "rubric_self_review_completed",
            "percentage": 17.0,
            "timestamps": {
                **state.get("timestamps", {}),
                "rubric_self_review_at": datetime.now().isoformat(),
            },
        })
        
    except Exception as e:
        logger.warning(f"[rubric_self_review] è‡ªåŠ¨å¤æ ¸å¤±è´¥: {e}", exc_info=True)
        await _broadcast_progress(
            batch_id,
            {
                "type": "agent_update",
                "agentId": "rubric-self-review",
                "agentName": "Rubric Self Review",
                "agentLabel": "Rubric Self Review",
                "parentNodeId": "rubric_self_review",
                "status": "failed",
                "progress": 100,
                "message": f"è‡ªåŠ¨å¤æ ¸å¤±è´¥: {str(e)}",
            },
        )
        # å¤±è´¥æ—¶ä¸é˜»å¡æµç¨‹ï¼Œç»§ç»­ä½¿ç”¨åŸæœ‰çš„ parsed_rubric
        return _preserve_images_in_result(state, {
            "current_stage": "rubric_self_review_failed",
            "percentage": 17.0,
            "timestamps": {
                **state.get("timestamps", {}),
                "rubric_self_review_at": datetime.now().isoformat(),
            },
        })


def _build_self_review_prompt(parsed_rubric: Dict[str, Any], confession: Dict[str, Any]) -> str:
    """æ„å»ºè‡ªåŠ¨å¤æ ¸çš„æç¤ºè¯"""
    risks = confession.get("risks", [])
    uncertainties = confession.get("uncertainties", [])
    blind_spots = confession.get("blindSpots") or confession.get("blind_spots") or []
    needs_review = confession.get("needsReview") or confession.get("needs_review") or []
    
    # æ”¶é›†é¢˜ç›®çº§åˆ«çš„é£é™©
    question_issues = []
    for q in parsed_rubric.get("questions", []):
        q_conf = q.get("confession", {})
        if q_conf.get("risk") or q_conf.get("uncertainty"):
            question_issues.append({
                "question_id": q.get("question_id"),
                "risk": q_conf.get("risk", ""),
                "uncertainty": q_conf.get("uncertainty", ""),
            })
    
    prompt = f"""ä½ æ˜¯ä¸€ä½ä¸“ä¸šçš„è¯„åˆ†æ ‡å‡†å¤æ ¸ä¸“å®¶ã€‚è¯·æ ¹æ®ä»¥ä¸‹è‡ªç™½ï¼ˆconfessionï¼‰ä¿¡æ¯ï¼Œé‡æ–°å®¡è§†åŸå›¾å¹¶ä¿®æ­£è§£æç»“æœã€‚

## å½“å‰è§£æç»“æœæ‘˜è¦
- æ€»é¢˜æ•°: {parsed_rubric.get('total_questions', 0)}
- æ€»åˆ†: {parsed_rubric.get('total_score', 0)}
- ç½®ä¿¡åº¦: {confession.get('confidence', 1.0):.2f}

## è‡ªç™½ä¿¡æ¯ï¼ˆéœ€è¦å¤æ ¸çš„é—®é¢˜ï¼‰
"""
    
    if risks:
        prompt += f"\n### é£é™©ç‚¹\n"
        for r in risks:
            prompt += f"- {r}\n"
    
    if uncertainties:
        prompt += f"\n### ä¸ç¡®å®šç‚¹\n"
        for u in uncertainties:
            prompt += f"- {u}\n"
    
    if blind_spots:
        prompt += f"\n### å¯èƒ½é—æ¼\n"
        for b in blind_spots:
            prompt += f"- {b}\n"
    
    if needs_review:
        prompt += f"\n### å»ºè®®å¤æ ¸\n"
        for n in needs_review:
            prompt += f"- {n}\n"
    
    if question_issues:
        prompt += f"\n### é¢˜ç›®çº§é—®é¢˜\n"
        for qi in question_issues:
            prompt += f"- Q{qi['question_id']}: "
            if qi['risk']:
                prompt += f"é£é™©={qi['risk']} "
            if qi['uncertainty']:
                prompt += f"ä¸ç¡®å®š={qi['uncertainty']}"
            prompt += "\n"
    
    prompt += """
## å½“å‰è§£æçš„é¢˜ç›®ç»“æ„
"""
    for q in parsed_rubric.get("questions", []):
        prompt += f"\n### é¢˜ç›® {q.get('question_id')} (æ»¡åˆ† {q.get('max_score')} åˆ†)\n"
        prompt += f"å¾—åˆ†ç‚¹: {len(q.get('scoring_points', []))} ä¸ª\n"
        for sp in q.get("scoring_points", []):
            prompt += f"  - [{sp.get('point_id')}] {sp.get('description')} ({sp.get('score')}åˆ†)\n"
    
    prompt += """
## ä»»åŠ¡
è¯·ä»”ç»†æŸ¥çœ‹åŸå›¾ï¼Œé’ˆå¯¹ä¸Šè¿°è‡ªç™½ä¸­çš„é—®é¢˜è¿›è¡Œå¤æ ¸ï¼š
1. ç¡®è®¤æˆ–ä¿®æ­£é¢˜ç›®æ•°é‡
2. ç¡®è®¤æˆ–ä¿®æ­£å„é¢˜åˆ†å€¼
3. ç¡®è®¤æˆ–ä¿®æ­£å¾—åˆ†ç‚¹
4. è¡¥å……å¯èƒ½é—æ¼çš„å†…å®¹

## è¾“å‡ºæ ¼å¼ï¼ˆä»…è¿”å› JSONï¼Œä¸è¦ markdown ä»£ç å—ï¼‰
å¦‚æœéœ€è¦ä¿®æ­£ï¼Œè¿”å›ï¼š
{{
  "has_changes": true,
  "changes": ["ä¿®æ­£1çš„æè¿°", "ä¿®æ­£2çš„æè¿°"],
  "updated_confidence": 0.95,
  "corrections": [
    {{
      "question_id": "ä¿®æ­£çš„é¢˜ç›®ID",
      "field": "max_score|scoring_points|standard_answer",
      "old_value": "åŸå€¼",
      "new_value": "æ–°å€¼",
      "reason": "ä¿®æ­£åŸå› "
    }}
  ]
}}

å¦‚æœç¡®è®¤æ— éœ€ä¿®æ­£ï¼Œè¿”å›ï¼š
{{
  "has_changes": false,
  "confirmation": "ç¡®è®¤å½“å‰è§£ææ­£ç¡®",
  "updated_confidence": 0.95
}}

æ³¨æ„ï¼š
- åªä¿®æ­£æœ‰æ˜ç¡®é—®é¢˜çš„å†…å®¹
- ä¸è¦è¿‡åº¦ä¿®æ­£æˆ–çŒœæµ‹
- ä¿®æ­£åçš„ç½®ä¿¡åº¦åº”è¯¥æé«˜"""
    
    return prompt


def _parse_self_review_result(result_text: str, original_rubric: Dict[str, Any]) -> Dict[str, Any]:
    """è§£æè‡ªåŠ¨å¤æ ¸ç»“æœ"""
    import json
    
    updated_rubric = copy.deepcopy(original_rubric)
    
    try:
        # æå– JSON
        json_text = result_text
        if "```json" in result_text:
            json_start = result_text.find("```json") + 7
            json_end = result_text.find("```", json_start)
            if json_end > json_start:
                json_text = result_text[json_start:json_end].strip()
        elif "```" in result_text:
            json_start = result_text.find("```") + 3
            json_end = result_text.find("```", json_start)
            if json_end > json_start:
                json_text = result_text[json_start:json_end].strip()
        
        if not json_text.startswith("{"):
            brace_start = json_text.find("{")
            if brace_start >= 0:
                json_text = json_text[brace_start:]
        
        data = json.loads(json_text)
        
        if data.get("has_changes"):
            # è®°å½•ä¿®æ­£
            updated_rubric["self_review_changes"] = data.get("changes", [])
            
            # åº”ç”¨ä¿®æ­£
            corrections = data.get("corrections", [])
            for correction in corrections:
                qid = correction.get("question_id")
                field = correction.get("field")
                new_value = correction.get("new_value")
                
                if not qid or not field or new_value is None:
                    continue
                
                # æ‰¾åˆ°å¯¹åº”é¢˜ç›®
                for q in updated_rubric.get("questions", []):
                    if q.get("question_id") == qid or q.get("id") == qid:
                        if field == "max_score":
                            try:
                                q["max_score"] = float(new_value)
                            except (ValueError, TypeError):
                                pass
                        elif field == "standard_answer":
                            q["standard_answer"] = str(new_value)
                        elif field == "scoring_points" and isinstance(new_value, list):
                            # æ›´æ–°å¾—åˆ†ç‚¹ï¼ˆæ›´å¤æ‚çš„é€»è¾‘ï¼‰
                            q["scoring_points"] = new_value
                        break
            
            # æ›´æ–°ç½®ä¿¡åº¦
            if "updated_confidence" in data:
                if "confession" not in updated_rubric:
                    updated_rubric["confession"] = {}
                updated_rubric["confession"]["confidence"] = float(data["updated_confidence"])
            
            # é‡æ–°è®¡ç®—æ€»åˆ†
            updated_rubric["total_score"] = sum(
                q.get("max_score", 0) for q in updated_rubric.get("questions", [])
            )
            
            logger.info(f"[rubric_self_review] åº”ç”¨äº† {len(corrections)} ä¸ªä¿®æ­£")
        else:
            # æ— ä¿®æ­£ï¼Œä½†å¯èƒ½æ›´æ–°ç½®ä¿¡åº¦
            updated_rubric["self_review_changes"] = []
            if "updated_confidence" in data:
                if "confession" not in updated_rubric:
                    updated_rubric["confession"] = {}
                updated_rubric["confession"]["confidence"] = float(data["updated_confidence"])
            logger.info("[rubric_self_review] ç¡®è®¤è§£ææ­£ç¡®ï¼Œæ— éœ€ä¿®æ­£")
            
    except json.JSONDecodeError as e:
        logger.warning(f"[rubric_self_review] æ— æ³•è§£æå¤æ ¸ç»“æœ JSON: {e}")
        updated_rubric["self_review_changes"] = []
    except Exception as e:
        logger.warning(f"[rubric_self_review] å¤„ç†å¤æ ¸ç»“æœå¤±è´¥: {e}")
        updated_rubric["self_review_changes"] = []
    
    return updated_rubric


async def rubric_review_node(state: BatchGradingGraphState) -> Dict[str, Any]:
    """
    Rubric review node with interrupt.
    """
    batch_id = state["batch_id"]
    parsed_rubric = state.get("parsed_rubric", {})
    api_key = state.get("api_key") or os.getenv("LLM_API_KEY") or os.getenv("OPENROUTER_API_KEY")
    enable_review = state.get("inputs", {}).get("enable_review", True)
    grading_mode = _resolve_grading_mode(state.get("inputs", {}), parsed_rubric)

    if grading_mode.startswith("assist"):
        logger.info(f"[rubric_review] skip (assist mode): batch_id={batch_id}")
        return _preserve_images_in_result(state, {
            "current_stage": "rubric_review_skipped",
            "percentage": 18.0,
            "timestamps": {
                **state.get("timestamps", {}),
                "rubric_review_at": datetime.now().isoformat(),
            },
        })

    if not parsed_rubric or not parsed_rubric.get("questions"):
        logger.info(f"[rubric_review] skip (no rubric): batch_id={batch_id}")
        return _preserve_images_in_result(state, {
            "current_stage": "rubric_review_skipped",
            "percentage": 18.0,
            "timestamps": {
                **state.get("timestamps", {}),
                "rubric_review_at": datetime.now().isoformat(),
            },
        })

    if not enable_review:
        logger.info(f"[rubric_review] skip (review disabled): batch_id={batch_id}")
        return _preserve_images_in_result(state, {
            "current_stage": "rubric_review_skipped",
            "percentage": 18.0,
            "timestamps": {
                **state.get("timestamps", {}),
                "rubric_review_at": datetime.now().isoformat(),
            },
        })

    review_request = {
        "type": "rubric_review_required",
        "batch_id": batch_id,
        "message": "Rubric review required",
        "requested_at": datetime.now().isoformat(),
        "parsed_rubric": parsed_rubric,
    }
    review_response = interrupt(review_request)

    action = (review_response or {}).get("action", "approve").lower()
    updated_rubric = parsed_rubric

    if action in ("update", "override"):
        updated_payload = (review_response or {}).get("parsed_rubric") or {}
        updated_rubric = _normalize_parsed_rubric_input(updated_payload, parsed_rubric)
    elif action == "reparse":
        selected_ids = (review_response or {}).get("selected_question_ids") or []
        notes = (review_response or {}).get("notes") or ""
        if selected_ids and api_key:
            try:
                from src.services.rubric_parser import RubricParserService

                parser = RubricParserService(api_key=api_key)
                selected_questions = [
                    q
                    for q in parsed_rubric.get("questions", [])
                    if q.get("question_id") in selected_ids or q.get("id") in selected_ids
                ]
                revised = await parser.revise_questions(selected_questions, notes=notes)
                revised_map = {
                    (q.get("question_id") or q.get("id")): q for q in revised if isinstance(q, dict)
                }
                updated_questions = []
                for q in parsed_rubric.get("questions", []):
                    qid = q.get("question_id") or q.get("id")
                    if qid in revised_map:
                        normalized = _normalize_parsed_rubric_input(
                            {
                                "questions": [revised_map[qid]],
                            },
                            parsed_rubric,
                        )
                        if normalized.get("questions"):
                            updated_questions.append(normalized["questions"][0])
                            continue
                    updated_questions.append(q)
                updated_rubric = {
                    **parsed_rubric,
                    "questions": updated_questions,
                }
            except Exception as exc:
                logger.warning(f"[rubric_review] reparse failed: {exc}", exc_info=True)

    if updated_rubric.get("questions"):
        updated_rubric["total_questions"] = len(updated_rubric["questions"])
        updated_rubric["total_score"] = sum(
            q.get("max_score", 0) for q in updated_rubric["questions"]
        )
        updated_rubric["rubric_context"] = _format_rubric_context_from_dict(updated_rubric)

    return _preserve_images_in_result(state, {
        "parsed_rubric": updated_rubric,
        "rubric_review_result": review_response,
        "current_stage": "rubric_review_completed",
        "percentage": 20.0,
        "timestamps": {
            **state.get("timestamps", {}),
            "rubric_review_at": datetime.now().isoformat(),
        },
    })


def grading_fanout_router(state: BatchGradingGraphState) -> List[Send]:
    """
    æ‰¹æ”¹æ‰‡å‡ºè·¯ç”±

    å°†æ‰€æœ‰é¡µé¢åˆ†æ‰¹ï¼Œæ¯æ‰¹å¹¶è¡Œæ‰¹æ”¹ã€‚
    ä¸é¢„å…ˆåˆ†å‰²å­¦ç”Ÿï¼Œè€Œæ˜¯æ‰¹æ”¹æ‰€æœ‰é¡µé¢ã€‚
    æ”¯æŒå¯é…ç½®çš„æ‰¹æ¬¡å¤§å°ã€‚

    **å…³é”®**: ä½¿ç”¨æ·±æ‹·è´ç¡®ä¿ Worker ä¹‹é—´ä¸å…±äº«å¯å˜çŠ¶æ€ (Requirement 3.2)

    Requirements: 3.1, 3.2, 10.1
    """
    import copy

    batch_id = state["batch_id"]
    inputs = state.get("inputs", {})
    rubric = state.get("rubric", "")
    parsed_rubric = state.get("parsed_rubric", {})
    api_key = state.get("api_key", "")
    student_boundaries = state.get("student_boundaries")
    
    # ğŸ”§ ä¿®å¤ï¼šä»å¤šä¸ªæ¥æºè·å–å›¾ç‰‡ï¼Œå¹¶æ·»åŠ è¯¦ç»†æ—¥å¿—è¯Šæ–­å¤§æ‰¹é‡å›¾ç‰‡ä¸¢å¤±é—®é¢˜
    processed_images = state.get("processed_images") or []
    answer_images = state.get("answer_images") or []
    
    # ä¼˜å…ˆä½¿ç”¨ processed_imagesï¼ˆå·²é¢„å¤„ç†ï¼‰ï¼Œfallback åˆ° answer_imagesï¼ˆåŸå§‹ï¼‰
    images_to_use = processed_images if processed_images else answer_images
    
    logger.info(
        f"[grading_fanout] å›¾ç‰‡æ¥æºè¯Šæ–­: batch_id={batch_id}, "
        f"processed_images={len(processed_images)}, answer_images={len(answer_images)}, "
        f"state_keys={list(state.keys())}"
    )
    
    if not student_boundaries:
        student_boundaries = _build_student_boundaries(state, len(images_to_use))
        if student_boundaries:
            logger.info(f"[grading_fanout] ç”Ÿæˆ {len(student_boundaries)} ä¸ªå­¦ç”Ÿè¾¹ç•Œ")

    if not images_to_use:
        logger.warning(f"[grading_fanout] âš ï¸ state ä¸­æ²¡æœ‰å›¾ç‰‡ï¼Œå°è¯•æ¢å¤: batch_id={batch_id}")
        logger.debug(f"[grading_fanout] ğŸ” è¯Šæ–­: state keys={sorted(list(state.keys()))}")
        logger.debug(f"[grading_fanout] ğŸ” inputs keys={sorted(list(inputs.keys())) if inputs else 'None'}")
        
        # 1. å…ˆå°è¯•ä» inputs ä¸­æ¢å¤
        input_answer_images = inputs.get("answer_images") or []
        if input_answer_images:
            logger.info(f"[grading_fanout] âœ… ä» inputs æ¢å¤ {len(input_answer_images)} å¼ å›¾ç‰‡")
            images_to_use = input_answer_images
        else:
            # 2. å°è¯•ä» PostgreSQL è¯»å–ï¼ˆæœ€åä¸€é“é˜²çº¿ï¼‰
            pg_reader = _get_pg_image_reader()
            if pg_reader:
                try:
                    # åœ¨åŒæ­¥ä¸Šä¸‹æ–‡ä¸­è¿è¡Œå¼‚æ­¥å‡½æ•°
                    loop = asyncio.get_event_loop()
                    if loop.is_running():
                        # å¦‚æœå·²æœ‰äº‹ä»¶å¾ªç¯ï¼Œåˆ›å»ºæ–°ä»»åŠ¡
                        import concurrent.futures
                        with concurrent.futures.ThreadPoolExecutor() as executor:
                            future = executor.submit(asyncio.run, pg_reader(batch_id, "answer"))
                            pg_images = future.result(timeout=60)
                    else:
                        pg_images = loop.run_until_complete(pg_reader(batch_id, "answer"))
                    
                    if pg_images:
                        logger.info(f"[grading_fanout] âœ… ä» PostgreSQL æ¢å¤ {len(pg_images)} å¼ å›¾ç‰‡")
                        # è½¬æ¢ä¸º base64 æ ¼å¼ï¼ˆä¸ answer_images æ ¼å¼ä¸€è‡´ï¼‰
                        import base64
                        images_to_use = [
                            f"data:image/jpeg;base64,{base64.b64encode(img).decode('utf-8')}"
                            for img in pg_images
                        ]
                except Exception as e:
                    logger.error(f"[grading_fanout] âŒ PostgreSQL è¯»å–å›¾ç‰‡å¤±è´¥: {e}")
            
            if not images_to_use:
                logger.error(f"[grading_fanout] âŒ æ— æ³•æ¢å¤å›¾ç‰‡ï¼Œè·³è¿‡æ‰¹æ”¹ç›´æ¥è¿›å…¥ confession")
                return [Send("logic_review", state)]
    
    # æ›´æ–°å˜é‡åä»¥ä¿æŒåç»­ä»£ç å…¼å®¹
    processed_images = images_to_use

    # ä¸å†ä» page_index_contexts æ¨å¯¼ student_boundaries
    # å¦‚æœå‰ç«¯æ²¡æœ‰æä¾› student_mappingï¼Œåˆ™æŒ‰æ‰¹æ¬¡å¤§å°åˆ†é…

    # è·å–æ‰¹æ¬¡é…ç½® (Requirements: 3.1, 10.1)
    config = get_batch_config()
    max_retries = config.max_retries
    total_pages = len(processed_images)

    # ğŸ”¥ ä¼˜å…ˆæŒ‰å­¦ç”Ÿè¾¹ç•ŒåŠ¨æ€åˆ†é…æ‰¹æ¬¡
    if student_boundaries and len(student_boundaries) > 0:
        num_batches = len(student_boundaries)
        logger.info(
            f"[grading_fanout] æŒ‰å­¦ç”Ÿè¾¹ç•Œåˆ›å»ºæ‰¹æ”¹ä»»åŠ¡: batch_id={batch_id}, "
            f"å­¦ç”Ÿæ•°={num_batches}, æ€»é¡µæ•°={total_pages}"
        )

        sends = []
        for batch_idx, boundary in enumerate(student_boundaries):
            student_key = boundary.get("student_key", f"student_{batch_idx}")
            student_name = boundary.get("student_name")
            student_id = boundary.get("student_id")
            pages = boundary.get("pages")
            if pages:
                page_indices = sorted(list(pages))
            else:
                start_page = boundary.get("start_page", 0)
                end_page = boundary.get("end_page", total_pages - 1)
                page_indices = list(range(start_page, end_page + 1))
            if page_indices:
                start_page = page_indices[0]
                end_page = page_indices[-1]
            else:
                start_page = 0
                end_page = 0

            batch_images = [processed_images[i] for i in page_indices if i < len(processed_images)]

            if not batch_images:
                logger.warning(f"[grading_fanout] å­¦ç”Ÿ {student_key} æ²¡æœ‰å›¾åƒï¼Œè·³è¿‡")
                continue

            task_state = {
                "batch_id": batch_id,
                "batch_index": batch_idx,
                "total_batches": num_batches,
                "student_key": student_key,
                "student_name": student_name,
                "student_id": student_id,
                "page_indices": page_indices,
                "images": batch_images,
                "rubric": rubric,
                "parsed_rubric": copy.deepcopy(parsed_rubric),
                "api_key": api_key,
                "retry_count": 0,
                "max_retries": max_retries,
                "inputs": copy.deepcopy(inputs),
            }

            sends.append(Send("grade_batch", task_state))
            logger.info(
                f"[grading_fanout] åˆ›å»ºå­¦ç”Ÿæ‰¹æ¬¡: student={student_key}, pages={start_page}-{end_page}"
            )

        if sends:
            logger.info(f"[grading_fanout] âœ… æˆåŠŸåˆ›å»º {len(sends)} ä¸ªå­¦ç”Ÿæ‰¹æ”¹ä»»åŠ¡")
            return sends
        logger.warning(f"[grading_fanout] âš ï¸ æ²¡æœ‰æœ‰æ•ˆçš„å­¦ç”Ÿæ‰¹æ¬¡")
        logger.warning(f"[grading_fanout] ğŸ” student_boundaries={student_boundaries}")

    # å›é€€ï¼šæŒ‰å›ºå®šæ‰¹æ¬¡å¤§å°åˆ†é…
    batch_size = config.batch_size
    if batch_size <= 0:
        batch_size = max(1, total_pages)
    num_batches = (total_pages + batch_size - 1) // batch_size

    logger.info(
        f"[grading_fanout] åˆ›å»ºæ‰¹æ”¹ä»»åŠ¡: batch_id={batch_id}, "
        f"æ€»é¡µæ•°={total_pages}, æ‰¹æ¬¡æ•°={num_batches}, "
        f"æ‰¹æ¬¡å¤§å°={batch_size}, æœ€å¤§é‡è¯•={max_retries}"
    )

    sends = []
    for batch_idx in range(num_batches):
        start_idx = batch_idx * batch_size
        end_idx = min(start_idx + batch_size, total_pages)
        batch_images = processed_images[start_idx:end_idx]

        # ğŸ”§ ä¿®å¤ï¼šä¸ºå›é€€é€»è¾‘æ·»åŠ é»˜è®¤ student_keyï¼ˆä¿®å¤ total_students=0 é—®é¢˜ï¼‰
        # å½“æ²¡æœ‰ student_mapping æ—¶ï¼Œåˆ›å»ºä¸€ä¸ªé»˜è®¤å­¦ç”Ÿè¦†ç›–æ‰€æœ‰é¡µé¢
        if num_batches == 1:
            # åªæœ‰ä¸€ä¸ªæ‰¹æ¬¡ï¼Œè§†ä¸ºå•ä¸ªå­¦ç”Ÿ
            default_student_key = "å­¦ç”Ÿ1"
        else:
            # å¤šä¸ªæ‰¹æ¬¡ï¼Œä¸ºæ¯ä¸ªæ‰¹æ¬¡åˆ†é…ä¸€ä¸ªå­¦ç”Ÿç¼–å·
            default_student_key = f"å­¦ç”Ÿ{batch_idx + 1}"

        task_state = {
            "batch_id": batch_id,
            "batch_index": batch_idx,
            "total_batches": num_batches,
            "student_key": default_student_key,  # âœ… æ·»åŠ  student_key
            "page_indices": list(range(start_idx, end_idx)),
            "images": batch_images,
            "rubric": rubric,
            "parsed_rubric": copy.deepcopy(parsed_rubric),
            "api_key": api_key,
            "retry_count": 0,
            "max_retries": max_retries,
            "inputs": copy.deepcopy(inputs),
        }

        logger.info(
            f"[grading_fanout] å›é€€æ‰¹æ¬¡: batch={batch_idx+1}/{num_batches}, "
            f"student_key={default_student_key}, pages={start_idx}-{end_idx-1}"
        )

        sends.append(Send("grade_batch", task_state))

    return sends


def _normalize_question_id(value: Any) -> str:
    if value is None:
        return ""
    text = str(value).strip()
    if not text:
        return ""
    for token in ["ç¬¬", "é¢˜ç›®", "é¢˜", "Q", "q"]:
        text = text.replace(token, "")
    return text.strip().rstrip(".:ï¼š")


def _normalize_logic_review_items(raw_items: Any) -> List[Dict[str, Any]]:
    """
    æ ‡å‡†åŒ–é€»è¾‘å¤æ ¸è¿”å›çš„é¢˜ç›®åˆ—è¡¨ã€‚
    
    å¤„ç†å„ç§å¯èƒ½çš„å­—æ®µåå˜ä½“ï¼ˆé©¼å³°/ä¸‹åˆ’çº¿ï¼‰å’Œæ•°æ®ç»“æ„ã€‚
    
    Args:
        raw_items: åŸå§‹çš„é¢˜ç›®å¤æ ¸æ•°æ®ï¼ˆå¯èƒ½æ˜¯åˆ—è¡¨æˆ–å…¶ä»–æ ¼å¼ï¼‰
    
    Returns:
        æ ‡å‡†åŒ–åçš„é¢˜ç›®å­—å…¸åˆ—è¡¨
    """
    if not raw_items:
        return []
    
    # å¦‚æœä¸æ˜¯åˆ—è¡¨ï¼Œå°è¯•è½¬æ¢
    if not isinstance(raw_items, list):
        if isinstance(raw_items, dict):
            # å¯èƒ½æ˜¯å•ä¸ªé¢˜ç›®ï¼ŒåŒ…è£…æˆåˆ—è¡¨
            raw_items = [raw_items]
        else:
            return []
    
    normalized = []
    for item in raw_items:
        if not isinstance(item, dict):
            continue
        
        # æ ‡å‡†åŒ–å­—æ®µåï¼ˆç»Ÿä¸€ä½¿ç”¨ä¸‹åˆ’çº¿å‘½åï¼‰
        normalized_item = {}
        
        # question_id / questionId
        qid = item.get("question_id") or item.get("questionId")
        if qid:
            normalized_item["question_id"] = qid
        
        # confidence
        if "confidence" in item:
            normalized_item["confidence"] = item["confidence"]
        
        # confidence_reason / confidenceReason
        conf_reason = item.get("confidence_reason") or item.get("confidenceReason")
        if conf_reason:
            normalized_item["confidence_reason"] = conf_reason
        
        # review_summary / reviewSummary
        review_summary = item.get("review_summary") or item.get("reviewSummary")
        if review_summary:
            normalized_item["review_summary"] = review_summary
        
        # review_corrections / reviewCorrections
        corrections = item.get("review_corrections") or item.get("reviewCorrections")
        if corrections:
            normalized_item["review_corrections"] = corrections
        
        # self_critique / selfCritique
        self_critique = item.get("self_critique") or item.get("selfCritique")
        if self_critique:
            normalized_item["self_critique"] = self_critique
        
        # self_critique_confidence / selfCritiqueConfidence
        self_conf = item.get("self_critique_confidence") or item.get("selfCritiqueConfidence")
        if self_conf:
            normalized_item["self_critique_confidence"] = self_conf
        
        # ä¿ç•™å…¶ä»–æ‰€æœ‰å­—æ®µ
        for key, value in item.items():
            if key not in normalized_item:
                normalized_item[key] = value
        
        normalized.append(normalized_item)
    
    return normalized


def _normalize_logic_review_self_audit(raw_audit: Any) -> Optional[Dict[str, Any]]:
    """
    æ ‡å‡†åŒ–é€»è¾‘å¤æ ¸è¿”å›çš„ self_audit æ•°æ®ã€‚
    
    å¤„ç†å„ç§å¯èƒ½çš„å­—æ®µåå˜ä½“ï¼ˆé©¼å³°/ä¸‹åˆ’çº¿ï¼‰å’Œæ•°æ®ç»“æ„ã€‚
    
    Args:
        raw_audit: åŸå§‹çš„ self_audit æ•°æ®
    
    Returns:
        æ ‡å‡†åŒ–åçš„ self_audit å­—å…¸ï¼Œå¦‚æœè¾“å…¥æ— æ•ˆåˆ™è¿”å› None
    """
    if not raw_audit or not isinstance(raw_audit, dict):
        return None
    
    normalized = {}
    
    # summary
    summary = raw_audit.get("summary")
    if summary:
        normalized["summary"] = summary
    
    # confidence
    confidence = raw_audit.get("confidence")
    if confidence is not None:
        normalized["confidence"] = _safe_float(confidence, 0.0)
    
    # issues
    issues = raw_audit.get("issues")
    if issues and isinstance(issues, list):
        normalized["issues"] = issues
    
    # compliance_analysis / complianceAnalysis
    compliance = raw_audit.get("compliance_analysis") or raw_audit.get("complianceAnalysis")
    if compliance and isinstance(compliance, list):
        normalized["compliance_analysis"] = compliance
    
    # uncertainties_and_conflicts / uncertaintiesAndConflicts
    uncertainties = raw_audit.get("uncertainties_and_conflicts") or raw_audit.get(
        "uncertaintiesAndConflicts"
    )
    if uncertainties and isinstance(uncertainties, list):
        normalized["uncertainties_and_conflicts"] = uncertainties
    
    # overall_compliance_grade / overallComplianceGrade
    grade = raw_audit.get("overall_compliance_grade") or raw_audit.get("overallComplianceGrade")
    if grade is not None:
        normalized["overall_compliance_grade"] = grade
    
    # honesty_note / honestyNote
    honesty = raw_audit.get("honesty_note") or raw_audit.get("honestyNote")
    if honesty:
        normalized["honesty_note"] = honesty
    
    # ä¿ç•™å…¶ä»–æ‰€æœ‰å­—æ®µ
    for key, value in raw_audit.items():
        if key not in normalized:
            normalized[key] = value
    
    return normalized if normalized else None


def _build_logic_review_summary(question_details: List[Dict[str, Any]]) -> str:
    """
    æ„å»ºé€»è¾‘å¤æ ¸æ‘˜è¦ã€‚
    
    åŸºäºé¢˜ç›®è¯¦æƒ…ç”Ÿæˆä¸€ä¸ªç®€çŸ­çš„æ‘˜è¦ï¼Œè¯´æ˜å¤æ ¸ç»“æœã€‚
    
    Args:
        question_details: é¢˜ç›®è¯¦æƒ…åˆ—è¡¨
    
    Returns:
        å¤æ ¸æ‘˜è¦å­—ç¬¦ä¸²
    """
    if not question_details:
        return "No questions to review"
    
    total_questions = len(question_details)
    total_score = sum(_safe_float(q.get("score", 0), 0.0) for q in question_details)
    max_score = sum(_safe_float(q.get("max_score", 0), 0.0) for q in question_details)
    
    # ç»Ÿè®¡ä½ç½®ä¿¡åº¦é¢˜ç›®
    low_confidence_count = sum(
        1 for q in question_details 
        if _safe_float(q.get("confidence", 1.0), 1.0) < 0.7
    )
    
    # ç»Ÿè®¡æœ‰ä¿®æ­£çš„é¢˜ç›®
    corrected_count = sum(
        1 for q in question_details 
        if q.get("review_corrections") and len(q.get("review_corrections", [])) > 0
    )
    
    # æ„å»ºæ‘˜è¦
    parts = [f"Reviewed {total_questions} question(s)"]
    
    if max_score > 0:
        percentage = (total_score / max_score) * 100
        parts.append(f"score {total_score:.1f}/{max_score:.1f} ({percentage:.0f}%)")
    
    if corrected_count > 0:
        parts.append(f"{corrected_count} correction(s)")
    
    if low_confidence_count > 0:
        parts.append(f"{low_confidence_count} low confidence")
    
    return ", ".join(parts)


def _estimate_page_max_score(
    parsed_rubric: Optional[Dict[str, Any]],
    page_context: Optional[Dict[str, Any]],
) -> float:
    if not parsed_rubric or not page_context:
        return 0.0
    if page_context.get("is_cover_page"):
        return 0.0
    question_numbers = page_context.get("question_numbers") or []
    if not question_numbers:
        return 0.0
    normalized = {_normalize_question_id(qnum) for qnum in question_numbers if qnum is not None}
    normalized = {qid for qid in normalized if qid}
    if not normalized:
        return 0.0
    total = 0.0
    for question in parsed_rubric.get("questions", []):
        qid = _normalize_question_id(question.get("question_id") or question.get("id"))
        if not qid or qid not in normalized:
            continue
        try:
            total += float(question.get("max_score", 0) or 0)
        except (TypeError, ValueError):
            continue
    return total


def _is_placeholder_evidence(text: Optional[str]) -> bool:
    if not text:
        return True
    content = text.strip()
    if not content:
        return True
    placeholders = [
        "æœªæ‰¾åˆ°",
        "æœªè¯†åˆ«",
        "ä¸æ¸…æ™°",
        "æ— æ³•è¾¨è®¤",
        "N/A",
        "null",
        "None",
        "ã€åŸæ–‡å¼•ç”¨ã€‘æœªæ‰¾åˆ°",
    ]
    return any(p in content for p in placeholders)


def _trim_text(value: Any, max_len: int) -> str:
    if value is None:
        return ""
    text = str(value)
    if max_len <= 0:
        return ""
    if len(text) <= max_len:
        return text
    if max_len <= 3:
        return text[:max_len]
    return text[: max_len - 3].rstrip() + "..."


def _normalize_scoring_point_results(
    raw_points: Any,
    question_id: str,
) -> List[Dict[str, Any]]:
    if not isinstance(raw_points, list):
        return []
    qid = _normalize_question_id(question_id) or str(question_id or "").strip()
    normalized = []
    for idx, spr in enumerate(raw_points, 1):
        if not isinstance(spr, dict):
            continue
        scoring_point = spr.get("scoring_point") or spr.get("scoringPoint") or {}
        point_id = (
            spr.get("point_id")
            or spr.get("pointId")
            or scoring_point.get("point_id")
            or f"{qid}.{idx}"
        )
        description = scoring_point.get("description") or spr.get("description") or ""
        rubric_reference = spr.get("rubric_reference") or spr.get("rubricReference") or ""
        rubric_reference_source = spr.get("rubric_reference_source") or spr.get(
            "rubricReferenceSource"
        )
        if not rubric_reference:
            rubric_reference = f"[{point_id}] {description}".strip()
            rubric_reference_source = "system"
        max_points = (
            spr.get("max_points")
            or spr.get("maxPoints")
            or spr.get("max_score")
            or spr.get("maxScore")
            or scoring_point.get("score")
            or 0
        )
        normalized.append(
            {
                **spr,
                "point_id": point_id,
                "rubric_reference": rubric_reference,
                "rubric_reference_source": rubric_reference_source,
                "max_points": max_points,
            }
        )
    return normalized


def _trim_list(items: Any, max_items: int) -> List[Any]:
    if items is None:
        return []
    if isinstance(items, list):
        values = items
    elif isinstance(items, tuple):
        values = list(items)
    else:
        values = [items]
    if max_items <= 0:
        return []
    return values[:max_items]


def _compact_evidence(evidence: Dict[str, Any], limits: Dict[str, int]) -> Dict[str, Any]:
    if not isinstance(evidence, dict):
        return evidence
    max_qnums = limits.get("max_question_numbers", 6)
    qnums = evidence.get("question_numbers")
    if isinstance(qnums, list):
        evidence["question_numbers"] = qnums[:max_qnums]
    evidence["page_summary"] = _trim_text(
        evidence.get("page_summary", ""),
        limits.get("max_page_summary_chars", 100),
    )
    answers = evidence.get("answers")
    if isinstance(answers, list):
        for answer in answers:
            if not isinstance(answer, dict):
                continue
            answer["answer_text"] = _trim_text(
                answer.get("answer_text", ""),
                limits.get("max_answer_chars", 160),
            )
            snippets = answer.get("evidence_snippets", [])
            snippets = _trim_list(snippets, limits.get("max_snippets", 1))
            answer["evidence_snippets"] = [
                _trim_text(snippet, limits.get("max_snippet_chars", 90))
                for snippet in snippets
                if snippet
            ]
            flags = answer.get("uncertainty_flags", [])
            answer["uncertainty_flags"] = _trim_list(
                flags,
                limits.get("max_uncertainty_flags", 3),
            )
    return evidence


def _compact_score_result(result: Dict[str, Any], limits: Dict[str, int]) -> Dict[str, Any]:
    if not isinstance(result, dict):
        return result
    result["page_summary"] = _trim_text(
        result.get("page_summary", ""),
        limits.get("max_page_summary_chars", 100),
    )
    q_details = result.get("question_details")
    if isinstance(q_details, list):
        for q in q_details:
            if not isinstance(q, dict):
                continue
            q["feedback"] = _trim_text(
                q.get("feedback", ""),
                limits.get("max_feedback_chars", 120),
            )
            q["student_answer"] = _trim_text(
                q.get("student_answer", ""),
                limits.get("max_student_answer_chars", 120),
            )
            typo_notes = q.get("typo_notes") or q.get("typoNotes") or []
            typo_notes = _trim_list(typo_notes, limits.get("max_typo_notes", 3))
            q["typo_notes"] = [
                _trim_text(note, limits.get("max_typo_chars", 24)) for note in typo_notes if note
            ]
            sprs = q.get("scoring_point_results") or q.get("scoring_results") or []
            if isinstance(sprs, list):
                for spr in sprs:
                    if not isinstance(spr, dict):
                        continue
                    spr["evidence"] = _trim_text(
                        spr.get("evidence", ""),
                        limits.get("max_evidence_chars", 90),
                    )
                    spr["reason"] = _trim_text(
                        spr.get("reason", ""),
                        limits.get("max_reason_chars", 120),
                    )
                    spr["decision"] = _trim_text(
                        spr.get("decision", ""),
                        limits.get("max_decision_chars", 24),
                    )
    return result


def _normalize_text(value: Any) -> str:
    if value is None:
        return ""
    return str(value).strip()


def _is_choice_question(question_text: str, standard_answer: str) -> bool:
    text = _normalize_text(question_text)
    answer = _normalize_text(standard_answer)
    if not text and not answer:
        return False
    text_no_space = re.sub(r"\s+", "", text)
    if re.search(r"[A-D][\\.ã€ï¼]", text_no_space):
        return True
    if any(token in text for token in ["é€‰æ‹©é¢˜", "å•é€‰", "å¤šé€‰", "é€‰é¡¹", "è¯·é€‰æ‹©", "ä¸‹åˆ—"]):
        return True
    if answer:
        answer_clean = re.sub(r"\s+", "", answer.upper())
        if re.fullmatch(r"[A-D](?:[ã€,/ï¼Œ ]*[A-D]){0,3}", answer_clean):
            return True
    return False


def _infer_question_type(question: Dict[str, Any]) -> str:
    raw_type = question.get("question_type") or question.get("questionType") or ""
    raw_type = str(raw_type).strip().lower()
    if raw_type:
        return raw_type

    question_text = _normalize_text(
        question.get("question_text") or question.get("questionText") or ""
    )
    grading_notes = _normalize_text(
        question.get("grading_notes") or question.get("gradingNotes") or ""
    )
    standard_answer = _normalize_text(
        question.get("standard_answer") or question.get("standardAnswer") or ""
    )
    alternative_solutions = (
        question.get("alternative_solutions") or question.get("alternativeSolutions") or []
    )

    if _is_choice_question(question_text, standard_answer):
        return "choice"

    text_blob = f"{question_text} {grading_notes}".lower()
    subjective_keywords = [
        "ç®€ç­”",
        "è®ºè¿°",
        "è¯æ˜",
        "æ¨å¯¼",
        "è§£é‡Š",
        "åˆ†æ",
        "è®¨è®º",
        "è®¾è®¡",
        "è¯´æ˜",
        "è¿‡ç¨‹",
        "æ­¥éª¤",
        "åº”ç”¨",
        "å®éªŒ",
    ]
    objective_keywords = [
        "åˆ¤æ–­",
        "å¡«ç©º",
        "å¯¹é”™",
        "æ˜¯é",
        "true",
        "false",
        "âˆš",
        "Ã—",
    ]

    if alternative_solutions:
        return "subjective"
    if any(token.lower() in text_blob for token in subjective_keywords):
        return "subjective"
    if any(token.lower() in text_blob for token in objective_keywords):
        return "objective"

    if standard_answer:
        answer_clean = re.sub(r"\s+", "", standard_answer)
        # æ³¨æ„: åœ¨å­—ç¬¦ç±»ä¸­ - éœ€è¦æ”¾åœ¨æœ«å°¾é¿å…è¢«è§£é‡Šä¸ºèŒƒå›´
        if len(answer_clean) <= 4 and re.fullmatch(r"[0-9A-Za-z+.=()ï¼ˆï¼‰/\\-]+", answer_clean):
            return "objective"
        if len(standard_answer) > 30 or "\n" in standard_answer:
            return "subjective"

    return "objective"


def _resolve_grading_mode(
    inputs: Optional[Dict[str, Any]],
    parsed_rubric: Optional[Dict[str, Any]],
) -> str:
    raw_mode = (inputs or {}).get("grading_mode") or (inputs or {}).get("gradingMode") or ""
    mode = str(raw_mode).strip().lower()
    mode_map = {
        "standard": "standard",
        "auto": "auto",
        "assist_teacher": "assist_teacher",
        "teacher_assist": "assist_teacher",
        "assistant_teacher": "assist_teacher",
        "assist_student": "assist_student",
        "student_assist": "assist_student",
        "assistant_student": "assist_student",
        "teacher": "assist_teacher",
        "student": "assist_student",
    }
    resolved = mode_map.get(mode, "auto" if not mode else "standard")
    has_rubric = bool((parsed_rubric or {}).get("questions"))
    if resolved == "auto":
        return "standard" if has_rubric else "assist_teacher"
    if resolved.startswith("assist"):
        return resolved
    return "standard"


def _build_rubric_question_map(parsed_rubric: Dict[str, Any]) -> Dict[str, Dict[str, Any]]:
    question_map: Dict[str, Dict[str, Any]] = {}
    for q in parsed_rubric.get("questions", []):
        qid = _normalize_question_id(q.get("question_id") or q.get("id"))
        if not qid:
            continue
        question_type = _infer_question_type(q)
        scoring_points = []
        for idx, sp in enumerate(q.get("scoring_points", [])):
            point_id = sp.get("point_id") or sp.get("pointId") or f"{qid}.{idx + 1}"
            scoring_points.append(
                {
                    "point_id": point_id,
                    "description": sp.get("description", ""),
                    "score": sp.get("score", 0),
                    "is_required": sp.get("is_required", True),
                    "expected_value": sp.get("expected_value") or sp.get("expectedValue") or "",
                    "keywords": sp.get("keywords") or [],
                }
            )
        alternative_solutions = []
        for alt in q.get("alternative_solutions") or q.get("alternativeSolutions") or []:
            if not isinstance(alt, dict):
                continue
            alternative_solutions.append(
                {
                    "description": alt.get("description", ""),
                    "scoring_criteria": alt.get("scoring_criteria")
                    or alt.get("scoringCriteria")
                    or alt.get("scoring_conditions")
                    or alt.get("scoringConditions")
                    or "",
                    "max_score": alt.get("max_score", alt.get("maxScore", q.get("max_score", 0))),
                }
            )
        question_map[qid] = {
            "question_id": qid,
            "max_score": q.get("max_score", 0),
            "question_text": q.get("question_text", ""),
            "question_type": question_type,
            "is_choice": question_type == "choice",
            "standard_answer": q.get("standard_answer", ""),
            "grading_notes": q.get("grading_notes", ""),
            "scoring_points": scoring_points,
            "deduction_rules": q.get("deduction_rules") or q.get("deductionRules") or [],
            "alternative_solutions": alternative_solutions,
        }
    return question_map


def _normalize_parsed_rubric_input(
    raw_rubric: Dict[str, Any],
    fallback: Optional[Dict[str, Any]] = None,
) -> Dict[str, Any]:
    fallback = fallback or {}
    raw_questions = raw_rubric.get("questions") or []
    normalized_questions = []

    for q in raw_questions:
        qid = q.get("question_id") or q.get("questionId") or q.get("id") or ""
        max_score = q.get("max_score", q.get("maxScore"))
        question_text = q.get("question_text") or q.get("questionText") or ""
        standard_answer = q.get("standard_answer") or q.get("standardAnswer") or ""
        question_type = q.get("question_type") or q.get("questionType") or ""
        grading_notes = q.get("grading_notes") or q.get("gradingNotes") or ""
        source_pages = q.get("source_pages") or q.get("sourcePages") or []
        if not isinstance(source_pages, list):
            source_pages = []

        scoring_points_raw = q.get("scoring_points") or q.get("scoringPoints") or []
        scoring_points = []
        for idx, sp in enumerate(scoring_points_raw):
            if isinstance(sp, dict):
                point_id = sp.get("point_id") or sp.get("pointId") or f"{qid}.{idx + 1}"
                keywords = sp.get("keywords") or []
                if isinstance(keywords, str):
                    keywords = [keywords]
                scoring_points.append(
                    {
                        "point_id": point_id,
                        "description": sp.get("description", ""),
                        "score": float(sp.get("score", sp.get("maxScore", 0)) or 0),
                        "is_required": sp.get("is_required", sp.get("isRequired", True)),
                        "keywords": keywords,
                        "expected_value": sp.get("expected_value") or sp.get("expectedValue") or "",
                    }
                )
            elif isinstance(sp, str):
                scoring_points.append(
                    {
                        "point_id": f"{qid}.{idx + 1}",
                        "description": sp,
                        "score": 0,
                        "is_required": True,
                        "keywords": [],
                        "expected_value": "",
                    }
                )

        if max_score is None:
            max_score = sum(sp.get("score", 0) for sp in scoring_points)
        max_score = float(max_score or 0)

        alternative_solutions_raw = (
            q.get("alternative_solutions") or q.get("alternativeSolutions") or []
        )
        alternative_solutions = []
        for alt in alternative_solutions_raw:
            if isinstance(alt, dict):
                alternative_solutions.append(
                    {
                        "description": alt.get("description", ""),
                        "scoring_criteria": alt.get("scoring_criteria")
                        or alt.get("scoringCriteria")
                        or "",
                        "note": alt.get("note", ""),
                    }
                )
            elif isinstance(alt, str):
                alternative_solutions.append(
                    {
                        "description": alt,
                        "scoring_criteria": "",
                        "note": "",
                    }
                )

        deduction_rules_raw = q.get("deduction_rules") or q.get("deductionRules") or []
        deduction_rules = []
        for idx, dr in enumerate(deduction_rules_raw):
            if isinstance(dr, dict):
                deduction_rules.append(
                    {
                        "rule_id": dr.get("rule_id") or dr.get("ruleId") or f"{qid}.d{idx + 1}",
                        "description": dr.get("description", ""),
                        "deduction": float(dr.get("deduction", dr.get("score", 0)) or 0),
                        "conditions": dr.get("conditions") or dr.get("when") or "",
                    }
                )
            elif isinstance(dr, str):
                deduction_rules.append(
                    {
                        "rule_id": f"{qid}.d{idx + 1}",
                        "description": dr,
                        "deduction": 0.0,
                        "conditions": "",
                    }
                )

        criteria = q.get("criteria")
        if not criteria:
            criteria = [sp.get("description", "") for sp in scoring_points]

        normalized_questions.append(
            {
                "id": qid,
                "question_id": qid,
                "max_score": max_score,
                "question_text": question_text,
                "question_type": question_type,
                "standard_answer": standard_answer,
                "criteria": criteria,
                "scoring_points": scoring_points,
                "alternative_solutions": alternative_solutions,
                "deduction_rules": deduction_rules,
                "grading_notes": grading_notes,
                "source_pages": source_pages,
            }
        )

    total_score = raw_rubric.get("total_score") or raw_rubric.get("totalScore")
    if total_score is None:
        total_score = sum(q.get("max_score", 0) for q in normalized_questions)

    return {
        "total_questions": int(
            raw_rubric.get("total_questions")
            or raw_rubric.get("totalQuestions")
            or len(normalized_questions)
        ),
        "total_score": float(total_score or 0),
        "rubric_format": raw_rubric.get("rubric_format")
        or raw_rubric.get("rubricFormat")
        or fallback.get("rubric_format", "standard"),
        "general_notes": raw_rubric.get("general_notes")
        or raw_rubric.get("generalNotes")
        or fallback.get("general_notes", ""),
        "questions": normalized_questions,
        "rubric_context": raw_rubric.get("rubric_context") or fallback.get("rubric_context"),
        "raw_text": raw_rubric.get("raw_text") or fallback.get("raw_text"),
    }


def _format_rubric_context_from_dict(parsed_rubric: Dict[str, Any]) -> str:
    def ensure_str(value: Any) -> str:
        if value is None:
            return ""
        if isinstance(value, list):
            return " ".join(str(item) for item in value)
        return str(value)

    lines = [
        "=" * 60,
        "RUBRIC SUMMARY",
        "=" * 60,
        f"Questions: {parsed_rubric.get('total_questions', 0)}",
        f"Total Score: {parsed_rubric.get('total_score', 0)}",
        f"Format: {ensure_str(parsed_rubric.get('rubric_format', 'standard'))}",
        "",
    ]

    general_notes = ensure_str(parsed_rubric.get("general_notes", ""))
    if general_notes:
        lines.append(f"General Notes: {general_notes}")
        lines.append("")

    for q in parsed_rubric.get("questions", []):
        lines.append("-" * 40)
        question_id = ensure_str(q.get("question_id", ""))
        lines.append(f"Question {question_id} max_score: {q.get('max_score', 0)}")

        question_text = ensure_str(q.get("question_text", ""))
        if question_text:
            preview = question_text[:100] if len(question_text) > 100 else question_text
            lines.append(f"Question text: {preview}")

        standard_answer = ensure_str(q.get("standard_answer", ""))
        if standard_answer:
            preview = standard_answer[:200] if len(standard_answer) > 200 else standard_answer
            lines.append(f"Standard answer: {preview}")

        scoring_points = q.get("scoring_points", [])
        if scoring_points:
            lines.append("Scoring points:")
            for idx, sp in enumerate(scoring_points, 1):
                required = "required" if sp.get("is_required", True) else "optional"
                keywords = sp.get("keywords") or []
                keywords_str = f" keywords:{keywords}" if keywords else ""
                expected_value = ensure_str(sp.get("expected_value", ""))
                expected_value_str = f" expected:{expected_value}" if expected_value else ""
                point_id = sp.get("point_id") or sp.get("pointId") or f"{question_id}.{idx}"
                lines.append(
                    f"  [{point_id}] {sp.get('score', 0)}?/{required} - "
                    f"{ensure_str(sp.get('description', ''))}{keywords_str}{expected_value_str}"
                )

        deduction_rules = q.get("deduction_rules") or q.get("deductionRules") or []
        if deduction_rules:
            lines.append("Deduction rules:")
            for idx, dr in enumerate(deduction_rules, 1):
                rule_id = dr.get("rule_id") or dr.get("ruleId") or f"{question_id}.d{idx}"
                deduction = dr.get("deduction", dr.get("score", 0))
                conditions = ensure_str(dr.get("conditions") or dr.get("when") or "")
                condition_text = f" conditions:{conditions}" if conditions else ""
                lines.append(
                    f"  [{rule_id}] -{deduction} {ensure_str(dr.get('description', ''))}{condition_text}"
                )

        alternative_solutions = q.get("alternative_solutions", [])
        if alternative_solutions:
            lines.append("Alternative solutions:")
            for alt in alternative_solutions:
                lines.append(f"  - {ensure_str(alt.get('description', ''))}")
                lines.append(f"    criteria: {ensure_str(alt.get('scoring_criteria', ''))}")

        grading_notes = ensure_str(q.get("grading_notes", ""))
        if grading_notes:
            lines.append(f"Notes: {grading_notes}")

        lines.append("")

    return "\n".join(lines)


def _finalize_scoring_result(
    raw_result: Dict[str, Any],
    evidence: Dict[str, Any],
    rubric_map: Dict[str, Dict[str, Any]],
    page_index: int,
) -> Dict[str, Any]:
    raw_questions = raw_result.get("question_details") or []
    raw_by_id = {}
    for q in raw_questions:
        qid = _normalize_question_id(q.get("question_id"))
        if qid:
            raw_by_id[qid] = q

    answer_map = {}
    for answer in evidence.get("answers", []):
        qid = _normalize_question_id(answer.get("question_id"))
        if qid:
            answer_map[qid] = answer

    question_ids = list(answer_map.keys())
    if not question_ids:
        question_ids = [
            _normalize_question_id(q.get("question_id"))
            for q in raw_questions
            if q.get("question_id")
        ]
    if not question_ids:
        question_ids = list(rubric_map.keys())
    seen = set()
    question_ids = [qid for qid in question_ids if qid and not (qid in seen or seen.add(qid))]

    question_details = []
    for qid in question_ids:
        rubric = rubric_map.get(qid, {})
        expected_points = rubric.get("scoring_points", [])
        raw_question = raw_by_id.get(qid, {})
        question_type = rubric.get("question_type") or (
            _infer_question_type(rubric) if rubric else ""
        )
        if not question_type:
            question_type = (
                raw_question.get("question_type") or raw_question.get("questionType") or ""
            )
        is_choice = bool(rubric.get("is_choice") or question_type == "choice")
        raw_scoring = (
            raw_question.get("scoring_point_results") or raw_question.get("scoring_results") or []
        )
        answer_info = answer_map.get(qid, {}) if isinstance(answer_map, dict) else {}
        evidence_snippets = answer_info.get("evidence_snippets") or []
        fallback_snippet = ""
        if isinstance(evidence_snippets, list) and evidence_snippets:
            fallback_snippet = _trim_text(evidence_snippets[0], 90)
        raw_scoring_by_id = {
            _normalize_question_id(spr.get("point_id") or spr.get("pointId")): spr
            for spr in raw_scoring
            if spr.get("point_id") or spr.get("pointId")
        }

        scoring_point_results = []
        review_corrections = []
        missing_points = 0
        missing_evidence = 0
        for idx, sp in enumerate(expected_points):
            point_id = _normalize_question_id(sp.get("point_id")) or f"{qid}.{idx + 1}"
            existing = raw_scoring_by_id.get(point_id, {})
            awarded = existing.get("awarded", existing.get("score", 0))
            max_points = sp.get("score", existing.get("max_points", 0))
            if awarded is None:
                awarded = 0
            if max_points is None:
                max_points = 0
            if awarded > max_points:
                review_corrections.append(
                    {
                        "point_id": point_id,
                        "review_reason": "Score exceeds max; capped to max.",
                    }
                )
                awarded = max_points
            if awarded < 0:
                review_corrections.append(
                    {
                        "point_id": point_id,
                        "review_reason": "Score below zero; clamped to 0.",
                    }
                )
                awarded = 0

            evidence_text = existing.get("evidence")
            if _is_placeholder_evidence(evidence_text):
                missing_evidence += 1
                if fallback_snippet:
                    evidence_text = f"ã€åŸæ–‡å¼•ç”¨ã€‘{fallback_snippet}"
                elif not evidence_text:
                    evidence_text = "ã€åŸæ–‡å¼•ç”¨ã€‘æœªæ‰¾åˆ°"
            if not existing:
                missing_points += 1
                review_corrections.append(
                    {
                        "point_id": point_id,
                        "review_reason": "Missing scoring point; added with 0 score.",
                    }
                )

            description = sp.get("description", "")
            expected_value = sp.get("expected_value") or sp.get("expectedValue") or ""
            
            # ğŸ”§ å¼ºåŒ– rubric_reference ç”Ÿæˆé€»è¾‘
            # ä¼˜å…ˆä½¿ç”¨è¯„åˆ†æ ‡å‡†ä¸­çš„æè¿°ï¼Œç¡®ä¿ Logic Review èƒ½è·å–åˆ°å®Œæ•´ä¿¡æ¯
            rubric_reference = f"[{point_id}] {description}".strip()
            if expected_value:
                rubric_reference = f"{rubric_reference}ï¼ˆæ ‡å‡†å€¼:{expected_value}ï¼‰"
            
            # å¦‚æœ description ä¸ºç©ºï¼Œå°è¯•ä» rubric_map ä¸­è·å–
            if not description and rubric:
                for rubric_sp in rubric.get("scoring_points", []):
                    if rubric_sp.get("point_id") == point_id:
                        rubric_desc = rubric_sp.get("description", "")
                        if rubric_desc:
                            rubric_reference = f"[{point_id}] {rubric_desc}".strip()
                            if expected_value:
                                rubric_reference = f"{rubric_reference}ï¼ˆæ ‡å‡†å€¼:{expected_value}ï¼‰"
                        break

            scoring_point_results.append(
                {
                    "point_id": point_id,
                    "rubric_reference": rubric_reference,
                    "rubric_reference_source": "system",
                    "decision": "å¾—åˆ†" if awarded > 0 else "æœªå¾—åˆ†",
                    "awarded": awarded,
                    "max_points": max_points,
                    "evidence": evidence_text,
                    "reason": existing.get("reason", ""),
                    "scoring_point": {
                        "description": sp.get("description", ""),
                        "score": max_points,
                        "is_required": sp.get("is_required", True),
                    },
                }
            )
            
            # ğŸ” è¯Šæ–­æ—¥å¿—ï¼šæ£€æŸ¥ rubric_reference æ˜¯å¦ä¸ºç©º
            if not rubric_reference or rubric_reference == f"[{point_id}]":
                logger.warning(
                    f"[grading] âš ï¸ rubric_reference ä¸ºç©ºæˆ–ä¸å®Œæ•´: "
                    f"qid={qid}, point_id={point_id}, description={description}, "
                    f"rubric_reference={rubric_reference}"
                )

        if not scoring_point_results and raw_scoring:
            for idx, spr in enumerate(raw_scoring, 1):
                point_id = spr.get("point_id") or spr.get("pointId") or f"{qid}.{idx}"
                scoring_point = spr.get("scoring_point") or spr.get("scoringPoint") or {}
                description = scoring_point.get("description") or spr.get("description") or ""
                rubric_reference = spr.get("rubric_reference") or spr.get("rubricReference") or ""
                rubric_reference_source = spr.get("rubric_reference_source") or spr.get(
                    "rubricReferenceSource"
                )
                
                # ğŸ”§ å¼ºåŒ– rubric_reference ç”Ÿæˆé€»è¾‘
                if not rubric_reference:
                    rubric_reference = f"[{point_id}] {description}".strip()
                    rubric_reference_source = "system"
                    
                    # å¦‚æœ description ä¸ºç©ºï¼Œå°è¯•ä» rubric_map ä¸­è·å–
                    if not description and rubric:
                        for rubric_sp in rubric.get("scoring_points", []):
                            if rubric_sp.get("point_id") == point_id:
                                rubric_desc = rubric_sp.get("description", "")
                                if rubric_desc:
                                    rubric_reference = f"[{point_id}] {rubric_desc}".strip()
                                break
                
                max_points = spr.get("max_points", spr.get("maxScore"))
                if max_points is None:
                    max_points = scoring_point.get("score", 0)
                scoring_point_results.append(
                    {
                        "point_id": point_id,
                        "rubric_reference": rubric_reference,
                        "rubric_reference_source": rubric_reference_source,
                        "decision": spr.get("decision") or spr.get("result") or "",
                        "awarded": spr.get("awarded", spr.get("score", 0)),
                        "max_points": max_points or 0,
                        "evidence": spr.get("evidence", ""),
                        "reason": spr.get("reason", ""),
                        "scoring_point": scoring_point if scoring_point else None,
                    }
                )

        sum_awarded = sum(r.get("awarded", 0) for r in scoring_point_results)
        max_score = rubric.get("max_score", raw_question.get("max_score", 0))
        if not max_score:
            max_score = sum(r.get("max_points", 0) for r in scoring_point_results)
        score = raw_question.get("score")
        score_adjusted = False
        if score is None:
            score = sum_awarded
        if abs(sum_awarded - score) > 0.25:
            score = sum_awarded
            score_adjusted = True
        if score > max_score:
            score = max_score
            score_adjusted = True
        if score_adjusted:
            review_corrections.append(
                {
                    "point_id": qid,
                    "review_reason": "Total mismatch; recalculated from point scores.",
                }
            )

        typo_notes = raw_question.get("typo_notes") or raw_question.get("typoNotes") or []
        if isinstance(typo_notes, str):
            typo_notes = [typo_notes]
        if not isinstance(typo_notes, list):
            typo_notes = []

        total_points = (
            max(1, len(expected_points)) if expected_points else max(1, len(scoring_point_results))
        )
        coverage = min(1.0, len(scoring_point_results) / total_points)
        evidence_ok = min(1.0, (total_points - missing_evidence) / total_points)
        consistency = 1.0 if not score_adjusted else 0.6
        confidence = 0.2 + coverage * 0.5 + evidence_ok * 0.2 + consistency * 0.1
        answer_confidence = answer_map.get(qid, {}).get("confidence")
        if isinstance(answer_confidence, (int, float)):
            confidence = max(0.0, min(1.0, confidence * max(0.4, min(1.0, answer_confidence))))
        used_alt = bool(
            raw_question.get("used_alternative_solution")
            or raw_question.get("usedAlternativeSolution")
            or raw_question.get("alternative_solution_ref")
            or raw_question.get("alternativeSolutionRef")
        )
        confidence_multiplier = 1.0
        if question_type in ("subjective", "essay", "stepwise"):
            confidence_multiplier *= 0.85
        if used_alt or rubric.get("alternative_solutions"):
            confidence_multiplier *= 0.9
        confidence = max(0.0, min(1.0, confidence * confidence_multiplier))

        rubric_ref_coverage = 1.0
        if scoring_point_results:
            rubric_ref_coverage = sum(
                1 for spr in scoring_point_results if spr.get("rubric_reference")
            ) / max(1, len(scoring_point_results))
            if rubric_ref_coverage < 1.0:
                confidence = max(0.0, min(1.0, confidence * (0.6 + 0.4 * rubric_ref_coverage)))

        issues = []
        if missing_points:
            issues.append(f"Scoring coverage incomplete (missing {missing_points} points)")
        if missing_evidence:
            issues.append("Insufficient evidence for some points")
        if score_adjusted:
            issues.append("Point sum mismatched; adjusted total")

        missing_rubric_ref = any(not spr.get("rubric_reference") for spr in scoring_point_results)
        missing_point_id = any(not spr.get("point_id") for spr in scoring_point_results)
        if missing_rubric_ref:
            issues.append("Missing rubric reference for some points")
        if missing_point_id:
            issues.append("Missing point_id for some points")

        audit_flags = []
        if missing_points:
            audit_flags.append("missing_scoring_points")
        if missing_evidence:
            audit_flags.append("missing_evidence")
        if score_adjusted:
            audit_flags.append("score_adjusted")
        if missing_rubric_ref:
            audit_flags.append("missing_rubric_reference")
        if missing_point_id:
            audit_flags.append("missing_point_id")

        review_summary = "; ".join(issues) if issues else "Logic consistent; no obvious issues"

        confidence_reason = (
            f"coverage={coverage:.2f}, evidence={evidence_ok:.2f}, consistency={consistency:.2f}"
        )
        if question_type:
            confidence_reason = f"{confidence_reason}, type={question_type}"
        if used_alt or rubric.get("alternative_solutions"):
            confidence_reason = f"{confidence_reason}, alt_solution=1"
        confidence_reason = f"{confidence_reason}, rubric_refs={rubric_ref_coverage:.2f}"

        feedback = raw_question.get("feedback", "")
        self_critique = raw_question.get("self_critique") or review_summary
        if is_choice:
            feedback = ""
            self_critique = ""

        question_details.append(
            {
                "question_id": qid,
                "score": score,
                "max_score": max_score,
                "confidence": confidence,
                "confidence_reason": confidence_reason,
                "feedback": feedback,
                "student_answer": raw_question.get("student_answer")
                or answer_map.get(qid, {}).get("answer_text", ""),
                "self_critique": self_critique,
                "self_critique_confidence": raw_question.get(
                    "self_critique_confidence", confidence
                ),
                "typo_notes": typo_notes,
                "rubric_refs": [
                    spr.get("rubric_reference")
                    for spr in scoring_point_results
                    if spr.get("rubric_reference")
                ],
                "scoring_point_results": scoring_point_results,
                "review_summary": review_summary,
                "review_corrections": review_corrections,
                "audit_flags": audit_flags,
                "page_indices": [page_index],
                "is_correct": max_score > 0 and score >= max_score,
                "question_type": question_type,
                "used_alternative_solution": used_alt,
                "alternative_solution_ref": raw_question.get("alternative_solution_ref")
                or raw_question.get("alternativeSolutionRef")
                or "",
            }
        )

    page_confidence = (
        sum(q.get("confidence", 0) for q in question_details) / len(question_details)
        if question_details
        else 0.0
    )
    return {
        "question_details": question_details,
        "score": sum(q.get("score", 0) for q in question_details),
        "max_score": sum(q.get("max_score", 0) for q in question_details),
        "page_confidence": page_confidence,
    }


def _finalize_assist_result(
    raw_result: Dict[str, Any],
    evidence: Dict[str, Any],
    page_index: int,
    grading_mode: str,
) -> Dict[str, Any]:
    raw_questions = raw_result.get("question_details") or []
    raw_by_id = {}
    for q in raw_questions:
        qid = _normalize_question_id(q.get("question_id"))
        if qid:
            raw_by_id[qid] = q

    answer_map = {}
    for answer in evidence.get("answers", []):
        qid = _normalize_question_id(answer.get("question_id"))
        if qid:
            answer_map[qid] = answer

    question_ids = list(answer_map.keys())
    if not question_ids:
        question_ids = [
            _normalize_question_id(q.get("question_id"))
            for q in raw_questions
            if q.get("question_id")
        ]
    seen = set()
    question_ids = [qid for qid in question_ids if qid and not (qid in seen or seen.add(qid))]

    question_details = []
    for qid in question_ids:
        raw_question = raw_by_id.get(qid, {})
        answer_info = answer_map.get(qid, {}) if isinstance(answer_map, dict) else {}
        feedback = raw_question.get("feedback", "")
        if not feedback:
            feedback = raw_question.get("explanation") or raw_question.get("analysis") or ""
        if not feedback:
            hints = raw_question.get("error_hints") or raw_question.get("errorHints") or []
            if isinstance(hints, list) and hints:
                feedback = "ï¼›".join([str(h).strip() for h in hints if h])
        confidence = raw_question.get("confidence", 0.4)
        if not isinstance(confidence, (int, float)):
            confidence = 0.4
        question_type = (
            raw_question.get("question_type") or raw_question.get("questionType") or "unknown"
        )

        question_details.append(
            {
                "question_id": qid,
                "score": 0.0,
                "max_score": 0.0,
                "confidence": float(confidence),
                "feedback": feedback,
                "student_answer": raw_question.get("student_answer")
                or answer_info.get("answer_text", ""),
                "self_critique": raw_question.get("self_critique") or "",
                "self_critique_confidence": raw_question.get(
                    "self_critique_confidence", confidence
                ),
                "typo_notes": raw_question.get("typo_notes") or raw_question.get("typoNotes") or [],
                "rubric_refs": [],
                "scoring_point_results": [],
                "review_summary": "",
                "review_corrections": [],
                "audit_flags": ["assist_mode", grading_mode],
                "page_indices": [page_index],
                "is_correct": False,
                "question_type": question_type,
                "grading_mode": grading_mode,
            }
        )

    page_confidence = (
        sum(q.get("confidence", 0) for q in question_details) / len(question_details)
        if question_details
        else 0.0
    )
    return {
        "question_details": question_details,
        "score": 0.0,
        "max_score": 0.0,
        "page_confidence": page_confidence,
    }


async def grade_batch_node(state: Dict[str, Any]) -> Dict[str, Any]:
    return await _grade_batch_node_impl(state)


async def _grade_batch_node_impl(state: Dict[str, Any]) -> Dict[str, Any]:
    """
    æ‰¹é‡æ‰¹æ”¹èŠ‚ç‚¹

    æ‰¹æ”¹ä¸€æ‰¹é¡µé¢ï¼Œè¿”å›æ¯é¡µçš„æ‰¹æ”¹ç»“æœã€‚

    **æ ¸å¿ƒæµç¨‹**:
    1. ä» parsed_rubric é‡å»º RubricRegistry
    2. åˆ›å»º GradingSkills å®ä¾‹
    3. æ‰¹æ”¹æ—¶è¯†åˆ«é¢˜ç›®ç¼–å·
    4. ä½¿ç”¨ GradingSkills.get_rubric_for_question è·å–è¯¥é¢˜ç›®çš„è¯„åˆ†æ ‡å‡†
    5. åŸºäºæŒ‡å®šè¯„åˆ†æ ‡å‡†è¿›è¡Œæ‰¹æ”¹

    ç‰¹æ€§ï¼š
    - Worker ç‹¬ç«‹æ€§ï¼šæ¯ä¸ª Worker ç‹¬ç«‹è·å–è¯„åˆ†æ ‡å‡†ï¼Œä¸å…±äº«å¯å˜çŠ¶æ€ (Req 3.2)
    - Agent Skill é›†æˆï¼šä½¿ç”¨ GradingSkills åŠ¨æ€è·å–é¢˜ç›®è¯„åˆ†æ ‡å‡† (Req 5.1)
    - æ‰¹æ¬¡å¤±è´¥é‡è¯•ï¼šå•æ‰¹æ¬¡å¤±è´¥ä¸å½±å“å…¶ä»–æ‰¹æ¬¡ï¼Œæ”¯æŒé‡è¯• (Req 3.3, 9.3)
    - è¿›åº¦æŠ¥å‘Šï¼šå®æ—¶æŠ¥å‘Šæ‰¹æ¬¡å¤„ç†è¿›åº¦ (Req 3.4)
    - é”™è¯¯éš”ç¦»ï¼šå•é¡µå¤±è´¥ä¸å½±å“å…¶ä»–é¡µé¢ï¼Œè®°å½•é”™è¯¯å¹¶ç»§ç»­å¤„ç† (Req 9.2)

    Requirements: 3.2, 3.3, 3.4, 5.1, 9.2, 9.3
    """
    batch_id = state["batch_id"]
    batch_index = state["batch_index"]
    total_batches = state["total_batches"]
    page_indices = state["page_indices"]
    images = state["images"]
    rubric = state.get("rubric", "")
    page_index_contexts = state.get("page_index_contexts", {})
    api_key = state.get("api_key") or os.getenv("LLM_API_KEY") or os.getenv("OPENROUTER_API_KEY")
    retry_count = state.get("retry_count", 0)
    max_retries = state.get("max_retries", 2)
    batch_student_key = state.get("student_key") or f"Student {batch_index + 1}"
    batch_student_name = state.get("student_name")
    batch_student_id = state.get("student_id")

    logger.info(
        f"[grade_batch] å¼€å§‹æ‰¹æ”¹æ‰¹æ¬¡ {batch_index + 1}/{total_batches}: "
        f"batch_id={batch_id}, é¡µé¢={page_indices}, é‡è¯•æ¬¡æ•°={retry_count}"
    )

    page_results = []
    batch_error = None
    output_limits = {
        "max_answer_chars": int(os.getenv("GRADING_MAX_ANSWER_CHARS", "160")),
        "max_student_answer_chars": int(os.getenv("GRADING_MAX_STUDENT_ANSWER_CHARS", "4000")),
        "max_snippet_chars": int(os.getenv("GRADING_MAX_SNIPPET_CHARS", "90")),
        "max_snippets": int(os.getenv("GRADING_MAX_SNIPPETS", "1")),
        "max_page_summary_chars": int(os.getenv("GRADING_MAX_PAGE_SUMMARY_CHARS", "100")),
        "max_feedback_chars": int(os.getenv("GRADING_MAX_FEEDBACK_CHARS", "120")),
        "max_evidence_chars": int(os.getenv("GRADING_MAX_EVIDENCE_CHARS", "90")),
        "max_reason_chars": int(os.getenv("GRADING_MAX_REASON_CHARS", "120")),
        "max_decision_chars": int(os.getenv("GRADING_MAX_DECISION_CHARS", "24")),
        "max_typo_notes": int(os.getenv("GRADING_MAX_TYPO_NOTES", "3")),
        "max_typo_chars": int(os.getenv("GRADING_MAX_TYPO_CHARS", "24")),
        "max_question_numbers": int(os.getenv("GRADING_MAX_QUESTION_NUMBERS", "6")),
        "max_uncertainty_flags": int(os.getenv("GRADING_MAX_UNCERTAINTY_FLAGS", "3")),
    }
    second_pass_threshold = float(os.getenv("GRADING_SECOND_PASS_CONFIDENCE", "0.65"))
    second_pass_max_ratio = float(os.getenv("GRADING_SECOND_PASS_MAX_RATIO", "0.2"))
    second_pass_budget_fraction = float(os.getenv("GRADING_SECOND_PASS_BUDGET_FRACTION", "0.25"))
    budget_per_page = float(os.getenv("GRADING_BUDGET_PER_PAGE_USD", "0.01"))
    cost_per_m_input = float(os.getenv("GRADING_COST_PER_M_INPUT_TOKENS", "0.5"))
    cost_per_m_output = float(os.getenv("GRADING_COST_PER_M_OUTPUT_TOKENS", "3.0"))
    strict_est_input_tokens = int(os.getenv("GRADING_STRICT_EST_INPUT_TOKENS", "1200"))
    strict_est_output_tokens = int(os.getenv("GRADING_STRICT_EST_OUTPUT_TOKENS", "600"))
    fast_pass_only = os.getenv("GRADING_FAST_PASS_ONLY", "false").strip().lower() in (
        "1",
        "true",
        "yes",
    )
    max_second_passes = int(len(page_indices) * max(0.0, second_pass_max_ratio))
    if second_pass_max_ratio > 0 and max_second_passes == 0:
        max_second_passes = 1
    second_pass_used = 0
    second_pass_lock = asyncio.Lock()
    est_second_pass_cost = (strict_est_input_tokens / 1_000_000.0) * cost_per_m_input + (
        strict_est_output_tokens / 1_000_000.0
    ) * cost_per_m_output
    budget_allows_second_pass = (
        budget_per_page > 0
        and est_second_pass_cost <= budget_per_page * second_pass_budget_fraction
    )
    grading_mode = _resolve_grading_mode(state.get("inputs", {}), state.get("parsed_rubric", {}))

    try:
        if not api_key:
            raise ValueError("API key æœªé…ç½®")

        # Worker ç‹¬ç«‹æ€§ä¿è¯ (Requirement 3.2)
        # æ¯ä¸ª Worker ç‹¬ç«‹åˆ›å»ºå®ä¾‹ï¼Œä¸å…±äº«å¯å˜çŠ¶æ€
        from src.services.llm_reasoning import LLMReasoningClient
        from src.utils.error_handling import execute_with_isolation, get_error_manager
        from src.services.rubric_registry import RubricRegistry

        # æ³¨æ„ï¼šå·²ç§»é™¤ Agent Skillï¼Œç›´æ¥ä½¿ç”¨ rubric_registry
        from src.models.grading_models import QuestionRubric, ScoringPoint

        # ç‹¬ç«‹è·å–è¯„åˆ†æ ‡å‡†å‰¯æœ¬ï¼ˆä¸å…±äº«å¯å˜çŠ¶æ€ï¼‰
        parsed_rubric = state.get("parsed_rubric", {})
        import copy

        local_parsed_rubric = copy.deepcopy(parsed_rubric)
        rubric_map = _build_rubric_question_map(local_parsed_rubric)
        grading_mode = _resolve_grading_mode(state.get("inputs", {}), local_parsed_rubric)
        if grading_mode == "assist_student":
            output_limits["max_feedback_chars"] = int(
                os.getenv("GRADING_ASSIST_FEEDBACK_CHARS", "600")
            )
            output_limits["max_page_summary_chars"] = int(
                os.getenv("GRADING_ASSIST_SUMMARY_CHARS", "180")
            )
            output_limits["max_student_answer_chars"] = int(
                os.getenv("GRADING_ASSIST_ANSWER_CHARS", "220")
            )
        logger.info(f"[grade_batch] grading_mode={grading_mode}")

        # ğŸ” è°ƒè¯•æ—¥å¿—ï¼šç¡®è®¤ parsed_rubric å†…å®¹
        logger.info(
            f"[grade_batch] æ¥æ”¶åˆ° parsed_rubric: "
            f"total_questions={local_parsed_rubric.get('total_questions', 0)}, "
            f"total_score={local_parsed_rubric.get('total_score', 0)}, "
            f"questions_count={len(local_parsed_rubric.get('questions', []))}"
        )

        # ğŸ”¥ å…³é”®ï¼šä» parsed_rubric é‡å»º RubricRegistry (Requirement 5.1)
        rubric_registry = RubricRegistry(total_score=local_parsed_rubric.get("total_score", 100.0))

        # å°†è§£æçš„é¢˜ç›®æ³¨å†Œåˆ° Registry
        questions_data = local_parsed_rubric.get("questions", [])
        if questions_data:
            question_rubrics = []
            for q in questions_data:
                # æ„å»º ScoringPoint åˆ—è¡¨
                qid = q.get("question_id") or q.get("id") or ""
                scoring_points = [
                    ScoringPoint(
                        description=sp.get("description", ""),
                        score=sp.get("score", 0),
                        is_required=sp.get("is_required", True),
                        point_id=sp.get("point_id") or sp.get("pointId") or f"{qid}.{idx + 1}",
                    )
                    for idx, sp in enumerate(q.get("scoring_points", []))
                ]

                # æ„å»º QuestionRubric
                question_rubric = QuestionRubric(
                    question_id=str(qid),
                    question_text=q.get("question_text", ""),
                    max_score=q.get("max_score", 0),
                    scoring_points=scoring_points,
                    standard_answer=q.get("standard_answer", ""),
                    grading_notes=q.get("grading_notes", ""),
                    alternative_solutions=[],  # ç®€åŒ–å¤„ç†
                )
                question_rubrics.append(question_rubric)

            rubric_registry.register_rubrics(question_rubrics, log=False)
            logger.info(f"[grade_batch] å·²é‡å»º RubricRegistryï¼Œæ³¨å†Œ {len(question_rubrics)} é“é¢˜ç›®")

        # åˆ›å»º LLMReasoningClientï¼ˆå·²ç§»é™¤ Agent Skillï¼‰
        reasoning_client = LLMReasoningClient(
            api_key=api_key,
            rubric_registry=rubric_registry,
        )
        # é”™è¯¯éš”ç¦»ï¼šå•é¡µå¤±è´¥ä¸å½±å“å…¶ä»–é¡µé¢ (Requirement 9.2)
        error_manager = get_error_manager()

        batch_agent_id = f"batch_{batch_index}"
        batch_student_key = state.get("student_key")
        batch_agent_label = batch_student_key or f"Student Batch {batch_index + 1}"
        total_pages_in_batch = len(page_indices)
        pages_done = 0
        pages_lock = asyncio.Lock()

        async def emit_agent_update(
            status: str,
            message: str = "",
            progress: Optional[int] = None,
        ) -> None:
            payload = {
                "type": "agent_update",
                "parentNodeId": "grade_batch",
                "agentId": batch_agent_id,
                "agentName": batch_agent_label,
                "agentLabel": batch_agent_label,
                "status": status,
                "message": message,
            }
            if progress is not None:
                payload["progress"] = progress
            await _broadcast_progress(batch_id, payload)

        async def emit_stage(message: str) -> None:
            await emit_agent_update("running", message)

        async def mark_page_done(page_idx: int, detail: str) -> None:
            nonlocal pages_done
            async with pages_lock:
                pages_done += 1
                progress = int((pages_done / max(1, total_pages_in_batch)) * 100)
            await emit_agent_update("running", detail, progress=progress)

        await emit_agent_update(
            "running",
            f"Start grading {total_pages_in_batch} pages",
            progress=0,
        )

        async def allow_second_pass() -> bool:
            nonlocal second_pass_used
            async with second_pass_lock:
                if second_pass_used >= max_second_passes:
                    return False
                second_pass_used += 1
                return True

        # ğŸš€ å§‹ç»ˆä½¿ç”¨ grade_student ä¸€æ¬¡ LLM call æ‰¹æ”¹æ•´ä¸ªå­¦ç”Ÿï¼ˆé¿å…é€é¡µæµªè´¹ tokenï¼‰
        async def stream_callback(stream_type: str, chunk: str) -> None:
            await _broadcast_progress(
                batch_id,
                {
                    "type": "llm_stream_chunk",
                    "nodeId": "grade_batch",
                    "nodeName": "Batch Grading",
                    "agentId": f"batch_{batch_index}",
                    "agentLabel": batch_student_key,
                    "streamType": stream_type,
                    "chunk": chunk,
                },
            )

        await _broadcast_progress(
            batch_id,
            {
                "type": "agent_update",
                "parentNodeId": "grade_batch",
                "agentId": f"batch_{batch_index}",
                "agentName": batch_student_key,
                "agentLabel": batch_student_key,
                "status": "running",
                "message": f"Grading {len(images)} pages...",
                "progress": 10,
            },
        )

        try:
            logger.info(f"[grade_batch] grade_student for {batch_student_key} pages={len(images)}")

            # grade_student - ä¸€æ¬¡æ€§æ‰¹æ”¹æ•´ä¸ªå­¦ç”Ÿ
            student_result = await reasoning_client.grade_student(
                images=images,
                student_key=batch_student_key,
                parsed_rubric=local_parsed_rubric,
                page_indices=page_indices,
                page_contexts=page_index_contexts,
                stream_callback=stream_callback,
            )

            # Convert to legacy page result format
            if student_result.get("status") == "completed":
                total_score = student_result.get("total_score", 0)
                max_score = student_result.get("max_score", 0)
                question_details = student_result.get("question_details", [])

                page_results.append(
                    {
                        "page_index": page_indices[0] if page_indices else 0,
                        "page_indices": page_indices,
                        "status": "completed",
                        "score": total_score,
                        "max_score": max_score,
                        "confidence": student_result.get("confidence", 0.8),
                        "feedback": student_result.get("overall_feedback", ""),
                        "question_details": question_details,
                        "student_key": batch_student_key,
                        "student_name": batch_student_name,
                        "student_id": batch_student_id,
                        "batch_index": batch_index,
                    }
                )
                
                await emit_agent_update(
                    "completed",
                    f"Grading completed: {total_score}/{max_score}",
                    progress=100,
                )
            else:
                error_msg = student_result.get("error", "Unknown error")
                logger.error(
                    f"[grade_batch] grade_student failed for {batch_student_key}: {error_msg}"
                )
                
                await emit_agent_update(
                    "failed",
                    f"Grading failed: {error_msg}",
                    progress=0,
                )
                
                page_results.append(
                    {
                        "page_index": page_indices[0] if page_indices else 0,
                        "page_indices": page_indices,
                        "status": "failed",
                        "error": error_msg,
                        "student_key": batch_student_key,
                        "student_name": batch_student_name,
                        "student_id": batch_student_id,
                        "batch_index": batch_index,
                    }
                )

        except Exception as exc:
            logger.error(
                f"[grade_batch] Unexpected exception for {batch_student_key}: {exc}",
                exc_info=True
            )
            
            await emit_agent_update(
                "failed",
                f"System error: {str(exc)[:100]}",
                progress=0,
            )
            
            page_results.append(
                {
                    "page_index": page_indices[0] if page_indices else 0,
                    "page_indices": page_indices,
                    "status": "failed",
                    "error": f"System exception: {str(exc)}",
                    "student_key": batch_student_key,
                    "student_name": batch_student_name,
                    "student_id": batch_student_id,
                    "batch_index": batch_index,
                }
            )

    except Exception as e:
        batch_error = str(e)
        logger.error(f"[grade_batch] æ‰¹æ¬¡ {batch_index} æ‰¹æ”¹å¤±è´¥: {e}", exc_info=True)
        try:
            await emit_agent_update("failed", "Batch failed", progress=100)
        except Exception:
            pass

        # è®°å½•æ‰¹æ¬¡çº§é”™è¯¯
        from src.utils.error_handling import get_error_manager

        error_manager = get_error_manager()
        error_manager.add_error(
            exc=e,
            context={
                "batch_id": batch_id,
                "batch_index": batch_index,
                "function": "grade_batch_node",
                "retry_count": retry_count,
            },
            batch_id=batch_id,
            retry_count=retry_count,
        )

        # æ‰¹æ¬¡å¤±è´¥é‡è¯•é€»è¾‘ (Requirements: 3.3, 9.3)
        if retry_count < max_retries:
            logger.info(
                f"[grade_batch] æ‰¹æ¬¡ {batch_index} å°†è¿›è¡Œé‡è¯• " f"({retry_count + 1}/{max_retries})"
            )
            # è¿”å›é‡è¯•æ ‡è®°ï¼Œè®©è°ƒåº¦å™¨é‡æ–°è°ƒåº¦
            return {
                "grading_results": [],
                "batch_retry_needed": {
                    "batch_index": batch_index,
                    "retry_count": retry_count + 1,
                    "error": batch_error,
                },
            }

        # æ‰€æœ‰é¡µé¢æ ‡è®°ä¸ºå¤±è´¥
        for page_idx in page_indices:
            page_results.append(
                {
                    "page_index": page_idx,
                    "status": "failed",
                    "error": batch_error,
                    "score": 0,
                    "max_score": 0,
                    "batch_index": batch_index,
                    "grading_mode": grading_mode,
                }
            )

    success_count = sum(1 for r in page_results if r["status"] == "completed")
    failed_count = sum(1 for r in page_results if r["status"] == "failed")
    total_score = sum(r.get("score", 0) for r in page_results if r["status"] == "completed")

    # è¿›åº¦æŠ¥å‘Š (Requirement 3.4)
    progress_info = {
        "batch_index": batch_index,
        "total_batches": total_batches,
        "pages_processed": success_count,
        "pages_failed": failed_count,
        "total_score": total_score,
        "status": "completed" if failed_count == 0 else "partial",
        "timestamp": datetime.now().isoformat(),
    }

    logger.info(
        f"[grade_batch] æ‰¹æ¬¡ {batch_index + 1}/{total_batches} å®Œæˆ: "
        f"æˆåŠŸ={success_count}/{len(page_results)}, å¤±è´¥={failed_count}, æ€»åˆ†={total_score}"
    )

    final_status = "completed" if success_count > 0 else "failed"
    await _broadcast_progress(
        batch_id,
        {
            "type": "agent_update",
            "parentNodeId": "grade_batch",
            "agentId": f"batch_{batch_index}",
            "agentName": batch_student_key,
            "agentLabel": batch_student_key,
            "status": final_status,
            "message": f"Completed {success_count}/{len(page_results)} students",
            "progress": 100,
        },
    )

    # ===== ç›´æ¥æ„å»º student_results æ ¼å¼ï¼ˆç§»é™¤ simple_aggregate_node çš„éœ€è¦ï¼‰=====
    student_results = _build_student_results_from_page_results(
        page_results,
        default_student_key=batch_student_key,
        grading_mode=grading_mode,
    )

    logger.debug(
        f"[grade_batch] Page results summary: total={len(page_results)}, "
        f"success={success_count}, failed={failed_count}"
    )
    logger.debug(f"[grade_batch] Student results count: {len(student_results)}")

    # ğŸ” DEBUG: å…³é”®æ—¥å¿— - è®°å½• grade_batch è¿”å›
    logger.warning(
        f"[grade_batch] ğŸ” DEBUG: å‡†å¤‡è¿”å›ç»“æœ, batch_index={batch_index}, "
        f"student_key={batch_student_key}, student_results_count={len(student_results)}, "
        f"page_results_count={len(page_results)}"
    )

    # è¿”å›ç»“æœï¼ˆä½¿ç”¨ add reducer èšåˆï¼Œç›´æ¥è¾“å‡º student_resultsï¼‰
    result = {
        "student_results": student_results,
        "grading_results": page_results,  # ä¿ç•™ç”¨äºè°ƒè¯•/æ—¥å¿—
        "batch_progress": progress_info,
    }
    
    logger.warning(f"[grade_batch] ğŸ” DEBUG: è¿”å› result keys={list(result.keys())}")
    return result


def _apply_student_result_overrides(
    student_results: List[Dict[str, Any]],
    overrides: List[Dict[str, Any]],
) -> List[Dict[str, Any]]:
    if not overrides:
        return student_results

    overrides_by_key = {}
    for override in overrides:
        key = (
            override.get("studentKey")
            or override.get("student_key")
            or override.get("studentName")
            or override.get("student_name")
        )
        if key:
            overrides_by_key[key] = override

    for student in student_results:
        student_key = (
            student.get("student_key") or student.get("student_id") or student.get("student_name")
        )
        override = overrides_by_key.get(student_key)
        if not override:
            continue

        question_overrides = {}
        for q in override.get("questionResults", []) or override.get("question_results", []):
            qid = _normalize_question_id(q.get("questionId") or q.get("question_id"))
            if qid:
                question_overrides[qid] = q

        if student.get("question_details"):
            for q in student.get("question_details", []):
                qid = _normalize_question_id(q.get("question_id"))
                if not qid or qid not in question_overrides:
                    continue
                update = question_overrides[qid]
                if update.get("score") is not None:
                    q["score"] = float(update.get("score", q.get("score", 0)))
                if update.get("feedback") is not None:
                    q["feedback"] = update.get("feedback", q.get("feedback", ""))

        if student.get("page_results"):
            for page in student.get("page_results", []):
                if not page.get("question_details"):
                    continue
                for q in page.get("question_details", []):
                    qid = _normalize_question_id(q.get("question_id"))
                    if not qid or qid not in question_overrides:
                        continue
                    update = question_overrides[qid]
                    if update.get("score") is not None:
                        q["score"] = float(update.get("score", q.get("score", 0)))
                    if update.get("feedback") is not None:
                        q["feedback"] = update.get("feedback", q.get("feedback", ""))
                page["score"] = sum(q.get("score", 0) for q in page.get("question_details", []))

        if student.get("question_details"):
            student["total_score"] = sum(
                q.get("score", 0) for q in student.get("question_details", [])
            )
        elif student.get("page_results"):
            student["total_score"] = sum(p.get("score", 0) for p in student.get("page_results", []))

    return student_results


def _safe_float(value: Any, default: float = 0.0) -> float:
    try:
        if value is None:
            return default
        return float(value)
    except (TypeError, ValueError):
        return default


def _build_student_results_from_page_results(
    page_results: List[Dict[str, Any]],
    *,
    default_student_key: Optional[str] = None,
    grading_mode: Optional[str] = None,
) -> List[Dict[str, Any]]:
    if not page_results:
        return []

    grouped: Dict[str, Dict[str, Any]] = {}
    for result in page_results:
        student_key = result.get("student_key") or default_student_key or "Student"
        entry = grouped.get(student_key)
        if not entry:
            entry = {
                "student_key": student_key,
                "student_id": None,
                "student_name": None,
                "start_page": None,
                "end_page": None,
                "total_score": 0.0,
                "max_total_score": 0.0,
                "question_details": [],
                "page_results": [],
                "grading_mode": grading_mode,
                "feedback": "",
                "confidence": 0.0,
                "_confidence_sum": 0.0,
                "_confidence_count": 0,
                "_has_completed": False,
                "_has_failed": False,
                "_errors": [],
            }
            grouped[student_key] = entry

        entry["page_results"].append(result)
        if result.get("grading_mode"):
            entry["grading_mode"] = result.get("grading_mode")

        if result.get("student_id") and not entry.get("student_id"):
            entry["student_id"] = result.get("student_id")
        if result.get("student_name") and not entry.get("student_name"):
            entry["student_name"] = result.get("student_name")

        if not entry["feedback"] and result.get("feedback"):
            entry["feedback"] = result.get("feedback")

        if result.get("question_details"):
            entry["question_details"].extend(result.get("question_details", []))

        page_indices = result.get("page_indices")
        if not page_indices:
            page_index = result.get("page_index")
            if page_index is not None:
                page_indices = [page_index]

        if page_indices:
            start_page = min(page_indices)
            end_page = max(page_indices)
            entry["start_page"] = (
                start_page if entry["start_page"] is None else min(entry["start_page"], start_page)
            )
            entry["end_page"] = (
                end_page if entry["end_page"] is None else max(entry["end_page"], end_page)
            )

        entry["total_score"] += _safe_float(result.get("score", 0))
        entry["max_total_score"] += _safe_float(result.get("max_score", 0))

        if result.get("status") == "completed":
            entry["_has_completed"] = True
        if result.get("status") == "failed":
            entry["_has_failed"] = True
            if result.get("error"):
                entry["_errors"].append(result.get("error"))

        confidence = result.get("confidence")
        if confidence is not None:
            entry["_confidence_sum"] += _safe_float(confidence)
            entry["_confidence_count"] += 1

    student_results: List[Dict[str, Any]] = []
    for entry in grouped.values():
        if entry["start_page"] is None:
            entry["start_page"] = 0
        if entry["end_page"] is None:
            entry["end_page"] = entry["start_page"]
        if entry["_confidence_count"]:
            entry["confidence"] = entry["_confidence_sum"] / entry["_confidence_count"]
        if entry["_has_failed"] and not entry["_has_completed"]:
            entry["status"] = "failed"
        elif entry["_has_failed"] and entry["_has_completed"]:
            entry["status"] = "partial"
        if entry["_errors"]:
            entry["error"] = entry["_errors"][0]
        entry.pop("_confidence_sum", None)
        entry.pop("_confidence_count", None)
        entry.pop("_has_completed", None)
        entry.pop("_has_failed", None)
        entry.pop("_errors", None)
        student_results.append(entry)

    return student_results


def _merge_logic_review_fields(
    original_question: Dict[str, Any],
    review_item: Dict[str, Any]
) -> Dict[str, Any]:
    """
    åˆå¹¶é€»è¾‘å¤æ ¸ç»“æœåˆ°åŸå§‹é¢˜ç›®æ•°æ®ã€‚
    
    é€»è¾‘å¤æ ¸è¾“å‡ºçš„å­—æ®µï¼ˆconfidence, review_summary, review_corrections ç­‰ï¼‰
    ä¼šè¦†ç›–æˆ–è¡¥å……åŸå§‹é¢˜ç›®çš„å¯¹åº”å­—æ®µã€‚
    
    å…³é”®ï¼šé€»è¾‘å¤æ ¸åçš„ confidence åº”è¯¥ä½œä¸ºæœ€ç»ˆæ˜¾ç¤ºçš„ç½®ä¿¡åº¦ã€‚
    """
    merged = dict(original_question)
    
    # 1. æ›´æ–°ç½®ä¿¡åº¦ï¼ˆé€»è¾‘å¤æ ¸å†³å®šæœ€ç»ˆç½®ä¿¡åº¦ï¼‰
    if "confidence" in review_item:
        new_confidence = _safe_float(review_item["confidence"])
        merged["confidence"] = new_confidence
        audit = merged.get("audit") if isinstance(merged.get("audit"), dict) else {}
        audit = dict(audit)
        audit["confidence"] = new_confidence
        audit["updated_at"] = datetime.now().isoformat()
        merged["audit"] = audit
    if "confidence_reason" in review_item:
        merged["confidence_reason"] = review_item["confidence_reason"]
    if "confidenceReason" in review_item:
        merged["confidence_reason"] = review_item["confidenceReason"]
    
    # 2. æ›´æ–°è‡ªæˆ‘åæ€ç›¸å…³å­—æ®µ
    if "self_critique" in review_item:
        merged["self_critique"] = review_item["self_critique"]
    if "selfCritique" in review_item:
        merged["self_critique"] = review_item["selfCritique"]
    if "self_critique_confidence" in review_item:
        merged["self_critique_confidence"] = review_item["self_critique_confidence"]
    if "selfCritiqueConfidence" in review_item:
        merged["self_critique_confidence"] = review_item["selfCritiqueConfidence"]
    
    # 3. æ›´æ–°å¤æ ¸æ‘˜è¦
    if "review_summary" in review_item:
        merged["review_summary"] = review_item["review_summary"]
    if "reviewSummary" in review_item:
        merged["review_summary"] = review_item["reviewSummary"]
    
    # 4. å¤„ç†åˆ†æ•°ä¿®æ­£
    review_corrections = review_item.get("review_corrections") or review_item.get("reviewCorrections") or []
    if review_corrections:
        merged["review_corrections"] = review_corrections
        
        # åº”ç”¨åˆ†æ•°ä¿®æ­£åˆ° scoring_point_results
        scoring_results = merged.get("scoring_point_results") or []
        correction_map = {}
        for corr in review_corrections:
            if isinstance(corr, dict):
                point_id = corr.get("point_id") or corr.get("pointId")
                if point_id:
                    correction_map[point_id] = corr
        
        if scoring_results and correction_map:
            updated_scoring = []
            total_score_delta = 0
            for spr in scoring_results:
                point_id = spr.get("point_id") or spr.get("pointId")
                if point_id and point_id in correction_map:
                    corr = correction_map[point_id]
                    original_awarded = _safe_float(spr.get("awarded", 0))
                    corrected_awarded = _safe_float(corr.get("correct_awarded", corr.get("correctAwarded", original_awarded)))
                    
                    updated_spr = dict(spr)
                    updated_spr["awarded"] = corrected_awarded
                    updated_spr["review_adjusted"] = True
                    updated_spr["review_before"] = {
                        "awarded": original_awarded,
                        "decision": spr.get("decision"),
                        "reason": spr.get("reason"),
                        "evidence": spr.get("evidence"),
                    }
                    updated_spr["review_reason"] = corr.get("review_reason") or corr.get("reviewReason") or ""
                    
                    # æ›´æ–° decision
                    if corrected_awarded > 0:
                        updated_spr["decision"] = "å¾—åˆ†ï¼ˆå¤æ ¸ä¿®æ­£ï¼‰"
                    else:
                        updated_spr["decision"] = "ä¸å¾—åˆ†ï¼ˆå¤æ ¸ä¿®æ­£ï¼‰"
                    
                    total_score_delta += corrected_awarded - original_awarded
                    updated_scoring.append(updated_spr)
                else:
                    updated_scoring.append(dict(spr))
            
            merged["scoring_point_results"] = updated_scoring
            
            # é‡æ–°è®¡ç®—æ€»åˆ†
            if total_score_delta != 0:
                original_score = _safe_float(merged.get("score", 0))
                merged["score"] = max(0, original_score + total_score_delta)
    
    # 5. æ›´æ–° honesty_note
    if "honesty_note" in review_item:
        merged["honesty_note"] = review_item["honesty_note"]
    if "honestyNote" in review_item:
        merged["honesty_note"] = review_item["honestyNote"]
    
    # 6. æ ‡è®°å·²å¤æ ¸
    merged["logic_reviewed"] = True
    
    return merged


def _recompute_student_totals(student: Dict[str, Any]) -> None:
    total_score = _safe_float(student.get("total_score", 0))
    max_total_score = _safe_float(student.get("max_total_score", 0))

    question_details = student.get("question_details") or []
    if question_details:
        computed_score = sum(_safe_float(q.get("score", 0)) for q in question_details)
        computed_max = sum(
            _safe_float(q.get("max_score", q.get("maxScore", 0))) for q in question_details
        )
        if total_score <= 0 and computed_score > 0:
            student["total_score"] = computed_score
        if max_total_score <= 0 and computed_max > 0:
            student["max_total_score"] = computed_max
        return

    page_results = student.get("page_results") or []
    if page_results:
        computed_score = sum(_safe_float(p.get("score", 0)) for p in page_results)
        computed_max = sum(_safe_float(p.get("max_score", 0)) for p in page_results)
        if total_score <= 0 and computed_score > 0:
            student["total_score"] = computed_score
        if max_total_score <= 0 and computed_max > 0:
            student["max_total_score"] = computed_max


def _resolve_student_key_for_page(
    student_results: List[Dict[str, Any]],
    page_index: int,
) -> str:
    for student in student_results:
        start_page = student.get("start_page")
        end_page = student.get("end_page")
        if start_page is None or end_page is None:
            continue
        if start_page <= page_index <= end_page:
            return (
                student.get("student_key")
                or student.get("student_id")
                or student.get("student_name")
                or ""
            )
    return ""


def _find_question_pages(
    student_results: List[Dict[str, Any]],
    student_key: str,
    question_id: str,
    total_pages: int,
) -> List[int]:
    normalized_qid = _normalize_question_id(question_id)
    for student in student_results:
        key = (
            student.get("student_key")
            or student.get("student_id")
            or student.get("student_name")
            or ""
        )
        if student_key and key != student_key:
            continue
        for question in student.get("question_details", []) or []:
            qid = _normalize_question_id(question.get("question_id") or question.get("questionId"))
            if qid != normalized_qid:
                continue
            pages = _sanitize_pages(
                question.get("page_indices") or question.get("pageIndices") or [],
                total_pages,
            )
            if pages:
                return pages
        for page in student.get("page_results", []) or []:
            page_index = page.get("page_index")
            if page_index is None:
                continue
            for question in page.get("question_details", []) or []:
                qid = _normalize_question_id(
                    question.get("question_id") or question.get("questionId")
                )
                if qid == normalized_qid:
                    return [page_index]
    return []


def _select_best_question_result(
    current: Optional[Dict[str, Any]],
    candidate: Optional[Dict[str, Any]],
) -> Optional[Dict[str, Any]]:
    if not candidate:
        return current
    if not current:
        return candidate
    current_conf = _safe_float(current.get("confidence", 0))
    candidate_conf = _safe_float(candidate.get("confidence", 0))
    if candidate_conf > current_conf + 1e-6:
        return candidate
    if candidate_conf < current_conf - 1e-6:
        return current
    current_score = _safe_float(current.get("score", 0))
    candidate_score = _safe_float(candidate.get("score", 0))
    if candidate_score > current_score:
        return candidate
    if candidate_score < current_score:
        return current
    current_feedback = current.get("feedback", "") or ""
    candidate_feedback = candidate.get("feedback", "") or ""
    if len(candidate_feedback) > len(current_feedback):
        return candidate
    return current


def _apply_question_result_update(
    question: Dict[str, Any],
    update: Dict[str, Any],
) -> None:
    if update.get("score") is not None:
        question["score"] = _safe_float(update.get("score", question.get("score", 0)))
    if update.get("max_score") is not None:
        question["max_score"] = _safe_float(update.get("max_score", question.get("max_score", 0)))
    if update.get("feedback") is not None:
        question["feedback"] = update.get("feedback", question.get("feedback", ""))
    if update.get("confidence") is not None:
        question["confidence"] = _safe_float(
            update.get("confidence", question.get("confidence", 0))
        )
    scoring_points = update.get("scoring_point_results") or update.get("scoring_results")
    if scoring_points is not None:
        question_id = question.get("question_id") or question.get("questionId") or ""
        question["scoring_point_results"] = _normalize_scoring_point_results(
            scoring_points, question_id
        )
    if update.get("student_answer"):
        question["student_answer"] = update.get(
            "student_answer", question.get("student_answer", "")
        )
    if update.get("page_indices"):
        question["page_indices"] = update.get("page_indices", question.get("page_indices", []))


def _apply_regrade_updates(
    student_results: List[Dict[str, Any]],
    updates_by_student: Dict[str, Dict[str, Dict[str, Any]]],
) -> List[Dict[str, Any]]:
    if not updates_by_student:
        return student_results

    for student in student_results:
        student_key = (
            student.get("student_key")
            or student.get("student_id")
            or student.get("student_name")
            or ""
        )
        question_updates = updates_by_student.get(student_key)
        if not question_updates:
            continue

        if student.get("question_details"):
            for q in student.get("question_details", []):
                qid = _normalize_question_id(q.get("question_id") or q.get("questionId"))
                update = question_updates.get(qid)
                if update:
                    _apply_question_result_update(q, update)

        if student.get("page_results"):
            for page in student.get("page_results", []):
                page_questions = page.get("question_details") or []
                updated = False
                for q in page_questions:
                    qid = _normalize_question_id(q.get("question_id") or q.get("questionId"))
                    update = question_updates.get(qid)
                    if update:
                        _apply_question_result_update(q, update)
                        updated = True
                if updated:
                    page["score"] = sum(q.get("score", 0) for q in page_questions)

        if student.get("question_details"):
            student["total_score"] = sum(
                _safe_float(q.get("score", 0)) for q in student.get("question_details", [])
            )
        elif student.get("page_results"):
            student["total_score"] = sum(
                _safe_float(p.get("score", 0)) for p in student.get("page_results", [])
            )

    return student_results


async def _regrade_selected_questions(
    state: BatchGradingGraphState,
    student_results: List[Dict[str, Any]],
    regrade_items: List[Dict[str, Any]],
) -> List[Dict[str, Any]]:
    if not regrade_items:
        return student_results

    api_key = state.get("api_key") or os.getenv("LLM_API_KEY") or os.getenv("OPENROUTER_API_KEY")
    if not api_key:
        logger.warning("[review] regrade skipped: missing API key")
        return student_results

    processed_images = state.get("processed_images") or state.get("answer_images") or []
    total_pages = len(processed_images)
    if total_pages == 0:
        logger.warning("[review] regrade skipped: missing images")
        return student_results

    parsed_rubric = state.get("parsed_rubric", {})
    questions_data = parsed_rubric.get("questions", []) if isinstance(parsed_rubric, dict) else []

    try:
        from src.services.llm_reasoning import LLMReasoningClient
        from src.services.rubric_registry import RubricRegistry
        from src.models.grading_models import QuestionRubric, ScoringPoint
    except Exception as exc:
        logger.warning(f"[review] regrade skipped: {exc}")
        return student_results

    rubric_registry = RubricRegistry(total_score=parsed_rubric.get("total_score", 100.0))
    question_rubrics = []
    for q in questions_data:
        qid = q.get("question_id") or q.get("id") or ""
        if not qid:
            continue
        scoring_points = [
            ScoringPoint(
                description=sp.get("description", ""),
                score=sp.get("score", 0),
                is_required=sp.get("is_required", True),
                point_id=sp.get("point_id") or sp.get("pointId") or f"{qid}.{idx + 1}",
            )
            for idx, sp in enumerate(q.get("scoring_points", []))
        ]
        question_rubrics.append(
            QuestionRubric(
                question_id=str(qid),
                question_text=q.get("question_text", ""),
                max_score=q.get("max_score", 0),
                scoring_points=scoring_points,
                standard_answer=q.get("standard_answer", ""),
                grading_notes=q.get("grading_notes", ""),
                alternative_solutions=[],
            )
        )
    if question_rubrics:
        rubric_registry.register_rubrics(question_rubrics, log=False)

    reasoning_client = LLMReasoningClient(
        api_key=api_key,
        rubric_registry=rubric_registry,
    )

    student_page_map = state.get("student_page_map") or {}
    resolved_items: List[Dict[str, Any]] = []

    for item in regrade_items:
        if not isinstance(item, dict):
            continue
        question_id = _normalize_question_id(item.get("question_id") or item.get("questionId"))
        if not question_id:
            continue
        student_key = (
            item.get("student_key")
            or item.get("studentKey")
            or item.get("studentName")
            or item.get("student_name")
            or ""
        )
        raw_pages = (
            item.get("page_indices")
            or item.get("pageIndices")
            or item.get("page_index")
            or item.get("pageIndex")
        )
        if raw_pages is not None and not isinstance(raw_pages, (list, tuple)):
            raw_pages = [raw_pages]
        pages = _sanitize_pages(raw_pages, total_pages)
        if not pages:
            pages = _find_question_pages(student_results, student_key, question_id, total_pages)
        if not student_key and pages:
            student_key = student_page_map.get(pages[0]) or _resolve_student_key_for_page(
                student_results, pages[0]
            )
        if not student_key or not pages:
            logger.warning(
                f"[review] regrade skipped item: question={question_id}, student={student_key or 'unknown'}"
            )
            continue
        for page_index in pages:
            resolved_items.append(
                {
                    "student_key": student_key,
                    "question_id": question_id,
                    "page_index": page_index,
                    "notes": item.get("notes") or item.get("note") or "",
                }
            )

    if not resolved_items:
        return student_results

    updates_by_student: Dict[str, Dict[str, Dict[str, Any]]] = {}
    for item in resolved_items:
        page_index = item["page_index"]
        if not (0 <= page_index < total_pages):
            continue
        image = processed_images[page_index]
        try:
            result = await reasoning_client.grade_with_detailed_scoring_points(
                image=image,
                question_id=item["question_id"],
                page_index=page_index,
                reviewer_notes=item.get("notes") or "",
            )
            result_dict = result.to_dict()
            student_key = item["student_key"]
            qid = _normalize_question_id(item["question_id"])
            bucket = updates_by_student.setdefault(student_key, {})
            bucket[qid] = _select_best_question_result(bucket.get(qid), result_dict)
        except Exception as exc:
            logger.warning(
                f"[review] regrade failed: question={item['question_id']} page={page_index} error={exc}"
            )

    return _apply_regrade_updates(student_results, updates_by_student)


def _extract_scoring_points(question: Dict[str, Any]) -> List[Dict[str, Any]]:
    qid = _normalize_question_id(question.get("question_id") or question.get("questionId"))
    raw_points = question.get("scoring_point_results") or question.get("scoring_results") or []
    points: List[Dict[str, Any]] = []
    for spr in raw_points:
        if not isinstance(spr, dict):
            continue
        scoring_point = spr.get("scoring_point") or spr.get("scoringPoint") or {}
        description = (
            scoring_point.get("description")
            or spr.get("description")
            or spr.get("rubric_reference")
            or spr.get("rubricReference")
            or ""
        )
        point_id = spr.get("point_id") or spr.get("pointId") or scoring_point.get("point_id") or ""
        awarded = _safe_float(spr.get("awarded", spr.get("score", 0)))
        max_points = _safe_float(
            spr.get("max_points") or spr.get("maxPoints") or scoring_point.get("score") or 0
        )
        points.append(
            {
                "question_id": qid,
                "point_id": str(point_id) if point_id is not None else "",
                "description": description,
                "score": awarded,
                "max_score": max_points,
                "evidence": spr.get("evidence") or "",
                "rubric_reference": spr.get("rubric_reference") or spr.get("rubricReference") or "",
            }
        )
    return points


def _build_student_summary(student: Dict[str, Any]) -> Dict[str, Any]:
    total_score = _safe_float(student.get("total_score", 0))
    max_total_score = _safe_float(student.get("max_total_score", 0))
    percentage = (total_score / max_total_score * 100) if max_total_score > 0 else 0.0

    knowledge_points: List[Dict[str, Any]] = []
    weak_points: List[Dict[str, Any]] = []

    question_details = student.get("question_details") or []
    for question in question_details:
        for point in _extract_scoring_points(question):
            max_score = point.get("max_score", 0) or 0
            ratio = (point.get("score", 0) / max_score) if max_score > 0 else 0.0
            if ratio >= 0.85:
                mastery = "mastered"
            elif ratio >= 0.6:
                mastery = "partial"
            else:
                mastery = "weak"
            enriched = {
                **point,
                "mastery_level": mastery,
                "ratio": ratio,
            }
            knowledge_points.append(enriched)
            if mastery == "weak":
                weak_points.append(enriched)

    if not knowledge_points:
        for question in question_details:
            qid = _normalize_question_id(question.get("question_id") or question.get("questionId"))
            score = _safe_float(question.get("score", 0))
            max_score = _safe_float(question.get("max_score", 0))
            ratio = (score / max_score) if max_score > 0 else 0.0
            mastery = "partial" if ratio >= 0.6 else "weak"
            knowledge_points.append(
                {
                    "question_id": qid,
                    "point_id": "",
                    "description": question.get("feedback", "") or f"Question {qid}",
                    "score": score,
                    "max_score": max_score,
                    "mastery_level": mastery,
                    "ratio": ratio,
                    "evidence": "",
                    "rubric_reference": "",
                }
            )
            if mastery == "weak":
                weak_points.append(knowledge_points[-1])

    suggestion_candidates = []
    for point in weak_points:
        label = point.get("description") or f"Question {point.get('question_id', '')}"
        if label:
            suggestion_candidates.append(f"å»ºè®®å¤ä¹ ï¼š{label}")
    if not suggestion_candidates:
        for point in knowledge_points:
            if point.get("ratio", 0) < 0.7:
                label = point.get("description") or f"Question {point.get('question_id', '')}"
                if label:
                    suggestion_candidates.append(f"å»ºè®®å¤ä¹ ï¼š{label}")

    improvement_suggestions = []
    seen = set()
    for item in suggestion_candidates:
        if item not in seen:
            improvement_suggestions.append(item)
            seen.add(item)
        if len(improvement_suggestions) >= 5:
            break

    overall_parts = [f"æ•´ä½“å¾—åˆ† {total_score}/{max_total_score}ï¼ˆ{percentage:.1f}%ï¼‰ã€‚"]
    if percentage >= 85:
        overall_parts.append("æ•´ä½“è¡¨ç°ä¼˜ç§€ã€‚")
    elif percentage >= 70:
        overall_parts.append("æ•´ä½“è¡¨ç°è‰¯å¥½ã€‚")
    elif percentage >= 60:
        overall_parts.append("æ•´ä½“è¾¾åˆ°åŠæ ¼æ°´å¹³ã€‚")
    else:
        overall_parts.append("æ•´ä½“è¡¨ç°éœ€é‡ç‚¹æå‡ã€‚")

    if weak_points:
        weak_labels = []
        for point in weak_points[:3]:
            label = point.get("description") or f"Question {point.get('question_id', '')}"
            if label:
                weak_labels.append(label)
        if weak_labels:
            overall_parts.append(f"è–„å¼±ç‚¹é›†ä¸­åœ¨ï¼š{'ï¼Œ'.join(weak_labels)}ã€‚")
    else:
        overall_parts.append("æš‚æ— æ˜æ˜¾è–„å¼±ç‚¹ã€‚")

    return {
        "overall": " ".join(overall_parts),
        "percentage": percentage,
        "knowledge_points": knowledge_points,
        "improvement_suggestions": improvement_suggestions,
        "generated_at": datetime.now().isoformat(),
    }


def _build_self_audit(student: Dict[str, Any]) -> Dict[str, Any]:
    question_details = student.get("question_details") or []
    issues: List[Dict[str, Any]] = []
    confidence_values: List[float] = []

    for question in question_details:
        qid = _normalize_question_id(question.get("question_id") or question.get("questionId"))
        confidence = _safe_float(question.get("confidence", 0), 0.0)
        if confidence:
            confidence_values.append(confidence)

        if confidence and confidence < 0.7:
            issues.append(
                {
                    "issue_type": "low_confidence",
                    "message": f"é¢˜ç›® {qid} è¯„åˆ†ç½®ä¿¡åº¦è¾ƒä½",
                    "question_id": qid,
                }
            )

        review_corrections = question.get("review_corrections") or []
        if review_corrections:
            issues.append(
                {
                    "issue_type": "logic_review_adjusted",
                    "message": f"é¢˜ç›® {qid} å­˜åœ¨é€»è¾‘å¤æ ¸ä¿®æ­£è®°å½•",
                    "question_id": qid,
                }
            )

        if not question.get("self_critique"):
            issues.append(
                {
                    "issue_type": "missing_self_critique",
                    "message": f"é¢˜ç›® {qid} ç¼ºå°‘è‡ªç™½è¯´æ˜",
                    "question_id": qid,
                }
            )

        scoring_points = (
            question.get("scoring_point_results") or question.get("scoring_results") or []
        )
        if not scoring_points:
            issues.append(
                {
                    "issue_type": "missing_scoring_points",
                    "message": f"é¢˜ç›® {qid} ç¼ºå°‘è¯„åˆ†ç‚¹æ˜ç»†",
                    "question_id": qid,
                }
            )
        else:
            missing_evidence = False
            missing_rubric_ref = False
            for spr in scoring_points:
                if not isinstance(spr, dict):
                    continue
                evidence = spr.get("evidence")
                if _is_placeholder_evidence(evidence):
                    missing_evidence = True
                rubric_ref = spr.get("rubric_reference") or spr.get("rubricReference")
                if not rubric_ref:
                    missing_rubric_ref = True
            if missing_evidence:
                issues.append(
                    {
                        "issue_type": "missing_evidence",
                        "message": f"é¢˜ç›® {qid} éƒ¨åˆ†è¯„åˆ†ç‚¹è¯æ®ä¸è¶³",
                        "question_id": qid,
                    }
                )
            if missing_rubric_ref and not question.get("rubric_refs"):
                issues.append(
                    {
                        "issue_type": "missing_rubric_ref",
                        "message": f"é¢˜ç›® {qid} éƒ¨åˆ†è¯„åˆ†ç‚¹ç¼ºå°‘æ ‡å‡†å¼•ç”¨",
                        "question_id": qid,
                    }
                )

        typo_notes = question.get("typo_notes") or question.get("typoNotes") or []
        if typo_notes:
            issues.append(
                {
                    "issue_type": "typo_detected",
                    "message": f"é¢˜ç›® {qid} å‘ç°é”™åˆ«å­—æ ‡æ³¨",
                    "question_id": qid,
                }
            )

    issue_types = {issue.get("issue_type") for issue in issues}
    low_confidence_questions = [
        issue.get("question_id")
        for issue in issues
        if issue.get("issue_type") == "low_confidence" and issue.get("question_id")
    ]

    compliance_analysis = [
        {
            "goal": "ä¸¥æ ¼æŒ‰è¯„åˆ†æ ‡å‡†ç»™åˆ†",
            "tag": (
                "unsure_not_reported" if "missing_rubric_ref" in issue_types else "fully_complied"
            ),
            "notes": (
                "éƒ¨åˆ†è¯„åˆ†ç‚¹ç¼ºå°‘æ ‡å‡†å¼•ç”¨"
                if "missing_rubric_ref" in issue_types
                else "æœªå‘ç°æ˜æ˜¾åç¦»è¯„åˆ†æ ‡å‡†"
            ),
        },
        {
            "goal": "æ‰£åˆ†ç‚¹éœ€æœ‰ç­”æ¡ˆè¯æ®",
            "tag": "failed_not_reported" if "missing_evidence" in issue_types else "fully_complied",
            "notes": (
                "å­˜åœ¨è¯æ®ä¸è¶³çš„è¯„åˆ†ç‚¹" if "missing_evidence" in issue_types else "è¯„åˆ†ç‚¹è¯æ®å……è¶³"
            ),
        },
        {
            "goal": "ä¸ç¡®å®šæ€§éœ€æ˜ç¡®æŠ«éœ²",
            "tag": "unsure_not_reported" if "low_confidence" in issue_types else "fully_complied",
            "notes": (
                "å­˜åœ¨ä½ç½®ä¿¡åº¦é¢˜ç›®" if "low_confidence" in issue_types else "æœªå‘ç°æ˜æ˜¾ä¸ç¡®å®šæ€§"
            ),
        },
    ]

    uncertainties_and_conflicts = []
    if low_confidence_questions:
        uncertainties_and_conflicts.append(
            {
                "issue": "éƒ¨åˆ†é¢˜ç›®è¯„åˆ†ç½®ä¿¡åº¦ä¸è¶³",
                "impact": "å¯èƒ½å¯¼è‡´è¯„åˆ†åå·®",
                "question_ids": low_confidence_questions,
                "reported_to_user": False,
            }
        )

    avg_confidence = sum(confidence_values) / len(confidence_values) if confidence_values else 0.7
    penalty = min(0.4, 0.05 * len(issues))
    audit_confidence = max(0.1, min(1.0, avg_confidence - penalty))
    base_grade = 7
    if "missing_evidence" in issue_types:
        base_grade -= 2
    if "missing_rubric_ref" in issue_types:
        base_grade -= 1
    if "low_confidence" in issue_types:
        base_grade -= 1
    if "missing_self_critique" in issue_types:
        base_grade -= 1
    overall_compliance_grade = max(1, min(7, base_grade))

    if issues:
        issue_labels = [issue.get("message", "") for issue in issues[:3] if issue.get("message")]
        summary = f"å‘ç° {len(issues)} é¡¹å¯ç–‘ç‚¹ï¼Œå»ºè®®å¤æ ¸ï¼š{'ï¼›'.join(issue_labels)}ã€‚"
    else:
        summary = "æœªå‘ç°æ˜æ˜¾å¯ç–‘ç‚¹ï¼Œç»“æœä¸€è‡´æ€§è‰¯å¥½ã€‚"

    return {
        "summary": summary,
        "confidence": audit_confidence,
        "issues": issues,
        "compliance_analysis": compliance_analysis,
        "uncertainties_and_conflicts": uncertainties_and_conflicts,
        "overall_compliance_grade": overall_compliance_grade,
        "generated_at": datetime.now().isoformat(),
    }


def _collect_review_reasons(
    question: Dict[str, Any],
    confidence_threshold: float,
) -> List[str]:
    reasons: List[str] = []
    confidence = _safe_float(question.get("confidence", 0))
    if confidence < confidence_threshold:
        reasons.append("low_confidence")

    audit_flags = question.get("audit_flags") or []
    for flag in audit_flags:
        if flag not in reasons:
            reasons.append(flag)

    if question.get("review_corrections"):
        reasons.append("logic_review_adjusted")

    return reasons


def _apply_review_flags_and_queue(
    student_results: List[Dict[str, Any]],
    confidence_threshold: float,
) -> Tuple[List[Dict[str, Any]], List[Dict[str, Any]]]:
    queue_map: Dict[str, Dict[str, Any]] = {}
    low_confidence_questions: List[Dict[str, Any]] = []

    for student in student_results:
        student_key = (
            student.get("student_key")
            or student.get("student_id")
            or student.get("student_name")
            or ""
        )

        if student.get("needs_confirmation"):
            key = f"boundary:{student_key}"
            queue_map.setdefault(
                key,
                {
                    "type": "boundary",
                    "student_key": student_key,
                    "start_page": student.get("start_page"),
                    "end_page": student.get("end_page"),
                    "confidence": _safe_float(student.get("confidence", 0)),
                    "reasons": ["boundary_needs_confirmation"],
                },
            )

        self_audit = student.get("self_audit") or {}
        compliance_grade = _safe_float(self_audit.get("overall_compliance_grade"))
        if compliance_grade and compliance_grade <= 3:
            key = f"confession:{student_key}"
            queue_map.setdefault(
                key,
                {
                    "type": "confession",
                    "student_key": student_key,
                    "confidence": _safe_float(self_audit.get("confidence", 0)),
                    "compliance_grade": compliance_grade,
                    "reasons": ["confession_low_grade"],
                },
            )

        for question in student.get("question_details", []) or []:
            qid = _normalize_question_id(question.get("question_id") or question.get("questionId"))
            if not qid:
                continue
            reasons = _collect_review_reasons(question, confidence_threshold)
            if not reasons:
                continue
            question["needs_review"] = True
            question["review_reasons"] = reasons

            if "low_confidence" in reasons:
                low_confidence_questions.append(
                    {
                        "student_key": student_key,
                        "question_id": qid,
                        "confidence": _safe_float(question.get("confidence", 0)),
                    }
                )

            page_indices = question.get("page_indices") or question.get("pageIndices") or []
            key = f"question:{student_key}:{qid}"
            existing = queue_map.get(key)
            if existing:
                merged_reasons = set(existing.get("reasons") or [])
                merged_reasons.update(reasons)
                existing["reasons"] = list(merged_reasons)
                if page_indices:
                    existing_pages = set(existing.get("page_indices") or [])
                    existing_pages.update(page_indices)
                    existing["page_indices"] = sorted(existing_pages)
                continue

            queue_map[key] = {
                "type": "question",
                "student_key": student_key,
                "question_id": qid,
                "page_indices": page_indices,
                "confidence": _safe_float(question.get("confidence", 0)),
                "reasons": reasons,
            }

        for page in student.get("page_results", []) or []:
            page_confidence = _safe_float(page.get("confidence", 1.0))
            if page_confidence < confidence_threshold:
                page["needs_review"] = True
                page["review_reasons"] = ["low_confidence"]

    return list(queue_map.values()), low_confidence_questions


def _build_class_report(student_results: List[Dict[str, Any]]) -> Dict[str, Any]:
    total_students = len(student_results)
    if total_students == 0:
        return {
            "total_students": 0,
            "generated_at": datetime.now().isoformat(),
        }

    total_scores = []
    total_percentages = []
    distribution = {"A": 0, "B": 0, "C": 0, "D": 0, "E": 0}
    knowledge_aggregate: Dict[str, Dict[str, Any]] = {}

    for student in student_results:
        total_score = _safe_float(student.get("total_score", 0))
        max_score = _safe_float(student.get("max_total_score", 0))
        percentage = (total_score / max_score * 100) if max_score > 0 else 0.0
        total_scores.append(total_score)
        total_percentages.append(percentage)

        if percentage >= 85:
            distribution["A"] += 1
        elif percentage >= 70:
            distribution["B"] += 1
        elif percentage >= 60:
            distribution["C"] += 1
        elif percentage >= 50:
            distribution["D"] += 1
        else:
            distribution["E"] += 1

        summary = student.get("student_summary") or _build_student_summary(student)
        for point in summary.get("knowledge_points", []):
            key = point.get("point_id") or point.get("description") or ""
            if not key:
                continue
            entry = knowledge_aggregate.setdefault(
                key,
                {
                    "point_id": point.get("point_id") or "",
                    "description": point.get("description") or "",
                    "total_score": 0.0,
                    "total_max_score": 0.0,
                },
            )
            entry["total_score"] += _safe_float(point.get("score", 0))
            entry["total_max_score"] += _safe_float(point.get("max_score", 0))

    average_score = sum(total_scores) / total_students if total_students else 0.0
    average_percentage = sum(total_percentages) / total_students if total_students else 0.0
    pass_rate = (
        sum(1 for pct in total_percentages if pct >= 60) / total_students if total_students else 0.0
    )

    weak_points = []
    strong_points = []
    for entry in knowledge_aggregate.values():
        max_score = entry.get("total_max_score", 0) or 0
        ratio = (entry.get("total_score", 0) / max_score) if max_score > 0 else 0.0
        record = {
            "point_id": entry.get("point_id"),
            "description": entry.get("description"),
            "mastery_ratio": ratio,
        }
        if ratio < 0.6:
            weak_points.append(record)
        elif ratio >= 0.85:
            strong_points.append(record)

    weak_points.sort(key=lambda x: x.get("mastery_ratio", 0))
    strong_points.sort(key=lambda x: x.get("mastery_ratio", 0), reverse=True)

    summary_parts = [
        f"ç­çº§å¹³å‡åˆ† {average_score:.1f}ï¼Œå¹³å‡å¾—åˆ†ç‡ {average_percentage:.1f}%ã€‚",
        f"åŠæ ¼ç‡ {pass_rate * 100:.1f}%ã€‚",
    ]
    if weak_points:
        weak_labels = [p.get("description", "") for p in weak_points[:3] if p.get("description")]
        if weak_labels:
            summary_parts.append(f"ä¸»è¦è–„å¼±çŸ¥è¯†ç‚¹ï¼š{'ï¼Œ'.join(weak_labels)}ã€‚")
    if strong_points:
        strong_labels = [
            p.get("description", "") for p in strong_points[:3] if p.get("description")
        ]
        if strong_labels:
            summary_parts.append(f"ä¼˜åŠ¿çŸ¥è¯†ç‚¹ï¼š{'ï¼Œ'.join(strong_labels)}ã€‚")

    return {
        "total_students": total_students,
        "average_score": average_score,
        "average_percentage": average_percentage,
        "pass_rate": pass_rate,
        "score_distribution": distribution,
        "weak_points": weak_points[:10],
        "strong_points": strong_points[:10],
        "summary": " ".join(summary_parts),
        "generated_at": datetime.now().isoformat(),
    }


def _apply_student_result_overrides(
    student_results: List[Dict[str, Any]],
    overrides: List[Dict[str, Any]],
) -> List[Dict[str, Any]]:
    """åº”ç”¨å­¦ç”Ÿç»“æœè¦†ç›–"""
    if not overrides:
        return student_results

    # æ„å»ºè¦†ç›–æ˜ å°„
    override_map = {}
    for item in overrides:
        key = item.get("student_key") or item.get("studentKey")
        if key:
            override_map[key] = item

    updated_results = []
    for student in student_results:
        student_key = student.get("student_key")
        if student_key not in override_map:
            updated_results.append(student)
            continue

        override = override_map[student_key]
        updated_student = student.copy()

        # æ„å»ºé¢˜ç›®è¦†ç›–æ˜ å°„
        q_override_map = {}
        for q in override.get("questionResults") or override.get("question_results") or []:
            qid = _normalize_question_id(q.get("questionId") or q.get("question_id"))
            if qid:
                q_override_map[qid] = q

        # æ›´æ–° question_details
        current_details = student.get("question_details") or []
        updated_details = []
        for q in current_details:
            qid = _normalize_question_id(q.get("question_id"))
            if qid in q_override_map:
                logger.info(f"[review] applying override for student={student_key} question={qid}")
                q_override = q_override_map[qid]
                updated_q = q.copy()

                # æ›´æ–°åˆ†æ•°
                if "score" in q_override:
                    updated_q["score"] = float(q_override["score"])

                # æ›´æ–°åé¦ˆ
                if "feedback" in q_override:
                    updated_q["feedback"] = q_override["feedback"]

                updated_details.append(updated_q)
            else:
                updated_details.append(q)

        updated_student["question_details"] = updated_details

        # é‡æ–°è®¡ç®—æ€»åˆ†
        updated_student["total_score"] = sum(float(q.get("score", 0)) for q in updated_details)

        updated_results.append(updated_student)

    return updated_results


def _collect_question_details(student: Dict[str, Any]) -> List[Dict[str, Any]]:
    """Collect question details from student or page_results with page index fallback."""
    details = student.get("question_details") or []
    if not isinstance(details, list) or not details:
        fallback: List[Dict[str, Any]] = []
        for page in student.get("page_results", []) or []:
            for q in page.get("question_details", []) or []:
                merged = dict(q)
                if not merged.get("page_indices") and page.get("page_index") is not None:
                    merged["page_indices"] = [page.get("page_index")]
                fallback.append(merged)
        details = fallback
    return details if isinstance(details, list) else []


# _extract_confession_questions å‡½æ•°å·²åˆ é™¤ï¼ˆæ‰¹æ”¹å’Œå®¡è®¡ä¸€ä½“åŒ–æ”¹é€ ï¼‰


def _extract_logic_review_questions(student: Dict[str, Any]) -> List[Dict[str, Any]]:
    """
    Extract questions that require logic review based on audit signals.
    
    æ”¹é€ è¯´æ˜ï¼š
    - ä¸å†ä¾èµ– confession æ•°æ®ï¼ˆå·²åˆ é™¤ï¼‰
    - æ”¹ä¸ºåŸºäºé¢˜ç›®çš„ audit ä¿¡æ¯ï¼ˆrisk_flags, needs_review, confidenceï¼‰
    - å¦‚æœ audit ä¿¡æ¯ä¸ºç©ºï¼Œåˆ™ä½¿ç”¨å¯å‘å¼è§„åˆ™
    """
    details = _collect_question_details(student)

    if not details:
        return []

    flagged_question_ids: set = set()
    confidence_threshold = float(os.getenv("LOGIC_REVIEW_CONFIDENCE_THRESHOLD", "0.7"))

    # åŸºäº audit ä¿¡æ¯ç­›é€‰éœ€è¦å¤æ ¸çš„é¢˜ç›®
    for q in details:
        qid = _normalize_question_id(q.get("question_id") or q.get("questionId") or "")
        audit = q.get("audit") or {}
        
        # 1. æ£€æŸ¥æ˜¯å¦æ ‡è®°ä¸ºéœ€è¦å¤æ ¸
        if audit.get("needs_review"):
            flagged_question_ids.add(qid)
            continue
        
        # 2. æ£€æŸ¥ç½®ä¿¡åº¦
        confidence = audit.get("confidence")
        if confidence is not None:
            try:
                if float(confidence) < confidence_threshold:
                    flagged_question_ids.add(qid)
                    continue
            except (ValueError, TypeError):
                pass
        
        # 3. æ£€æŸ¥é£é™©æ ‡è®°
        risk_flags = audit.get("risk_flags") or []
        high_risk_flags = ["full_marks", "zero_marks", "boundary_score", "low_confidence", "evidence_gap"]
        if isinstance(risk_flags, list) and any(flag in high_risk_flags for flag in risk_flags):
            flagged_question_ids.add(qid)
            continue
        
        # 4. æ£€æŸ¥ä¸ç¡®å®šç‚¹
        uncertainties = audit.get("uncertainties") or []
        if isinstance(uncertainties, list) and len(uncertainties) > 0:
            flagged_question_ids.add(qid)

    # å¦‚æœæ²¡æœ‰æ ‡è®°ä»»ä½•é¢˜ç›®ï¼Œä½¿ç”¨å¯å‘å¼è§„åˆ™
    if not flagged_question_ids:
        force_all = os.getenv("LOGIC_REVIEW_FORCE_ALL", "true").lower() in ("1", "true", "yes")
        if not force_all:
            logger.debug("[_extract_logic_review_questions] no flagged questions, skipping logic review")
            return []
        
        # å¼ºåˆ¶å…¨éƒ¨å¤æ ¸æ¨¡å¼ï¼šåŸºäºå¯å‘å¼è§„åˆ™é€‰æ‹©é«˜é£é™©é¢˜ç›®
        for q in details:
            qid = _normalize_question_id(q.get("question_id") or q.get("questionId") or "")
            score = q.get("score", 0)
            max_score = q.get("max_score", 0)
            confidence = q.get("confidence", 1.0)
            
            # æ»¡åˆ†æˆ–é›¶åˆ†
            if max_score > 0 and (score >= max_score or score == 0):
                flagged_question_ids.add(qid)
            # ä½ç½®ä¿¡åº¦
            elif confidence < confidence_threshold:
                flagged_question_ids.add(qid)

    max_questions = int(os.getenv("LOGIC_REVIEW_MAX_QUESTIONS", "0"))
    
    # å¦‚æœä»ç„¶æ²¡æœ‰æ ‡è®°ä»»ä½•é¢˜ç›®ï¼Œè¿”å›ç©ºåˆ—è¡¨æˆ–å…¨éƒ¨é¢˜ç›®
    if not flagged_question_ids:
        if max_questions > 0:
            return details[:max_questions]
        return details if force_all else []

    # æ”¶é›†è¢«æ ‡è®°çš„é¢˜ç›®
    review_questions: List[Dict[str, Any]] = []
    for q in details:
        qid = _normalize_question_id(q.get("question_id") or q.get("questionId") or "")
        if qid in flagged_question_ids:
            review_questions.append(q)

    if max_questions > 0:
        return review_questions[:max_questions]
    return review_questions


# _build_confession_prompt å‡½æ•°å·²åˆ é™¤ï¼ˆæ‰¹æ”¹å’Œå®¡è®¡ä¸€ä½“åŒ–æ”¹é€ ï¼‰


def _extract_json_from_response(text: str) -> str:
    """
    ä» LLM å“åº”ä¸­æå– JSON å†…å®¹
    
    æ”¯æŒä»¥ä¸‹æ ¼å¼ï¼š
    1. ```json ... ```
    2. çº¯ JSONï¼ˆä»¥ { å¼€å¤´ï¼‰
    3. åŒ…å«å…¶ä»–æ–‡æœ¬çš„æ··åˆå†…å®¹
    """
    if not text:
        return "{}"
    
    # å°è¯•æå– ```json ... ``` å—
    json_match = re.search(r'```json\s*\n(.*?)\n```', text, re.DOTALL)
    if json_match:
        return json_match.group(1).strip()
    
    # å°è¯•æå– ``` ... ``` å—ï¼ˆä¸å¸¦ json æ ‡è®°ï¼‰
    code_match = re.search(r'```\s*\n(.*?)\n```', text, re.DOTALL)
    if code_match:
        content = code_match.group(1).strip()
        if content.startswith('{') or content.startswith('['):
            return content
    
    # å°è¯•æå– { ... } æˆ– [ ... ]
    start = text.find('{')
    if start == -1:
        start = text.find('[')
    
    if start != -1:
        # æ‰¾åˆ°åŒ¹é…çš„ç»“æŸæ‹¬å·
        bracket_count = 0
        is_array = text[start] == '['
        end_char = ']' if is_array else '}'
        start_char = '[' if is_array else '{'
        
        for i in range(start, len(text)):
            if text[i] == start_char:
                bracket_count += 1
            elif text[i] == end_char:
                bracket_count -= 1
                if bracket_count == 0:
                    return text[start:i+1]
    
    # å¦‚æœéƒ½å¤±è´¥äº†ï¼Œè¿”å›åŸæ–‡æœ¬
    return text.strip()


# confession_node å‡½æ•°å·²åˆ é™¤ï¼ˆæ‰¹æ”¹å’Œå®¡è®¡ä¸€ä½“åŒ–æ”¹é€ ï¼‰
# åŸ confession èŠ‚ç‚¹çš„åŠŸèƒ½å·²åˆå¹¶åˆ° grade_batch èŠ‚ç‚¹ä¸­


def _build_logic_review_prompt(
    student: Dict[str, Any],
    question_details: List[Dict[str, Any]],
    rubric_map: Dict[str, Dict[str, Any]],
    limits: Dict[str, int],
) -> str:
    """
    æ„å»ºé€»è¾‘å¤æ ¸ (Logic Review) LLM æç¤ºè¯
    
    æ”¹é€ è¯´æ˜ï¼ˆæ‰¹æ”¹å’Œå®¡è®¡ä¸€ä½“åŒ–ï¼‰ï¼š
    - ç§»é™¤äº† confession å‚æ•°ï¼ˆå·²åˆ é™¤ confession èŠ‚ç‚¹ï¼‰
    - æ”¹ä¸ºç›´æ¥ä½¿ç”¨é¢˜ç›®ä¸­çš„ audit ä¿¡æ¯è¿›è¡Œå¤æ ¸å†³ç­–
    - audit ä¿¡æ¯åŒ…å«ï¼šconfidence, uncertainties, risk_flags, needs_review

    é€»è¾‘å¤æ ¸çš„æ ¸å¿ƒåŠŸèƒ½ï¼šéªŒè¯/å®¡è®¡ + ä¸€è‡´æ€§ä¿®å¤
    - åªèƒ½åŸºäºæ‰¹æ”¹ç»“æœã€è¯„åˆ†æ ‡å‡†è§£æç»“æœå’Œå®¡è®¡ä¿¡æ¯
    - ä¸å…è®¸å¼•å…¥æ–°äº‹å®/æ–°æ¨ç†
    - è¦æœ‰æ‰¹åˆ¤æ€§æ€ç»´ï¼ŒæŸ¥æ¼è¡¥ç¼º
    - å…·å¤‡æœ‰é™çš„ä¿®æ­£èƒ½åŠ›ï¼ˆæ˜æ˜¾é”™è¯¯ï¼‰

    âš ï¸ é‡è¦ï¼šé€»è¾‘å¤æ ¸ç‹¬ç«‹æ€§åŸåˆ™ (P3)
    =========================================
    æ­¤å‡½æ•°æ„å»ºçš„ prompt ä¸èƒ½åŒ…å«ä»»ä½•è®°å¿†ç³»ç»Ÿçš„æ•°æ®ï¼

    é€»è¾‘å¤æ ¸å¿…é¡»æ˜¯"æ— çŠ¶æ€"çš„ï¼š
    1. ä¸èƒ½å¼•ç”¨å†å²æ‰¹æ”¹ç»éªŒæˆ–è®°å¿†
    2. è¯„åˆ†å†³ç­–å®Œå…¨åŸºäºå½“å‰è¯„åˆ†æ ‡å‡†å’Œå­¦ç”Ÿç­”æ¡ˆ
    3. audit ä¿¡æ¯ä»…ç”¨äºç¡®å®šå¤æ ¸é‡ç‚¹ï¼Œä¸ç›´æ¥å½±å“è¯„åˆ†

    å…è®¸çš„è¾“å…¥ï¼š
    - student: å½“å‰å­¦ç”Ÿçš„æ‰¹æ”¹ç»“æœ
    - question_details: å½“å‰æ‰¹æ”¹çš„é¢˜ç›®è¯¦æƒ…ï¼ˆå« audit ä¿¡æ¯ï¼‰
    - rubric_map: è¯„åˆ†æ ‡å‡†ï¼ˆä» parsed_rubric æ„å»ºï¼‰
    =========================================
    """
    student_key = student.get("student_key") or student.get("student_name") or "Unknown"
    max_questions = limits.get("max_questions", 20)
    if max_questions <= 0:
        max_questions = len(question_details)
    max_answer_chars = limits.get("max_answer_chars", 400)
    max_feedback_chars = limits.get("max_feedback_chars", 200)
    max_rubric_chars = limits.get("max_rubric_chars", 240)
    max_points = limits.get("max_scoring_points", 4)
    max_evidence_chars = limits.get("max_evidence_chars", 120)

    lines = [
        "# è§’è‰²ï¼šé€»è¾‘å¤æ ¸å®¡è®¡å‘˜ (Logic Review Auditor)",
        "",
        "ä½ æ˜¯ä¸€ä½ä¸¥è°¨çš„é€»è¾‘å¤æ ¸å®¡è®¡å‘˜ï¼Œä¸“é—¨è´Ÿè´£å®¡è®¡æ‰¹æ”¹ç»“æœä¸­çš„**æ˜æ˜¾é”™è¯¯**ã€‚",
        "",
        "## æ ¸å¿ƒåŸåˆ™ï¼ˆå¿…é¡»ä¸¥æ ¼éµå®ˆï¼‰",
        "",
        "### âš ï¸ æœ€é«˜ä¼˜å…ˆçº§ï¼šåªçº æ­£æ˜æ˜¾é”™è¯¯",
        "1. **åªä¿®æ­£æ˜æ˜¾çš„ã€æ— å¯äº‰è®®çš„é”™è¯¯**",
        "   - è¯æ®æ˜ç¡®è¯´æ­£ç¡®ä½†ç»™äº† 0 åˆ†",
        "   - è¯æ®æ˜ç¡®è¯´é”™è¯¯ä½†ç»™äº†åˆ†",
        "   - åˆ†æ•°è¶…å‡ºæ»¡åˆ†æˆ–ä¸ºè´Ÿæ•°",
        "   - å¾—åˆ†ç‚¹åˆ†æ•°ç´¯åŠ é”™è¯¯",
        "",
        "2. **ç»å¯¹ç¦æ­¢é…Œæƒ…ç»™åˆ†**",
        '   - ä¸å¾—å› ä¸º"å­¦ç”Ÿå¯èƒ½ç†è§£äº†"è€Œç»™åˆ†',
        '   - ä¸å¾—å› ä¸º"ç­”æ¡ˆæ¥è¿‘æ­£ç¡®"è€Œç»™éƒ¨åˆ†åˆ†ï¼ˆé™¤éè¯„åˆ†æ ‡å‡†æ˜ç¡®å…è®¸ï¼‰',
        '   - ä¸å¾—å› ä¸º"è§£é¢˜æ€è·¯æ­£ç¡®"è€Œç»™åˆ†ï¼ˆé™¤éè¯„åˆ†æ ‡å‡†æ˜ç¡®å…è®¸ï¼‰',
        "",
        "3. **ä¸¥æ ¼åŸºäºè¯„åˆ†æ ‡å‡†**",
        "   - æ‰€æœ‰ä¿®æ­£å¿…é¡»æœ‰è¯„åˆ†æ ‡å‡†ä¸­çš„æ˜ç¡®ä¾æ®",
        "   - å¦‚æœè¯„åˆ†æ ‡å‡†æœªè¦†ç›–æŸç§æƒ…å†µï¼Œ**ä¿ç•™åŸåˆ¤**",
        "   - ä¸å¾—è‡ªè¡Œè§£é‡Šæˆ–æ‰©å±•è¯„åˆ†æ ‡å‡†",
        "",
        "4. **æ‰¹åˆ¤æ€§æ€ç»´**",
        "   - å¯¹è‡ªç™½ä¸­æŠ«éœ²çš„é£é™©ç‚¹æŒæ€€ç–‘æ€åº¦ï¼Œç‹¬ç«‹éªŒè¯",
        '   - ä¸è¦è½»ä¿¡ä»»ä½•"å¯èƒ½"ã€"åº”è¯¥"çš„æ¨æµ‹',
        "   - å®å¯æ¼çº ä¹Ÿä¸å¯é”™çº ",
        "",
        "### ğŸ”´ æ— æ³•åˆ¤æ–­æ—¶çš„å¤„ç†",
        "å½“é‡åˆ°ä»¥ä¸‹æƒ…å†µæ—¶ï¼Œ**ä¸ä¿®æ­£**ï¼Œä½†å¿…é¡»ï¼š",
        "- é™ä½è¯¥é¢˜çš„ `confidence` å€¼ï¼ˆè®¾ä¸º 0.3-0.5ï¼‰",
        "- åœ¨ `honesty_note` ä¸­è¯¦ç»†è¯´æ˜æ— æ³•åˆ¤æ–­çš„åŸå› ",
        "- æ ‡è®° `self_critique_confidence` ä¸ºä½å€¼",
        "",
        "æ— æ³•åˆ¤æ–­çš„æƒ…å†µåŒ…æ‹¬ï¼š",
        "- è¯„åˆ†æ ‡å‡†ä¸å¤Ÿæ¸…æ™°",
        "- å­¦ç”Ÿç­”æ¡ˆè¡¨è¿°æ¨¡ç³Š",
        "- è¯æ®ä¸è¯„åˆ†æ ‡å‡†çš„å¯¹åº”å…³ç³»ä¸æ˜ç¡®",
        "- å­˜åœ¨å¤šç§åˆç†è§£é‡Š",
        "",
        "## æ£€æŸ¥ç»´åº¦",
        "1. **è¯æ®ä¸€è‡´æ€§**ï¼ševidence ä¸ awarded æ˜¯å¦ä¸€è‡´ï¼Ÿ",
        "2. **æ•°å­¦æ­£ç¡®æ€§**ï¼šåˆ†æ•°ç´¯åŠ æ˜¯å¦æ­£ç¡®ï¼Ÿæ˜¯å¦æº¢å‡ºï¼Ÿ",
        "3. **æ ‡å‡†ç¬¦åˆæ€§**ï¼šè¯„åˆ†æ˜¯å¦ç¬¦åˆè¯„åˆ†æ ‡å‡†çš„å­—é¢è¦æ±‚ï¼Ÿ",
        "",
        "## ä¿®æ­£å†³ç­–ï¼ˆä¸¥æ ¼æŒ‰æ­¤æ‰§è¡Œï¼‰",
        "```",
        "if è¯æ®ã€æ˜ç¡®ä¸”æ— æ­§ä¹‰åœ°ã€‘è¯´æ­£ç¡® and awarded == 0:",
        "    â†’ ä¿®æ­£ä¸ºå¾—åˆ†",
        "elif è¯æ®ã€æ˜ç¡®ä¸”æ— æ­§ä¹‰åœ°ã€‘è¯´é”™è¯¯ and awarded > 0:",
        "    â†’ ä¿®æ­£ä¸ºæ‰£åˆ†",
        "elif å¾—åˆ†è¶…å‡ºæ»¡åˆ† or å¾—åˆ†ä¸ºè´Ÿ:",
        "    â†’ ä¿®æ­£ä¸ºåˆç†è¾¹ç•Œå€¼",
        "elif åˆ†æ•°ç´¯åŠ æ˜æ˜¾é”™è¯¯:",
        "    â†’ ä¿®æ­£ç´¯åŠ ç»“æœ",
        "elif å­˜åœ¨ä»»ä½•ä¸ç¡®å®šæ€§:",
        "    â†’ ä¿ç•™åŸåˆ¤ + é™ä½ç½®ä¿¡åº¦ + å†™æ˜ honesty_note",
        "else:",
        "    â†’ ä¿ç•™åŸåˆ¤",
        "```",
        "",
        "## å¯ç”¨ä¿¡æ¯æºï¼ˆä»…é™è¿™äº›ï¼‰",
        "- æ‰¹æ”¹ç»“æœï¼ˆè¯„åˆ†ã€è¯æ®ã€åé¦ˆï¼‰",
        "- è¯„åˆ†æ ‡å‡†ï¼ˆrubricï¼‰â€”â€” **ä¿®æ­£çš„å”¯ä¸€ä¾æ®**",
        "- å®¡è®¡ä¿¡æ¯ï¼ˆä»…ä¾›å‚è€ƒï¼Œä¸ä½œä¸ºä¿®æ­£ä¾æ®ï¼‰",
        "",
        "## è¾“å‡ºå†…å®¹",
        "- **review_corrections**ï¼šåªåŒ…å«æ˜æ˜¾é”™è¯¯çš„ä¿®æ­£",
        "- **confidence**ï¼šè¯„åˆ†ç½®ä¿¡åº¦ï¼ˆæ— æ³•åˆ¤æ–­æ—¶è®¾ä¸º 0.3-0.5ï¼‰",
        "- **honesty_note**ï¼šæ— æ³•åˆ¤æ–­æ—¶çš„è¯¦ç»†è¯´æ˜",
        "",
        f"## å­¦ç”Ÿæ ‡è¯†: {student_key}",
        "",
    ]

    # æ·»åŠ å®¡è®¡ä¿¡æ¯æ‘˜è¦ï¼ˆåŸºäº audit å­—æ®µï¼‰
    high_risk_count = 0
    low_confidence_count = 0
    needs_review_count = 0
    risk_summary = []
    
    for q in question_details[:max_questions]:
        audit = q.get("audit") or {}
        qid = _normalize_question_id(q.get("question_id") or q.get("questionId")) or "?"
        
        # ç»Ÿè®¡é£é™©
        if audit.get("needs_review"):
            needs_review_count += 1
        
        confidence = audit.get("confidence", 1.0)
        if confidence < 0.6:
            low_confidence_count += 1
        
        risk_flags = audit.get("risk_flags") or []
        if any(flag in ["full_marks", "zero_marks", "evidence_gap"] for flag in risk_flags):
            high_risk_count += 1
            risk_summary.append(f"Q{qid}: {', '.join(risk_flags)}")
    
    if high_risk_count > 0 or low_confidence_count > 0 or needs_review_count > 0:
        lines.append("## å®¡è®¡é£é™©æ‘˜è¦ï¼ˆä¾›ä½ é‡ç‚¹å…³æ³¨ï¼‰")
        lines.append(f"- éœ€è¦å¤æ ¸é¢˜ç›®æ•°: {needs_review_count}")
        lines.append(f"- ä½ç½®ä¿¡åº¦é¢˜ç›®æ•°: {low_confidence_count}")
        lines.append(f"- é«˜é£é™©é¢˜ç›®æ•°: {high_risk_count}")
        
        if risk_summary:
            lines.append("- å…·ä½“é£é™©æ ‡è®°:")
            for summary in risk_summary[:5]:
                lines.append(f"  - {summary}")
        lines.append("")

    lines.append("## é¢˜ç›®æ‘˜è¦ï¼ˆä¾›ä½ åšä¸€è‡´æ€§æ£€æŸ¥ï¼‰")

    for idx, question in enumerate(question_details[:max_questions]):
        qid = _normalize_question_id(
            question.get("question_id") or question.get("questionId")
        ) or str(idx + 1)
        rubric = rubric_map.get(qid, {})
        score = question.get("score", 0)
        max_score = question.get("max_score", rubric.get("max_score", 0))
        question_text = _trim_text(rubric.get("question_text", ""), max_rubric_chars)
        standard_answer = _trim_text(rubric.get("standard_answer", ""), max_rubric_chars)
        student_answer = _trim_text(question.get("student_answer", ""), max_answer_chars)
        feedback = _trim_text(question.get("feedback", ""), max_feedback_chars)

        lines.append(f"- Q{qid}: score {score}/{max_score}")
        if question_text:
            lines.append(f"  prompt: {question_text}")
        if standard_answer:
            lines.append(f"  standard_answer: {standard_answer}")
        if student_answer:
            lines.append(f"  student_answer: {student_answer}")
        if feedback:
            lines.append(f"  feedback: {feedback}")

        scoring_points = (
            question.get("scoring_point_results") or question.get("scoring_results") or []
        )
        if scoring_points:
            lines.append("  scoring_points:")
            for sp in scoring_points[:max_points]:
                if not isinstance(sp, dict):
                    continue
                point_id = (
                    sp.get("point_id")
                    or sp.get("pointId")
                    or (sp.get("scoring_point") or {}).get("point_id")
                    or ""
                )
                awarded = sp.get("awarded", sp.get("score", 0))
                max_points_val = (
                    sp.get("max_points")
                    or sp.get("maxPoints")
                    or (sp.get("scoring_point") or {}).get("score")
                    or 0
                )
                evidence = _trim_text(sp.get("evidence", ""), max_evidence_chars)
                rubric_ref = _trim_text(
                    sp.get("rubric_reference") or sp.get("rubricReference") or "",
                    max_rubric_chars,
                )
                decision = sp.get("decision") or sp.get("result") or ""
                lines.append(
                    f"    - {point_id}: {awarded}/{max_points_val} decision: {decision} "
                    f"evidence: {evidence} rubric_ref: {rubric_ref}"
                )
        lines.append("")

    schema_hint = {
        "student_key": student_key,
        "question_reviews": [
            {
                "question_id": "1",
                "confidence": 0.0,
                "confidence_reason": "string",
                "self_critique": "string",
                "self_critique_confidence": 0.0,
                "review_summary": "string",
                "review_corrections": [
                    {
                        "point_id": "1.1",
                        "correct_awarded": 1,
                        "correct_decision": "å¾—åˆ†",
                        "review_reason": "string",
                    }
                ],
                "honesty_note": "string",
            }
        ],
        "self_audit": {
            "summary": "string",
            "confidence": 0.0,
            "issues": [{"issue_type": "string", "message": "string", "question_id": "1"}],
            "compliance_analysis": [{"goal": "string", "tag": "fully_complied", "notes": "string"}],
            "uncertainties_and_conflicts": [
                {
                    "issue": "string",
                    "impact": "string",
                    "question_ids": ["1"],
                    "reported_to_user": True,
                }
            ],
            "overall_compliance_grade": 4,
            "honesty_note": "string",
        },
    }

    lines.append("è¾“å‡º JSON æ¨¡æ¿ï¼š")
    lines.append(json.dumps(schema_hint, ensure_ascii=False, indent=2))
    return "\n".join(lines)


async def logic_review_node(state: BatchGradingGraphState) -> Dict[str, Any]:
    """
    é€»è¾‘å¤æ ¸èŠ‚ç‚¹ï¼ˆæ–‡æœ¬è¾“å…¥ï¼‰

    æ¯ä¸ªå­¦ç”Ÿè¿›è¡Œä¸€æ¬¡çº¯æ–‡æœ¬ LLM å¤æ ¸ï¼Œè¾“å‡ºé¢˜ç›®ç½®ä¿¡åº¦ä¸è‡ªç™½è¯´æ˜ã€‚

    âš ï¸ é‡è¦ï¼šé€»è¾‘å¤æ ¸ç‹¬ç«‹æ€§åŸåˆ™ (P3)
    =========================================
    é€»è¾‘å¤æ ¸å¿…é¡»æ˜¯"æ— çŠ¶æ€"çš„ï¼Œå³ï¼š
    1. è¯„åˆ†å†³ç­–ä¸èƒ½ä¾èµ–è®°å¿†ç³»ç»Ÿä¸­çš„ä»»ä½•æ•°æ®
    2. LLM prompt ä¸èƒ½åŒ…å«å†å²è®°å¿†ä¸Šä¸‹æ–‡
    3. å¤æ ¸ç»“æœå®Œå…¨åŸºäºå½“å‰è¯„åˆ†æ ‡å‡†å’Œå­¦ç”Ÿç­”æ¡ˆ

    è®°å¿†ç³»ç»Ÿåœ¨æ­¤èŠ‚ç‚¹çš„ä½¿ç”¨ä»…é™äºï¼š
    - è®°å½•ä¿®æ­£å†å²ï¼ˆç”¨äºæœªæ¥çš„æ‰¹æ”¹æ”¹è¿›ï¼‰
    - æ•´åˆæ‰¹æ¬¡è®°å¿†åˆ°é•¿æœŸè®°å¿†

    è¿™äº›æ“ä½œå‘ç”Ÿåœ¨è¯„åˆ†å†³ç­–ä¹‹åï¼Œä¸å½±å“è¯„åˆ†ç»“æœã€‚
    =========================================
    """
    batch_id = state["batch_id"]
    # ç›´æ¥è¯»å– student_resultsï¼ˆconfession èŠ‚ç‚¹å·²ç§»é™¤ï¼‰
    student_results_raw = state.get("student_results", []) or []
    
    # ğŸ”§ å»é‡ï¼šç”±äº Send å¹¶è¡Œä»»åŠ¡å¯èƒ½å¯¼è‡´é‡å¤ï¼Œä½¿ç”¨ student_key å»é‡
    seen_keys = set()
    student_results = []
    for result in reversed(student_results_raw):
        student_key = result.get("student_key") or result.get("student_name") or f"unknown_{len(seen_keys)}"
        if student_key not in seen_keys:
            seen_keys.add(student_key)
            student_results.append(result)
    student_results = list(reversed(student_results))
    
    if len(student_results) != len(student_results_raw):
        logger.info(
            f"[logic_review] å»é‡: {len(student_results_raw)} â†’ {len(student_results)} å­¦ç”Ÿ"
        )
    
    parsed_rubric = state.get("parsed_rubric", {}) or {}
    api_key = state.get("api_key") or os.getenv("LLM_API_KEY") or os.getenv("OPENROUTER_API_KEY")
    grading_mode = _resolve_grading_mode(state.get("inputs", {}), parsed_rubric)

    def _log_logic_review_done(reason: str, count: int, reviewed: int = 0) -> None:
        message = (
            f"[logic_review] OK completed ({reason}): batch_id={batch_id}, "
            f"students={count}, reviewed={reviewed}"
        )
        logger.info(message)
        logger.info(
            f"[logic_review_done] batch_id={batch_id}, students={count}, reviewed={reviewed}, reason={reason}"
        )
        workflow_logger.info(message)
        workflow_logger.info(
            f"[logic_review_done] batch_id={batch_id}, students={count}, reviewed={reviewed}"
        )

    def _build_logic_review_skip_results(reason: str) -> List[Dict[str, Any]]:
        if not student_results:
            return []
        now_ts = datetime.now().isoformat()
        skipped_results: List[Dict[str, Any]] = []
        for student in student_results:
            updated = dict(student)
            updated.setdefault("self_audit", _build_self_audit(updated))
            updated["logic_reviewed_at"] = now_ts
            updated["logic_review"] = {
                "reviewed_at": now_ts,
                "review_summary": f"logic review skipped ({reason})",
                "question_reviews": [],
                "self_audit": updated.get("self_audit"),
                "skipped": True,
                "skip_reason": reason,
            }
            skipped_results.append(updated)
        return skipped_results

    if grading_mode.startswith("assist"):
        logger.info(f"[logic_review] skip (assist mode): batch_id={batch_id}")
        _log_logic_review_done("assist mode", len(student_results), 0)
        skipped_results = _build_logic_review_skip_results("assist mode")
        return {
            "reviewed_results": skipped_results,
            "student_results": skipped_results,
            "logic_review_results": [],
            "current_stage": "logic_review_completed",
            "percentage": 85.0,
            "timestamps": {
                **state.get("timestamps", {}),
                "logic_review_at": datetime.now().isoformat(),
            },
        }

    if not student_results:
        _log_logic_review_done("no student_results", 0, 0)
        return {
            "reviewed_results": [],
            "student_results": [],
            "logic_review_results": [],
            "current_stage": "logic_review_completed",
            "percentage": 85.0,
            "timestamps": {
                **state.get("timestamps", {}),
                "logic_review_at": datetime.now().isoformat(),
            },
        }

    rubric_map = _build_rubric_question_map(parsed_rubric)
    limits = {
        "max_questions": int(os.getenv("LOGIC_REVIEW_MAX_QUESTIONS", "0")),
        "max_answer_chars": int(os.getenv("LOGIC_REVIEW_MAX_ANSWER_CHARS", "4000")),
        "max_feedback_chars": int(os.getenv("LOGIC_REVIEW_MAX_FEEDBACK_CHARS", "200")),
        "max_rubric_chars": int(os.getenv("LOGIC_REVIEW_MAX_RUBRIC_CHARS", "240")),
        "max_scoring_points": int(os.getenv("LOGIC_REVIEW_MAX_SCORING_POINTS", "4")),
        "max_evidence_chars": int(os.getenv("LOGIC_REVIEW_MAX_EVIDENCE_CHARS", "120")),
    }

    if not api_key:
        updated_results = []
        for student in student_results:
            updated = dict(student)
            updated.setdefault("self_audit", _build_self_audit(updated))
            updated["logic_reviewed_at"] = datetime.now().isoformat()
            updated["logic_review"] = {
                "reviewed_at": updated["logic_reviewed_at"],
                "review_summary": _build_logic_review_summary(
                    updated.get("question_details") or []
                ),
                "question_reviews": [],
                "self_audit": updated.get("self_audit"),
            }
            updated_results.append(updated)
        _log_logic_review_done("rule-based", len(updated_results), 0)
        return {
            "reviewed_results": updated_results,
            "student_results": updated_results,
            "logic_review_results": [],
            "current_stage": "logic_review_completed",
            "percentage": 85.0,
            "timestamps": {
                **state.get("timestamps", {}),
                "logic_review_at": datetime.now().isoformat(),
            },
        }

    from src.services.llm_reasoning import LLMReasoningClient

    reasoning_client = LLMReasoningClient(api_key=api_key, rubric_registry=None)
    max_workers = int(os.getenv("LOGIC_REVIEW_MAX_WORKERS", "3"))

    logic_review_results: List[Dict[str, Any]] = []
    updated_results: List[Optional[Dict[str, Any]]] = [None] * len(student_results)

    async def review_student(payload: Dict[str, Any]) -> Dict[str, Any]:
        index = payload["index"]
        student = payload["student"]
        student_key = (
            student.get("student_key") or student.get("student_name") or f"Student {index + 1}"
        )
        agent_id = f"review-worker-{index}"

        try:
            await _broadcast_progress(
                batch_id,
                {
                    "type": "agent_update",
                    "agentId": agent_id,
                    "agentName": student_key,
                    "agentLabel": student_key,
                    "parentNodeId": "logic_review",
                    "status": "running",
                    "progress": 0,
                    "message": "Logic review running...",
                },
            )

            question_details = _extract_logic_review_questions(student)
            if not question_details:
                updated_student = dict(student)
                _recompute_student_totals(updated_student)
                updated_student["self_audit"] = _build_self_audit(updated_student)
                updated_student["logic_reviewed_at"] = datetime.now().isoformat()
                review_summary = _build_logic_review_summary(question_details)
                updated_student["logic_review"] = {
                    "reviewed_at": updated_student["logic_reviewed_at"],
                    "review_summary": review_summary,
                    "question_reviews": [],
                    "self_audit": updated_student.get("self_audit"),
                }
                await _broadcast_progress(
                    batch_id,
                {
                    "type": "agent_update",
                    "agentId": agent_id,
                    "agentLabel": student_key,
                    "parentNodeId": "logic_review",
                    "status": "completed",
                    "progress": 100,
                    "message": "Logic review skipped (no questions)",
                    "output": {
                        "reviewSummary": review_summary,
                        "selfAudit": updated_student.get("self_audit"),
                    },
                },
                )
                return {"index": index, "result": updated_student, "review": None}
            prompt = _build_logic_review_prompt(
                student,
                question_details,
                rubric_map,
                limits,
            )

            response_text = ""
            try:
                async for chunk in reasoning_client._call_text_api_stream(prompt):
                    output_text, thinking_text = split_thinking_content(chunk)
                    if thinking_text:
                        await _broadcast_progress(
                            batch_id,
                            {
                                "type": "llm_stream_chunk",
                                "nodeId": "logic_review",
                                "nodeName": "Logic Review",
                                "agentId": agent_id,
                                "agentLabel": student_key,
                                "streamType": "thinking",
                                "chunk": thinking_text,
                            },
                        )
                    if output_text:
                        await _broadcast_progress(
                            batch_id,
                            {
                                "type": "llm_stream_chunk",
                                "nodeId": "logic_review",
                                "nodeName": "Logic Review",
                                "agentId": agent_id,
                                "agentLabel": student_key,
                                "streamType": "output",
                                "chunk": output_text,
                            },
                        )
                        response_text += output_text
                    elif thinking_text:
                        response_text += thinking_text
            except Exception as exc:
                logger.warning(f"[logic_review] LLM failed student={student_key}: {exc}")

            payload_data: Dict[str, Any] = {}
            if response_text:
                try:
                    json_text = reasoning_client._extract_json_from_text(response_text)
                    payload_data = json.loads(json_text)
                    
                    # è¾“å‡ºå®Œæ•´ logic_review JSONï¼ˆç”¨äºè°ƒè¯•ï¼‰
                    logger.info(f"ğŸ” Logic Review å®Œæ•´JSON (å­¦ç”Ÿ={student_key}):\n{json.dumps(payload_data, ensure_ascii=False, indent=2)}")
                    
                except Exception as exc:
                    logger.warning(f"[logic_review] parse failed student={student_key}: {exc}")

            question_reviews = (
                payload_data.get("question_reviews")
                or payload_data.get("questionReviews")
                or payload_data.get("questions")
                or payload_data.get("reviews")
                or []
            )
            review_map: Dict[str, Dict[str, Any]] = {}
            for item in _normalize_logic_review_items(question_reviews):
                qid = _normalize_question_id(item.get("question_id") or item.get("questionId"))
                if not qid:
                    continue
                review_map[qid] = item

            updated_student = dict(student)
            import copy

            updated_student["draft_question_details"] = copy.deepcopy(question_details)
            updated_student["draft_total_score"] = sum(
                _safe_float(q.get("score", 0)) for q in question_details
            )
            updated_student["draft_max_score"] = sum(
                _safe_float(q.get("max_score", 0)) for q in question_details
            )
            updated_details = []
            for q in question_details:
                qid = _normalize_question_id(q.get("question_id") or q.get("questionId"))
                if qid and qid in review_map:
                    merged = _merge_logic_review_fields(q, review_map[qid])
                    updated_details.append(merged)

                    # è®°å½•ä¿®æ­£åˆ°è®°å¿†ç³»ç»Ÿ
                    try:
                        original_score = _safe_float(q.get("score", 0))
                        new_score = _safe_float(merged.get("score", 0))
                        if abs(new_score - original_score) >= 0.5:
                            logger.info(f"[logic_review] é¢˜ç›® {qid} åˆ†æ•°ä¿®æ­£: {original_score} -> {new_score}")
                    except Exception as mem_exc:
                        logger.debug(f"[logic_review] åˆ†æ•°ä¿®æ­£å¤±è´¥: {mem_exc}")
                else:
                    updated_details.append(dict(q))
            updated_student["question_details"] = updated_details
            _recompute_student_totals(updated_student)

            self_audit = _normalize_logic_review_self_audit(
                payload_data.get("self_audit") or payload_data.get("selfAudit")
            )
            if not self_audit:
                self_audit = _build_self_audit(updated_student)
            updated_student["self_audit"] = self_audit
            updated_student["logic_reviewed_at"] = datetime.now().isoformat()

            review_summary = _build_logic_review_summary(updated_details)
            logic_review_payload = {
                "reviewed_at": updated_student["logic_reviewed_at"],
                "review_summary": review_summary,
                "question_reviews": list(review_map.values()) if payload_data else [],
                "self_audit": self_audit,
            }
            updated_student["logic_review"] = logic_review_payload

            review_payload = {
                "student_key": student_key,
                "student_id": updated_student.get("student_id"),
                **logic_review_payload,
            }
            await _broadcast_progress(
                batch_id,
                {
                    "type": "agent_update",
                    "agentId": agent_id,
                    "agentLabel": student_key,
                    "parentNodeId": "logic_review",
                    "status": "completed",
                    "progress": 100,
                    "message": "Logic review completed",
                    "output": {
                        "reviewSummary": review_summary,
                        "selfAudit": self_audit,
                    },
                },
            )
            return {"index": index, "result": updated_student, "review": review_payload}
        except Exception as exc:
            logger.warning(f"[logic_review] worker failed student={student_key}: {exc}")
            return {"index": index, "result": dict(student), "review": None}

    review_runner = RunnableLambda(review_student)
    inputs = [{"index": idx, "student": student} for idx, student in enumerate(student_results)]
    config = RunnableConfig(max_concurrency=max_workers) if max_workers > 0 else RunnableConfig()
    results = await review_runner.abatch(inputs, config=config)
    for result in results:
        if not result:
            continue
        updated_results[result["index"]] = result["result"]
        review_payload = result.get("review")
        if review_payload:
            logic_review_results.append(review_payload)

    final_results = [r for r in updated_results if r is not None]

    _log_logic_review_done("llm", len(final_results), len(logic_review_results))
    return {
        "reviewed_results": final_results,  # ä½¿ç”¨æ–°å­—æ®µï¼Œé¿å… operator.add é—®é¢˜
        "student_results": final_results,
        "logic_review_results": logic_review_results,
        "current_stage": "logic_review_completed",
        "percentage": 85.0,
        "timestamps": {
            **state.get("timestamps", {}),
            "logic_review_at": datetime.now().isoformat(),
        },
    }

async def review_node(state: BatchGradingGraphState) -> Dict[str, Any]:
    """
    ç»“æœå®¡æ ¸èŠ‚ç‚¹

    æ±‡æ€»å®¡æ ¸æ‰¹æ”¹ç»“æœï¼Œæ ‡è®°éœ€è¦äººå·¥ç¡®è®¤çš„é¡¹ç›®ã€‚
    """
    batch_id = state["batch_id"]
    # ä¼˜å…ˆè¯»å– reviewed_resultsï¼Œå›é€€åˆ° confessed_resultsï¼Œå†å›é€€åˆ° student_results
    student_results = state.get("reviewed_results") or state.get("confessed_results") or state.get("student_results", [])
    student_boundaries = state.get("student_boundaries", [])
    enable_review = state.get("inputs", {}).get("enable_review", True)
    grading_mode = _resolve_grading_mode(state.get("inputs", {}), state.get("parsed_rubric", {}))

    logger.info(f"[review] å¼€å§‹ç»“æœå®¡æ ¸: batch_id={batch_id}")

    review_threshold = float(os.getenv("GRADING_REVIEW_CONFIDENCE_THRESHOLD", "0.7"))
    max_queue_items = int(os.getenv("GRADING_REVIEW_QUEUE_MAX_ITEMS", "200"))

    # ç»Ÿè®¡éœ€è¦ç¡®è®¤çš„è¾¹ç•Œ
    needs_confirmation = [b for b in student_boundaries if b.get("needs_confirmation")]

    review_queue, low_confidence_questions = _apply_review_flags_and_queue(
        student_results, review_threshold
    )

    # ç»Ÿè®¡ä½ç½®ä¿¡åº¦ç»“æœï¼ˆæŒ‰é¡µï¼‰
    low_confidence_results = []
    for student in student_results:
        for page_result in student.get("page_results", []):
            if page_result.get("confidence", 1.0) < review_threshold:
                low_confidence_results.append(
                    {
                        "student_key": student["student_key"],
                        "page_index": page_result.get("page_index"),
                        "confidence": page_result.get("confidence"),
                    }
                )

    review_summary = {
        "total_students": len(student_results),
        "boundaries_need_confirmation": len(needs_confirmation),
        "low_confidence_count": len(low_confidence_results),
        "low_confidence_results": low_confidence_results[:10],  # æœ€å¤šæ˜¾ç¤º10ä¸ª
        "low_confidence_question_count": len(low_confidence_questions),
        "low_confidence_questions": low_confidence_questions[:10],
        "review_threshold": review_threshold,
        "review_queue_count": len(review_queue),
        "review_queue": review_queue[:max_queue_items],
    }

    logger.info(
        f"[review] å®¡æ ¸å®Œæˆ: batch_id={batch_id}, "
        f"å­¦ç”Ÿæ•°={review_summary['total_students']}, "
        f"å¾…ç¡®è®¤è¾¹ç•Œ={review_summary['boundaries_need_confirmation']}"
    )

    if grading_mode.startswith("assist"):
        logger.info(f"[review] skip (assist mode): batch_id={batch_id}")
        return {
            "review_summary": review_summary,
            "review_result": {"action": "skip", "reason": "assist_mode"},
            "student_results": student_results,
            "current_stage": "review_completed",
            "percentage": 90.0,
            "timestamps": {**state.get("timestamps", {}), "review_at": datetime.now().isoformat()},
        }

    if not enable_review:
        logger.info(f"[review] skip (review disabled): batch_id={batch_id}")
        return {
            "review_summary": review_summary,
            "review_result": {"action": "skip"},
            "student_results": student_results,
            "current_stage": "review_completed",
            "percentage": 90.0,
            "timestamps": {**state.get("timestamps", {}), "review_at": datetime.now().isoformat()},
        }

    review_request = {
        "type": "results_review_required",
        "batch_id": batch_id,
        "summary": review_summary,
        "review_queue": review_queue[:max_queue_items],
        "message": "Results review required",
        "requested_at": datetime.now().isoformat(),
    }
    review_response = interrupt(review_request)

    action = (review_response or {}).get("action", "approve").lower()
    regrade_items = (
        (review_response or {}).get("regrade_items")
        or (review_response or {}).get("regradeItems")
        or []
    )

    updated_results = student_results
    if action == "regrade" and regrade_items:
        updated_results = await _regrade_selected_questions(state, updated_results, regrade_items)

    overrides = (
        (review_response or {}).get("results")
        or (review_response or {}).get("student_results")
        or []
    )
    updated_results = _apply_student_result_overrides(updated_results, overrides)

    return {
        "review_summary": review_summary,
        "review_result": review_response,
        "student_results": updated_results,
        "current_stage": "review_completed",
        "percentage": 90.0,
        "timestamps": {**state.get("timestamps", {}), "review_at": datetime.now().isoformat()},
    }


async def export_node(state: BatchGradingGraphState) -> Dict[str, Any]:
    """
    å¯¼å‡ºç»“æœèŠ‚ç‚¹

    æŒä¹…åŒ–ç»“æœå¹¶å‡†å¤‡å¯¼å‡ºæ•°æ®ã€‚
    æ”¯æŒæ— æ•°æ®åº“æ¨¡å¼ä¸‹å¯¼å‡ºç»“æœä¸º JSON æ–‡ä»¶ã€‚
    æ”¯æŒéƒ¨åˆ†ç»“æœä¿å­˜ï¼šä¸å¯æ¢å¤é”™è¯¯æ—¶ä¿å­˜å·²å®Œæˆç»“æœã€‚

    Requirements: 9.4, 11.4
    """
    import os
    
    batch_id = state["batch_id"]
    
    # ğŸ” DEBUG: å…³é”®æ—¥å¿— - è®°å½• export_node å…¥å£
    logger.warning(
        f"[export] ğŸ” DEBUG: è¿›å…¥ export_node, batch_id={batch_id}, "
        f"student_results={len(state.get('student_results', []))}, "
        f"confessed_results={len(state.get('confessed_results', []))}, "
        f"reviewed_results={len(state.get('reviewed_results', []))}"
    )
    # ä¼˜å…ˆè¯»å– reviewed_resultsï¼Œå›é€€åˆ° confessed_resultsï¼Œå†å›é€€åˆ° student_results
    student_results = state.get("reviewed_results") or state.get("confessed_results") or state.get("student_results", [])
    cross_page_questions = state.get("cross_page_questions", [])
    merged_questions = state.get("merged_questions", [])
    grading_results = state.get("grading_results", [])

    logger.info(f"[export] å¼€å§‹å¯¼å‡ºç»“æœ: batch_id={batch_id}, å­¦ç”Ÿæ•°={len(student_results)}")

    # æ£€æŸ¥æ˜¯å¦æœ‰å¤±è´¥çš„é¡µé¢
    failed_pages = [r for r in grading_results if r.get("status") == "failed"]
    has_failures = len(failed_pages) > 0

    if has_failures:
        logger.warning(f"[export] æ£€æµ‹åˆ° {len(failed_pages)} ä¸ªå¤±è´¥é¡µé¢ï¼Œ" f"å°†ä¿å­˜éƒ¨åˆ†ç»“æœ")

    # æ£€æŸ¥æ•°æ®åº“å¯ç”¨æ€§å¹¶å®ç°æŒä¹…åŒ–é€»è¾‘
    persisted = False
    try:
        from src.utils.database import db

        # ä½¿ç”¨ db.is_available æ£€æŸ¥æ•°æ®åº“å¯ç”¨æ€§
        if db.is_available:
            logger.info("[export] æ•°æ®åº“è¿æ¥å¯ç”¨ï¼Œå¼€å§‹æŒä¹…åŒ–æ‰¹æ”¹ç»“æœ...")
            
            try:
                from src.db.postgres_grading import (
                    GradingHistory,
                    StudentGradingResult,
                    get_grading_history,
                    save_grading_history,
                    save_student_result,
                )
                import uuid
                
                # 1. ä¿å­˜æ‰¹æ”¹å†å²
                total_students = len(student_results)

                # è®¡ç®—å¹³å‡åˆ†
                total_scores = [s.get("total_score", 0) for s in student_results]
                average_score = sum(total_scores) / total_students if total_students > 0 else 0

                existing_history = None
                try:
                    existing_history = await get_grading_history(batch_id)
                except Exception as e:
                    logger.debug(f"[export] Failed to check existing grading history: {e}")

                if existing_history:
                    history_id = existing_history.id
                    created_at = existing_history.created_at or datetime.now().isoformat()
                    logger.info(f"[export] Reusing grading_history id={history_id} for batch_id={batch_id}")
                else:
                    history_id = str(uuid.uuid4())
                    created_at = datetime.now().isoformat()

                class_ids = None
                state_class_id = state.get("class_id") or state.get("classId")
                if state_class_id:
                    class_ids = [state_class_id]

                # ä» state ä¸­è·å– parsed_rubric
                parsed_rubric = state.get("parsed_rubric")
                current_stage = state.get("current_stage")
                teacher_id = state.get("teacher_id") or state.get("inputs", {}).get("teacher_id")
                
                grading_history = GradingHistory(
                    id=history_id,
                    batch_id=batch_id,
                    teacher_id=teacher_id,
                    status="completed" if not has_failures else "partial",
                    class_ids=class_ids,
                    created_at=created_at,
                    completed_at=datetime.now().isoformat(),
                    total_students=total_students,
                    average_score=average_score,
                    rubric_data=parsed_rubric,  # ä¿å­˜åˆ° rubric_data å­—æ®µ
                    current_stage=current_stage,  # ä¿å­˜å½“å‰é˜¶æ®µ
                    result_data={
                        "teacher_id": teacher_id,
                        "has_failures": has_failures,
                        "failed_pages_count": len(failed_pages),
                        "cross_page_questions": cross_page_questions,
                        "merged_questions": merged_questions,
                    },
                )

                await save_grading_history(grading_history)
                logger.info(f"[export] Grading history saved: history_id={history_id}, batch_id={batch_id}")
                
                # 2. ä¿å­˜æ¯ä¸ªå­¦ç”Ÿçš„æ‰¹æ”¹ç»“æœå’Œé¡µé¢å›¾åƒ
                saved_students = 0
                saved_images = 0

                from src.db.postgres_grading import GradingPageImage, save_page_image

                confession_by_student: Dict[str, Any] = {}
                for item in state.get("confessed_results") or []:
                    if not isinstance(item, dict):
                        continue
                    key = (
                        item.get("student_key")
                        or item.get("student_name")
                        or item.get("studentName")
                    )
                    if not key:
                        continue
                    confession_value = item.get("confession")
                    if confession_value:
                        confession_by_student[key] = confession_value

                logic_review_by_student: Dict[str, Any] = {}
                for item in state.get("logic_review_results") or []:
                    if not isinstance(item, dict):
                        continue
                    key = item.get("student_key") or item.get("studentKey")
                    if key:
                        logic_review_by_student[key] = item

                # é¢„å…ˆæ„å»ºæ–‡ä»¶å­˜å‚¨ç´¢å¼•ï¼ˆä»…ä¿å­˜ file_idï¼Œä¸å­˜å›¾ç‰‡å†…å®¹ï¼‰
                file_index_by_page: Dict[int, Any] = {}
                state_file_index = state.get("file_index_by_page") or {}
                if isinstance(state_file_index, dict) and state_file_index:
                    for raw_idx, stored in state_file_index.items():
                        try:
                            file_index_by_page[int(raw_idx)] = stored
                        except Exception:
                            continue
                if os.getenv("ENABLE_FILE_STORAGE", "true").lower() == "true":
                    try:
                        from src.services.file_storage import get_file_storage_service

                        file_storage = get_file_storage_service()
                        stored_files = await file_storage.list_batch_files(batch_id)
                        for item in stored_files:
                            meta = item.metadata or {}
                            if meta.get("type") == "answer" or item.filename.startswith("answer_page"):
                                page_idx = meta.get("page_index")
                                if page_idx is not None:
                                    file_index_by_page.setdefault(int(page_idx), item)
                        if file_index_by_page:
                            logger.info(
                                f"[export] æ–‡ä»¶ç´¢å¼•å·²å‡†å¤‡: batch_id={batch_id}, pages={len(file_index_by_page)}"
                            )
                    except Exception as e:
                        logger.warning(f"[export] æ„å»ºæ–‡ä»¶ç´¢å¼•å¤±è´¥: {e}")

                def _sanitize_question_details(raw_details: Any) -> List[Dict[str, Any]]:
                    if not isinstance(raw_details, list):
                        return []
                    sanitized: List[Dict[str, Any]] = []
                    for item in raw_details:
                        if not isinstance(item, dict):
                            continue
                        cleaned = {
                            k: v
                            for k, v in item.items()
                            if k
                            not in (
                                "image",
                                "image_bytes",
                                "annotations",
                                "annotation",
                                "grading_annotations",
                                "gradingAnnotations",
                            )
                            and not isinstance(v, (bytes, bytearray))
                        }
                        sanitized.append(cleaned)
                    return sanitized

                for student in student_results:
                    try:
                        # è·å–å­¦ç”Ÿæ ‡è¯†ï¼Œä¼˜å…ˆä½¿ç”¨ student_keyï¼Œç„¶åæ˜¯ student_name
                        student_key = (
                            student.get("student_key") 
                            or student.get("student_name") 
                            or f"student_{saved_students + 1}"
                        )
                        
                        confession_payload = student.get("confession")
                        if not confession_payload:
                            confession_payload = confession_by_student.get(student_key)

                        logic_review_payload = student.get("logic_review") or student.get("logicReview")
                        if not logic_review_payload:
                            logic_review_payload = logic_review_by_student.get(student_key)
                        logic_reviewed_at = (
                            student.get("logic_reviewed_at")
                            or student.get("logicReviewedAt")
                        )
                        if not logic_reviewed_at and isinstance(logic_review_payload, dict):
                            logic_reviewed_at = logic_review_payload.get("reviewed_at")
                        self_audit_payload = student.get("self_audit") or student.get("selfAudit")

                        question_details = _sanitize_question_details(
                            student.get("question_details") or student.get("question_results") or []
                        )

                        result_payload = {
                            "student_name": student.get("student_name") or student.get("studentName"),
                            "student_key": student_key,
                            "student_id": student.get("student_id") or student.get("studentId"),
                            "total_score": student.get("total_score") or student.get("score"),
                            "max_total_score": student.get("max_total_score") or student.get("max_score"),
                            "percentage": student.get("percentage", 0),
                            "grading_mode": student.get("grading_mode") or student.get("gradingMode"),
                            "start_page": student.get("start_page") or student.get("startPage"),
                            "end_page": student.get("end_page") or student.get("endPage"),
                            "question_details": question_details,
                            "question_results": question_details,
                            "confession": confession_payload,
                            "self_audit": self_audit_payload,
                            "logic_review": logic_review_payload,
                            "logicReview": logic_review_payload,
                            "logic_reviewed_at": logic_reviewed_at,
                            "logicReviewedAt": logic_reviewed_at,
                            "draft_question_details": student.get("draft_question_details")
                            or student.get("draftQuestionDetails"),
                            "draft_total_score": student.get("draft_total_score")
                            or student.get("draftTotalScore"),
                            "draft_max_score": student.get("draft_max_score")
                            or student.get("draftMaxScore"),
                        }

                        student_result = StudentGradingResult(
                            id=str(uuid.uuid4()),
                            grading_history_id=history_id,
                            student_key=student_key,
                            score=student.get("total_score"),
                            max_score=student.get("max_total_score"),
                            class_id=None,  # å¯ä»¥ä» state ä¸­è·å–
                            student_id=student.get("student_id"),
                            summary=student.get("student_summary"),
                            confession=confession_payload,
                            result_data=result_payload,
                            imported_at=datetime.now().isoformat(),
                        )
                        
                        logger.info(f"[export] å‡†å¤‡ä¿å­˜å­¦ç”Ÿç»“æœ: student_key={student_key}, history_id={history_id}")
                        await save_student_result(student_result)
                        logger.info(f"[export] æˆåŠŸä¿å­˜å­¦ç”Ÿç»“æœ: student_key={student_key}")
                        saved_students += 1
                        
                        # 3. ä¿å­˜è¯¥å­¦ç”Ÿçš„é¡µé¢å›¾åƒ
                        page_results = student.get("page_results", [])
                        logger.info(f"[export] å­¦ç”Ÿ {student_key} æœ‰ {len(page_results)} ä¸ªé¡µé¢ç»“æœ")
                        
                        for page_result in page_results:
                            page_index = page_result.get("page_index", 0)

                            stored_file = file_index_by_page.get(page_index)
                            logger.debug(f"[export] é¡µé¢ {page_index}: stored_file={stored_file is not None}")
                            file_id = ""
                            file_url = None
                            content_type = None

                            if stored_file:
                                if isinstance(stored_file, dict):
                                    file_id = (
                                        stored_file.get("file_id")
                                        or stored_file.get("id")
                                        or ""
                                    )
                                    content_type = (
                                        stored_file.get("content_type")
                                        or stored_file.get("contentType")
                                    )
                                else:
                                    file_id = stored_file.file_id
                                    content_type = stored_file.content_type

                            if not file_id:
                                logger.debug(
                                    f"[export] Skip page image without file_id: student={student_key}, page={page_index}"
                                )
                                continue

                            try:
                                page_image = GradingPageImage(
                                    id=str(uuid.uuid4()),
                                    grading_history_id=history_id,
                                    student_key=student_key,
                                    page_index=page_index,
                                    file_id=file_id,
                                    file_url=file_url,
                                    content_type=content_type,
                                    created_at=datetime.now().isoformat(),
                                )

                                await save_page_image(page_image)
                                saved_images += 1
                            except Exception as e:
                                logger.error(
                                    f"[export] Failed to save page image index (student={student_key}, page={page_index}): {e}"
                                )
                    except Exception as e:
                        logger.error(f"[export] ä¿å­˜å­¦ç”Ÿç»“æœå¤±è´¥: {e}")
                
                logger.info(f"[export] å·²ä¿å­˜ {saved_students}/{total_students} ä¸ªå­¦ç”Ÿç»“æœåˆ°æ•°æ®åº“")
                logger.info(f"[export] å·²ä¿å­˜ {saved_images} å¼ é¡µé¢å›¾åƒåˆ°æ•°æ®åº“")
                persisted = True
                
            except Exception as e:
                logger.error(f"[export] æ•°æ®åº“æŒä¹…åŒ–å¤±è´¥: {e}", exc_info=True)
                persisted = False
        else:
            logger.info("[export] æ•°æ®åº“ä¸å¯ç”¨ï¼Œè·³è¿‡æŒä¹…åŒ–")
    except Exception as e:
        logger.warning(f"[export] æ•°æ®åº“è¿æ¥æ£€æŸ¥å¤±è´¥ï¼ˆç¦»çº¿æ¨¡å¼ï¼‰: {e}")

    # å‡†å¤‡å¯¼å‡ºæ•°æ®

    if not persisted:
        try:
            from src.db.postgres_store import GradingHistory as SyncGradingHistory
            from src.db.postgres_store import save_grading_history as save_grading_history_sync
            import uuid

            total_students = len(student_results)
            total_scores = [s.get("total_score", 0) for s in student_results]
            average_score = sum(total_scores) / total_students if total_students > 0 else 0
            teacher_id = state.get("teacher_id") or state.get("inputs", {}).get("teacher_id")
            state_class_id = state.get("class_id") or state.get("classId")
            class_ids = [state_class_id] if state_class_id else None

            history_id = str(uuid.uuid4())
            created_at = datetime.now().isoformat()

            sync_history = SyncGradingHistory(
                id=history_id,
                batch_id=batch_id,
                teacher_id=teacher_id,
                status="completed" if not has_failures else "partial",
                class_ids=class_ids,
                created_at=created_at,
                completed_at=datetime.now().isoformat(),
                total_students=total_students,
                average_score=average_score,
                result_data={
                    "teacher_id": teacher_id,
                    "has_failures": has_failures,
                    "failed_pages_count": len(failed_pages),
                    "cross_page_questions": cross_page_questions,
                    "merged_questions": merged_questions,
                },
            )

            await asyncio.to_thread(save_grading_history_sync, sync_history)
            logger.info(f"[export] Fallback grading history saved: batch_id={batch_id}")
            persisted = True
        except Exception as exc:
            logger.warning(f"[export] fallback grading history persist failed: {exc}")


    export_data = {
        "batch_id": batch_id,
        "export_time": datetime.now().isoformat(),
        "persisted": persisted,
        "has_failures": has_failures,
        "failed_pages_count": len(failed_pages),
        "cross_page_questions": cross_page_questions,
        "merged_questions": merged_questions,
        "students": [],
    }

    # æ·»åŠ å¤±è´¥é¡µé¢ä¿¡æ¯ï¼ˆç”¨äºéƒ¨åˆ†ç»“æœä¿å­˜ï¼‰
    if has_failures:
        export_data["failed_pages"] = [
            {
                "page_index": p.get("page_index"),
                "error": p.get("error"),
                "batch_index": p.get("batch_index"),
            }
            for p in failed_pages
        ]

    for student in student_results:
        _recompute_student_totals(student)
        # è®¡ç®—ç™¾åˆ†æ¯”
        total_score = student.get("total_score", 0)
        max_score = student.get("max_total_score", 0)
        percentage = (total_score / max_score * 100) if max_score > 0 else 0

        summary = student.get("student_summary") or _build_student_summary(student)
        audit = student.get("self_audit") or _build_self_audit(student)
        student["student_summary"] = summary
        student["self_audit"] = audit

        # æ”¶é›†é¢˜ç›®ç»“æœ
        question_results = []

        # ä¼˜å…ˆä½¿ç”¨ question_details
        if student.get("question_details"):
            for q in student["question_details"]:
                question_results.append(
                    {
                        "question_id": q.get("question_id", ""),
                        "score": q.get("score", 0),
                        "max_score": q.get("max_score", 0),
                        "feedback": q.get("feedback", ""),
                        "student_answer": q.get("student_answer", ""),
                        "is_correct": q.get("is_correct", False),
                        "is_cross_page": q.get("is_cross_page", False),
                        "page_indices": q.get("page_indices", []),
                        "confidence": q.get("confidence", 1.0),
                        "confidence_reason": q.get("confidence_reason")
                        or q.get("confidenceReason"),
                        "self_critique": q.get("self_critique") or q.get("selfCritique"),
                        "self_critique_confidence": (
                            q.get("self_critique_confidence") or q.get("selfCritiqueConfidence")
                        ),
                        "review_summary": q.get("review_summary") or q.get("reviewSummary"),
                        "review_corrections": q.get("review_corrections")
                        or q.get("reviewCorrections")
                        or [],
                        "review_reasons": q.get("review_reasons") or q.get("reviewReasons") or [],
                        "needs_review": (
                            q.get("needs_review")
                            if q.get("needs_review") is not None
                            else q.get("needsReview")
                        ),
                        "audit_flags": q.get("audit_flags") or q.get("auditFlags") or [],
                        "typo_notes": q.get("typo_notes") or q.get("typoNotes") or [],
                        "rubric_refs": q.get("rubric_refs") or q.get("rubricRefs") or [],
                        "honesty_note": q.get("honesty_note") or q.get("honestyNote"),
                        "question_type": q.get("question_type") or q.get("questionType"),
                        "merge_source": q.get("merge_source") or q.get("mergeSource"),
                        "scoring_point_results": (
                            q.get("scoring_point_results") or q.get("scoring_results") or []
                        ),
                    }
                )
        # å¦åˆ™ä» page_results æå–
        elif student.get("page_results"):
            for page in student["page_results"]:
                if page.get("status") == "completed" and not page.get("is_blank_page", False):
                    for q in page.get("question_details", []):
                        question_results.append(
                            {
                                "question_id": q.get("question_id", ""),
                                "score": q.get("score", 0),
                                "max_score": q.get("max_score", 0),
                                "feedback": q.get("feedback", ""),
                                "student_answer": q.get("student_answer", ""),
                                "is_correct": q.get("is_correct", False),
                                "confidence": q.get("confidence", 1.0),
                                "confidence_reason": q.get("confidence_reason")
                                or q.get("confidenceReason"),
                                "self_critique": q.get("self_critique") or q.get("selfCritique"),
                                "self_critique_confidence": (
                                    q.get("self_critique_confidence")
                                    or q.get("selfCritiqueConfidence")
                                ),
                                "review_summary": q.get("review_summary") or q.get("reviewSummary"),
                                "review_corrections": q.get("review_corrections")
                                or q.get("reviewCorrections")
                                or [],
                                "review_reasons": q.get("review_reasons")
                                or q.get("reviewReasons")
                                or [],
                                "needs_review": (
                                    q.get("needs_review")
                                    if q.get("needs_review") is not None
                                    else q.get("needsReview")
                                ),
                                "audit_flags": q.get("audit_flags") or q.get("auditFlags") or [],
                                "typo_notes": q.get("typo_notes") or q.get("typoNotes") or [],
                                "rubric_refs": q.get("rubric_refs") or q.get("rubricRefs") or [],
                                "honesty_note": q.get("honesty_note") or q.get("honestyNote"),
                                "question_type": q.get("question_type") or q.get("questionType"),
                                "is_cross_page": q.get("is_cross_page", False),
                                "page_indices": q.get("page_indices") or [page.get("page_index")],
                                "merge_source": q.get("merge_source") or q.get("mergeSource"),
                                "scoring_point_results": (
                                    q.get("scoring_point_results") or q.get("scoring_results") or []
                                ),
                            }
                        )

        export_data["students"].append(
            {
                "student_name": student["student_key"],
                "student_id": student.get("student_id"),
                "score": total_score,
                "max_score": max_score,
                "percentage": round(percentage, 1),
                "question_results": question_results,
                "confidence": student.get("confidence", 0),
                "needs_confirmation": student.get("needs_confirmation", False),
                "start_page": student.get("start_page", 0),
                "end_page": student.get("end_page", 0),
                "student_summary": summary,
                "self_audit": audit,
                "draft_question_details": student.get("draft_question_details"),
                "draft_total_score": student.get("draft_total_score"),
                "draft_max_score": student.get("draft_max_score"),
                "missing_question_ids": student.get("missing_question_ids"),
            }
        )

    class_report = _build_class_report(student_results)
    export_data["class_report"] = class_report

    # å¯¼å‡ºä¸º JSON æ–‡ä»¶ (Requirements: 9.4, 11.4)
    # æ— æ•°æ®åº“æ¨¡å¼æˆ–æœ‰å¤±è´¥æ—¶éƒ½å¯¼å‡º
    if not persisted or has_failures:
        try:
            import os

            # åˆ›å»ºå¯¼å‡ºç›®å½•
            export_dir = os.getenv("EXPORT_DIR", "./exports")
            os.makedirs(export_dir, exist_ok=True)

            # ç”Ÿæˆæ–‡ä»¶å
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")

            # å¦‚æœæœ‰å¤±è´¥ï¼Œæ ‡è®°ä¸ºéƒ¨åˆ†ç»“æœ (Requirement 9.4)
            if has_failures:
                filename = f"partial_result_{batch_id}_{timestamp}.json"
                logger.info(f"[export] ä¿å­˜éƒ¨åˆ†ç»“æœï¼ˆ{len(failed_pages)} ä¸ªé¡µé¢å¤±è´¥ï¼‰: {filename}")
            else:
                filename = f"grading_result_{batch_id}_{timestamp}.json"

            filepath = os.path.join(export_dir, filename)

            # å†™å…¥ JSON æ–‡ä»¶
            with open(filepath, "w", encoding="utf-8") as f:
                json.dump(export_data, f, ensure_ascii=False, indent=2)

            export_data["json_file"] = filepath

            if has_failures:
                logger.warning(
                    f"[export] éƒ¨åˆ†ç»“æœå·²ä¿å­˜: {filepath}. "
                    f"å®Œæˆ={len(grading_results) - len(failed_pages)}/{len(grading_results)} é¡µ"
                )
            else:
                logger.info(f"[export] ç»“æœå·²å¯¼å‡ºä¸º JSON: {filepath}")

        except Exception as e:
            logger.error(f"[export] JSON å¯¼å‡ºå¤±è´¥: {e}", exc_info=True)
            export_data["json_export_error"] = str(e)

            # è®°å½•é”™è¯¯
            from src.utils.error_handling import get_error_manager

            error_manager = get_error_manager()
            error_manager.add_error(
                exc=e,
                context={
                    "batch_id": batch_id,
                    "function": "export_node",
                    "export_type": "json",
                },
                batch_id=batch_id,
            )

    # å¯¼å‡ºé”™è¯¯æ—¥å¿—ï¼ˆå¦‚æœæœ‰é”™è¯¯ï¼‰
    try:
        from src.utils.error_handling import get_error_manager

        error_manager = get_error_manager()

        batch_errors = error_manager.get_errors_by_batch(batch_id)
        if batch_errors:
            import os

            export_dir = os.getenv("EXPORT_DIR", "./exports")
            os.makedirs(export_dir, exist_ok=True)

            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            error_log_file = os.path.join(export_dir, f"error_log_{batch_id}_{timestamp}.json")

            error_manager.export_to_file(error_log_file)
            export_data["error_log_file"] = error_log_file

            logger.info(
                f"[export] é”™è¯¯æ—¥å¿—å·²å¯¼å‡º: {error_log_file} " f"({len(batch_errors)} ä¸ªé”™è¯¯)"
            )
    except Exception as e:
        logger.error(f"[export] é”™è¯¯æ—¥å¿—å¯¼å‡ºå¤±è´¥: {e}", exc_info=True)

    logger.info(
        f"[export] å¯¼å‡ºå®Œæˆ: batch_id={batch_id}, "
        f"å­¦ç”Ÿæ•°={len(export_data['students'])}, "
        f"è·¨é¡µé¢˜ç›®æ•°={len(cross_page_questions)}, "
        f"å¤±è´¥é¡µé¢æ•°={len(failed_pages)}"
    )

    return {
        "export_data": export_data,
        "student_results": student_results,
        "class_report": class_report,
        "current_stage": "completed",
        "percentage": 100.0,
        "timestamps": {
            **state.get("timestamps", {}),
            "export_at": datetime.now().isoformat(),
            "completed_at": datetime.now().isoformat(),
        },
    }


# ==================== Graph ç¼–è¯‘ ====================


def create_batch_grading_graph(
    checkpointer: Optional[AsyncPostgresSaver] = None,
    batch_config: Optional[BatchConfig] = None,
) -> StateGraph:
    """åˆ›å»ºæ‰¹é‡æ‰¹æ”¹ Graphï¼ˆç®€åŒ–ç‰ˆï¼‰

    å·¥ä½œæµï¼š
    1. intake: æ¥æ”¶æ–‡ä»¶
    2. preprocess: å›¾åƒé¢„å¤„ç†
    3. rubric_parse: è§£æè¯„åˆ†æ ‡å‡†ï¼ˆå«è‡ªç™½/confession ç”Ÿæˆï¼‰
    4. rubric_self_review: è‡ªåŠ¨å¤æ ¸ï¼ˆåŸºäºè‡ªç™½ï¼ŒLLM è‡ªåŠ¨ä¿®æ­£ï¼‰
    5. rubric_review: äººå·¥å®¡æ ¸ï¼ˆå¯è·³è¿‡ï¼‰
    6. grade_batch (å¹¶è¡Œ): æŒ‰å­¦ç”Ÿæˆ–æ‰¹æ¬¡å¤§å°å¹¶è¡Œæ‰¹æ”¹ï¼ˆå«å®¡è®¡ä¿¡æ¯ï¼‰
    7. logic_review: é€»è¾‘å¤æ ¸ï¼ˆåŸºäºå®¡è®¡ä¿¡æ¯ï¼‰
    8. review: ç»“æœå®¡æ ¸
    9. export: å¯¼å‡ºç»“æœ

    æµç¨‹å›¾ï¼š
    ```
    intake
      â†“
    preprocess
      â†“
    rubric_parse  â† è§£æè¯„åˆ†æ ‡å‡† + ç”Ÿæˆè‡ªç™½ï¼ˆconfessionï¼‰
      â†“
    rubric_self_review  â† è‡ªåŠ¨å¤æ ¸ï¼ˆåŸºäºè‡ªç™½å’ŒåŸå›¾ä¿®æ­£ï¼‰
      â†“
    rubric_review (å¯è·³è¿‡)  â† äººå·¥å¤æ ¸
      â†“
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚ grade_batch (N) â”‚  â† å¹¶è¡Œæ‰¹æ”¹ï¼ˆæŒ‰å­¦ç”Ÿåˆ†æ‰¹ï¼Œå«å®¡è®¡ä¿¡æ¯ï¼‰
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
      â†“
    logic_review  â† é€»è¾‘å¤æ ¸ï¼ˆåŸºäºå®¡è®¡ä¿¡æ¯ï¼‰
      â†“
    review
      â†“
    export
      â†“
    END
    ```

    ç‰¹æ€§ï¼š
    - æŒ‰å­¦ç”Ÿåˆ†æ‰¹æ‰¹æ”¹ï¼ˆå‰ç«¯æä¾› student_mappingï¼‰
    - Worker ç‹¬ç«‹æ€§ä¿è¯ (Requirements: 3.2)
    - æ‰¹æ¬¡å¤±è´¥é‡è¯• (Requirements: 3.3, 9.3)
    - å®æ—¶è¿›åº¦æŠ¥å‘Š (Requirements: 3.4)
    - è®°å¿†ç³»ç»Ÿé›†æˆï¼ˆç§‘ç›®éš”ç¦»ï¼‰

    å·²ç§»é™¤ï¼š
    - index èŠ‚ç‚¹ï¼ˆä¸å†éœ€è¦ç´¢å¼•å±‚ï¼‰
    - cross_page_merge èŠ‚ç‚¹ï¼ˆä¸å†éœ€è¦è·¨é¡µåˆå¹¶ï¼‰
    - index_merge èŠ‚ç‚¹ï¼ˆä¸å†éœ€è¦ç´¢å¼•èšåˆï¼‰

    Args:
        checkpointer: PostgreSQL Checkpointerï¼ˆå¯é€‰ï¼‰
        batch_config: æ‰¹æ¬¡é…ç½®ï¼ˆå¯é€‰ï¼Œé»˜è®¤ä»ç¯å¢ƒå˜é‡åŠ è½½ï¼‰

    Returns:
        ç¼–è¯‘åçš„ Graph
    """
    # è®¾ç½®æ‰¹æ¬¡é…ç½®
    if batch_config:
        set_batch_config(batch_config)

    config = get_batch_config()
    logger.info(
        f"åˆ›å»ºæ‰¹é‡æ‰¹æ”¹ Graph: batch_size={config.batch_size}, "
        f"max_workers={config.max_concurrent_workers}, "
        f"max_retries={config.max_retries}"
    )

    graph = StateGraph(BatchGradingGraphState)

    # æ·»åŠ èŠ‚ç‚¹
    graph.add_node("intake", intake_node)
    graph.add_node("preprocess", preprocess_node)
    # graph.add_node("index", index_node)  # å·²ç§»é™¤ï¼šä¸å†éœ€è¦ç´¢å¼•å±‚
    graph.add_node("rubric_parse", rubric_parse_node)
    graph.add_node("rubric_self_review", rubric_self_review_node)  # è‡ªåŠ¨å¤æ ¸èŠ‚ç‚¹ï¼ˆåŸºäºè‡ªç™½ï¼‰
    graph.add_node("rubric_review", rubric_review_node)
    graph.add_node("grade_batch", grade_batch_node)
    # graph.add_node("simple_aggregate", simple_aggregate_node)  # å·²ç§»é™¤ï¼šgrade_batch ç›´æ¥è¾“å‡º student_results
    # graph.add_node("cross_page_merge", cross_page_merge_node)  # å·²ç§»é™¤ï¼šä¸å†éœ€è¦è·¨é¡µåˆå¹¶
    # graph.add_node("index_merge", index_merge_node)  # å·²ç§»é™¤ï¼šä¸å†éœ€è¦ç´¢å¼•èšåˆ
    # graph.add_node("confession", confession_node)  # å·²ç§»é™¤ï¼šæ‰¹æ”¹å’Œå®¡è®¡ä¸€ä½“åŒ–æ”¹é€ 
    graph.add_node("logic_review", logic_review_node)

    graph.add_node("review", review_node)
    graph.add_node("export", export_node)

    # å…¥å£ç‚¹
    graph.set_entry_point("intake")

    # ç®€åŒ–æµç¨‹ï¼šintake â†’ preprocess â†’ rubric_parse â†’ rubric_self_review â†’ rubric_review (å¯é€‰)
    graph.add_edge("intake", "preprocess")
    graph.add_edge("preprocess", "rubric_parse")
    graph.add_edge("rubric_parse", "rubric_self_review")  # è§£æåå…ˆè¿›è¡Œè‡ªåŠ¨å¤æ ¸
    
    # âœ… å…ˆæ·»åŠ å ä½èŠ‚ç‚¹,ç”¨äºè·³è¿‡ review æ—¶çš„è·¯ç”±
    async def grading_fanout_placeholder_node(state: BatchGradingGraphState) -> Dict[str, Any]:
        """å ä½èŠ‚ç‚¹,ç”¨äºè·³è¿‡ review æ—¶ç›´æ¥è¿›å…¥ grading_fanout"""
        batch_id = state.get("batch_id", "unknown")
        logger.info(f"[grading_fanout_placeholder] è·³è¿‡ review,å‡†å¤‡è¿›å…¥æ‰¹æ”¹: batch_id={batch_id}")
        return {
            "current_stage": "grading_fanout_placeholder",
            "percentage": 20.0,
        }
    
    graph.add_node("grading_fanout_placeholder", grading_fanout_placeholder_node)
    
    # âœ… ä¿®å¤:æ·»åŠ æ¡ä»¶è·¯ç”±,æ ¹æ® enable_review å†³å®šæ˜¯å¦éœ€è¦ rubric_review
    def should_review_rubric(state: BatchGradingGraphState) -> str:
        """å†³å®šæ˜¯å¦éœ€è¦ rubric reviewï¼ˆåœ¨è‡ªåŠ¨å¤æ ¸ä¹‹åï¼‰"""
        batch_id = state.get("batch_id", "unknown")
        enable_review = state.get("inputs", {}).get("enable_review", True)
        parsed_rubric = state.get("parsed_rubric", {})
        grading_mode = _resolve_grading_mode(state.get("inputs", {}), parsed_rubric)
        
        # å¦‚æœæ˜¯ assist æ¨¡å¼æˆ– review è¢«ç¦ç”¨,ç›´æ¥è·³åˆ° grading_fanout
        if grading_mode.startswith("assist") or not enable_review:
            logger.info(f"[should_review_rubric] è·³è¿‡ review,ç›´æ¥è¿›å…¥æ‰¹æ”¹: batch_id={batch_id}, mode={grading_mode}, enable_review={enable_review}")
            return "skip_review"
        
        # å¦‚æœæ²¡æœ‰ rubric,ä¹Ÿè·³è¿‡
        if not parsed_rubric or not parsed_rubric.get("questions"):
            logger.info(f"[should_review_rubric] æ²¡æœ‰ rubric,è·³è¿‡ review: batch_id={batch_id}")
            return "skip_review"
        
        logger.info(f"[should_review_rubric] éœ€è¦ review: batch_id={batch_id}")
        return "do_review"
    
    # rubric_self_review åè¿›è¡Œæ¡ä»¶è·¯ç”±ï¼ˆå†³å®šæ˜¯å¦éœ€è¦äººå·¥å¤æ ¸ï¼‰
    graph.add_conditional_edges(
        "rubric_self_review",
        should_review_rubric,
        {
            "do_review": "rubric_review",
            "skip_review": "grading_fanout_placeholder",
        },
    )

    # rubric_review åä¹Ÿè¿›å…¥ grading_fanout
    graph.add_conditional_edges(
        "rubric_review",
        grading_fanout_router,
        [
            "grade_batch",
            "logic_review",  # è·³è¿‡ confessionï¼Œç›´æ¥åˆ° logic_review
        ],
    )
    
    # grading_fanout_placeholder ä¹Ÿä½¿ç”¨ç›¸åŒçš„è·¯ç”±
    graph.add_conditional_edges(
        "grading_fanout_placeholder",
        grading_fanout_router,
        [
            "grade_batch",
            "logic_review",  # è·³è¿‡ confessionï¼Œç›´æ¥åˆ° logic_review
        ],
    )

    # ğŸ”¥ ä¿®å¤ï¼šç§»é™¤æœ‰é—®é¢˜çš„ grading_merge_gate æ¡ä»¶è¾¹
    # é—®é¢˜ï¼šå¹¶è¡Œ Send ä»»åŠ¡å®Œæˆæ—¶ï¼Œæ¯ä¸ªä»»åŠ¡éƒ½ä¼šç‹¬ç«‹è§¦å‘æ¡ä»¶è¾¹ï¼Œ
    # å¯¼è‡´çŠ¶æ€èšåˆå‰å°±æ£€æŸ¥ student_results æ•°é‡ï¼Œäº§ç”Ÿç«æ€æ¡ä»¶ã€‚
    # 
    # è§£å†³æ–¹æ¡ˆï¼šç›´æ¥ä½¿ç”¨æ™®é€šè¾¹ï¼ŒLangGraph ä¼šè‡ªåŠ¨ç­‰å¾…æ‰€æœ‰ Send ä»»åŠ¡å®Œæˆã€
    # çŠ¶æ€èšåˆåï¼Œå†è¿›å…¥ä¸‹ä¸€ä¸ªèŠ‚ç‚¹ï¼ˆlogic_reviewï¼‰ã€‚
    # confession èŠ‚ç‚¹å·²ç§»é™¤ï¼Œæ‰¹æ”¹å’Œå®¡è®¡ä¸€ä½“åŒ–
    graph.add_edge("grade_batch", "logic_review")

    # ç®€åŒ–æµç¨‹ï¼šlogic_review â†’ review â†’ export â†’ END
    # ï¼ˆconfession èŠ‚ç‚¹å·²ç§»é™¤ï¼‰
    graph.add_edge("logic_review", "review")
    graph.add_edge("review", "export")
    graph.add_edge("export", END)

    # ç¼–è¯‘
    compile_kwargs = {}
    if checkpointer:
        compile_kwargs["checkpointer"] = checkpointer

    compiled_graph = graph.compile(**compile_kwargs)

    logger.info("æ‰¹é‡æ‰¹æ”¹ Graph å·²ç¼–è¯‘")

    return compiled_graph


def _count_graded_pages(grading_results: List[Dict[str, Any]]) -> int:
    """Count unique graded pages from grading_results (supports multi-page student batches)."""
    if not grading_results:
        return 0
    pages = set()
    for result in grading_results:
        page_indices = result.get("page_indices") if isinstance(result, dict) else None
        if isinstance(page_indices, list) and page_indices:
            for idx in page_indices:
                if idx is None:
                    continue
                pages.add(idx)
            continue
        page_index = result.get("page_index") if isinstance(result, dict) else None
        if page_index is None:
            continue
        pages.add(page_index)
    return len(pages)


def grading_merge_gate(state: BatchGradingGraphState) -> str:
    """
    æ‰¹æ”¹æ±‡èšé—¨æ§ï¼ˆå·²å¼ƒç”¨ï¼‰

    âš ï¸ æ­¤å‡½æ•°å½“å‰æœªè¢«ä½¿ç”¨ï¼
    
    åŸé—®é¢˜ï¼šå½“ä½¿ç”¨ Send è¿›è¡Œå¹¶è¡Œæ‰¹æ”¹æ—¶ï¼Œæ¯ä¸ªå¹¶è¡Œä»»åŠ¡å®Œæˆåéƒ½ä¼šç‹¬ç«‹è§¦å‘æ­¤æ¡ä»¶è¾¹ï¼Œ
    ä½†æ­¤æ—¶çŠ¶æ€èšåˆå¯èƒ½è¿˜æœªå®Œæˆï¼Œå¯¼è‡´ student_results æ•°é‡æ£€æŸ¥å¤±è´¥ï¼Œè¿”å› "wait" â†’ ENDï¼Œ
    æ•´ä¸ªå›¾è¢«æå‰æ ‡è®°ä¸º "completed"ï¼Œè·³è¿‡äº† confession å’Œ logic_reviewã€‚
    
    ä¿®å¤æ–¹æ¡ˆï¼šç§»é™¤æ¡ä»¶è¾¹ï¼Œæ”¹ä¸ºç›´æ¥ä½¿ç”¨æ™®é€šè¾¹ (add_edge)ï¼Œè®© LangGraph è‡ªåŠ¨ç­‰å¾…
    æ‰€æœ‰ Send ä»»åŠ¡å®Œæˆå¹¶èšåˆçŠ¶æ€åï¼Œå†è¿›å…¥ä¸‹ä¸€ä¸ªèŠ‚ç‚¹ã€‚
    
    ä¿ç•™æ­¤å‡½æ•°ä»¥ä¾¿æœªæ¥è°ƒè¯•æˆ–å‚è€ƒã€‚
    """
    batch_id = state.get("batch_id", "unknown")
    grading_results = state.get("grading_results") or []
    student_results = state.get("student_results") or []
    student_boundaries = state.get("student_boundaries") or []
    
    total_students = len(student_boundaries) if student_boundaries else 0
    completed_students = len(student_results)
    
    # ğŸ” DEBUG: è¯¦ç»†æ—¥å¿—è®°å½•æ¯æ¬¡è°ƒç”¨çš„çŠ¶æ€
    logger.warning(
        f"[grading_merge] ğŸ” DEBUG entry: batch_id={batch_id}, "
        f"completed={completed_students}, total={total_students}, "
        f"student_keys={[s.get('student_key') for s in student_results]}, "
        f"boundary_keys={[b.get('student_key') for b in student_boundaries]}, "
        f"state_keys={sorted(list(state.keys()))}"
    )
    
    logger.info(
        f"[grading_merge] è¯Šæ–­: batch_id={batch_id}, "
        f"completed_students={completed_students}, total_students={total_students}, "
        f"student_results={len(student_results)}, grading_results={len(grading_results)}"
    )

    # ğŸ”§ ä¿®å¤ï¼šä¼˜å…ˆæ£€æŸ¥ student_resultsï¼ˆgrade_student æ¨¡å¼ï¼‰
    # å¦‚æœæœ‰ student_boundariesï¼Œå°±æŒ‰å­¦ç”Ÿæ•°é‡åˆ¤æ–­
    if total_students > 0:
        if completed_students >= total_students:
            logger.info(f"[grading_merge] âœ… æ‰€æœ‰ {total_students} ä¸ªå­¦ç”Ÿæ‰¹æ”¹å®Œæˆï¼Œè¿›å…¥è‡ªç™½é˜¶æ®µ")
            logger.warning(f"[grading_merge] ğŸ” DEBUG: returning 'continue' - all students done")
            return "continue"
        else:
            logger.info(f"[grading_merge] â³ å­¦ç”Ÿæ‰¹æ”¹è¿›åº¦: {completed_students}/{total_students}")
            logger.warning(f"[grading_merge] ğŸ” DEBUG: returning 'wait' - {completed_students}/{total_students} students")
            return "wait"
    
    # ğŸ”§ Fallbackï¼šå¦‚æœæ²¡æœ‰ student_boundariesï¼Œæ£€æŸ¥æ˜¯å¦æœ‰ä»»ä½•æ‰¹æ”¹ç»“æœ
    if student_results:
        logger.info(f"[grading_merge] âœ… æœ‰ {len(student_results)} ä¸ªå­¦ç”Ÿç»“æœï¼ˆæ— è¾¹ç•Œä¿¡æ¯ï¼‰ï¼Œè¿›å…¥è‡ªç™½é˜¶æ®µ")
        logger.warning(f"[grading_merge] ğŸ” DEBUG: returning 'continue' - fallback with {len(student_results)} results")
        return "continue"
    
    if grading_results:
        logger.info(f"[grading_merge] âœ… æœ‰ {len(grading_results)} ä¸ªé¡µé¢ç»“æœï¼Œè¿›å…¥è‡ªç™½é˜¶æ®µ")
        return "continue"
    
    # æ²¡æœ‰ä»»ä½•ç»“æœï¼Œç»§ç»­ç­‰å¾…ï¼ˆå¯èƒ½è¿˜åœ¨å¤„ç†ä¸­ï¼‰
    logger.warning("[grading_merge] âš ï¸ æ²¡æœ‰æ‰¹æ”¹ç»“æœï¼Œç»§ç»­ç­‰å¾…")
    logger.warning(f"[grading_merge] ğŸ” DEBUG: returning 'wait' - no results at all")
    return "wait"


# ==================== å¯¼å‡º ====================

__all__ = [
    # é…ç½®ç±»
    "BatchConfig",
    "get_batch_config",
    "set_batch_config",
    # è¿›åº¦ç±»
    "BatchProgress",
    "BatchTaskState",
    # èŠ‚ç‚¹å‡½æ•°
    "intake_node",
    "preprocess_node",
    "rubric_parse_node",
    "grade_batch_node",
    "confession_node",  # åŸ confession_node
    "logic_review_node",

    "review_node",
    "export_node",
    # è·¯ç”±å‡½æ•°
    "grading_fanout_router",
    # Graph åˆ›å»º
    "create_batch_grading_graph",
]
