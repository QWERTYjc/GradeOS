"""EssayAgent - ä½œæ–‡/ç®€ç­”é¢˜æ‰¹æ”¹æ™ºèƒ½ä½“

ä¾æ®å†…å®¹ã€ç»“æ„ã€è¯­è¨€ç­‰ç»´åº¦è¯„åˆ†ï¼Œç”Ÿæˆè¯æ®é“¾
"""

import json
import logging
from typing import List, Dict, Any, Optional

from langchain_google_genai import ChatGoogleGenerativeAI
from langchain_core.messages import HumanMessage

from src.models.enums import QuestionType
from src.models.state import ContextPack, GradingState, EvidenceItem
from src.agents.base import BaseGradingAgent
from src.config.models import get_default_model
from src.utils.llm_thinking import get_thinking_kwargs


logger = logging.getLogger(__name__)


# ä½œæ–‡è¯„åˆ†ç»´åº¦
ESSAY_DIMENSIONS = [
    {"name": "å†…å®¹", "weight": 0.4, "description": "è§‚ç‚¹æ˜ç¡®ã€è®ºæ®å……åˆ†ã€åˆ‡é¢˜"},
    {"name": "ç»“æ„", "weight": 0.25, "description": "å±‚æ¬¡æ¸…æ™°ã€é€»è¾‘è¿è´¯ã€æ®µè½åˆ†æ˜"},
    {"name": "è¯­è¨€", "weight": 0.25, "description": "è¡¨è¾¾å‡†ç¡®ã€è¯­å¥é€šé¡ºã€ç”¨è¯æ°å½“"},
    {"name": "ä¹¦å†™", "weight": 0.1, "description": "å­—è¿¹å·¥æ•´ã€å·é¢æ•´æ´"},
]


class EssayAgent(BaseGradingAgent):
    """ä½œæ–‡/ç®€ç­”é¢˜æ‰¹æ”¹æ™ºèƒ½ä½“
    
    ä¸“é—¨å¤„ç†ä½œæ–‡å’Œç®€ç­”é¢˜ï¼Œä»å¤šä¸ªç»´åº¦è¿›è¡Œç»¼åˆè¯„åˆ†ã€‚
    
    ç‰¹ç‚¹ï¼š
    - å¤šç»´åº¦è¯„åˆ†ï¼ˆå†…å®¹ã€ç»“æ„ã€è¯­è¨€ã€ä¹¦å†™ï¼‰
    - æ•´ä½“å°è±¡ä¸ç»†èŠ‚åˆ†æç»“åˆ
    - ç”Ÿæˆè¯¦ç»†çš„æ”¹è¿›å»ºè®®
    - ç”Ÿæˆå®Œæ•´è¯æ®é“¾
    """
    
    def __init__(
        self,
        api_key: str,
        model_name: Optional[str] = None
    ):
        """åˆå§‹åŒ– EssayAgent
        
        Args:
            api_key: Google AI API å¯†é’¥
            model_name: ä½¿ç”¨çš„æ¨¡å‹åç§°ï¼Œé»˜è®¤ä½¿ç”¨å…¨å±€é…ç½®
        """
        if model_name is None:
            model_name = get_default_model()
        thinking_kwargs = get_thinking_kwargs(model_name, enable_thinking=True)
        self.llm = ChatGoogleGenerativeAI(
            model=model_name,
            google_api_key=api_key,
            temperature=0.3,  # slightly higher temperature for flexible evaluation
            **thinking_kwargs,
        )
        self._api_key = api_key
    
    @property
    def agent_type(self) -> str:
        return "essay"
    
    @property
    def supported_question_types(self) -> List[QuestionType]:
        return [QuestionType.ESSAY]
    
    async def grade(self, context_pack: ContextPack) -> GradingState:
        """æ‰§è¡Œä½œæ–‡/ç®€ç­”é¢˜æ‰¹æ”¹
        
        Args:
            context_pack: ä¸Šä¸‹æ–‡åŒ…
            
        Returns:
            GradingState: æ‰¹æ”¹ç»“æœ
        """
        question_image = context_pack.get("question_image", "")
        rubric = context_pack.get("rubric", "")
        max_score = context_pack.get("max_score", 0.0)
        standard_answer = context_pack.get("standard_answer", "")
        previous_result = context_pack.get("previous_result")
        
        reasoning_trace: List[str] = []
        
        try:
            # æ­¥éª¤1ï¼šè§†è§‰æå– - è¯†åˆ«å­¦ç”Ÿä½œç­”å†…å®¹
            reasoning_trace.append("å¼€å§‹è§†è§‰æå–ï¼šè¯†åˆ«å­¦ç”Ÿä½œç­”å†…å®¹")
            vision_analysis = await self._extract_essay_content(
                question_image, rubric
            )
            reasoning_trace.append(f"è§†è§‰æå–å®Œæˆï¼šå­—æ•°çº¦ {vision_analysis.get('word_count', 0)}")
            
            # æ­¥éª¤2ï¼šå¤šç»´åº¦è¯„åˆ†
            reasoning_trace.append("å¼€å§‹å¤šç»´åº¦è¯„åˆ†")
            scoring_result = await self._score_essay(
                vision_analysis, rubric, max_score, standard_answer
            )
            reasoning_trace.append(f"è¯„åˆ†å®Œæˆï¼šæ€»åˆ† {scoring_result['total_score']}/{max_score}")
            
            # æ­¥éª¤3ï¼šç”Ÿæˆè¯æ®é“¾
            evidence_chain = self._build_evidence_chain(
                scoring_result.get("dimension_scores", []),
                rubric
            )
            
            # æ­¥éª¤4ï¼šè®¡ç®—ç½®ä¿¡åº¦
            confidence = self._calculate_confidence(
                vision_analysis, scoring_result, previous_result
            )
            reasoning_trace.append(f"ç½®ä¿¡åº¦ï¼š{confidence:.2f}")
            
            # æ­¥éª¤5ï¼šç”Ÿæˆå­¦ç”Ÿåé¦ˆ
            student_feedback = self._generate_feedback(scoring_result)
            
            # æ„å»º rubric_mapping
            rubric_mapping = []
            for dim_score in scoring_result.get("dimension_scores", []):
                rubric_mapping.append({
                    "rubric_point": dim_score.get("dimension", ""),
                    "evidence": dim_score.get("evidence", ""),
                    "score_awarded": dim_score.get("score", 0),
                    "max_score": dim_score.get("max_score", 0)
                })
            
            # æ„å»ºè§†è§‰æ ‡æ³¨
            visual_annotations = [{
                "type": "essay_region",
                "bounding_box": vision_analysis.get("text_region", [0, 0, 1000, 1000]),
                "label": "ä½œç­”åŒºåŸŸ",
                "word_count": vision_analysis.get("word_count", 0)
            }]
            
            # æ·»åŠ äº®ç‚¹å’Œé—®é¢˜æ ‡æ³¨
            for highlight in scoring_result.get("highlights", []):
                visual_annotations.append({
                    "type": "highlight",
                    "bounding_box": highlight.get("location", [0, 0, 100, 100]),
                    "label": highlight.get("text", ""),
                    "category": "positive"
                })
            
            for issue in scoring_result.get("issues", []):
                visual_annotations.append({
                    "type": "issue",
                    "bounding_box": issue.get("location", [0, 0, 100, 100]),
                    "label": issue.get("text", ""),
                    "category": "negative"
                })
            
            return GradingState(
                context_pack=context_pack,
                vision_analysis=vision_analysis.get("content", ""),
                rubric_mapping=rubric_mapping,
                initial_score=scoring_result["total_score"],
                reasoning_trace=reasoning_trace,
                critique_feedback=None,
                evidence_chain=evidence_chain,
                final_score=scoring_result["total_score"],
                max_score=max_score,
                confidence=confidence,
                visual_annotations=visual_annotations,
                student_feedback=student_feedback,
                agent_type=self.agent_type,
                revision_count=0,
                is_finalized=True,
                needs_secondary_review=confidence < 0.75
            )
            
        except Exception as e:
            logger.error(f"EssayAgent æ‰¹æ”¹å¤±è´¥: {e}")
            reasoning_trace.append(f"é”™è¯¯: {str(e)}")
            return GradingState(
                context_pack=context_pack,
                vision_analysis="",
                rubric_mapping=[],
                initial_score=0.0,
                reasoning_trace=reasoning_trace,
                critique_feedback=None,
                evidence_chain=[],
                final_score=0.0,
                max_score=max_score,
                confidence=0.0,
                visual_annotations=[],
                student_feedback="æ‰¹æ”¹è¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯ï¼Œéœ€è¦äººå·¥å®¡æ ¸",
                agent_type=self.agent_type,
                revision_count=0,
                is_finalized=False,
                needs_secondary_review=True,
                error=str(e)
            )
    
    async def _extract_essay_content(
        self,
        question_image: str,
        rubric: str
    ) -> dict:
        """ä»å›¾åƒä¸­æå–ä½œæ–‡/ç®€ç­”å†…å®¹
        
        Args:
            question_image: Base64 ç¼–ç çš„é¢˜ç›®å›¾åƒ
            rubric: è¯„åˆ†ç»†åˆ™
            
        Returns:
            åŒ…å«ä½œç­”å†…å®¹çš„å­—å…¸
        """
        prompt = f"""è¯·åˆ†æè¿™å¼ ä½œæ–‡/ç®€ç­”é¢˜çš„ç­”é¢˜å›¾åƒï¼Œæå–å­¦ç”Ÿçš„ä½œç­”å†…å®¹ã€‚

è¯„åˆ†ç»†åˆ™ï¼š
{rubric}

è¯·è¿”å› JSON æ ¼å¼ï¼š
{{
    "content": "å­¦ç”Ÿä½œç­”çš„å®Œæ•´æ–‡å­—å†…å®¹",
    "word_count": ä¼°è®¡å­—æ•°,
    "text_region": [ymin, xmin, ymax, xmax],  // ä½œç­”åŒºåŸŸä½ç½®
    "handwriting_quality": "excellent/good/fair/poor",  // ä¹¦å†™è´¨é‡
    "structure_analysis": {{
        "has_title": true/false,
        "paragraph_count": æ®µè½æ•°,
        "has_clear_structure": true/false
    }},
    "key_points": ["å…³é”®ç‚¹1", "å…³é”®ç‚¹2", ...],  // æå–çš„è¦ç‚¹
    "readability": "high/medium/low"  // å¯è¯»æ€§
}}

æ³¨æ„ï¼š
- å°½å¯èƒ½å®Œæ•´åœ°æå–æ–‡å­—å†…å®¹
- è¯†åˆ«æ®µè½ç»“æ„
- è¯„ä¼°ä¹¦å†™è´¨é‡
- åæ ‡ä½¿ç”¨å½’ä¸€åŒ–æ ¼å¼ï¼ˆ0-1000ï¼‰"""

        message = HumanMessage(
            content=[
                {"type": "text", "text": prompt},
                {
                    "type": "image_url",
                    "image_url": f"data:image/jpeg;base64,{question_image}"
                }
            ]
        )
        
        response = await self.llm.ainvoke([message])
        result_text = self._extract_text(response.content)
        
        try:
            if "```json" in result_text:
                json_start = result_text.find("```json") + 7
                json_end = result_text.find("```", json_start)
                result_text = result_text[json_start:json_end].strip()
            return json.loads(result_text)
        except json.JSONDecodeError:
            logger.warning(f"æ— æ³•è§£æè§†è§‰æå–ç»“æœ: {result_text}")
            return {
                "content": result_text,
                "word_count": len(result_text),
                "text_region": [0, 0, 1000, 1000],
                "handwriting_quality": "fair",
                "structure_analysis": {
                    "has_title": False,
                    "paragraph_count": 1,
                    "has_clear_structure": False
                },
                "key_points": [],
                "readability": "medium"
            }
    
    async def _score_essay(
        self,
        vision_analysis: dict,
        rubric: str,
        max_score: float,
        standard_answer: str
    ) -> dict:
        """å¯¹ä½œæ–‡è¿›è¡Œå¤šç»´åº¦è¯„åˆ†
        
        Args:
            vision_analysis: è§†è§‰åˆ†æç»“æœ
            rubric: è¯„åˆ†ç»†åˆ™
            max_score: æ»¡åˆ†
            standard_answer: å‚è€ƒç­”æ¡ˆ
            
        Returns:
            è¯„åˆ†ç»“æœ
        """
        content = vision_analysis.get("content", "")
        structure = vision_analysis.get("structure_analysis", {})
        handwriting = vision_analysis.get("handwriting_quality", "fair")
        
        # æ„å»ºè¯„åˆ†æç¤º
        dimensions_text = "\n".join([
            f"- {d['name']}ï¼ˆæƒé‡{d['weight']*100}%ï¼‰: {d['description']}"
            for d in ESSAY_DIMENSIONS
        ])
        
        prompt = f"""è¯·å¯¹ä»¥ä¸‹ä½œæ–‡/ç®€ç­”è¿›è¡Œå¤šç»´åº¦è¯„åˆ†ã€‚

å­¦ç”Ÿä½œç­”å†…å®¹ï¼š
{content}

ç»“æ„åˆ†æï¼š
- æ˜¯å¦æœ‰æ ‡é¢˜ï¼š{structure.get('has_title', False)}
- æ®µè½æ•°ï¼š{structure.get('paragraph_count', 0)}
- ç»“æ„æ¸…æ™°ï¼š{structure.get('has_clear_structure', False)}

ä¹¦å†™è´¨é‡ï¼š{handwriting}

è¯„åˆ†ç»†åˆ™ï¼š
{rubric}

å‚è€ƒç­”æ¡ˆ/è¦ç‚¹ï¼š
{standard_answer if standard_answer else "æœªæä¾›"}

æ»¡åˆ†ï¼š{max_score}

è¯„åˆ†ç»´åº¦ï¼š
{dimensions_text}

è¯·è¿”å› JSON æ ¼å¼ï¼š
{{
    "total_score": æ€»å¾—åˆ†,
    "dimension_scores": [
        {{
            "dimension": "ç»´åº¦åç§°",
            "max_score": è¯¥ç»´åº¦æ»¡åˆ†,
            "score": è¯¥ç»´åº¦å¾—åˆ†,
            "evidence": "è¯„åˆ†ä¾æ®ï¼ˆå¼•ç”¨å­¦ç”Ÿä½œç­”ä¸­çš„å…·ä½“å†…å®¹ï¼‰",
            "feedback": "è¯¥ç»´åº¦çš„è¯„ä»·"
        }}
    ],
    "highlights": [
        {{
            "text": "äº®ç‚¹å†…å®¹",
            "location": [ymin, xmin, ymax, xmax],
            "reason": "ä¸ºä»€ä¹ˆæ˜¯äº®ç‚¹"
        }}
    ],
    "issues": [
        {{
            "text": "é—®é¢˜å†…å®¹",
            "location": [ymin, xmin, ymax, xmax],
            "suggestion": "æ”¹è¿›å»ºè®®"
        }}
    ],
    "overall_comment": "æ•´ä½“è¯„è¯­",
    "grade_level": "A/B/C/D/E"  // ç­‰çº§è¯„å®š
}}"""

        response = await self.llm.ainvoke([HumanMessage(content=prompt)])
        result_text = self._extract_text(response.content)
        
        try:
            if "```json" in result_text:
                json_start = result_text.find("```json") + 7
                json_end = result_text.find("```", json_start)
                result_text = result_text[json_start:json_end].strip()
            return json.loads(result_text)
        except json.JSONDecodeError:
            logger.warning(f"æ— æ³•è§£æè¯„åˆ†ç»“æœ: {result_text}")
            # è¿”å›é»˜è®¤è¯„åˆ†
            return {
                "total_score": max_score * 0.6,  # é»˜è®¤ç»™60%
                "dimension_scores": [],
                "highlights": [],
                "issues": [],
                "overall_comment": "è¯„åˆ†è§£æå¤±è´¥ï¼Œéœ€è¦äººå·¥å®¡æ ¸",
                "grade_level": "C"
            }
    
    def _build_evidence_chain(
        self,
        dimension_scores: List[Dict[str, Any]],
        rubric: str
    ) -> List[EvidenceItem]:
        """æ„å»ºè¯æ®é“¾
        
        Args:
            dimension_scores: å„ç»´åº¦è¯„åˆ†
            rubric: è¯„åˆ†ç»†åˆ™
            
        Returns:
            è¯æ®é“¾åˆ—è¡¨
        """
        evidence_chain: List[EvidenceItem] = []
        
        for dim in dimension_scores:
            evidence: EvidenceItem = {
                "scoring_point": dim.get("dimension", ""),
                "image_region": [0, 0, 1000, 1000],  # ä½œæ–‡é€šå¸¸æ˜¯æ•´ä½“è¯„åˆ†
                "text_description": dim.get("evidence", ""),
                "reasoning": dim.get("feedback", ""),
                "rubric_reference": rubric[:100] if rubric else "ä½œæ–‡è¯„åˆ†æ ‡å‡†",
                "points_awarded": dim.get("score", 0)
            }
            evidence_chain.append(evidence)
        
        return evidence_chain
    
    def _calculate_confidence(
        self,
        vision_analysis: dict,
        scoring_result: dict,
        previous_result: Optional[dict]
    ) -> float:
        """è®¡ç®—ç½®ä¿¡åº¦
        
        Args:
            vision_analysis: è§†è§‰åˆ†æç»“æœ
            scoring_result: è¯„åˆ†ç»“æœ
            previous_result: å‰åºç»“æœ
            
        Returns:
            ç½®ä¿¡åº¦åˆ†æ•°
        """
        base_confidence = 0.75  # ä½œæ–‡è¯„åˆ†ä¸»è§‚æ€§è¾ƒå¼ºï¼ŒåŸºç¡€ç½®ä¿¡åº¦è¾ƒä½
        
        # æ ¹æ®å¯è¯»æ€§è°ƒæ•´
        readability = vision_analysis.get("readability", "medium")
        if readability == "high":
            base_confidence += 0.1
        elif readability == "low":
            base_confidence -= 0.15
        
        # æ ¹æ®ä¹¦å†™è´¨é‡è°ƒæ•´
        handwriting = vision_analysis.get("handwriting_quality", "fair")
        if handwriting == "poor":
            base_confidence -= 0.1
        
        # æ ¹æ®å­—æ•°è°ƒæ•´ï¼ˆå¤ªçŸ­å¯èƒ½è¯†åˆ«ä¸å®Œæ•´ï¼‰
        word_count = vision_analysis.get("word_count", 0)
        if word_count < 50:
            base_confidence -= 0.1
        
        # äºŒæ¬¡è¯„ä¼°ä¸€è‡´æ€§
        if previous_result:
            prev_score = previous_result.get("score", -1)
            curr_score = scoring_result.get("total_score", 0)
            max_score = scoring_result.get("max_score", 100)
            # å…è®¸10%çš„è¯¯å·®
            if abs(prev_score - curr_score) < max_score * 0.1:
                base_confidence = min(1.0, base_confidence + 0.1)
            else:
                base_confidence -= 0.1
        
        return max(0.0, min(1.0, base_confidence))
    
    def _generate_feedback(self, scoring_result: dict) -> str:
        """ç”Ÿæˆå­¦ç”Ÿåé¦ˆ
        
        Args:
            scoring_result: è¯„åˆ†ç»“æœ
            
        Returns:
            åé¦ˆæ–‡æœ¬
        """
        feedback_parts = []
        
        # ç­‰çº§å’Œæ€»è¯„
        grade = scoring_result.get("grade_level", "")
        overall = scoring_result.get("overall_comment", "")
        if grade:
            feedback_parts.append(f"ç­‰çº§ï¼š{grade}")
        if overall:
            feedback_parts.append(f"\n{overall}")
        
        # äº®ç‚¹
        highlights = scoring_result.get("highlights", [])
        if highlights:
            feedback_parts.append("\n\nâœ¨ äº®ç‚¹ï¼š")
            for h in highlights[:2]:
                feedback_parts.append(f"- {h.get('reason', h.get('text', ''))}")
        
        # æ”¹è¿›å»ºè®®
        issues = scoring_result.get("issues", [])
        if issues:
            feedback_parts.append("\n\nğŸ“ æ”¹è¿›å»ºè®®ï¼š")
            for i in issues[:3]:
                feedback_parts.append(f"- {i.get('suggestion', i.get('text', ''))}")
        
        if not feedback_parts:
            total = scoring_result.get("total_score", 0)
            feedback_parts.append(f"å¾—åˆ†ï¼š{total}åˆ†")
        
        return "".join(feedback_parts)
    
    def _extract_text(self, content) -> str:
        """ä»å“åº”ä¸­æå–æ–‡æœ¬"""
        if isinstance(content, list):
            text_parts = []
            for item in content:
                if isinstance(item, dict):
                    text_parts.append(item.get('text', ''))
                else:
                    text_parts.append(str(item))
            return '\n'.join(text_parts)
        return str(content)
