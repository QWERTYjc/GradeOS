"""LabDesignAgent - å®éªŒè®¾è®¡é¢˜æ‰¹æ”¹æ™ºèƒ½ä½“

è¯„ä¼°å®éªŒæ–¹æ¡ˆçš„å®Œæ•´æ€§å’Œç§‘å­¦æ€§ï¼Œç”Ÿæˆè¯æ®é“¾
"""

import json
import logging
from typing import List, Dict, Any, Optional

from langchain_core.messages import HumanMessage

from src.services.chat_model_factory import get_chat_model
from src.models.enums import QuestionType
from src.models.state import ContextPack, GradingState, EvidenceItem
from src.agents.base import BaseGradingAgent
from src.config.models import get_default_model


logger = logging.getLogger(__name__)


# å®éªŒè®¾è®¡è¯„åˆ†ç»´åº¦
LAB_DESIGN_DIMENSIONS = [
    {"name": "å®éªŒç›®çš„", "weight": 0.1, "description": "ç›®çš„æ˜ç¡®ã€ä¸é¢˜ç›®è¦æ±‚ä¸€è‡´"},
    {"name": "å®éªŒåŸç†", "weight": 0.15, "description": "åŸç†æ­£ç¡®ã€è¡¨è¿°æ¸…æ™°"},
    {"name": "å®éªŒå™¨æ", "weight": 0.1, "description": "å™¨æé€‰æ‹©åˆç†ã€å®Œæ•´"},
    {"name": "å®éªŒæ­¥éª¤", "weight": 0.25, "description": "æ­¥éª¤å®Œæ•´ã€é¡ºåºåˆç†ã€å¯æ“ä½œæ€§å¼º"},
    {"name": "å˜é‡æ§åˆ¶", "weight": 0.2, "description": "è‡ªå˜é‡ã€å› å˜é‡ã€æ§åˆ¶å˜é‡æ˜ç¡®"},
    {"name": "æ•°æ®å¤„ç†", "weight": 0.1, "description": "æ•°æ®è®°å½•è¡¨æ ¼è®¾è®¡åˆç†ã€å¤„ç†æ–¹æ³•æ­£ç¡®"},
    {"name": "å®‰å…¨è§„èŒƒ", "weight": 0.1, "description": "æ³¨æ„äº‹é¡¹å®Œæ•´ã€å®‰å…¨æ„è¯†å¼º"},
]


class LabDesignAgent(BaseGradingAgent):
    """å®éªŒè®¾è®¡é¢˜æ‰¹æ”¹æ™ºèƒ½ä½“
    
    ä¸“é—¨å¤„ç†ç‰©ç†ã€åŒ–å­¦ã€ç”Ÿç‰©ç­‰å­¦ç§‘çš„å®éªŒè®¾è®¡é¢˜ã€‚
    è¯„ä¼°å®éªŒæ–¹æ¡ˆçš„å®Œæ•´æ€§ã€ç§‘å­¦æ€§å’Œå¯æ“ä½œæ€§ã€‚
    
    ç‰¹ç‚¹ï¼š
    - å¤šç»´åº¦è¯„ä¼°ï¼ˆç›®çš„ã€åŸç†ã€å™¨æã€æ­¥éª¤ã€å˜é‡æ§åˆ¶ç­‰ï¼‰
    - ç§‘å­¦æ€§éªŒè¯
    - å®‰å…¨è§„èŒƒæ£€æŸ¥
    - ç”Ÿæˆå®Œæ•´è¯æ®é“¾
    """
    
    def __init__(
        self,
        api_key: Optional[str] = None,
        model_name: Optional[str] = None,
    ):
        """åˆå§‹åŒ– LabDesignAgent
        
        Args:
            api_key: Google AI API å¯†é’¥
            model_name: ä½¿ç”¨çš„æ¨¡å‹åç§°ï¼Œé»˜è®¤ä½¿ç”¨å…¨å±€é…ç½®
        """
        if model_name is None:
            model_name = get_default_model()
        self.llm = get_chat_model(
            api_key=api_key,
            model_name=model_name,
            temperature=0.2,
            purpose="vision",
            enable_thinking=True,
        )
        self._api_key = api_key
    
    @property
    def agent_type(self) -> str:
        return "lab_design"
    
    @property
    def supported_question_types(self) -> List[QuestionType]:
        return [QuestionType.LAB_DESIGN]
    
    async def grade(self, context_pack: ContextPack) -> GradingState:
        """æ‰§è¡Œå®éªŒè®¾è®¡é¢˜æ‰¹æ”¹
        
        Args:
            context_pack: ä¸Šä¸‹æ–‡åŒ…
            
        Returns:
            GradingState: æ‰¹æ”¹ç»“æœ
        """
        question_image = context_pack.get("question_image", "")
        rubric = context_pack.get("rubric", "")
        max_score = context_pack.get("max_score", 0.0)
        standard_answer = context_pack.get("standard_answer", "")
        previous_result = context_pack.get("previous_result")
        terminology = context_pack.get("terminology", [])
        
        reasoning_trace: List[str] = []
        
        try:
            # æ­¥éª¤1ï¼šè§†è§‰æå– - è¯†åˆ«å®éªŒè®¾è®¡å†…å®¹
            reasoning_trace.append("å¼€å§‹è§†è§‰æå–ï¼šè¯†åˆ«å®éªŒè®¾è®¡æ–¹æ¡ˆ")
            vision_analysis = await self._extract_lab_design(
                question_image, rubric, terminology
            )
            reasoning_trace.append(f"è§†è§‰æå–å®Œæˆï¼šè¯†åˆ«åˆ° {len(vision_analysis.get('components', {}))} ä¸ªç»„æˆéƒ¨åˆ†")
            
            # æ­¥éª¤2ï¼šç§‘å­¦æ€§éªŒè¯
            reasoning_trace.append("å¼€å§‹ç§‘å­¦æ€§éªŒè¯")
            validation_result = await self._validate_scientific_rigor(
                vision_analysis, rubric, standard_answer
            )
            reasoning_trace.append(f"ç§‘å­¦æ€§éªŒè¯å®Œæˆï¼š{validation_result.get('overall_validity', 'unknown')}")
            
            # æ­¥éª¤3ï¼šå¤šç»´åº¦è¯„åˆ†
            reasoning_trace.append("å¼€å§‹å¤šç»´åº¦è¯„åˆ†")
            scoring_result = await self._score_lab_design(
                vision_analysis, validation_result, rubric, max_score, standard_answer
            )
            reasoning_trace.append(f"è¯„åˆ†å®Œæˆï¼šæ€»åˆ† {scoring_result['total_score']}/{max_score}")
            
            # æ­¥éª¤4ï¼šç”Ÿæˆè¯æ®é“¾
            evidence_chain = self._build_evidence_chain(
                scoring_result.get("dimension_scores", []),
                rubric
            )
            
            # æ­¥éª¤5ï¼šè®¡ç®—ç½®ä¿¡åº¦
            confidence = self._calculate_confidence(
                vision_analysis, scoring_result, previous_result
            )
            reasoning_trace.append(f"ç½®ä¿¡åº¦ï¼š{confidence:.2f}")
            
            # æ­¥éª¤6ï¼šç”Ÿæˆå­¦ç”Ÿåé¦ˆ
            student_feedback = self._generate_feedback(scoring_result, validation_result)
            
            # æ„å»º rubric_mapping
            rubric_mapping = []
            for dim_score in scoring_result.get("dimension_scores", []):
                rubric_mapping.append({
                    "rubric_point": dim_score.get("dimension", ""),
                    "evidence": dim_score.get("evidence", ""),
                    "score_awarded": dim_score.get("score", 0),
                    "max_score": dim_score.get("max_score", 0)
                })
            
            # æ„å»ºè§†è§‰æ ‡æ³¨
            visual_annotations = self._build_visual_annotations(
                vision_analysis, scoring_result
            )
            
            return GradingState(
                context_pack=context_pack,
                vision_analysis=json.dumps(vision_analysis.get("components", {}), ensure_ascii=False),
                rubric_mapping=rubric_mapping,
                initial_score=scoring_result["total_score"],
                reasoning_trace=reasoning_trace,
                critique_feedback=None,
                evidence_chain=evidence_chain,
                final_score=scoring_result["total_score"],
                max_score=max_score,
                confidence=confidence,
                visual_annotations=visual_annotations,
                student_feedback=student_feedback,
                agent_type=self.agent_type,
                revision_count=0,
                is_finalized=True,
                needs_secondary_review=confidence < 0.75
            )
            
        except Exception as e:
            logger.error(f"LabDesignAgent æ‰¹æ”¹å¤±è´¥: {e}")
            reasoning_trace.append(f"é”™è¯¯: {str(e)}")
            return GradingState(
                context_pack=context_pack,
                vision_analysis="",
                rubric_mapping=[],
                initial_score=0.0,
                reasoning_trace=reasoning_trace,
                critique_feedback=None,
                evidence_chain=[],
                final_score=0.0,
                max_score=max_score,
                confidence=0.0,
                visual_annotations=[],
                student_feedback="æ‰¹æ”¹è¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯ï¼Œéœ€è¦äººå·¥å®¡æ ¸",
                agent_type=self.agent_type,
                revision_count=0,
                is_finalized=False,
                needs_secondary_review=True,
                error=str(e)
            )
    
    async def _extract_lab_design(
        self,
        question_image: str,
        rubric: str,
        terminology: List[str]
    ) -> dict:
        """ä»å›¾åƒä¸­æå–å®éªŒè®¾è®¡å†…å®¹
        
        Args:
            question_image: Base64 ç¼–ç çš„é¢˜ç›®å›¾åƒ
            rubric: è¯„åˆ†ç»†åˆ™
            terminology: ç›¸å…³æœ¯è¯­
            
        Returns:
            åŒ…å«å®éªŒè®¾è®¡å„éƒ¨åˆ†çš„å­—å…¸
        """
        terminology_text = "ã€".join(terminology) if terminology else "æ— ç‰¹å®šæœ¯è¯­"
        
        prompt = f"""è¯·åˆ†æè¿™å¼ å®éªŒè®¾è®¡é¢˜çš„ç­”é¢˜å›¾åƒï¼Œæå–å­¦ç”Ÿçš„å®éªŒè®¾è®¡æ–¹æ¡ˆã€‚

è¯„åˆ†ç»†åˆ™ï¼š
{rubric}

ç›¸å…³æœ¯è¯­ï¼š{terminology_text}

è¯·è¿”å› JSON æ ¼å¼ï¼š
{{
    "components": {{
        "purpose": {{
            "content": "å®éªŒç›®çš„å†…å®¹",
            "location": [ymin, xmin, ymax, xmax],
            "is_present": true/false
        }},
        "principle": {{
            "content": "å®éªŒåŸç†å†…å®¹",
            "location": [ymin, xmin, ymax, xmax],
            "is_present": true/false
        }},
        "materials": {{
            "content": "å®éªŒå™¨æåˆ—è¡¨",
            "items": ["å™¨æ1", "å™¨æ2", ...],
            "location": [ymin, xmin, ymax, xmax],
            "is_present": true/false
        }},
        "procedure": {{
            "content": "å®éªŒæ­¥éª¤å†…å®¹",
            "steps": ["æ­¥éª¤1", "æ­¥éª¤2", ...],
            "location": [ymin, xmin, ymax, xmax],
            "is_present": true/false
        }},
        "variables": {{
            "independent": "è‡ªå˜é‡",
            "dependent": "å› å˜é‡",
            "controlled": ["æ§åˆ¶å˜é‡1", ...],
            "location": [ymin, xmin, ymax, xmax],
            "is_present": true/false
        }},
        "data_table": {{
            "content": "æ•°æ®è®°å½•è¡¨æè¿°",
            "location": [ymin, xmin, ymax, xmax],
            "is_present": true/false
        }},
        "safety": {{
            "content": "å®‰å…¨æ³¨æ„äº‹é¡¹",
            "location": [ymin, xmin, ymax, xmax],
            "is_present": true/false
        }}
    }},
    "overall_completeness": "complete/partial/incomplete",
    "diagram_present": true/false,
    "readability": "high/medium/low"
}}

æ³¨æ„ï¼š
- è¯†åˆ«å®éªŒè®¾è®¡çš„å„ä¸ªç»„æˆéƒ¨åˆ†
- æ ‡æ³¨æ¯ä¸ªéƒ¨åˆ†çš„ä½ç½®
- è¯„ä¼°æ–¹æ¡ˆçš„å®Œæ•´æ€§
- åæ ‡ä½¿ç”¨å½’ä¸€åŒ–æ ¼å¼ï¼ˆ0-1000ï¼‰"""

        message = HumanMessage(
            content=[
                {"type": "text", "text": prompt},
                {
                    "type": "image_url",
                    "image_url": f"data:image/jpeg;base64,{question_image}"
                }
            ]
        )
        
        response = await self.llm.ainvoke([message])
        result_text = self._extract_text(response.content)
        
        try:
            if "```json" in result_text:
                json_start = result_text.find("```json") + 7
                json_end = result_text.find("```", json_start)
                result_text = result_text[json_start:json_end].strip()
            return json.loads(result_text)
        except json.JSONDecodeError:
            logger.warning(f"æ— æ³•è§£æè§†è§‰æå–ç»“æœ: {result_text}")
            return {
                "components": {},
                "overall_completeness": "incomplete",
                "diagram_present": False,
                "readability": "low"
            }
    
    async def _validate_scientific_rigor(
        self,
        vision_analysis: dict,
        rubric: str,
        standard_answer: str
    ) -> dict:
        """éªŒè¯å®éªŒè®¾è®¡çš„ç§‘å­¦æ€§
        
        Args:
            vision_analysis: è§†è§‰åˆ†æç»“æœ
            rubric: è¯„åˆ†ç»†åˆ™
            standard_answer: æ ‡å‡†ç­”æ¡ˆ
            
        Returns:
            ç§‘å­¦æ€§éªŒè¯ç»“æœ
        """
        components = vision_analysis.get("components", {})
        
        prompt = f"""è¯·éªŒè¯ä»¥ä¸‹å®éªŒè®¾è®¡æ–¹æ¡ˆçš„ç§‘å­¦æ€§ã€‚

å®éªŒè®¾è®¡å†…å®¹ï¼š
{json.dumps(components, ensure_ascii=False, indent=2)}

è¯„åˆ†ç»†åˆ™ï¼š
{rubric}

å‚è€ƒç­”æ¡ˆï¼š
{standard_answer if standard_answer else "æœªæä¾›"}

è¯·ä»ä»¥ä¸‹æ–¹é¢éªŒè¯å¹¶è¿”å› JSON æ ¼å¼ï¼š
{{
    "overall_validity": "valid/partially_valid/invalid",
    "principle_correct": true/false,
    "principle_issues": ["é—®é¢˜1", ...],
    "procedure_feasible": true/false,
    "procedure_issues": ["é—®é¢˜1", ...],
    "variable_control_proper": true/false,
    "variable_issues": ["é—®é¢˜1", ...],
    "safety_adequate": true/false,
    "safety_issues": ["é—®é¢˜1", ...],
    "scientific_errors": [
        {{
            "error": "é”™è¯¯æè¿°",
            "severity": "critical/major/minor",
            "suggestion": "ä¿®æ­£å»ºè®®"
        }}
    ]
}}"""

        response = await self.llm.ainvoke([HumanMessage(content=prompt)])
        result_text = self._extract_text(response.content)
        
        try:
            if "```json" in result_text:
                json_start = result_text.find("```json") + 7
                json_end = result_text.find("```", json_start)
                result_text = result_text[json_start:json_end].strip()
            return json.loads(result_text)
        except json.JSONDecodeError:
            logger.warning(f"æ— æ³•è§£æç§‘å­¦æ€§éªŒè¯ç»“æœ: {result_text}")
            return {
                "overall_validity": "partially_valid",
                "principle_correct": True,
                "procedure_feasible": True,
                "variable_control_proper": True,
                "safety_adequate": True,
                "scientific_errors": []
            }
    
    async def _score_lab_design(
        self,
        vision_analysis: dict,
        validation_result: dict,
        rubric: str,
        max_score: float,
        standard_answer: str
    ) -> dict:
        """å¯¹å®éªŒè®¾è®¡è¿›è¡Œå¤šç»´åº¦è¯„åˆ†
        
        Args:
            vision_analysis: è§†è§‰åˆ†æç»“æœ
            validation_result: ç§‘å­¦æ€§éªŒè¯ç»“æœ
            rubric: è¯„åˆ†ç»†åˆ™
            max_score: æ»¡åˆ†
            standard_answer: æ ‡å‡†ç­”æ¡ˆ
            
        Returns:
            è¯„åˆ†ç»“æœ
        """
        components = vision_analysis.get("components", {})
        
        # æ„å»ºè¯„åˆ†æç¤º
        dimensions_text = "\n".join([
            f"- {d['name']}ï¼ˆæƒé‡{d['weight']*100}%ï¼‰: {d['description']}"
            for d in LAB_DESIGN_DIMENSIONS
        ])
        
        prompt = f"""è¯·å¯¹ä»¥ä¸‹å®éªŒè®¾è®¡æ–¹æ¡ˆè¿›è¡Œå¤šç»´åº¦è¯„åˆ†ã€‚

å®éªŒè®¾è®¡å†…å®¹ï¼š
{json.dumps(components, ensure_ascii=False, indent=2)}

ç§‘å­¦æ€§éªŒè¯ç»“æœï¼š
{json.dumps(validation_result, ensure_ascii=False, indent=2)}

è¯„åˆ†ç»†åˆ™ï¼š
{rubric}

å‚è€ƒç­”æ¡ˆï¼š
{standard_answer if standard_answer else "æœªæä¾›"}

æ»¡åˆ†ï¼š{max_score}

è¯„åˆ†ç»´åº¦ï¼š
{dimensions_text}

è¯·è¿”å› JSON æ ¼å¼ï¼š
{{
    "total_score": æ€»å¾—åˆ†,
    "dimension_scores": [
        {{
            "dimension": "ç»´åº¦åç§°",
            "max_score": è¯¥ç»´åº¦æ»¡åˆ†,
            "score": è¯¥ç»´åº¦å¾—åˆ†,
            "evidence": "è¯„åˆ†ä¾æ®",
            "feedback": "è¯¥ç»´åº¦çš„è¯„ä»·"
        }}
    ],
    "strengths": ["ä¼˜ç‚¹1", "ä¼˜ç‚¹2", ...],
    "weaknesses": ["ä¸è¶³1", "ä¸è¶³2", ...],
    "overall_comment": "æ•´ä½“è¯„è¯­",
    "improvement_suggestions": ["å»ºè®®1", "å»ºè®®2", ...]
}}"""

        response = await self.llm.ainvoke([HumanMessage(content=prompt)])
        result_text = self._extract_text(response.content)
        
        try:
            if "```json" in result_text:
                json_start = result_text.find("```json") + 7
                json_end = result_text.find("```", json_start)
                result_text = result_text[json_start:json_end].strip()
            return json.loads(result_text)
        except json.JSONDecodeError:
            logger.warning(f"æ— æ³•è§£æè¯„åˆ†ç»“æœ: {result_text}")
            return {
                "total_score": max_score * 0.5,
                "dimension_scores": [],
                "strengths": [],
                "weaknesses": [],
                "overall_comment": "è¯„åˆ†è§£æå¤±è´¥ï¼Œéœ€è¦äººå·¥å®¡æ ¸",
                "improvement_suggestions": []
            }
    
    def _build_evidence_chain(
        self,
        dimension_scores: List[Dict[str, Any]],
        rubric: str
    ) -> List[EvidenceItem]:
        """æ„å»ºè¯æ®é“¾
        
        Args:
            dimension_scores: å„ç»´åº¦è¯„åˆ†
            rubric: è¯„åˆ†ç»†åˆ™
            
        Returns:
            è¯æ®é“¾åˆ—è¡¨
        """
        evidence_chain: List[EvidenceItem] = []
        
        for dim in dimension_scores:
            evidence: EvidenceItem = {
                "scoring_point": dim.get("dimension", ""),
                "image_region": [0, 0, 1000, 1000],
                "text_description": dim.get("evidence", ""),
                "reasoning": dim.get("feedback", ""),
                "rubric_reference": rubric[:100] if rubric else "å®éªŒè®¾è®¡è¯„åˆ†æ ‡å‡†",
                "points_awarded": dim.get("score", 0)
            }
            evidence_chain.append(evidence)
        
        return evidence_chain
    
    def _build_visual_annotations(
        self,
        vision_analysis: dict,
        scoring_result: dict
    ) -> List[Dict[str, Any]]:
        """æ„å»ºè§†è§‰æ ‡æ³¨
        
        Args:
            vision_analysis: è§†è§‰åˆ†æç»“æœ
            scoring_result: è¯„åˆ†ç»“æœ
            
        Returns:
            è§†è§‰æ ‡æ³¨åˆ—è¡¨
        """
        annotations = []
        components = vision_analysis.get("components", {})
        
        # ä¸ºæ¯ä¸ªç»„æˆéƒ¨åˆ†æ·»åŠ æ ‡æ³¨
        component_names = {
            "purpose": "å®éªŒç›®çš„",
            "principle": "å®éªŒåŸç†",
            "materials": "å®éªŒå™¨æ",
            "procedure": "å®éªŒæ­¥éª¤",
            "variables": "å˜é‡æ§åˆ¶",
            "data_table": "æ•°æ®è®°å½•",
            "safety": "å®‰å…¨æ³¨æ„äº‹é¡¹"
        }
        
        for key, name in component_names.items():
            comp = components.get(key, {})
            if comp.get("is_present", False):
                annotations.append({
                    "type": "component_region",
                    "bounding_box": comp.get("location", [0, 0, 1000, 1000]),
                    "label": name,
                    "is_present": True
                })
        
        return annotations
    
    def _calculate_confidence(
        self,
        vision_analysis: dict,
        scoring_result: dict,
        previous_result: Optional[dict]
    ) -> float:
        """è®¡ç®—ç½®ä¿¡åº¦
        
        Args:
            vision_analysis: è§†è§‰åˆ†æç»“æœ
            scoring_result: è¯„åˆ†ç»“æœ
            previous_result: å‰åºç»“æœ
            
        Returns:
            ç½®ä¿¡åº¦åˆ†æ•°
        """
        base_confidence = 0.80
        
        # æ ¹æ®å®Œæ•´æ€§è°ƒæ•´
        completeness = vision_analysis.get("overall_completeness", "incomplete")
        if completeness == "complete":
            base_confidence += 0.05
        elif completeness == "incomplete":
            base_confidence -= 0.15
        
        # æ ¹æ®å¯è¯»æ€§è°ƒæ•´
        readability = vision_analysis.get("readability", "medium")
        if readability == "low":
            base_confidence -= 0.1
        
        # æ ¹æ®ç»„ä»¶è¯†åˆ«æ•°é‡è°ƒæ•´
        components = vision_analysis.get("components", {})
        present_count = sum(1 for c in components.values() if c.get("is_present", False))
        if present_count < 3:
            base_confidence -= 0.1
        
        # äºŒæ¬¡è¯„ä¼°ä¸€è‡´æ€§
        if previous_result:
            prev_score = previous_result.get("score", -1)
            curr_score = scoring_result.get("total_score", 0)
            max_score = scoring_result.get("max_score", 100)
            if abs(prev_score - curr_score) < max_score * 0.1:
                base_confidence = min(1.0, base_confidence + 0.1)
            else:
                base_confidence -= 0.1
        
        return max(0.0, min(1.0, base_confidence))
    
    def _generate_feedback(
        self,
        scoring_result: dict,
        validation_result: dict
    ) -> str:
        """ç”Ÿæˆå­¦ç”Ÿåé¦ˆ
        
        Args:
            scoring_result: è¯„åˆ†ç»“æœ
            validation_result: ç§‘å­¦æ€§éªŒè¯ç»“æœ
            
        Returns:
            åé¦ˆæ–‡æœ¬
        """
        feedback_parts = []
        
        # æ•´ä½“è¯„è¯­
        overall = scoring_result.get("overall_comment", "")
        if overall:
            feedback_parts.append(overall)
        
        # ä¼˜ç‚¹
        strengths = scoring_result.get("strengths", [])
        if strengths:
            feedback_parts.append("\n\nâœ… ä¼˜ç‚¹ï¼š")
            for s in strengths[:3]:
                feedback_parts.append(f"- {s}")
        
        # ä¸è¶³
        weaknesses = scoring_result.get("weaknesses", [])
        if weaknesses:
            feedback_parts.append("\n\nâš ï¸ éœ€è¦æ”¹è¿›ï¼š")
            for w in weaknesses[:3]:
                feedback_parts.append(f"- {w}")
        
        # ç§‘å­¦æ€§é—®é¢˜
        errors = validation_result.get("scientific_errors", [])
        critical_errors = [e for e in errors if e.get("severity") == "critical"]
        if critical_errors:
            feedback_parts.append("\n\nâŒ é‡è¦ç§‘å­¦æ€§é—®é¢˜ï¼š")
            for e in critical_errors[:2]:
                feedback_parts.append(f"- {e.get('error', '')}")
                if e.get("suggestion"):
                    feedback_parts.append(f"  å»ºè®®ï¼š{e.get('suggestion')}")
        
        # æ”¹è¿›å»ºè®®
        suggestions = scoring_result.get("improvement_suggestions", [])
        if suggestions:
            feedback_parts.append("\n\nğŸ’¡ æ”¹è¿›å»ºè®®ï¼š")
            for s in suggestions[:3]:
                feedback_parts.append(f"- {s}")
        
        if not feedback_parts:
            total = scoring_result.get("total_score", 0)
            feedback_parts.append(f"å¾—åˆ†ï¼š{total}åˆ†")
        
        return "".join(feedback_parts)
    
    def _extract_text(self, content) -> str:
        """ä»å“åº”ä¸­æå–æ–‡æœ¬"""
        if isinstance(content, list):
            text_parts = []
            for item in content:
                if isinstance(item, dict):
                    text_parts.append(item.get('text', ''))
                else:
                    text_parts.append(str(item))
            return '\n'.join(text_parts)
        return str(content)
