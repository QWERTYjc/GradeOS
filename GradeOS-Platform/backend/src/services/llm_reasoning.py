"""LLM æ·±åº¦æŽ¨ç†å®¢æˆ·ç«¯ - ä½¿ç”¨ LLM è¿›è¡Œæ‰¹æ”¹æŽ¨ç†

æœ¬æ¨¡å—å®žçŽ°äº†æ‰¹æ”¹å·¥ä½œæµçš„æ ¸å¿ƒæŽ¨ç†èƒ½åŠ›ï¼Œé›†æˆäº†ï¼š
- RubricRegistry: åŠ¨æ€èŽ·å–è¯„åˆ†æ ‡å‡†
- GradingSkills: Agent æŠ€èƒ½æ¨¡å—
- å¾—åˆ†ç‚¹é€ä¸€æ ¸å¯¹é€»è¾‘
- å¦ç±»è§£æ³•æ”¯æŒ
- æŒ‡æ•°é€€é¿é‡è¯•æœºåˆ¶ (Requirement 9.1)

Requirements: 1.1, 1.2, 1.3, 9.1
"""

import base64
import json
import logging
import os
import re
from typing import (
    Dict,
    Any,
    List,
    Optional,
    TYPE_CHECKING,
    AsyncIterator,
    Callable,
    Awaitable,
    Literal,
)

from langchain_core.messages import HumanMessage

from src.services.chat_model_factory import get_chat_model
from ..models.grading import RubricMappingItem
from ..models.grading_models import (
    QuestionRubric,
    QuestionResult,
    ScoringPoint,
    ScoringPointResult,
    PageGradingResult,
    StudentInfo,
)
from ..config.models import get_default_model
from ..utils.error_handling import with_retry, get_error_manager
from ..utils.llm_thinking import split_thinking_content

if TYPE_CHECKING:
    from ..services.rubric_registry import RubricRegistry


logger = logging.getLogger(__name__)

# ç²¾ç®€çš„ System Prompt - æå–æ‰€æœ‰é€šç”¨çº¦æŸ
SYSTEM_PROMPT = """ä½ æ˜¯ä¸€ä½ä¸“ä¸šé˜…å·æ•™å¸ˆã€‚è¯·ä¸¥æ ¼æŒ‰ä»¥ä¸‹è§„åˆ™æ‰¹æ”¹ï¼š

ã€åæ ‡ç³»ç»Ÿã€‘
- åŽŸç‚¹ï¼šå·¦ä¸Šè§’ï¼Œxå‘å³(0-1)ï¼Œyå‘ä¸‹(0-1)
- ä½¿ç”¨ {{x_min, y_min, x_max, y_max}} æ ¼å¼

ã€è¾“å‡ºè¦æ±‚ã€‘
- å¿…é¡»è¿”å›žæœ‰æ•ˆJSON
- æ¯ä¸ªå¾—åˆ†ç‚¹å¿…é¡»åŒ…å«ï¼špoint_id, awarded, max_points, evidence
- evidenceå¿…é¡»ä»¥ã€åŽŸæ–‡å¼•ç”¨ã€‘å¼€å¤´ï¼Œå¼•ç”¨å­¦ç”ŸåŽŸæ–‡
- åæ ‡å¿…é¡»ç²¾ç¡®åˆ°å›¾ç‰‡ä¸­çš„å…·ä½“ä½ç½®

ã€è¯„åˆ†åŽŸåˆ™ã€‘
1. ä¸¥æ ¼æŒ‰è¯„åˆ†æ ‡å‡†ç»™åˆ†ï¼Œç¦æ­¢è‡ªåˆ›åˆ†å€¼
2. è¿‡ç¨‹æ­£ç¡®ä½†ç»“æžœé”™è¯¯ï¼Œä»ç»™è¿‡ç¨‹åˆ†
3. å‰æ­¥é”™è¯¯å¯¼è‡´åŽæ­¥é”™è¯¯ï¼Œåªæ‰£ä¸€æ¬¡åˆ†
4. éžæ ‡å‡†ä½†æ­£ç¡®çš„æ–¹æ³•åŒæ ·ç»™åˆ†
5. è¯æ®ä¸è¶³æ—¶ç»™0åˆ†å¹¶è¯´æ˜Ž"æœªæ‰¾åˆ°"

ã€é¢˜åž‹å¤„ç†ã€‘
- é€‰æ‹©é¢˜/å¡«ç©ºé¢˜ï¼šå¿«é€Ÿåˆ¤æ–­æ­£è¯¯
- è®¡ç®—é¢˜ï¼šé€æ­¥éª¤è¯„åˆ†
- è¯æ˜Žé¢˜ï¼šæ£€æŸ¥é€»è¾‘é“¾æ¡
- åº”ç”¨é¢˜ï¼šæ£€æŸ¥å»ºæ¨¡ã€è®¡ç®—ã€ç»“è®º

ã€ç©ºç™½é¡µã€‘
- ç©ºç™½é¡µ/å°é¢é¡µï¼šis_blank_page=true, score=0, max_score=0
"""


class LLMReasoningClient:
    """
    LLM æ·±åº¦æŽ¨ç†å®¢æˆ·ç«¯ï¼Œç”¨äºŽæ‰¹æ”¹æ™ºèƒ½ä½“çš„å„ä¸ªæŽ¨ç†èŠ‚ç‚¹

    é›†æˆäº† RubricRegistry å’Œ GradingSkillsï¼Œæ”¯æŒï¼š
    - åŠ¨æ€è¯„åˆ†æ ‡å‡†èŽ·å– (Requirement 1.1)
    - å¾—åˆ†ç‚¹é€ä¸€æ ¸å¯¹ (Requirement 1.2)
    - å¦ç±»è§£æ³•æ”¯æŒ (Requirement 1.3)

    Requirements: 1.1, 1.2, 1.3
    """

    # ç±»å¸¸é‡ï¼šé¿å…é­”æ³•æ•°å­—
    MAX_QUESTIONS_IN_PROMPT = 0  # æç¤ºè¯ä¸­æœ€å¤šæ˜¾ç¤ºçš„é¢˜ç›®æ•°
    MAX_CRITERIA_PER_QUESTION = 0  # æ¯é“é¢˜æœ€å¤šæ˜¾ç¤ºçš„è¯„åˆ†è¦ç‚¹æ•°

    def __init__(
        self,
        api_key: Optional[str] = None,
        model_name: Optional[str] = None,
        rubric_registry: Optional["RubricRegistry"] = None,
    ):
        """
        åˆå§‹åŒ– LLM æŽ¨ç†å®¢æˆ·ç«¯

        Args:
            api_key: Google AI API å¯†é’¥
            model_name: ä½¿ç”¨çš„æ¨¡åž‹åç§°ï¼Œé»˜è®¤ä½¿ç”¨å…¨å±€é…ç½®
            rubric_registry: è¯„åˆ†æ ‡å‡†æ³¨å†Œä¸­å¿ƒï¼ˆå¯é€‰ï¼‰
        """
        if model_name is None:
            model_name = get_default_model()
        # ç§»é™¤ token é™åˆ¶ï¼šè®¾ç½®ä¸º None è¡¨ç¤ºä¸é™åˆ¶è¾“å‡ºé•¿åº¦
        # å¯é€šè¿‡çŽ¯å¢ƒå˜é‡ GRADING_MAX_OUTPUT_TOKENS è¦†ç›–ï¼ˆè®¾ä¸º 0 æˆ–è´Ÿæ•°è¡¨ç¤ºä¸é™åˆ¶ï¼‰
        raw_max_tokens = self._read_int_env("GRADING_MAX_OUTPUT_TOKENS", 0)
        self._max_output_tokens = raw_max_tokens if raw_max_tokens > 0 else None
        self._max_prompt_questions = self._read_int_env(
            "GRADING_PROMPT_MAX_QUESTIONS",
            self.MAX_QUESTIONS_IN_PROMPT,
        )
        self._max_prompt_criteria = self._read_int_env(
            "GRADING_PROMPT_MAX_CRITERIA",
            self.MAX_CRITERIA_PER_QUESTION,
        )
        self.llm = get_chat_model(
            api_key=api_key,
            model_name=model_name,
            temperature=0.2,
            purpose="vision",
            enable_thinking=True,
            max_output_tokens=self._max_output_tokens,
        )
        self.model_name = model_name
        self.temperature = 0.2  # ä½Žæ¸©åº¦ä»¥ä¿æŒä¸€è‡´æ€§

        # é›†æˆ RubricRegistry (Requirement 1.1)ï¼ˆå·²ç§»é™¤ Agent Skillï¼‰
        self._rubric_registry = rubric_registry

    @staticmethod
    def _read_int_env(key: str, default: int) -> int:
        raw = os.getenv(key)
        if raw is None:
            return default
        try:
            return int(raw)
        except ValueError:
            return default

    def _limit_questions_for_prompt(self, questions: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        max_questions = self._max_prompt_questions
        if max_questions <= 0:
            return questions
        return questions[:max_questions]

    def _limit_criteria_for_prompt(self, criteria: List[Any]) -> List[Any]:
        max_criteria = self._max_prompt_criteria
        if max_criteria <= 0:
            return criteria
        return criteria[:max_criteria]

    @property
    def rubric_registry(self) -> Optional["RubricRegistry"]:
        """èŽ·å–è¯„åˆ†æ ‡å‡†æ³¨å†Œä¸­å¿ƒ"""
        return self._rubric_registry

    @rubric_registry.setter
    def rubric_registry(self, registry: "RubricRegistry") -> None:
        """è®¾ç½®è¯„åˆ†æ ‡å‡†æ³¨å†Œä¸­å¿ƒ"""
        self._rubric_registry = registry

    def _extract_text_from_response(self, content: Any) -> str:
        """
        ä»Žå“åº”ä¸­æå–æ–‡æœ¬å†…å®¹

        Args:
            content: LLM å“åº”å†…å®¹

        Returns:
            str: æå–çš„æ–‡æœ¬
        """
        if isinstance(content, str):
            return content
        elif isinstance(content, list):
            # å¤„ç†å¤šéƒ¨åˆ†å“åº”ï¼ˆå¦‚åŒ…å« tool_calls æˆ– imageï¼‰
            text_parts = []
            for part in content:
                if isinstance(part, str):
                    text_parts.append(part)
                elif isinstance(part, dict) and "text" in part:
                    text_parts.append(part["text"])
            return "".join(text_parts)
        return str(content)

    def _extract_json_from_text(self, text: str) -> str:
        """
        ä»Žæ–‡æœ¬ä¸­æå– JSON éƒ¨åˆ†

        Args:
            text: åŒ…å« JSON çš„æ–‡æœ¬

        Returns:
            str: æå–çš„ JSON å­—ç¬¦ä¸²
        """
        if "```json" in text:
            json_start = text.find("```json") + 7
            json_end = text.find("```", json_start)
            return text[json_start:json_end].strip()
        elif "```" in text:
            json_start = text.find("```") + 3
            json_end = text.find("```", json_start)
            return text[json_start:json_end].strip()
        return text

    def _escape_invalid_backslashes(self, text: str) -> str:
        return re.sub(r'\\(?!["\\/bfnrtu])', r"\\\\", text)

    def _strip_control_chars(self, text: str) -> str:
        cleaned = re.sub(r"[\x00-\x1F]", " ", text)
        return re.sub(r"[\u2028\u2029]", " ", cleaned)

    def _load_json_with_repair(self, text: str) -> Dict[str, Any]:
        try:
            return json.loads(text)
        except json.JSONDecodeError:
            repaired = self._escape_invalid_backslashes(text)
            try:
                return json.loads(repaired, strict=False)
            except json.JSONDecodeError:
                repaired = self._strip_control_chars(repaired)
                return json.loads(repaired, strict=False)

    def _extract_json_block(self, text: str) -> Optional[str]:
        start = text.find("{")
        end = text.rfind("}")
        if start == -1 or end <= start:
            return None
        return text[start : end + 1]

    def _normalize_question_detail(
        self,
        detail: Dict[str, Any],
        page_index: Optional[int],
    ) -> Dict[str, Any]:
        question_id = (
            detail.get("question_id") or detail.get("questionId") or detail.get("id") or "unknown"
        )
        score = float(detail.get("score") or 0)
        max_score = float(detail.get("max_score") or detail.get("maxScore") or 0)
        student_answer = detail.get("student_answer") or detail.get("studentAnswer") or ""
        feedback = detail.get("feedback") or ""
        is_correct = detail.get("is_correct") if "is_correct" in detail else detail.get("isCorrect")
        confidence = detail.get("confidence")
        source_pages = (
            detail.get("source_pages")
            or detail.get("sourcePages")
            or detail.get("page_indices")
            or detail.get("pageIndices")
            or []
        )
        if not source_pages and page_index is not None:
            source_pages = [page_index]
        scoring_point_results = (
            detail.get("scoring_point_results")
            or detail.get("scoringPointResults")
            or detail.get("scoring_results")
            or detail.get("scoringResults")
            or []
        )
        # æå–æ‰¹æ³¨åæ ‡
        annotations = detail.get("annotations") or []

        # æå–æ­¥éª¤ä¿¡æ¯ï¼ˆåŒ…å«åæ ‡ï¼‰
        steps = detail.get("steps") or []

        # æå–ç­”æ¡ˆåŒºåŸŸåæ ‡
        answer_region = detail.get("answer_region") or detail.get("answerRegion")

        # ðŸ”¥ åŽå¤‡é€»è¾‘ï¼šå¦‚æžœ LLM æ²¡æœ‰è¿”å›ž annotationsï¼Œä»Ž scoring_point_results æž„å»ºåŸºæœ¬æ‰¹æ³¨
        if not annotations and scoring_point_results:
            fallback_annotations = []
            for idx, spr in enumerate(scoring_point_results):
                # ä»Ž error_region æž„å»ºé”™è¯¯åœˆé€‰æ‰¹æ³¨
                error_region = spr.get("error_region") or spr.get("errorRegion")
                if error_region:
                    fallback_annotations.append(
                        {
                            "type": "error_circle",
                            "page_index": page_index,
                            "bounding_box": error_region,
                            "text": spr.get("evidence", ""),
                            "color": "#FF0000",
                        }
                    )

                # ä»Ž mark_type æž„å»º M/A mark æ‰¹æ³¨
                mark_type = spr.get("mark_type") or spr.get("markType")
                awarded = spr.get("awarded") or spr.get("score") or 0
                if mark_type and error_region:
                    mark_text = f"{mark_type}{1 if awarded > 0 else 0}"
                    mark_color = "#00AA00" if awarded > 0 else "#FF0000"
                    fallback_annotations.append(
                        {
                            "type": f"{mark_type.lower()}_mark",
                            "page_index": page_index,
                            "bounding_box": {
                                "x_min": min(error_region.get("x_max", 0.9) + 0.02, 0.95),
                                "y_min": error_region.get("y_min", 0.1),
                                "x_max": min(error_region.get("x_max", 0.9) + 0.08, 1.0),
                                "y_max": error_region.get("y_max", 0.15),
                            },
                            "text": mark_text,
                            "color": mark_color,
                        }
                    )

            if fallback_annotations:
                annotations = fallback_annotations
                logger.debug(
                    f"[_normalize_question_detail] ä»Ž scoring_point_results æž„å»ºäº† "
                    f"{len(fallback_annotations)} ä¸ªåŽå¤‡æ‰¹æ³¨"
                )

        # ðŸ”¥ åŽå¤‡é€»è¾‘ï¼šå¦‚æžœ LLM æ²¡æœ‰è¿”å›ž stepsï¼Œä»Ž scoring_point_results æž„å»ºåŸºæœ¬æ­¥éª¤
        if not steps and scoring_point_results:
            fallback_steps = []
            for idx, spr in enumerate(scoring_point_results):
                point_id = spr.get("point_id") or spr.get("pointId") or f"{question_id}.{idx + 1}"
                description = spr.get("description") or ""
                awarded = spr.get("awarded") or spr.get("score") or 0
                max_points = (
                    spr.get("max_points") or spr.get("maxPoints") or spr.get("max_score") or 0
                )
                mark_type = spr.get("mark_type") or spr.get("markType") or "M"
                error_region = spr.get("error_region") or spr.get("errorRegion")

                fallback_steps.append(
                    {
                        "step_id": point_id,
                        "step_content": description,
                        "step_region": error_region,  # å¯èƒ½ä¸º None
                        "is_correct": awarded > 0,
                        "mark_type": mark_type,
                        "mark_value": 1 if awarded > 0 else 0,
                        "feedback": spr.get("reason") or spr.get("evidence") or "",
                    }
                )

            if fallback_steps:
                steps = fallback_steps
                logger.debug(
                    f"[_normalize_question_detail] ä»Ž scoring_point_results æž„å»ºäº† "
                    f"{len(fallback_steps)} ä¸ªåŽå¤‡æ­¥éª¤"
                )

        return {
            "question_id": question_id,
            "score": score,
            "max_score": max_score,
            "student_answer": student_answer,
            "is_correct": is_correct,
            "feedback": feedback,
            "confidence": confidence,
            "source_pages": source_pages,
            "scoring_point_results": scoring_point_results,
            "self_critique": detail.get("self_critique") or detail.get("selfCritique"),
            "self_critique_confidence": detail.get("self_critique_confidence")
            or detail.get("selfCritiqueConfidence"),
            "rubric_refs": detail.get("rubric_refs") or detail.get("rubricRefs"),
            "question_type": detail.get("question_type") or detail.get("questionType"),
            "annotations": annotations,
            "steps": steps,
            "answer_region": answer_region,
            # æ–°å¢žå­—æ®µï¼šå¦ç±»è§£æ³•æ ‡è®°
            "used_alternative_solution": detail.get("used_alternative_solution")
            or detail.get("usedAlternativeSolution")
            or False,
            "alternative_solution_ref": detail.get("alternative_solution_ref")
            or detail.get("alternativeSolutionRef")
            or "",
        }

    def _merge_page_break_results(
        self,
        page_results: List[Dict[str, Any]],
        student_key: str,
    ) -> Dict[str, Any]:
        question_map: Dict[str, Dict[str, Any]] = {}
        page_summaries: List[Dict[str, Any]] = []
        overall_feedback = ""
        student_info = None

        for page in page_results:
            page_index = page.get("page_index")
            if isinstance(page_index, str) and page_index.isdigit():
                page_index = int(page_index)
            if page_index is not None:
                page_summaries.append(
                    {
                        "page_index": page_index,
                        "question_numbers": page.get("question_numbers")
                        or page.get("questionNumbers")
                        or [],
                        "summary": page.get("page_summary") or page.get("summary") or "",
                    }
                )
            if student_info is None and page.get("student_info"):
                student_info = page.get("student_info")
            if not overall_feedback and page.get("overall_feedback"):
                overall_feedback = page.get("overall_feedback")

            for detail in page.get("question_details", []) or []:
                normalized = self._normalize_question_detail(detail, page_index)
                key = str(normalized.get("question_id") or "unknown")
                existing = question_map.get(key)
                if not existing:
                    question_map[key] = normalized
                    continue

                existing_pages = set(existing.get("source_pages") or [])
                existing_pages.update(normalized.get("source_pages") or [])
                existing["source_pages"] = sorted(existing_pages)

                existing["scoring_point_results"] = (
                    existing.get("scoring_point_results") or []
                ) + (normalized.get("scoring_point_results") or [])

                existing_answer = (existing.get("student_answer") or "").strip()
                new_answer = (normalized.get("student_answer") or "").strip()
                if new_answer and new_answer not in existing_answer:
                    existing["student_answer"] = "\n".join(
                        filter(None, [existing_answer, new_answer])
                    )

                existing_feedback = (existing.get("feedback") or "").strip()
                new_feedback = (normalized.get("feedback") or "").strip()
                if new_feedback and new_feedback not in existing_feedback:
                    existing["feedback"] = "\n".join(
                        filter(None, [existing_feedback, new_feedback])
                    )

                merged_max = max(
                    float(existing.get("max_score") or 0),
                    float(normalized.get("max_score") or 0),
                )
                merged_score = float(existing.get("score") or 0) + float(
                    normalized.get("score") or 0
                )
                if merged_max > 0:
                    merged_score = min(merged_score, merged_max)
                existing["score"] = merged_score
                existing["max_score"] = merged_max

                existing_conf = existing.get("confidence")
                new_conf = normalized.get("confidence")
                if existing_conf is None:
                    existing["confidence"] = new_conf
                elif new_conf is not None:
                    existing["confidence"] = (float(existing_conf) + float(new_conf)) / 2

                if not existing.get("question_type") and normalized.get("question_type"):
                    existing["question_type"] = normalized.get("question_type")

        question_details = list(question_map.values())
        confidence_values = [
            float(q.get("confidence"))
            for q in question_details
            if isinstance(q.get("confidence"), (int, float))
        ]
        total_score = sum(q.get("score", 0) for q in question_details)
        max_score = sum(q.get("max_score", 0) for q in question_details)

        result = {
            "student_key": student_key,
            "status": "completed",
            "total_score": total_score,
            "max_score": max_score,
            "confidence": (
                sum(confidence_values) / len(confidence_values) if confidence_values else 0.8
            ),
            "question_details": question_details,
            "page_summaries": page_summaries,
        }
        if student_info is not None:
            result["student_info"] = student_info
        if overall_feedback:
            result["overall_feedback"] = overall_feedback
        return result

    def _parse_page_break_output(
        self,
        full_response: str,
        student_key: str,
    ) -> Optional[Dict[str, Any]]:
        text = self._extract_json_from_text(full_response)
        sections = [
            section.strip() for section in text.split("---PAGE_BREAK---") if section.strip()
        ]
        if not sections:
            return None

        page_results: List[Dict[str, Any]] = []
        for section in sections:
            candidate = self._extract_json_from_text(section).strip()
            if not candidate:
                continue
            try:
                page_results.append(self._load_json_with_repair(candidate))
                continue
            except json.JSONDecodeError:
                pass

            trimmed = self._extract_json_block(candidate)
            if not trimmed:
                continue
            try:
                page_results.append(self._load_json_with_repair(trimmed))
            except json.JSONDecodeError:
                continue

        if not page_results:
            return None
        return self._merge_page_break_results(page_results, student_key)

    @with_retry(max_retries=3, initial_delay=1.0, max_delay=60.0)
    async def _call_vision_api(
        self,
        image_b64: str,
        prompt: str,
        stream_callback: Optional[Callable[[str, str], Awaitable[None]]] = None,
    ) -> str:
        """
        è°ƒç”¨è§†è§‰ API (å¸¦æŒ‡æ•°é€€é¿é‡è¯•)

        API è°ƒç”¨å¤±è´¥æ—¶ä½¿ç”¨æŒ‡æ•°é€€é¿ç­–ç•¥é‡è¯•æœ€å¤š3æ¬¡ã€‚

        Args:
            image_b64: Base64 ç¼–ç çš„å›¾åƒ
            prompt: æç¤ºè¯
            stream_callback: æµå¼å›žè°ƒå‡½æ•° (stream_type, chunk) -> None

        Returns:
            str: LLM å“åº”æ–‡æœ¬

        éªŒè¯ï¼šéœ€æ±‚ 9.1
        """
        try:
            message = HumanMessage(
                content=[
                    {"type": "text", "text": prompt},
                    {"type": "image_url", "image_url": f"data:image/png;base64,{image_b64}"},
                ]
            )

            if stream_callback:
                # æµå¼è°ƒç”¨
                full_response = ""
                async for chunk in self.llm.astream([message]):
                    content = chunk.content
                    if content:
                        if isinstance(content, str):
                            full_response += content
                            await stream_callback("text", content)
                        elif isinstance(content, list):
                            # å¤„ç†å¤æ‚å†…å®¹
                            for part in content:
                                if isinstance(part, str):
                                    full_response += part
                                    await stream_callback("text", part)

                return self._extract_text_from_response(full_response)
            else:
                # éžæµå¼è°ƒç”¨
                response = await self.llm.ainvoke([message])
                return self._extract_text_from_response(response.content)
        except Exception as e:
            # è®°å½•é”™è¯¯åˆ°å…¨å±€é”™è¯¯ç®¡ç†å™¨
            error_manager = get_error_manager()
            error_manager.add_error(
                exc=e,
                context={
                    "function": "_call_vision_api",
                    "prompt_length": len(prompt),
                    "image_size": len(image_b64),
                },
            )
            raise

    @with_retry(max_retries=3, initial_delay=1.0, max_delay=60.0)
    async def _call_text_api(self, prompt: str) -> str:
        """
        è°ƒç”¨çº¯æ–‡æœ¬ API (å¸¦æŒ‡æ•°é€€é¿é‡è¯•)

        ç”¨äºŽå¤„ç†çº¯æ–‡æœ¬è¾“å…¥ï¼ˆå¦‚æ–‡æœ¬æ–‡ä»¶å†…å®¹ï¼‰ï¼Œä¸åŒ…å«å›¾åƒã€‚

        Args:
            prompt: æç¤ºè¯ï¼ˆåŒ…å«å­¦ç”Ÿç­”æ¡ˆæ–‡æœ¬ï¼‰

        Returns:
            str: LLM å“åº”æ–‡æœ¬
        """
        try:
            message = HumanMessage(content=prompt)
            response = await self.llm.ainvoke([message])
            return self._extract_text_from_response(response.content)
        except Exception as e:
            error_manager = get_error_manager()
            error_manager.add_error(
                exc=e,
                context={
                    "function": "_call_text_api",
                    "prompt_length": len(prompt),
                },
            )
            raise

    async def _call_vision_api_stream(self, image_b64: str, prompt: str) -> AsyncIterator[str]:
        """æµå¼è°ƒç”¨è§†è§‰ API"""
        message = HumanMessage(
            content=[
                {"type": "text", "text": prompt},
                {"type": "image_url", "image_url": f"data:image/png;base64,{image_b64}"},
            ]
        )
        async for chunk in self.llm.astream([message]):
            yield self._extract_text_from_response(chunk.content)

    async def _call_text_api_stream(self, prompt: str) -> AsyncIterator[str]:
        """æµå¼è°ƒç”¨çº¯æ–‡æœ¬ API"""
        message = HumanMessage(content=prompt)
        async for chunk in self.llm.astream([message]):
            yield self._extract_text_from_response(chunk.content)

    def _is_text_content(self, data: bytes) -> bool:
        """
        æ£€æµ‹è¾“å…¥æ˜¯å¦ä¸ºçº¯æ–‡æœ¬å†…å®¹

        Args:
            data: è¾“å…¥æ•°æ®ï¼ˆbytesï¼‰

        Returns:
            bool: å¦‚æžœæ˜¯å¯è§£ç çš„ UTF-8 æ–‡æœ¬è¿”å›ž True
        """
        try:
            # å°è¯•è§£ç ä¸º UTF-8 æ–‡æœ¬
            text = data.decode("utf-8")
            # æ£€æŸ¥æ˜¯å¦åŒ…å«å¸¸è§çš„æ–‡æœ¬ç‰¹å¾ï¼ˆä¸­æ–‡å­—ç¬¦ã€æ¢è¡Œç¬¦ç­‰ï¼‰
            # æŽ’é™¤äºŒè¿›åˆ¶æ–‡ä»¶ï¼ˆå¦‚ PNG/PDF çš„é­”æ•°ï¼‰
            if data[:4] in [b"\x89PNG", b"%PDF", b"\xff\xd8\xff"]:
                return False
            # å¦‚æžœèƒ½æˆåŠŸè§£ç ä¸”åŒ…å«å¯æ‰“å°å­—ç¬¦ï¼Œè®¤ä¸ºæ˜¯æ–‡æœ¬
            printable_ratio = sum(1 for c in text if c.isprintable() or c in "\n\r\t") / len(text)
            return printable_ratio > 0.8
        except (UnicodeDecodeError, ZeroDivisionError):
            return False

    async def vision_extraction(
        self, question_image_b64: str, rubric: str, standard_answer: Optional[str] = None
    ) -> str:
        """
        è§†è§‰æå–èŠ‚ç‚¹ï¼šåˆ†æžå­¦ç”Ÿç­”æ¡ˆå›¾åƒï¼Œç”Ÿæˆè¯¦ç»†çš„æ–‡å­—æè¿°

        Args:
            question_image_b64: Base64 ç¼–ç çš„é¢˜ç›®å›¾åƒ
            rubric: è¯„åˆ†ç»†åˆ™
            standard_answer: æ ‡å‡†ç­”æ¡ˆï¼ˆå¯é€‰ï¼‰

        Returns:
            str: å­¦ç”Ÿè§£é¢˜æ­¥éª¤çš„è¯¦ç»†æ–‡å­—æè¿°
        """
        # æž„å»ºæç¤ºè¯
        prompt = f"""è¯·ä»”ç»†åˆ†æžè¿™å¼ å­¦ç”Ÿç­”é¢˜å›¾åƒï¼Œæä¾›è¯¦ç»†çš„æ–‡å­—æè¿°ã€‚

è¯„åˆ†ç»†åˆ™ï¼š
{rubric}

{f"æ ‡å‡†ç­”æ¡ˆï¼š{standard_answer}" if standard_answer else ""}

è¯·æè¿°ï¼š
1. å­¦ç”Ÿå†™äº†ä»€ä¹ˆå†…å®¹ï¼ˆå…¬å¼ã€æ–‡å­—ã€å›¾è¡¨ç­‰ï¼‰
2. å­¦ç”Ÿçš„è§£é¢˜æ­¥éª¤å’Œæ€è·¯
3. å­¦ç”Ÿçš„è®¡ç®—è¿‡ç¨‹
4. ä»»ä½•å¯è§çš„é”™è¯¯æˆ–é—æ¼

è¯·æä¾›è¯¦ç»†ã€å®¢è§‚çš„æè¿°ï¼Œä¸è¦è¿›è¡Œè¯„åˆ†ï¼Œåªæè¿°ä½ çœ‹åˆ°çš„å†…å®¹ã€‚"""

        # æž„å»ºæ¶ˆæ¯
        message = HumanMessage(
            content=[
                {"type": "text", "text": prompt},
                {"type": "image_url", "image_url": f"data:image/jpeg;base64,{question_image_b64}"},
            ]
        )

        # è°ƒç”¨ LLM
        response = await self.llm.ainvoke([message])

        # æå–æ–‡æœ¬å†…å®¹
        return self._extract_text_from_response(response.content)

    async def rubric_mapping(
        self,
        vision_analysis: str,
        rubric: str,
        max_score: float,
        standard_answer: Optional[str] = None,
        critique_feedback: Optional[str] = None,
    ) -> Dict[str, Any]:
        """
        è¯„åˆ†æ˜ å°„èŠ‚ç‚¹ï¼šå°†è¯„åˆ†ç»†åˆ™çš„æ¯ä¸ªè¯„åˆ†ç‚¹æ˜ å°„åˆ°å­¦ç”Ÿç­”æ¡ˆä¸­çš„è¯æ®

        Args:
            vision_analysis: è§†è§‰åˆ†æžç»“æžœ
            rubric: è¯„åˆ†ç»†åˆ™
            max_score: æ»¡åˆ†
            standard_answer: æ ‡å‡†ç­”æ¡ˆï¼ˆå¯é€‰ï¼‰
            critique_feedback: åæ€åé¦ˆï¼ˆå¦‚æžœæ˜¯ä¿®æ­£å¾ªçŽ¯ï¼‰

        Returns:
            Dict: åŒ…å« rubric_mapping å’Œ initial_score
        """
        # æž„å»ºæç¤ºè¯
        prompt = f"""åŸºäºŽä»¥ä¸‹å­¦ç”Ÿç­”æ¡ˆçš„è§†è§‰åˆ†æžï¼Œè¯·é€æ¡æ ¸å¯¹è¯„åˆ†ç»†åˆ™ï¼Œå¹¶ç»™å‡ºè¯„åˆ†ã€‚

è§†è§‰åˆ†æžï¼š
{vision_analysis}

è¯„åˆ†ç»†åˆ™ï¼š
{rubric}

æ»¡åˆ†ï¼š{max_score}

{f"æ ‡å‡†ç­”æ¡ˆï¼š{standard_answer}" if standard_answer else ""}

{f"ä¿®æ­£åé¦ˆï¼š{critique_feedback}" if critique_feedback else ""}

è¯·å¯¹æ¯ä¸ªè¯„åˆ†ç‚¹è¿›è¡Œè¯„ä¼°ï¼Œè¿”å›ž JSON æ ¼å¼ï¼š
{{
    "rubric_mapping": [
        {{
            "rubric_point": "è¯„åˆ†ç‚¹æè¿°",
            "evidence": "ã€å¿…é¡»ã€‘åœ¨å­¦ç”Ÿç­”æ¡ˆä¸­æ‰¾åˆ°çš„è¯æ®ã€‚å¦‚æžœæ˜¯æ–‡æœ¬ï¼Œè¯·å¼•ç”¨åŽŸæ–‡ï¼›å¦‚æžœæ˜¯å›¾åƒï¼Œè¯·æè¿°ä½ç½®ï¼ˆå¦‚'å·¦ä¸Šè§’'ã€'ç¬¬xè¡Œ'ï¼‰ã€‚",
            "score_awarded": èŽ·å¾—çš„åˆ†æ•°,
            "max_score": è¯¥è¯„åˆ†ç‚¹çš„æ»¡åˆ†
        }}
    ],
    "initial_score": æ€»å¾—åˆ†,
    "reasoning": "è¯„åˆ†ç†ç”±"
}}"""

        # è°ƒç”¨ LLM (ä½¿ç”¨æµå¼ä»¥è§¦å‘äº‹ä»¶)
        message = HumanMessage(content=prompt)
        full_response = ""
        try:
            async for chunk in self.llm.astream([message]):
                content_chunk = chunk.content
                if content_chunk:
                    full_response += str(content_chunk)
        except Exception as e:
            logger.error(f"Rubric mapping streaming error: {e}")
            raise

        # æå–æ–‡æœ¬å†…å®¹
        result_text = self._extract_text_from_response(full_response)
        result_text = self._extract_json_from_text(result_text)

        result = json.loads(result_text)
        return result

    async def critique(
        self,
        vision_analysis: str,
        rubric: str,
        rubric_mapping: List[Dict[str, Any]],
        initial_score: float,
        max_score: float,
        standard_answer: Optional[str] = None,
    ) -> Dict[str, Any]:
        """
        è‡ªæˆ‘åæ€èŠ‚ç‚¹ï¼šå®¡æŸ¥è¯„åˆ†é€»è¾‘ï¼Œè¯†åˆ«æ½œåœ¨çš„è¯„åˆ†é”™è¯¯

        Args:
            vision_analysis: è§†è§‰åˆ†æžç»“æžœ
            rubric: è¯„åˆ†ç»†åˆ™
            rubric_mapping: è¯„åˆ†ç‚¹æ˜ å°„
            initial_score: åˆå§‹è¯„åˆ†
            max_score: æ»¡åˆ†
            standard_answer: æ ‡å‡†ç­”æ¡ˆï¼ˆå¯é€‰ï¼‰

        Returns:
            Dict: åŒ…å« critique_feedback å’Œ needs_revision
        """
        # æž„å»ºæç¤ºè¯
        prompt = f"""è¯·å®¡æŸ¥ä»¥ä¸‹è¯„åˆ†ç»“æžœï¼Œè¯†åˆ«æ½œåœ¨çš„è¯„åˆ†é”™è¯¯æˆ–ä¸ä¸€è‡´ä¹‹å¤„ã€‚

è§†è§‰åˆ†æžï¼š
{vision_analysis}

è¯„åˆ†ç»†åˆ™ï¼š
{rubric}

è¯„åˆ†æ˜ å°„ï¼š
{json.dumps(rubric_mapping, ensure_ascii=False, indent=2)}

åˆå§‹è¯„åˆ†ï¼š{initial_score}/{max_score}

{f"æ ‡å‡†ç­”æ¡ˆï¼š{standard_answer}" if standard_answer else ""}

è¯·æ£€æŸ¥ï¼š
1. è¯„åˆ†ç‚¹æ˜¯å¦éƒ½è¢«æ­£ç¡®è¯„ä¼°ï¼Ÿ
2. è¯æ®æ˜¯å¦å……åˆ†æ”¯æŒç»™å‡ºçš„åˆ†æ•°ï¼Ÿ
3. æ˜¯å¦æœ‰é—æ¼çš„è¯„åˆ†ç‚¹ï¼Ÿ
4. è¯„åˆ†æ˜¯å¦è¿‡äºŽä¸¥æ ¼æˆ–å®½æ¾ï¼Ÿ
5. æ€»åˆ†æ˜¯å¦æ­£ç¡®è®¡ç®—ï¼Ÿ

è¿”å›ž JSON æ ¼å¼ï¼š
{{
    "critique_feedback": "åæ€åé¦ˆï¼ˆå¦‚æžœæ²¡æœ‰é—®é¢˜ï¼Œè¿”å›ž nullï¼‰",
    "needs_revision": true/false,
    "confidence": 0.0-1.0 ä¹‹é—´çš„ç½®ä¿¡åº¦åˆ†æ•°
}}"""

        # è°ƒç”¨ LLM (ä½¿ç”¨æµå¼)
        message = HumanMessage(content=prompt)
        full_response = ""
        try:
            async for chunk in self.llm.astream([message]):
                content_chunk = chunk.content
                if content_chunk:
                    full_response += str(content_chunk)
        except Exception as e:
            logger.error(f"Critique streaming error: {e}")
            raise

        # æå–æ–‡æœ¬å†…å®¹
        result_text = self._extract_text_from_response(full_response)
        result_text = self._extract_json_from_text(result_text)

        result = json.loads(result_text)
        return result

    async def analyze_with_vision(
        self,
        images: List[bytes],
        prompt: str,
        stream_callback: Optional[Callable[[str, str], Awaitable[None]]] = None,
    ) -> Dict[str, Any]:
        """
        é€šç”¨è§†è§‰åˆ†æžæ–¹æ³•ï¼šåˆ†æžå¤šå¼ å›¾åƒå¹¶è¿”å›žç»“æž„åŒ–ç»“æžœ

        Args:
            images: å›¾åƒå­—èŠ‚åˆ—è¡¨
            prompt: åˆ†æžæç¤ºè¯

        Returns:
            Dict: åŒ…å« response çš„ç»“æžœ
        """
        # æž„å»ºæ¶ˆæ¯å†…å®¹
        content = [{"type": "text", "text": prompt}]

        # æ·»åŠ å›¾åƒ
        for img_bytes in images:
            if isinstance(img_bytes, bytes):
                img_b64 = base64.b64encode(img_bytes).decode("utf-8")
            else:
                img_b64 = img_bytes  # å·²ç»æ˜¯ base64 å­—ç¬¦ä¸²

            content.append({"type": "image_url", "image_url": f"data:image/png;base64,{img_b64}"})

        # è°ƒç”¨ LLM
        # è°ƒç”¨ LLM (ä½¿ç”¨æµå¼)
        message = HumanMessage(content=content)
        full_response = ""
        try:
            async for chunk in self.llm.astream([message]):
                content_chunk = chunk.content
                if content_chunk:
                    # æ­£ç¡®æå–æ–‡æœ¬å†…å®¹
                    if isinstance(content_chunk, str):
                        full_response += content_chunk
                        if stream_callback:
                            await stream_callback("output", content_chunk)
                    elif isinstance(content_chunk, list):
                        # å¤„ç†å¤šéƒ¨åˆ†å“åº”
                        for part in content_chunk:
                            if isinstance(part, str):
                                full_response += part
                                if stream_callback:
                                    await stream_callback("output", part)
                            elif isinstance(part, dict) and "text" in part:
                                full_response += part["text"]
                                if stream_callback:
                                    await stream_callback("output", part["text"])
                    else:
                        # å°è¯•è½¬æ¢ä¸ºå­—ç¬¦ä¸²ï¼Œä½†è®°å½•è­¦å‘Š
                        logger.warning(f"Unexpected chunk type: {type(content_chunk)}")
                        full_response += str(content_chunk)
        except Exception as e:
            logger.error(f"Vision streaming error: {e}")
            # Fallback to non-streaming if needed, or just re-raise
            raise

        # æå–æ–‡æœ¬å†…å®¹
        result_text = self._extract_text_from_response(full_response)

        return {"response": result_text}

    def _format_page_index_context(self, page_context: Optional[Dict[str, Any]]) -> str:
        """æ ¼å¼åŒ–ç´¢å¼•ä¸Šä¸‹æ–‡ï¼Œç”¨äºŽæç¤ºè¯æ³¨å…¥"""
        if not page_context:
            return ""

        student_info = page_context.get("student_info") or {}
        student_parts = []
        if student_info:
            name = student_info.get("name") or "æœªçŸ¥"
            student_id = student_info.get("student_id") or "æœªçŸ¥"
            class_name = student_info.get("class_name") or "æœªçŸ¥"
            confidence = student_info.get("confidence", 0.0)
            student_parts.append(f"å§“å={name}")
            student_parts.append(f"å­¦å·={student_id}")
            student_parts.append(f"ç­çº§={class_name}")
            student_parts.append(f"ç½®ä¿¡åº¦={confidence}")

        question_numbers = page_context.get("question_numbers") or []
        continuation_of = page_context.get("continuation_of") or "æ— "
        notes = page_context.get("index_notes") or []

        return (
            "## ç´¢å¼•ä¸Šä¸‹æ–‡ï¼ˆä¼˜å…ˆä½¿ç”¨ï¼‰\n"
            f"- page_index: {page_context.get('page_index')}\n"
            f"- question_numbers: {', '.join(question_numbers) if question_numbers else 'æ— '}\n"
            f"- continuation_of: {continuation_of}\n"
            f"- is_cover_page: {page_context.get('is_cover_page', False)}\n"
            f"- student_key: {page_context.get('student_key', 'æœªçŸ¥')}\n"
            f"- student_info: {', '.join(student_parts) if student_parts else 'æ— '}\n"
            f"- notes: {', '.join(notes) if notes else 'æ— '}\n"
        )

    # ==================== grade_page æ‹†åˆ†ä¸ºå¤šä¸ªç§æœ‰æ–¹æ³• ====================

    def _build_compact_rubric_info(
        self, parsed_rubric: Optional[Dict[str, Any]], rubric: str
    ) -> str:
        """æž„å»ºç²¾ç®€çš„è¯„åˆ†æ ‡å‡†ä¿¡æ¯"""
        if parsed_rubric and parsed_rubric.get("rubric_context"):
            return parsed_rubric["rubric_context"]

        if parsed_rubric and parsed_rubric.get("questions"):
            lines = []
            for q in self._limit_questions_for_prompt(parsed_rubric.get("questions", [])):
                qid = q.get("question_id", "?")
                max_score = q.get("max_score", 0)
                lines.append(f"ç¬¬{qid}é¢˜(æ»¡åˆ†{max_score}åˆ†):")

                scoring_points = q.get("scoring_points", [])
                for idx, sp in enumerate(self._limit_criteria_for_prompt(scoring_points), 1):
                    point_id = sp.get("point_id") or f"{qid}.{idx}"
                    lines.append(
                        f"  [{point_id}] {sp.get('score', 0)}åˆ†: {sp.get('description', '')}"
                    )
            return "\n".join(lines)

        return rubric or "è¯·æ ¹æ®ç­”æ¡ˆæ­£ç¡®æ€§è¯„åˆ†"

    def _build_grading_prompt(
        self,
        rubric: str,
        parsed_rubric: Optional[Dict[str, Any]],
        page_context: Optional[Dict[str, Any]] = None,
    ) -> str:
        """æž„å»ºè¯„åˆ†æç¤ºè¯ï¼ˆç²¾ç®€ç‰ˆï¼‰"""
        rubric_info = self._build_compact_rubric_info(parsed_rubric, rubric)
        index_context = self._format_page_index_context(page_context)

        return f"""## è¯„åˆ†æ ‡å‡†
{rubric_info}
{index_context}

## ä»»åŠ¡
1. åˆ¤æ–­é¡µé¢ç±»åž‹ï¼ˆç©ºç™½é¡µ/å°é¢é¡µç›´æŽ¥è¿”å›žis_blank_page=trueï¼‰
2. è¯†åˆ«é¢˜ç›®å¹¶å®šä½ä½œç­”åŒºåŸŸï¼ˆåæ ‡0-1ï¼‰
3. æŒ‰è¯„åˆ†æ ‡å‡†é€é¢˜æ‰¹æ”¹ï¼Œè¾“å‡ºJSONæ ¼å¼

## è¾“å‡ºJSONæ ¼å¼
```json
{{
  "score": æ€»å¾—åˆ†,
  "max_score": æ»¡åˆ†,
  "confidence": 0.0-1.0,
  "is_blank_page": false,
  "question_numbers": ["1", "2"],
  "question_details": [
    {{
      "question_id": "1",
      "score": 8,
      "max_score": 10,
      "student_answer": "å­¦ç”Ÿç­”æ¡ˆæ‘˜è¦",
      "feedback": "è¯„è¯­",
      "answer_region": {{"x_min": 0.05, "y_min": 0.15, "x_max": 0.95, "y_max": 0.45}},
      "scoring_point_results": [
        {{
          "point_id": "1.1",
          "awarded": 3,
          "max_points": 3,
          "evidence": "ã€åŽŸæ–‡å¼•ç”¨ã€‘å­¦ç”Ÿå†™äº†...",
          "evidence_region": {{"x_min": 0.1, "y_min": 0.2, "x_max": 0.4, "y_max": 0.25}}
        }}
      ]
    }}
  ],
  "page_summary": "ç®€è¦æ€»ç»“",
  "student_info": {{"name": "", "student_id": ""}}
}}
```"""

    def _parse_grading_response(self, response_text: str, max_score: float) -> Dict[str, Any]:
        """
        è§£æžè¯„åˆ†å“åº”ï¼Œå¹¶ç¡®ä¿ evidence å­—æ®µè¢«æ­£ç¡®å¡«å……

        Args:
            response_text: LLM å“åº”æ–‡æœ¬
            max_score: æ»¡åˆ†

        Returns:
            Dict: è§£æžåŽçš„è¯„åˆ†ç»“æžœ
        """
        json_text = self._extract_json_from_text(response_text)
        result = json.loads(json_text)

        # ç¡®ä¿æ‰€æœ‰ scoring_point_results éƒ½æœ‰ evidence å­—æ®µ
        for q in result.get("question_details", []):
            for spr in q.get("scoring_point_results", []):
                # æ£€æŸ¥ evidence æ˜¯å¦ä¸ºç©ºæˆ–æ— æ•ˆ
                evidence = spr.get("evidence", "")
                if not evidence or evidence.strip() in ["", "æ— ", "N/A", "null", "None"]:
                    # è‡ªåŠ¨è¡¥å……é»˜è®¤ evidence
                    awarded = spr.get("awarded", 0)
                    max_sp_score = spr.get("max_score", 0)
                    description = spr.get("description", "è¯¥è¯„åˆ†ç‚¹")

                    if awarded == max_sp_score:
                        spr["evidence"] = f"å­¦ç”Ÿæ­£ç¡®å®Œæˆäº†{description}ï¼ŒèŽ·å¾—æ»¡åˆ†"
                    elif awarded == 0:
                        spr["evidence"] = f"å­¦ç”Ÿæœªä½œç­”æˆ–æœªæ­£ç¡®å®Œæˆ{description}"
                    else:
                        spr["evidence"] = (
                            f"å­¦ç”Ÿéƒ¨åˆ†å®Œæˆäº†{description}ï¼ŒèŽ·å¾—{awarded}/{max_sp_score}åˆ†"
                        )

                    logger.warning(f"evidence å­—æ®µä¸ºç©ºï¼Œå·²è‡ªåŠ¨è¡¥å……: {spr['evidence']}")

        return result

    def _generate_feedback(self, result: Dict[str, Any]) -> str:
        """
        ä»Žè¯„åˆ†ç»“æžœç”Ÿæˆç»¼åˆåé¦ˆ

        Args:
            result: è¯„åˆ†ç»“æžœå­—å…¸

        Returns:
            str: ç»¼åˆåé¦ˆæ–‡æœ¬
        """
        feedback_parts = []

        if result.get("page_summary"):
            feedback_parts.append(result["page_summary"])

        for q in result.get("question_details", []):
            q_feedback = (
                f"ç¬¬{q.get('question_id', '?')}é¢˜: {q.get('score', 0)}/{q.get('max_score', 0)}åˆ†"
            )
            if q.get("feedback"):
                q_feedback += f" - {q['feedback']}"
            feedback_parts.append(q_feedback)

        return "\n".join(feedback_parts) if feedback_parts else "è¯„åˆ†å®Œæˆ"

    def _build_text_grading_prompt(
        self,
        text_content: str,
        rubric: str,
        parsed_rubric: Optional[Dict[str, Any]],
        page_context: Optional[Dict[str, Any]] = None,
    ) -> str:
        """
        æž„å»ºçº¯æ–‡æœ¬æ‰¹æ”¹çš„æç¤ºè¯

        Args:
            text_content: å­¦ç”Ÿç­”æ¡ˆæ–‡æœ¬å†…å®¹
            rubric: è¯„åˆ†ç»†åˆ™æ–‡æœ¬
            parsed_rubric: è§£æžåŽçš„è¯„åˆ†æ ‡å‡†

        Returns:
            str: å®Œæ•´çš„è¯„åˆ†æç¤ºè¯
        """
        # èŽ·å–è¯„åˆ†æ ‡å‡†ä¿¡æ¯
        rubric_info = ""
        if parsed_rubric and parsed_rubric.get("rubric_context"):
            rubric_info = parsed_rubric["rubric_context"]
        elif rubric:
            rubric_info = rubric
        else:
            rubric_info = "è¯·æ ¹æ®ç­”æ¡ˆçš„æ­£ç¡®æ€§ã€å®Œæ•´æ€§å’Œæ¸…æ™°åº¦è¿›è¡Œè¯„åˆ†"

        index_context = self._format_page_index_context(page_context)

        return f"""ä½ æ˜¯ä¸€ä½ä¸“ä¸šçš„é˜…å·æ•™å¸ˆï¼Œè¯·ä»”ç»†åˆ†æžä»¥ä¸‹å­¦ç”Ÿç­”æ¡ˆæ–‡æœ¬å¹¶è¿›è¡Œç²¾ç¡®è¯„åˆ†ã€‚

## è¯„åˆ†æ ‡å‡†
{rubric_info}
{index_context}

## å­¦ç”Ÿç­”æ¡ˆæ–‡æœ¬
```
{text_content}
```

## è¯„åˆ†ä»»åŠ¡

### ç¬¬ä¸€æ­¥ï¼šå†…å®¹åˆ¤æ–­
é¦–å…ˆåˆ¤æ–­è¿™æ˜¯å¦æ˜¯æœ‰æ•ˆçš„ç­”é¢˜å†…å®¹ï¼š
- å¦‚æžœæ˜¯ç©ºç™½æˆ–æ— æ„ä¹‰å†…å®¹ï¼Œè¿”å›ž score=0, max_score=0, is_blank_page=true
- å¦‚æžœç´¢å¼•ä¸Šä¸‹æ–‡æ ‡è®° is_cover_page=trueï¼Œä¹ŸæŒ‰ç©ºç™½é¡µå¤„ç†

### ç¬¬äºŒæ­¥ï¼šé¢˜ç›®è¯†åˆ«ä¸Žè¯„åˆ†
å¦‚æžœåŒ…å«æœ‰æ•ˆç­”é¢˜å†…å®¹ï¼š
1. è¯†åˆ«æ–‡æœ¬ä¸­å‡ºçŽ°çš„æ‰€æœ‰é¢˜ç›®ç¼–å·ï¼ˆå¦‚æä¾›äº†ç´¢å¼•ä¸Šä¸‹æ–‡ï¼Œå¿…é¡»ä»¥ç´¢å¼•ä¸ºå‡†ï¼‰
2. å¯¹æ¯é“é¢˜é€ä¸€è¯„åˆ†ï¼Œä¸¥æ ¼æŒ‰ç…§è¯„åˆ†æ ‡å‡†
3. è®°å½•å­¦ç”Ÿç­”æ¡ˆçš„å…³é”®å†…å®¹
4. ç»™å‡ºè¯¦ç»†çš„è¯„åˆ†è¯´æ˜Ž

### ç¬¬ä¸‰æ­¥ï¼šå­¦ç”Ÿä¿¡æ¯æå–
å°è¯•ä»Žæ–‡æœ¬ä¸­è¯†åˆ«ï¼š
- å­¦ç”Ÿå§“å
- å­¦å·
- ç­çº§ä¿¡æ¯

## è¾“å‡ºæ ¼å¼ï¼ˆJSONï¼‰
```json
{{
    "score": æœ¬é¡µæ€»å¾—åˆ†,
    "max_score": æœ¬é¡µæ¶‰åŠé¢˜ç›®çš„æ»¡åˆ†æ€»å’Œ,
    "confidence": è¯„åˆ†ç½®ä¿¡åº¦ï¼ˆ0.0-1.0ï¼‰,
    "is_blank_page": false,
    "question_numbers": ["1", "2", "3"],
    "question_details": [
        {{
            "question_id": "1",
            "score": 8,
            "max_score": 10,
            "student_answer": "å­¦ç”Ÿå†™äº†ï¼š...",
            "is_correct": false,
            "feedback": "ç¬¬1æ­¥æ­£ç¡®å¾—3åˆ†ï¼Œç¬¬2æ­¥è®¡ç®—é”™è¯¯æ‰£2åˆ†...",
            "scoring_point_results": [
                {{
                    "point_index": 1,
                    "description": "ç¬¬1æ­¥è®¡ç®—",
                    "max_score": 3,
                    "awarded": 3,
                    "evidence": "ã€å¿…å¡«ã€‘æ–‡æœ¬ç¬¬3æ®µä¸­å­¦ç”Ÿå†™é“ï¼š'ä»£å…¥x=2å¾—y=4'ï¼Œè®¡ç®—æ­£ç¡®"
                }},
                {{
                    "point_index": 2,
                    "description": "ç¬¬2æ­¥é€»è¾‘",
                    "max_score": 7,
                    "awarded": 5,
                    "evidence": "ã€å¿…å¡«ã€‘å­¦ç”Ÿåœ¨ç»“è®ºå¤„å†™'å› æ­¤ç­”æ¡ˆä¸º5'ï¼Œä½†æ­£ç¡®ç­”æ¡ˆåº”ä¸º4ï¼Œæ‰£2åˆ†"
                }}
            ]
        }}
    ],
    "page_summary": "æœ¬é¡µåŒ…å«ç¬¬1-3é¢˜ï¼Œå­¦ç”Ÿæ•´ä½“è¡¨çŽ°è‰¯å¥½ï¼Œä¸»è¦åœ¨è®¡ç®—æ–¹é¢æœ‰å¤±è¯¯",
    "student_info": {{
        "name": "å¼ ä¸‰",
        "student_id": "2024001"
    }}
}}
```

## é‡è¦è¯„åˆ†åŽŸåˆ™
1. **ä¸¥æ ¼éµå¾ªè¯„åˆ†æ ‡å‡†**ï¼šæ¯ä¸ªå¾—åˆ†ç‚¹å¿…é¡»æœ‰æ˜Žç¡®ä¾æ®
2. **éƒ¨åˆ†åˆ†æ•°**ï¼šå¦‚æžœå­¦ç”Ÿç­”æ¡ˆéƒ¨åˆ†æ­£ç¡®ï¼Œç»™äºˆç›¸åº”çš„éƒ¨åˆ†åˆ†æ•°
3. **max_score è®¡ç®—**ï¼šåªè®¡ç®—æœ¬é¡µå®žé™…å‡ºçŽ°çš„é¢˜ç›®çš„æ»¡åˆ†ï¼Œä¸æ˜¯æ•´å¼ è¯•å·çš„æ€»åˆ†
4. **è¯¦ç»†åé¦ˆ**ï¼šæ˜Žç¡®æŒ‡å‡ºæ­£ç¡®å’Œé”™è¯¯çš„éƒ¨åˆ†ï¼Œç»™å‡ºå…·ä½“çš„æ‰£åˆ†åŽŸå› 

## ã€å…³é”®ã€‘è¯æ®å­—æ®µè¦æ±‚
**evidence å­—æ®µæ˜¯å¿…å¡«é¡¹**ï¼Œå¿…é¡»æ»¡è¶³ä»¥ä¸‹è¦æ±‚ï¼š
1. **å…·ä½“ä½ç½®**ï¼šè¯´æ˜Žè¯æ®åœ¨æ–‡æœ¬ä¸­çš„ä½ç½®ï¼ˆå¦‚"ç¬¬Xæ®µ"ã€"ç¬¬Xè¡Œ"ã€"ç­”æ¡ˆæœ«å°¾"ï¼‰
2. **åŽŸæ–‡å¼•ç”¨**ï¼šå°½å¯èƒ½ç›´æŽ¥å¼•ç”¨å­¦ç”Ÿçš„åŽŸå§‹æ–‡å­—
3. **å¯¹æ¯”è¯´æ˜Ž**ï¼šå¦‚æžœç­”æ¡ˆé”™è¯¯ï¼Œè¯´æ˜Žå­¦ç”Ÿå†™çš„å†…å®¹ä¸Žæ­£ç¡®ç­”æ¡ˆçš„å·®å¼‚
4. **æœªæ‰¾åˆ°æƒ…å†µ**ï¼šå¦‚æžœæ‰¾ä¸åˆ°ç›¸å…³å†…å®¹ï¼Œå†™æ˜Ž"å­¦ç”Ÿæœªä½œç­”æ­¤éƒ¨åˆ†"æˆ–"æ–‡æœ¬ä¸­æœªæ‰¾åˆ°ç›¸å…³å†…å®¹"

ç¦æ­¢åœ¨ evidence ä¸­å†™ç©ºå­—ç¬¦ä¸²æˆ–æ¨¡ç³Šæè¿°ï¼"""

    async def grade_page(
        self,
        image: bytes,
        rubric: str,
        max_score: float = 10.0,
        parsed_rubric: Optional[Dict[str, Any]] = None,
        page_context: Optional[Dict[str, Any]] = None,
        stream_callback: Optional[Callable[[str, str], Awaitable[None]]] = None,
    ) -> Dict[str, Any]:
        """
        æ‰¹æ”¹å•é¡µï¼šåˆ†æžå›¾åƒæˆ–æ–‡æœ¬å¹¶ç»™å‡ºè¯¦ç»†è¯„åˆ†

        è‡ªåŠ¨æ£€æµ‹è¾“å…¥ç±»åž‹ï¼ˆå›¾åƒæˆ–æ–‡æœ¬ï¼‰ï¼Œä½¿ç”¨ç›¸åº”çš„ API è¿›è¡Œæ‰¹æ”¹ã€‚

        Args:
            image: å›¾åƒå­—èŠ‚æˆ–æ–‡æœ¬å­—èŠ‚
            rubric: è¯„åˆ†ç»†åˆ™æ–‡æœ¬
            max_score: æ»¡åˆ†
            parsed_rubric: è§£æžåŽçš„è¯„åˆ†æ ‡å‡†ï¼ˆåŒ…å«é¢˜ç›®ä¿¡æ¯ï¼‰

        Returns:
            Dict: åŒ…å«è¯¦ç»†è¯„åˆ†ç»“æžœ
        """
        logger.debug(f"å¼€å§‹æ‰¹æ”¹å•é¡µ, rubricé•¿åº¦={len(rubric)}")

        # æ£€æµ‹è¾“å…¥ç±»åž‹ï¼šæ–‡æœ¬è¿˜æ˜¯å›¾åƒ
        is_text = isinstance(image, bytes) and self._is_text_content(image)

        try:
            if is_text:
                # çº¯æ–‡æœ¬è¾“å…¥ï¼šä½¿ç”¨æ–‡æœ¬ API
                text_content = image.decode("utf-8")
                logger.info(f"æ£€æµ‹åˆ°æ–‡æœ¬è¾“å…¥ï¼Œé•¿åº¦={len(text_content)}å­—ç¬¦ï¼Œä½¿ç”¨æ–‡æœ¬APIæ‰¹æ”¹")

                # æž„å»ºæ–‡æœ¬æ‰¹æ”¹æç¤ºè¯
                prompt = self._build_text_grading_prompt(
                    text_content, rubric, parsed_rubric, page_context
                )

                # è°ƒç”¨æ–‡æœ¬ API
                response_text = await self._call_text_api(prompt, stream_callback)
            else:
                # å›¾åƒè¾“å…¥ï¼šä½¿ç”¨è§†è§‰ API
                logger.info("æ£€æµ‹åˆ°å›¾åƒè¾“å…¥ï¼Œä½¿ç”¨è§†è§‰APIæ‰¹æ”¹")

                # æž„å»ºå›¾åƒæ‰¹æ”¹æç¤ºè¯
                prompt = self._build_grading_prompt(rubric, parsed_rubric, page_context)

                # è½¬æ¢å›¾åƒä¸º base64
                if isinstance(image, bytes):
                    img_b64 = base64.b64encode(image).decode("utf-8")
                else:
                    img_b64 = image

                # è°ƒç”¨è§†è§‰ API
                response_text = await self._call_vision_api(img_b64, prompt, stream_callback)

            # è§£æžå“åº”
            result = self._parse_grading_response(response_text, max_score)

            # ç”Ÿæˆç»¼åˆåé¦ˆ
            result["feedback"] = self._generate_feedback(result)

            logger.info(
                f"æ‰¹æ”¹å®Œæˆ: score={result.get('score')}, confidence={result.get('confidence')}"
            )

            return result

        except json.JSONDecodeError as e:
            logger.error(f"è¯„åˆ† JSON è§£æžå¤±è´¥: {e}")
            return {
                "score": 0.0,
                "max_score": max_score,
                "confidence": 0.0,
                "feedback": f"è¯„åˆ†è§£æžå¤±è´¥: {str(e)}",
                "question_numbers": [],
                "question_details": [],
                "student_info": None,
            }
        except Exception as e:
            logger.error(f"è¯„åˆ†å¤±è´¥: {e}", exc_info=True)
            return {
                "score": 0.0,
                "max_score": max_score,
                "confidence": 0.0,
                "feedback": f"è¯„åˆ†å¤±è´¥: {str(e)}",
                "question_numbers": [],
                "question_details": [],
                "student_info": None,
            }

    def _normalize_question_id(self, value: Any) -> str:
        if value is None:
            return ""
        text = str(value).strip()
        if not text:
            return ""
        for token in ["ç¬¬", "é¢˜ç›®", "é¢˜", "Q", "q"]:
            text = text.replace(token, "")
        return text.strip().rstrip(".:ï¼š")

    def _build_question_hints(
        self, parsed_rubric: Optional[Dict[str, Any]], page_context: Optional[Dict[str, Any]] = None
    ) -> str:
        if not parsed_rubric:
            return ""

        preferred = []
        if page_context:
            preferred = page_context.get("question_numbers") or []

        questions = parsed_rubric.get("questions", [])
        lines = []
        for q in self._limit_questions_for_prompt(questions):
            qid = self._normalize_question_id(q.get("question_id") or q.get("id"))
            if not qid:
                continue
            if preferred and qid not in [self._normalize_question_id(p) for p in preferred]:
                continue
            text = q.get("question_text") or ""
            if text:
                text = text[:80] + "..." if len(text) > 80 else text
                lines.append(f"- é¢˜å· {qid}: {text}")
            else:
                lines.append(f"- é¢˜å· {qid}")

        if not lines and questions:
            for q in self._limit_questions_for_prompt(questions):
                qid = self._normalize_question_id(q.get("question_id") or q.get("id"))
                if qid:
                    lines.append(f"- é¢˜å· {qid}")

        return "\n".join(lines)

    def _infer_question_type(self, question: Dict[str, Any]) -> str:
        raw_type = question.get("question_type") or question.get("questionType") or ""
        raw_type = str(raw_type).strip().lower()
        if raw_type:
            return raw_type

        question_text = (
            question.get("question_text") or question.get("questionText") or ""
        ).strip()
        grading_notes = (
            question.get("grading_notes") or question.get("gradingNotes") or ""
        ).strip()
        standard_answer = (
            question.get("standard_answer") or question.get("standardAnswer") or ""
        ).strip()
        alternative_solutions = (
            question.get("alternative_solutions") or question.get("alternativeSolutions") or []
        )

        text_blob = f"{question_text} {grading_notes}".lower()
        if question_text:
            text_no_space = re.sub(r"\s+", "", question_text)
            if re.search(r"[A-D][\\.ã€ï¼Ž]", text_no_space):
                return "choice"
        if standard_answer:
            answer_clean = re.sub(r"\s+", "", standard_answer.upper())
            if re.fullmatch(r"[A-D](?:[ã€,/ï¼Œ ]*[A-D]){0,3}", answer_clean):
                return "choice"
        if any(
            token in text_blob for token in ["é€‰æ‹©é¢˜", "å•é€‰", "å¤šé€‰", "é€‰é¡¹", "è¯·é€‰æ‹©", "ä¸‹åˆ—"]
        ):
            return "choice"

        if alternative_solutions:
            return "subjective"
        if any(
            token in text_blob
            for token in [
                "ç®€ç­”",
                "è®ºè¿°",
                "è¯æ˜Ž",
                "æŽ¨å¯¼",
                "è§£é‡Š",
                "åˆ†æž",
                "è®¨è®º",
                "è®¾è®¡",
                "è¯´æ˜Ž",
                "è¿‡ç¨‹",
                "æ­¥éª¤",
            ]
        ):
            return "subjective"
        if any(token in text_blob for token in ["åˆ¤æ–­", "å¡«ç©º", "å¯¹é”™", "æ˜¯éž", "true", "false"]):
            return "objective"

        if standard_answer:
            answer_compact = re.sub(r"\s+", "", standard_answer)
            if len(answer_compact) <= 4 and re.fullmatch(
                r"[0-9A-Za-z+\\-.=()ï¼ˆï¼‰/\\\\]+", answer_compact
            ):
                return "objective"
            if len(standard_answer) > 30 or "\n" in standard_answer:
                return "subjective"

        return "objective"

    def _build_rubric_payload(
        self, parsed_rubric: Optional[Dict[str, Any]], question_ids: List[str]
    ) -> Dict[str, Any]:
        if not parsed_rubric:
            return {"questions": []}

        questions = parsed_rubric.get("questions", [])
        normalized_targets = [self._normalize_question_id(qid) for qid in question_ids if qid]
        selected = []
        for q in questions:
            qid = self._normalize_question_id(q.get("question_id") or q.get("id"))
            if normalized_targets and qid not in normalized_targets:
                continue
            question_type = self._infer_question_type(q)
            scoring_points = []
            for idx, sp in enumerate(q.get("scoring_points", [])):
                point_id = sp.get("point_id") or sp.get("pointId") or f"{qid}.{idx + 1}"
                scoring_points.append(
                    {
                        "point_id": point_id,
                        "description": sp.get("description", ""),
                        "score": sp.get("score", 0),
                        "is_required": sp.get("is_required", True),
                        "keywords": sp.get("keywords") or [],
                        "expected_value": sp.get("expected_value") or sp.get("expectedValue") or "",
                    }
                )
            deduction_rules = []
            for idx, dr in enumerate(q.get("deduction_rules") or q.get("deductionRules") or []):
                deduction_rules.append(
                    {
                        "rule_id": dr.get("rule_id") or dr.get("ruleId") or f"{qid}.d{idx + 1}",
                        "description": dr.get("description", ""),
                        "deduction": dr.get("deduction", dr.get("score", 0)),
                        "conditions": dr.get("conditions") or dr.get("when") or "",
                    }
                )
            alternative_solutions = []
            for alt in q.get("alternative_solutions") or q.get("alternativeSolutions") or []:
                if not isinstance(alt, dict):
                    continue
                alternative_solutions.append(
                    {
                        "description": (alt.get("description", "") or "")[:200],
                        "scoring_criteria": (
                            alt.get("scoring_criteria")
                            or alt.get("scoringCriteria")
                            or alt.get("scoring_conditions")
                            or alt.get("scoringConditions")
                            or ""
                        )[:200],
                        "max_score": alt.get(
                            "max_score", alt.get("maxScore", q.get("max_score", 0))
                        ),
                    }
                )
            selected.append(
                {
                    "question_id": qid,
                    "max_score": q.get("max_score", 0),
                    "question_type": question_type,
                    "question_text": (q.get("question_text") or "")[:200],
                    "standard_answer": (q.get("standard_answer") or "")[:300],
                    "grading_notes": (q.get("grading_notes") or "")[:300],
                    "scoring_points": scoring_points,
                    "deduction_rules": deduction_rules,
                    "alternative_solutions": alternative_solutions,
                }
            )

        if not selected:
            for q in self._limit_questions_for_prompt(questions):
                qid = self._normalize_question_id(q.get("question_id") or q.get("id"))
                if not qid:
                    continue
                question_type = self._infer_question_type(q)
                scoring_points = []
                for idx, sp in enumerate(q.get("scoring_points", [])):
                    point_id = sp.get("point_id") or sp.get("pointId") or f"{qid}.{idx + 1}"
                    scoring_points.append(
                        {
                            "point_id": point_id,
                            "description": sp.get("description", ""),
                            "score": sp.get("score", 0),
                            "is_required": sp.get("is_required", True),
                            "keywords": sp.get("keywords") or [],
                            "expected_value": sp.get("expected_value")
                            or sp.get("expectedValue")
                            or "",
                        }
                    )
                deduction_rules = []
                for idx, dr in enumerate(q.get("deduction_rules") or q.get("deductionRules") or []):
                    deduction_rules.append(
                        {
                            "rule_id": dr.get("rule_id") or dr.get("ruleId") or f"{qid}.d{idx + 1}",
                            "description": dr.get("description", ""),
                            "deduction": dr.get("deduction", dr.get("score", 0)),
                            "conditions": dr.get("conditions") or dr.get("when") or "",
                        }
                    )
                alternative_solutions = []
                for alt in q.get("alternative_solutions") or q.get("alternativeSolutions") or []:
                    if not isinstance(alt, dict):
                        continue
                    alternative_solutions.append(
                        {
                            "description": (alt.get("description", "") or "")[:200],
                            "scoring_criteria": (
                                alt.get("scoring_criteria")
                                or alt.get("scoringCriteria")
                                or alt.get("scoring_conditions")
                                or alt.get("scoringConditions")
                                or ""
                            )[:200],
                            "max_score": alt.get(
                                "max_score", alt.get("maxScore", q.get("max_score", 0))
                            ),
                        }
                    )
                selected.append(
                    {
                        "question_id": qid,
                        "max_score": q.get("max_score", 0),
                        "question_type": question_type,
                        "question_text": (q.get("question_text") or "")[:200],
                        "standard_answer": (q.get("standard_answer") or "")[:300],
                        "grading_notes": (q.get("grading_notes") or "")[:300],
                        "scoring_points": scoring_points,
                        "deduction_rules": deduction_rules,
                        "alternative_solutions": alternative_solutions,
                    }
                )

        return {
            "total_score": parsed_rubric.get("total_score", 0),
            "questions": selected,
        }

    def _safe_json_loads(self, text: str, fallback: Dict[str, Any]) -> Dict[str, Any]:
        if not text:
            return fallback
        json_text = self._extract_json_from_text(text)
        try:
            return self._load_json_with_repair(json_text)
        except Exception as e:
            logger.warning(f"JSON è§£æžå¤±è´¥: {e}")
            return fallback

    async def extract_answer_evidence(
        self,
        image: bytes,
        parsed_rubric: Optional[Dict[str, Any]] = None,
        page_context: Optional[Dict[str, Any]] = None,
        stream_callback: Optional[Callable[[str, str], Awaitable[None]]] = None,
    ) -> Dict[str, Any]:
        """
        ä»…è¿›è¡Œç­”æ¡ˆè¯æ®æŠ½å–ï¼Œç¦æ­¢è¯„åˆ†ã€‚
        """
        question_hints = self._build_question_hints(parsed_rubric, page_context)
        index_context = self._format_page_index_context(page_context)
        # é¢„å…ˆæž„å»ºé¢˜å·æç¤ºï¼Œé¿å… f-string ä¸­ä½¿ç”¨åæ–œæ 
        hints_section = f"å¯ç”¨é¢˜å·æç¤º:\n{question_hints}" if question_hints else ""
        prompt = f"""ä½ æ˜¯é˜…å·åŠ©ç†ï¼Œåªåšâ€œç­”æ¡ˆè¯æ®æŠ½å–â€ï¼Œä¸è¦è¯„åˆ†ã€‚
{index_context}
{hints_section}

è¦æ±‚ï¼š
1. åªå†™å›¾ä¸­æ˜Žç¡®å¯è§çš„åŽŸæ–‡/å…¬å¼/æ­¥éª¤ï¼Œæ— æ³•è¾¨è®¤å°±æ³¨æ˜Žä¸æ¸…æ™°ã€‚
2. ä¸è¦æŽ¨æ–­ã€ä¸è¡¥å†™ã€ä¸è¯„åˆ†ã€‚
3. If the page is a cover/instruction page, set is_cover_page=true and is_blank_page=true with answers=[]. If it is blank, set is_blank_page=true and is_cover_page=false.
4. answer_text ä¿ç•™å…³é”®æ­¥éª¤ä¸Žå…¬å¼ï¼Œé¿å…é•¿ç¯‡å¤è¿°ã€‚
5. Output limits: answer_text<=160 chars; evidence_snippets<=1 item (<=90 chars); page_summary<=100 chars; question_numbers<=6.

è¾“å‡º JSONï¼š
```json
{{
  "is_blank_page": false,
  "is_cover_page": false,
  "question_numbers": ["1"],
  "page_summary": "æœ¬é¡µå†…å®¹æ¦‚è¿°ï¼ˆä¸è¯„åˆ†ï¼‰",
  "student_info": {{
    "name": "",
    "student_id": "",
    "class_name": "",
    "confidence": 0.0
  }},
  "answers": [
    {{
      "question_id": "1",
      "answer_text": "å­¦ç”ŸåŽŸæ–‡/å…¬å¼/æ­¥éª¤",
      "evidence_snippets": ["ã€åŽŸæ–‡å¼•ç”¨ã€‘..."],
      "uncertainty_flags": ["handwriting_unclear"],
      "confidence": 0.0
    }}
  ],
  "warnings": []
}}
```
"""
        if isinstance(image, bytes):
            img_b64 = base64.b64encode(image).decode("utf-8")
        else:
            img_b64 = image

        if stream_callback:
            response_text = ""
            async for chunk in self._call_vision_api_stream(img_b64, prompt):
                response_text += chunk
                await stream_callback("text", chunk)
        else:
            response_text = await self._call_vision_api(img_b64, prompt)
        fallback = {
            "is_blank_page": False,
            "is_cover_page": False,
            "question_numbers": [],
            "page_summary": "",
            "student_info": None,
            "answers": [],
            "warnings": ["parse_error"],
        }
        return self._safe_json_loads(response_text, fallback)

    async def score_from_evidence(
        self,
        evidence: Dict[str, Any],
        parsed_rubric: Optional[Dict[str, Any]] = None,
        page_context: Optional[Dict[str, Any]] = None,
        mode: Literal["fast", "strict"] = "fast",
        stream_callback: Optional[Callable[[str, str], Awaitable[None]]] = None,
    ) -> Dict[str, Any]:
        """
        åŸºäºŽè¯æ®ä¸Žè¯„åˆ†æ ‡å‡†è¿›è¡Œè¯„åˆ†ï¼ˆçº¯æ–‡æœ¬è°ƒç”¨ï¼‰ã€‚
        """
        answer_ids = []
        for item in evidence.get("answers", []):
            qid = self._normalize_question_id(item.get("question_id"))
            if qid:
                answer_ids.append(qid)
        question_numbers = (
            evidence.get("question_numbers") or (page_context or {}).get("question_numbers") or []
        )
        for qid in question_numbers:
            normalized = self._normalize_question_id(qid)
            if normalized and normalized not in answer_ids:
                answer_ids.append(normalized)

        if not answer_ids and parsed_rubric and parsed_rubric.get("questions"):
            for q in self._limit_questions_for_prompt(parsed_rubric.get("questions", [])):
                qid = self._normalize_question_id(q.get("question_id") or q.get("id"))
                if qid and qid not in answer_ids:
                    answer_ids.append(qid)

        rubric_payload = self._build_rubric_payload(parsed_rubric, answer_ids)
        mode_label = "FAST" if mode == "fast" else "STRICT"
        fast_note = (
            "FAST mode: keep output minimal; if full score, feedback must be empty."
            if mode == "fast"
            else ""
        )
        output_constraints = (
            "Output constraints: feedback<=120 chars (empty if full score); "
            "student_answer<=120 chars; evidence<=90 chars; reason<=120 chars; "
            "typo_notes<=3 items."
        )
        question_type_rules = (
            "Question type rules:\n"
            "- choice: no analysis; feedback/self_critique must be empty; keep output minimal.\n"
            "- objective: strictly follow rubric/scoring_points/deduction_rules; no speculation.\n"
            "- subjective: allow partial credit; if using alternative_solutions, set "
            "used_alternative_solution=true and fill alternative_solution_ref; lower confidence.\n"
        )
        # ç½®ä¿¡åº¦è®¡ç®—è§„åˆ™è¯´æ˜Žï¼ˆè¯„åˆ†æ ‡å‡†å¼•ç”¨ä¸Žè®°å¿†ç³»ç»Ÿä¼˜åŒ–ï¼‰
        confidence_rules = (
            "ç½®ä¿¡åº¦è®¡ç®—è§„åˆ™ï¼š\n"
            "- æœ‰ç²¾ç¡®å¼•ç”¨(citation_quality=exact)ï¼šç½®ä¿¡åº¦ 0.9\n"
            "- éƒ¨åˆ†å¼•ç”¨(citation_quality=partial)ï¼šç½®ä¿¡åº¦ 0.81\n"
            "- æ— å¼•ç”¨(citation_quality=none)ï¼šç½®ä¿¡åº¦æœ€é«˜ 0.7\n"
            "- å¦ç±»è§£æ³•(is_alternative_solution=true)ï¼šç½®ä¿¡åº¦å†é™ 25%\n"
        )
        prompt = f"""ä½ æ˜¯ä¸¥è°¨çš„é˜…å·è€å¸ˆï¼Œåªèƒ½åŸºäºŽâ€œè¯„åˆ†æ ‡å‡†â€å’Œâ€œç­”æ¡ˆè¯æ®â€è¯„åˆ†ã€‚
Mode: {mode_label}
{fast_note}
{output_constraints}
{question_type_rules}
{confidence_rules}
ç¦æ­¢è‡†æµ‹ï¼›è¯æ®ä¸è¶³æ—¶å¿…é¡»ç»™ 0 åˆ†å¹¶è¯´æ˜Žã€‚
å¦‚è¯„åˆ†æ ‡å‡†åŒ…å«æ‰£åˆ†è§„åˆ™ï¼ˆdeduction_rulesï¼‰ï¼Œè¯·æŒ‰è§„åˆ™æ‰£åˆ†å¹¶åœ¨åŽŸå› ä¸­è¯´æ˜Žã€‚
å¦‚å‘çŽ°é”™åˆ«å­—/æ‹¼å†™é”™è¯¯ï¼Œè¯·åœ¨æ¯é“é¢˜çš„ typo_notes ä¸­æ ‡å‡ºã€‚
æ¯ä¸ª scoring_point_results å¿…é¡»åŒ…å« point_idã€rubric_reference å’Œ evidenceï¼›è¯æ®ä¸è¶³æ—¶ evidence å†™â€œã€åŽŸæ–‡å¼•ç”¨ã€‘æœªæ‰¾åˆ°â€ã€‚

è¯„åˆ†æ ‡å‡†(JSON)ï¼š
{json.dumps(rubric_payload, ensure_ascii=False, indent=2)}

ç­”æ¡ˆè¯æ®(JSON)ï¼š
{json.dumps(evidence, ensure_ascii=False, indent=2)}

è¾“å‡º JSONï¼š
```json
{{
  "score": 0,
  "max_score": 0,
  "confidence": 0.0,
  "question_numbers": ["1"],
  "question_details": [
    {{
      "question_id": "1",
      "score": 0,
      "max_score": 0,
      "confidence": 0.0,
      "question_type": "objective",
      "student_answer": "",
      "feedback": "",
      "used_alternative_solution": false,
      "alternative_solution_ref": "",
      "typo_notes": ["å‘çŽ°çš„é”™åˆ«å­—/æ‹¼å†™é”™è¯¯ï¼ˆå¦‚æœ‰ï¼‰"],
      "scoring_point_results": [
        {{
          "point_id": "1.1",
          "rubric_reference": "[1.1] è¯„åˆ†ç‚¹æè¿°ï¼ˆå¿…é¡»å¼•ç”¨å…·ä½“è¯„åˆ†æ ‡å‡†æ¡ç›®ï¼‰",
          "citation_quality": "exact|partial|none",
          "is_alternative_solution": false,
          "alternative_description": "",
          "decision": "å¾—åˆ†/æœªå¾—åˆ†",
          "awarded": 0,
          "max_points": 0,
          "evidence": "ã€åŽŸæ–‡å¼•ç”¨ã€‘...",
          "reason": ""
        }}
      ]
    }}
  ],
  "page_summary": "",
  "flags": []
}}
```
"""
        response_text = await self._call_text_api(prompt, stream_callback)
        fallback = {
            "score": 0.0,
            "max_score": 0.0,
            "confidence": 0.0,
            "question_numbers": [],
            "question_details": [],
            "page_summary": "",
            "flags": ["parse_error"],
        }
        return self._safe_json_loads(response_text, fallback)

    async def assist_from_evidence(
        self,
        evidence: Dict[str, Any],
        parsed_rubric: Optional[Dict[str, Any]] = None,
        page_context: Optional[Dict[str, Any]] = None,
        mode: Literal["teacher", "student"] = "teacher",
        stream_callback: Optional[Callable[[str, str], Awaitable[None]]] = None,
    ) -> Dict[str, Any]:
        """
        Provide assistive feedback without grading or scoring.
        """
        answer_ids: List[str] = []
        for item in evidence.get("answers", []):
            qid = self._normalize_question_id(item.get("question_id"))
            if qid:
                answer_ids.append(qid)

        question_numbers = (
            evidence.get("question_numbers") or (page_context or {}).get("question_numbers") or []
        )
        for qid in question_numbers:
            normalized = self._normalize_question_id(qid)
            if normalized and normalized not in answer_ids:
                answer_ids.append(normalized)

        if not answer_ids and parsed_rubric and parsed_rubric.get("questions"):
            for q in self._limit_questions_for_prompt(parsed_rubric.get("questions", [])):
                qid = self._normalize_question_id(q.get("question_id") or q.get("id"))
                if qid and qid not in answer_ids:
                    answer_ids.append(qid)

        rubric_payload = self._build_rubric_payload(parsed_rubric, answer_ids)
        mode_label = "TEACHER_ASSIST" if mode == "teacher" else "STUDENT_ASSIST"
        output_constraints = (
            "Output constraints: feedback<=160 chars (teacher) or <=600 chars (student); "
            "student_answer<=200 chars; error_hints<=3 items."
        )
        prompt = f"""ä½ æ˜¯æ‰¹æ”¹åŠ©ç†ï¼Œåªåšé—®é¢˜åˆ†æžä¸Žå»ºè®®ï¼Œä¸è¦æ‰“åˆ†ã€ä¸è¾“å‡ºåˆ†æ•°ã€‚
Mode: {mode_label}
{output_constraints}
åªåŸºäºŽâ€œç­”æ¡ˆè¯æ®â€å’Œå¯ç”¨è¯„åˆ†æ ‡å‡†ï¼ˆå¦‚æœ‰ï¼‰ç»™å‡ºæç¤ºï¼›è¯æ®ä¸è¶³æ—¶æ˜Žç¡®è¯´æ˜Žä¸ç¡®å®šã€‚
Teacher assist: focus on concise error hints and likely missing steps.
Student assist: explain mistakes and how to improve, step-by-step if needed.

è¯„åˆ†æ ‡å‡†(JSONï¼Œå¯ä¸ºç©º)ï¼š
{json.dumps(rubric_payload, ensure_ascii=False, indent=2)}

ç­”æ¡ˆè¯æ®(JSON)ï¼š
{json.dumps(evidence, ensure_ascii=False, indent=2)}

è¾“å‡º JSONï¼š
```json
{{
  "question_numbers": ["1"],
  "question_details": [
    {{
      "question_id": "1",
      "question_type": "objective",
      "student_answer": "",
      "feedback": "",
      "error_hints": ["..."],
      "confidence": 0.0
    }}
  ],
  "page_summary": "",
  "flags": []
}}
```
"""
        response_text = await self._call_text_api(prompt, stream_callback)
        fallback = {
            "question_numbers": [],
            "question_details": [],
            "page_summary": "",
            "flags": ["parse_error"],
        }
        return self._safe_json_loads(response_text, fallback)

    def _safe_float(self, value: Any, default: float = 0.0) -> float:
        """å®‰å…¨è½¬æ¢ä¸ºæµ®ç‚¹æ•°"""
        try:
            if value is None:
                return default
            return float(value)
        except (TypeError, ValueError):
            return default

    def _sum_question_detail_scores(self, details: List[Dict[str, Any]]) -> tuple[float, float]:
        total = 0.0
        max_total = 0.0
        for detail in details:
            total += self._safe_float(detail.get("score", 0))
            max_total += self._safe_float(detail.get("max_score", detail.get("maxScore", 0)))
        return total, max_total

    def _collect_question_detail_ids(self, details: List[Dict[str, Any]]) -> set[str]:
        ids: set[str] = set()
        for detail in details:
            if not isinstance(detail, dict):
                continue
            qid = self._normalize_question_id(
                detail.get("question_id") or detail.get("questionId") or detail.get("id")
            )
            if qid:
                ids.add(qid)
        return ids

    def _get_expected_question_ids(self, parsed_rubric: Dict[str, Any]) -> List[str]:
        questions = parsed_rubric.get("questions") or []
        expected = []
        for question in questions:
            qid = self._normalize_question_id(question.get("question_id") or question.get("id"))
            if qid:
                expected.append(qid)
        return expected

    def _merge_question_details(
        self,
        existing: List[Dict[str, Any]],
        incoming: List[Dict[str, Any]],
    ) -> List[Dict[str, Any]]:
        merged = list(existing)
        existing_ids = self._collect_question_detail_ids(existing)
        for detail in incoming:
            if not isinstance(detail, dict):
                continue
            qid = self._normalize_question_id(
                detail.get("question_id") or detail.get("questionId") or detail.get("id")
            )
            if qid and qid in existing_ids:
                continue
            merged.append(detail)
        return merged

    def _build_missing_question_placeholders(
        self,
        missing_ids: List[str],
        parsed_rubric: Dict[str, Any],
    ) -> List[Dict[str, Any]]:
        placeholders = []
        rubric_map = {}
        for question in parsed_rubric.get("questions") or []:
            qid = self._normalize_question_id(question.get("question_id") or question.get("id"))
            if qid:
                rubric_map[qid] = question
        for qid in missing_ids:
            rubric = rubric_map.get(qid, {})
            max_score = self._safe_float(rubric.get("max_score", 0))
            placeholders.append(
                {
                    "question_id": qid,
                    "score": 0.0,
                    "max_score": max_score,
                    "student_answer": "",
                    "is_correct": False,
                    "feedback": "No answer detected.",
                    "confidence": 0.0,
                    "self_critique": "Insufficient evidence to grade; manual review recommended.",
                    "self_critique_confidence": 0.0,
                    "scoring_point_results": [],
                    "page_indices": [],
                    "question_type": rubric.get("question_type") or rubric.get("questionType"),
                }
            )
        return placeholders

    async def _grade_missing_questions(
        self,
        images: List[bytes],
        student_key: str,
        parsed_rubric: Dict[str, Any],
        missing_ids: List[str],
        context_info: str,
        stream_callback: Optional[Callable[[str, str], Awaitable[None]]] = None,
    ) -> List[Dict[str, Any]]:
        if not missing_ids:
            return []
        missing_ids_text = ", ".join(missing_ids)
        rubric_info = self._build_student_grading_rubric_info(
            parsed_rubric,
            question_ids=missing_ids,
        )
        prompt = (
            "You are a grading assistant. Grade ONLY the following questions for "
            f"{student_key}: {missing_ids_text}.\n\n"
            f"Rubric:\n{rubric_info}\n\n"
            f"Context:\n{context_info}\n\n"
            "Return JSON only with this structure:\n"
            '{"question_details": [{"question_id": "1", "score": 0, "max_score": 0, '
            '"student_answer": "", "is_correct": false, "feedback": "", '
            '"confidence": 0.0, "self_critique": "", '
            '"self_critique_confidence": 0.0, "scoring_point_results": []}]}\n'
            "Rules:\n"
            "- Only include the specified questions.\n"
            "- If an answer is missing or unclear, score 0 and explain in self_critique.\n"
            "- Return valid JSON only.\n"
        )
        content = [{"type": "text", "text": prompt}]
        for img_bytes in images:
            if isinstance(img_bytes, bytes):
                img_b64 = base64.b64encode(img_bytes).decode("utf-8")
            else:
                img_b64 = img_bytes
            content.append(
                {
                    "type": "image_url",
                    "image_url": f"data:image/jpeg;base64,{img_b64}",
                }
            )
        message = HumanMessage(content=content)
        full_response = ""
        async for chunk in self.llm.astream([message]):
            content_chunk = chunk.content
            if not content_chunk:
                continue
            if isinstance(content_chunk, str):
                full_response += content_chunk
                if stream_callback:
                    await stream_callback("output", content_chunk)
            elif isinstance(content_chunk, list):
                for part in content_chunk:
                    text_part = ""
                    if isinstance(part, str):
                        text_part = part
                    elif isinstance(part, dict) and "text" in part:
                        text_part = part["text"]
                    if text_part:
                        full_response += text_part
                        if stream_callback:
                            await stream_callback("output", text_part)
        if not full_response:
            return []
        try:
            json_text = self._extract_json_from_text(full_response)
            payload = self._load_json_with_repair(json_text)
        except Exception:
            return []
        raw_details = (
            payload.get("question_details")
            or payload.get("questionDetails")
            or payload.get("questions")
            or []
        )
        if not isinstance(raw_details, list):
            return []
        normalized_missing = {self._normalize_question_id(qid) for qid in missing_ids if qid}
        normalized = []
        for detail in raw_details:
            if not isinstance(detail, dict):
                continue
            normalized_detail = self._normalize_question_detail(detail, None)
            qid = self._normalize_question_id(normalized_detail.get("question_id"))
            if qid and qid in normalized_missing:
                normalized.append(normalized_detail)
        return normalized

    async def _ensure_student_result_complete(
        self,
        result: Dict[str, Any],
        parsed_rubric: Dict[str, Any],
        student_key: str,
        images: List[bytes],
        context_info: str,
        stream_callback: Optional[Callable[[str, str], Awaitable[None]]] = None,
    ) -> Dict[str, Any]:
        max_passes = self._read_int_env("GRADING_COMPLETION_PASSES", 1)
        if max_passes <= 0:
            return result
        expected_ids = self._get_expected_question_ids(parsed_rubric)
        if not expected_ids:
            return result
        details = result.get("question_details") or []
        if not isinstance(details, list):
            details = []
        existing_ids = self._collect_question_detail_ids(details)
        missing_ids = [qid for qid in expected_ids if qid not in existing_ids]
        if not missing_ids:
            return result
        completion_details = await self._grade_missing_questions(
            images=images,
            student_key=student_key,
            parsed_rubric=parsed_rubric,
            missing_ids=missing_ids,
            context_info=context_info,
            stream_callback=stream_callback,
        )
        if completion_details:
            merged = self._merge_question_details(details, completion_details)
        else:
            merged = self._merge_question_details(
                details,
                self._build_missing_question_placeholders(missing_ids, parsed_rubric),
            )
        result["question_details"] = merged
        result["missing_question_ids"] = missing_ids
        total_score, max_score = self._sum_question_detail_scores(merged)
        result["total_score"] = total_score
        result["max_score"] = max_score
        return result

    def _build_student_grading_rubric_info(
        self,
        parsed_rubric: Dict[str, Any],
        question_ids: Optional[List[str]] = None,
    ) -> str:
        """æž„å»ºå­¦ç”Ÿæ‰¹æ”¹ç”¨çš„è¯„åˆ†æ ‡å‡†ä¿¡æ¯"""
        if not parsed_rubric:
            return "è¯·æ ¹æ®ç­”æ¡ˆçš„æ­£ç¡®æ€§ã€å®Œæ•´æ€§å’Œæ¸…æ™°åº¦è¿›è¡Œè¯„åˆ†"

        if parsed_rubric.get("rubric_context"):
            return parsed_rubric["rubric_context"]

        questions = parsed_rubric.get("questions", [])
        if question_ids:
            normalized_ids = {self._normalize_question_id(qid) for qid in question_ids if qid}
            questions = [
                q
                for q in questions
                if self._normalize_question_id(q.get("question_id") or q.get("id"))
                in normalized_ids
            ]
        if not questions:
            return "è¯·æ ¹æ®ç­”æ¡ˆçš„æ­£ç¡®æ€§ã€å®Œæ•´æ€§å’Œæ¸…æ™°åº¦è¿›è¡Œè¯„åˆ†"

        lines = [
            f"è¯„åˆ†æ ‡å‡†ï¼ˆå…±{parsed_rubric.get('total_questions', len(questions))}é¢˜ï¼Œ"
            f"æ€»åˆ†{parsed_rubric.get('total_score', 0)}åˆ†ï¼‰ï¼š",
            "",
        ]

        for q in self._limit_questions_for_prompt(questions):
            qid = q.get("question_id") or q.get("id") or "?"
            max_score = q.get("max_score", 0)
            lines.append(f"ç¬¬{qid}é¢˜ (æ»¡åˆ†{max_score}åˆ†):")

            # è¯„åˆ†è¦ç‚¹
            scoring_points = q.get("scoring_points", [])
            for idx, sp in enumerate(self._limit_criteria_for_prompt(scoring_points), 1):
                point_id = sp.get("point_id") or f"{qid}.{idx}"
                lines.append(
                    f"  - [{point_id}] [{sp.get('score', 0)}åˆ†] {sp.get('description', '')}"
                )

            # æ ‡å‡†ç­”æ¡ˆ
            if q.get("standard_answer"):
                answer = q["standard_answer"]
                preview = answer[:150] + "..." if len(answer) > 150 else answer
                lines.append(f"  æ ‡å‡†ç­”æ¡ˆ: {preview}")

            lines.append("")

        return "\n".join(lines)

    async def grade_batch_pages_stream(
        self,
        images: List[bytes],
        page_indices: List[int],
        parsed_rubric: Dict[str, Any],
        page_contexts: Optional[Dict[int, Dict[str, Any]]] = None,
    ) -> AsyncIterator[Dict[str, str]]:
        """
        æ‰¹é‡æ‰¹æ”¹å¤šé¡µï¼ˆä¸€æ¬¡ LLM è°ƒç”¨ï¼‰

        å°†å¤šå¼ å›¾ç‰‡ä¸€èµ·å‘é€ç»™ LLMï¼Œè¦æ±‚æŒ‰é¡µé¡ºåºæ‰¹æ”¹ã€‚
        è¿™æ¯”æ¯é¡µå•ç‹¬è°ƒç”¨æ›´é«˜æ•ˆï¼Œè¾“å‡ºä¹Ÿæ›´æœ‰åºã€‚

        Args:
            images: å›¾åƒå­—èŠ‚åˆ—è¡¨
            page_indices: å¯¹åº”çš„é¡µé¢ç´¢å¼•åˆ—è¡¨
            parsed_rubric: è§£æžåŽçš„è¯„åˆ†æ ‡å‡†
            page_contexts: é¡µé¢ä¸Šä¸‹æ–‡å­—å…¸ {page_index: context}

        Yields:
            str: LLM å“åº”çš„æ–‡æœ¬æ•°æ®å—
        """
        if not images:
            return

        # æž„å»ºè¯„åˆ†æ ‡å‡†ä¿¡æ¯ï¼ˆä¸è®© LLM å†³å®š max_scoreï¼‰
        rubric_info = self._build_batch_rubric_info(parsed_rubric)

        # æž„å»ºé¡µé¢ä¸Šä¸‹æ–‡ä¿¡æ¯
        page_context_info = ""
        if page_contexts:
            context_lines = []
            for idx in page_indices:
                ctx = page_contexts.get(idx, {})
                if ctx:
                    q_nums = ctx.get("question_numbers", [])
                    if q_nums:
                        context_lines.append(
                            f"- å›¾ç‰‡ {page_indices.index(idx) + 1} (é¡µ {idx + 1}): é¢„æœŸé¢˜å· {q_nums}"
                        )
            if context_lines:
                page_context_info = f"\n\n## é¡µé¢ç´¢å¼•ä¿¡æ¯ï¼ˆè¯·ä»¥æ­¤ä¸ºå‡†ï¼‰\n" + "\n".join(
                    context_lines
                )

        # æž„å»ºæ‰¹é‡æ‰¹æ”¹ prompt
        prompt = f"""ä½ æ˜¯ä¸€ä½**ä¸¥æ ¼ä½†å…¬å¹³**çš„é˜…å·æ•™å¸ˆã€‚è¯·æ‰¹æ”¹ä»¥ä¸‹ {len(images)} å¼ å­¦ç”Ÿç­”é¢˜å›¾ç‰‡ã€‚

## è¯„åˆ†æ ‡å‡†ï¼ˆå¸¦ç¼–å·ï¼‰
{rubric_info}
{page_context_info}

## æ‰¹æ”¹ä»»åŠ¡
è¯·æŒ‰é¡ºåºæ‰¹æ”¹æ¯å¼ å›¾ç‰‡ï¼Œå¯¹æ¯å¼ å›¾ç‰‡ï¼š
1. è¯†åˆ«é¡µé¢ä¸­çš„é¢˜ç›®ç¼–å·
2. **é€æ¡è¯„åˆ†**ï¼šæ¯é“é¢˜å¿…é¡»é€ä¸€è¦†ç›–è¯¥é¢˜æ‰€æœ‰è¯„åˆ†ç‚¹ï¼ˆpoint_idï¼‰ï¼Œæ¯ä¸ªè¯„åˆ†ç‚¹è¾“å‡ºä¸€ä¸ª scoring_results æ¡ç›®
   - è‹¥æŸè¯„åˆ†ç‚¹æœªåœ¨ä½œç­”ä¸­å‡ºçŽ°ï¼Œä»éœ€è¾“å‡ºè¯¥æ¡ç›®ï¼Œawarded=0ï¼Œevidence å¡«å†™ "ã€åŽŸæ–‡å¼•ç”¨ã€‘æœªæ‰¾åˆ°"
 3. æ¯ä¸ªè¯„åˆ†ç‚¹å¿…é¡»è¾“å‡ºå¯¹åº”çš„ rubric_referenceï¼ˆåŒ…å« point_idã€æè¿°ï¼Œä»¥åŠè‹¥å·²æä¾›åˆ™åŒ…å«æ ‡å‡†å€¼ï¼‰ä»¥åŠ**å…·ä½“è¯„åˆ†ä¾æ®**ï¼ˆå¼•ç”¨å›¾ç‰‡ä¸­çš„åŽŸæ–‡è¯æ®ï¼‰
 4. **ä¸¥æ ¼ä¾æ®è¯„åˆ†æ ‡å‡†**ï¼šè¯„åˆ†æ ‡å‡†æ˜¯æ ¸å¿ƒä¾æ®ï¼Œä½†å…è®¸ç­‰ä»·è¡¨è¾¾ä¸Žåˆç†å˜å½¢ï¼›ä¸è¦è¦æ±‚ä¸€å­—ä¸å·®ï¼Œåªè¦è¯æ®å……åˆ†ä¸”é€»è¾‘ç­‰ä»·å³å¯ç»™åˆ†
 5. **åˆ¤å®šä¸Žå¾—åˆ†ä¸€è‡´**ï¼šè‹¥åˆ¤å®šâ€œå¾—åˆ†/æ­£ç¡®â€ï¼Œawarded å¿…é¡» > 0ï¼›è‹¥åˆ¤å®šâ€œä¸å¾—åˆ†/é”™è¯¯â€ï¼Œawarded å¿…é¡» = 0
 6. **åå¹»è§‰çº¦æŸ**ï¼šåªèƒ½åŸºäºŽå›¾ç‰‡ä¸­æ˜Žç¡®å¯è§çš„å†…å®¹å¼•ç”¨è¯æ®ï¼›ä¸å¾—è‡†æµ‹æˆ–æ›¿å­¦ç”Ÿâ€œè¡¥å†™â€ã€‚è‹¥è¯æ®ä¸è¶³ï¼Œawarded=0ï¼Œevidence=ã€åŽŸæ–‡å¼•ç”¨ã€‘æœªæ‰¾åˆ°ï¼Œå¹¶åœ¨ self_critique æ ‡æ³¨ä¸ç¡®å®š
 7. **è¯æ®å†²çªç›´æŽ¥æ‰£åˆ†**ï¼šè‹¥è¯æ®ä¸Žè¯„åˆ†ç‚¹è¦æ±‚å†²çªï¼ˆå¦‚å†™ AAA å´è¢«åˆ¤ AAS/ASAï¼‰ï¼Œç›´æŽ¥åˆ¤ 0 åˆ†å¹¶è¯´æ˜Žå†²çªç‚¹
 8. **è¯æ˜Ž/ç†ç”±ç±»ä¸å¾—â€œç¢°è¿æ°”â€**ï¼šéœ€è¦ç†ç”±/è¯æ˜Žçš„è¯„åˆ†ç‚¹ï¼Œè‹¥åªç»™ç»“è®ºæˆ–ä»…å†™â€œdisagree/unchangedâ€ç­‰æ— è®ºè¯è¡¨è¿°ï¼Œå¿…é¡»åˆ¤ 0 åˆ†
 9. **å¤šæ¡ä»¶è¯„åˆ†ç‚¹**ï¼šè‹¥è¯„åˆ†ç‚¹æè¿°åŒ…å«â€œåŒæ—¶/å¹¶ä¸”/ä»¥åŠ/both/and/å«â€¦ä¸Žâ€¦â€ï¼Œå¿…é¡»é€é¡¹æ»¡è¶³ï¼›ç¼ºä»»ä¸€é¡¹ç›´æŽ¥åˆ¤ 0 åˆ†
 10. ä¸å¾—æ ¹æ®æœ€ç»ˆç­”æ¡ˆâ€œå€’æŽ¨â€è¿‡ç¨‹æ­£ç¡®ï¼›è¿‡ç¨‹/ç†ç”±é”™è¯¯å³æ‰£åˆ†ï¼Œé™¤éžè¯„åˆ†æ ‡å‡†æ˜Žç¡®å…è®¸â€œä»…ç­”æ¡ˆæ­£ç¡®â€
 11. **å¦ç±»æ–¹æ³•**ï¼šå¦‚å­¦ç”Ÿä½¿ç”¨ä¸åŒä½†æœ‰æ•ˆçš„æŽ¨ç†æ–¹æ³•ï¼Œå¯ç»™åˆ†ï¼›ä½†éœ€åœ¨ self_critique ä¸­è¯´æ˜Žå…¶ä¸Žæ ‡å‡†çš„å·®å¼‚ï¼Œå¹¶é™ä½Ž confidence
 12. æ¯é“é¢˜å¿…é¡»è¾“å‡º self_critiqueï¼Œå¹¶æä¾›è‡ªè¯„ç½®ä¿¡åº¦ self_critique_confidenceï¼ˆ0-1ï¼‰
 13. è‡ªç™½éœ€å¯¹è‡ªå·±çš„æ‰¹æ”¹è¿›è¡Œè¯„åˆ¤ï¼šä¸»åŠ¨æŒ‡å‡ºä¸ç¡®å®šä¸Žé—æ¼
    - **å¥–åŠ±è¯šå®ž**ï¼šå¦è¯šæŒ‡å‡ºè¯æ®ä¸è¶³/ä¸ç¡®å®šä¹‹å¤„ï¼Œå¯ç»´æŒæˆ–ç•¥å‡ self_critique_confidence
    - **æƒ©ç½šä¸è¯šå®ž**ï¼šè‹¥ç»“è®ºç¼ºä¹è¯æ®æˆ–å­˜åœ¨å¤¸å¤§ï¼Œè‡ªç™½å¿…é¡»é™ä½Ž self_critique_confidence å¹¶è¯´æ˜Ž
 14. è‹¥æ— æ³•åŒ¹é…è¯„åˆ†æ ‡å‡†çš„ point_idï¼Œä»å¯ç»™åˆ†ï¼Œä½†éœ€åœ¨ self_critique ä¸­è¯´æ˜Žï¼Œå¹¶æ˜¾è‘—é™ä½Ž confidence
 15. **ä¸€è‡´æ€§è‡ªæ£€**ï¼šè¾“å‡ºå‰é€æ¡æ ¸å¯¹ awarded / decision / reason / evidence / summary æ˜¯å¦ä¸€è‡´ï¼›è‹¥å†²çªï¼Œä»¥è¯æ®ä¸Žè¯„åˆ†æ ‡å‡†ä¸ºå‡†è¿›è¡Œä¿®æ­£

## è¾“å‡ºæ ¼å¼
è¯·ä¸ºæ¯å¼ å›¾ç‰‡è¾“å‡ºä¸€ä¸ª JSON å¯¹è±¡ï¼Œå›¾ç‰‡ä¹‹é—´ç”¨ `---PAGE_BREAK---` åˆ†éš”ã€‚

```json
{{
    "page_index": 0,
    "score": æœ¬é¡µæ€»å¾—åˆ†,
    "confidence": è¯„åˆ†ç½®ä¿¡åº¦ï¼ˆ0.0-1.0ï¼‰,
    "is_blank_page": false,
    "question_numbers": ["1", "2"],
    "question_details": [
        {{
            "question_id": "1",
            "score": 8,
            "confidence": 0.82,
            "student_answer": "å­¦ç”Ÿçš„è§£ç­”å†…å®¹æ‘˜è¦...",
            "feedback": "æ•´ä½“è¯„ä»·ï¼šxxxã€‚æ‰£åˆ†åŽŸå› ï¼šxxxã€‚",
            "self_critique": "è¯æ®åªè¦†ç›–å…³é”®ä¸€æ­¥ï¼Œä»å¯èƒ½é—æ¼ä¸­é—´æŽ¨å¯¼ï¼Œå»ºè®®å¤æ ¸ã€‚",
            "self_critique_confidence": 0.62,
            "rubric_refs": ["1.1", "1.2"],
            "scoring_results": [
                {{
                    "point_id": "1.1",
                    "rubric_reference": "[1.1] æ­£ç¡®åˆ—å‡ºæ–¹ç¨‹",
                    "decision": "å¾—åˆ†",
                    "awarded": 2,
                    "max_points": 2,
                    "evidence": "ã€åŽŸæ–‡å¼•ç”¨ã€‘å­¦ç”Ÿå†™äº† 'x + 2 = 5'ï¼Œæ­£ç¡®åˆ—å‡ºæ–¹ç¨‹",
                    "reason": "æ–¹ç¨‹åˆ—å¼æ­£ç¡®"
                }},
                {{
                    "point_id": "1.2",
                    "rubric_reference": "[1.2] æ­£ç¡®æ±‚è§£",
                    "decision": "ä¸å¾—åˆ†",
                    "awarded": 0,
                    "max_points": 3,
                    "evidence": "ã€åŽŸæ–‡å¼•ç”¨ã€‘å­¦ç”Ÿå†™ 'x = 2'ï¼ˆé”™è¯¯ï¼Œæ­£ç¡®ç­”æ¡ˆæ˜¯ x = 3ï¼‰",
                    "reason": "æœ€ç»ˆè§£é”™è¯¯"
                }}
            ]
        }}
    ],
    "page_summary": "æœ¬é¡µåŒ…å«ç¬¬1-2é¢˜çš„è§£ç­”"
}}
---PAGE_BREAK---
{{
    "page_index": 1,
    ...
}}
```

## è¯„åˆ†åŽŸåˆ™
1. **å¼•ç”¨ç¼–å·**ï¼š`point_id` å¿…é¡»ä¸Žè¯„åˆ†æ ‡å‡†ä¸­çš„ç¼–å·ä¸€è‡´ï¼ˆå¦‚ "1.1", "2.3"ï¼‰
2. **æä¾›è¯æ®**ï¼š`evidence` å¿…é¡»å¼•ç”¨å­¦ç”Ÿç­”å·ä¸­çš„åŽŸæ–‡ï¼Œç”¨ã€åŽŸæ–‡å¼•ç”¨ã€‘å¼€å¤´
3. **å¼•ç”¨æ ‡å‡†å€¼**ï¼šè‹¥è¯„åˆ†æ ‡å‡†æä¾› expected_valueï¼Œrubric_reference ä¸­å¿…é¡»åŒ…å«è¯¥æ ‡å‡†å€¼
4. **æŒ‰é¡ºåºæ‰¹æ”¹**ï¼šå›¾ç‰‡é¡ºåºå¯¹åº”é¡µé¢é¡ºåºï¼ˆå›¾ç‰‡1=é¡µé¢1ï¼Œå›¾ç‰‡2=é¡µé¢2...ï¼‰
5. **ç©ºç™½é¡µå¤„ç†**ï¼šå¦‚æžœæ˜¯ç©ºç™½é¡µï¼Œè®¾ç½® is_blank_page=true, score=0
6. **è¯¦ç»†åé¦ˆ**ï¼šfeedback ä¸­è¯´æ˜Žæ•´ä½“è¯„ä»·å’Œå…·ä½“æ‰£åˆ†åŽŸå› 
7. **è‡ªç™½ä¸Žç½®ä¿¡åº¦**ï¼šæ¯é“é¢˜å¿…é¡»æœ‰ self_critique ä¸Ž self_critique_confidence
8. **åˆ¤å®šä¸€è‡´æ€§**ï¼šdecision ä¸Ž awarded å¿…é¡»ä¸€è‡´ï¼Œé¿å…â€œåˆ¤å®šå¾—åˆ†ä½† awarded=0â€çš„è¾“å‡º
9. **æ€»ç»“ä¸€è‡´æ€§**ï¼šsummary/feedback å¿…é¡»ä¸Žè¯„åˆ†ç‚¹ä¸€è‡´ï¼›è‹¥ä»»ä¸€è¯„åˆ†ç‚¹ä¸º 0ï¼Œä¸å¾—å†™â€œå®Œå…¨æ­£ç¡®â€
10. **é€æ¡è¾“å‡º**ï¼šä¸è¦åªè¾“å‡º summaryï¼›è¯„åˆ†ç‚¹æ¡ç›®å¿…é¡»å®Œæ•´è¦†ç›–è¯¥é¢˜å…¨éƒ¨ rubric è¯„åˆ†ç‚¹

çŽ°åœ¨è¯·å¼€å§‹æ‰¹æ”¹ã€‚"""

        # æž„å»ºåŒ…å«æ‰€æœ‰å›¾ç‰‡çš„æ¶ˆæ¯å†…å®¹
        content = [{"type": "text", "text": prompt}]
        for i, image in enumerate(images):
            img_b64 = base64.b64encode(image).decode("utf-8")
            content.append({"type": "image_url", "image_url": f"data:image/png;base64,{img_b64}"})

        message = HumanMessage(content=content)

        # æµå¼è°ƒç”¨
        async for chunk in self.llm.astream([message]):
            output_text, thinking_text = split_thinking_content(chunk.content)
            if thinking_text:
                yield {"type": "thinking", "content": thinking_text}
            if output_text:
                yield {"type": "output", "content": output_text}

    def _build_batch_rubric_info(self, parsed_rubric: Dict[str, Any]) -> str:
        """æž„å»ºæ‰¹é‡æ‰¹æ”¹çš„è¯„åˆ†æ ‡å‡†ä¿¡æ¯"""
        if not parsed_rubric:
            return "è¯·æ ¹æ®ç­”æ¡ˆçš„æ­£ç¡®æ€§ã€å®Œæ•´æ€§å’Œæ¸…æ™°åº¦è¿›è¡Œè¯„åˆ†"

        if parsed_rubric.get("rubric_context"):
            rubric_context = parsed_rubric["rubric_context"]
            questions = parsed_rubric.get("questions", [])
            point_lines = []
            for q in self._limit_questions_for_prompt(questions):
                q_id = q.get("question_id", "?")
                scoring_points = q.get("scoring_points", [])
                if not scoring_points:
                    continue
                entries = []
                for sp in self._limit_criteria_for_prompt(scoring_points):
                    point_id = sp.get("point_id", "")
                    description = sp.get("description", "")
                    score = sp.get("score", 0)
                    expected_value = sp.get("expected_value") or sp.get("expectedValue") or ""
                    expected_value = str(expected_value).strip()
                    expected_value_snippet = ""
                    if expected_value:
                        snippet = (
                            expected_value
                            if len(expected_value) <= 80
                            else f"{expected_value[:80]}..."
                        )
                        expected_value_snippet = f"ï¼›æ ‡å‡†å€¼:{snippet}"
                    if point_id:
                        entries.append(
                            f"[{point_id}] {description}ï¼ˆ{score}åˆ†{expected_value_snippet}ï¼‰"
                        )
                if entries:
                    point_lines.append(f"ç¬¬ {q_id} é¢˜: " + "ï¼›".join(entries))
            if point_lines:
                return rubric_context + "\n\n## å¾—åˆ†ç‚¹ç¼–å·ç´¢å¼•\n" + "\n".join(point_lines)
            return rubric_context

        # ä»Žé¢˜ç›®åˆ—è¡¨æž„å»º
        questions = parsed_rubric.get("questions", [])
        if not questions:
            return "è¯·æ ¹æ®ç­”æ¡ˆçš„æ­£ç¡®æ€§ã€å®Œæ•´æ€§å’Œæ¸…æ™°åº¦è¿›è¡Œè¯„åˆ†"

        total_score = parsed_rubric.get("total_score", 0)
        lines = [f"å…± {len(questions)} é¢˜ï¼Œæ€»åˆ† {total_score} åˆ†ã€‚\n"]

        for q in self._limit_questions_for_prompt(questions):
            q_id = q.get("question_id", "?")
            max_score = q.get("max_score", 0)
            lines.append(f"ç¬¬ {q_id} é¢˜ï¼ˆæ»¡åˆ† {max_score} åˆ†ï¼‰")

            # æ·»åŠ å¾—åˆ†ç‚¹ï¼ˆåŒ…å« point_idï¼‰
            scoring_points = q.get("scoring_points", [])
            for sp in self._limit_criteria_for_prompt(scoring_points):
                point_id = sp.get("point_id", "")
                point_label = f"[{point_id}]" if point_id else ""
                expected_value = sp.get("expected_value") or sp.get("expectedValue") or ""
                expected_value = str(expected_value).strip()
                expected_value_snippet = ""
                if expected_value:
                    snippet = (
                        expected_value if len(expected_value) <= 80 else f"{expected_value[:80]}..."
                    )
                    expected_value_snippet = f"ï¼›æ ‡å‡†å€¼:{snippet}"
                lines.append(
                    f"  - {point_label} {sp.get('description', '')}ï¼ˆ{sp.get('score', 0)}åˆ†{expected_value_snippet}ï¼‰"
                )

        return "\n".join(lines)

    async def get_rubric_for_question(
        self,
        question_id: str,
    ) -> Optional[QuestionRubric]:
        """
        åŠ¨æ€èŽ·å–æŒ‡å®šé¢˜ç›®çš„è¯„åˆ†æ ‡å‡† (Requirement 1.1)

        ä»Ž RubricRegistry èŽ·å–æŒ‡å®šé¢˜ç›®çš„è¯„åˆ†æ ‡å‡†ï¼ŒåŒ…æ‹¬å¾—åˆ†ç‚¹ã€æ ‡å‡†ç­”æ¡ˆã€å¦ç±»è§£æ³•ã€‚

        Args:
            question_id: é¢˜ç›®ç¼–å·ï¼ˆå¦‚ "1", "7a", "15"ï¼‰

        Returns:
            QuestionRubric: è¯¥é¢˜ç›®çš„å®Œæ•´è¯„åˆ†æ ‡å‡†ï¼Œå¦‚æžœæœªæ‰¾åˆ°è¿”å›ž None
        """
        if self._rubric_registry is None:
            logger.warning("æœªè®¾ç½® RubricRegistryï¼Œæ— æ³•èŽ·å–è¯„åˆ†æ ‡å‡†")
            return None

        result = self._rubric_registry.get_rubric_for_question(question_id)

        if result.is_default:
            logger.warning(f"é¢˜ç›® {question_id} ä½¿ç”¨é»˜è®¤è¯„åˆ†æ ‡å‡†ï¼Œç½®ä¿¡åº¦: {result.confidence}")

        return result.rubric

    def _build_scoring_point_prompt(
        self,
        rubric: QuestionRubric,
        student_answer: str,
        reviewer_notes: Optional[str] = None,
    ) -> str:
        """
        æž„å»ºå¾—åˆ†ç‚¹é€ä¸€æ ¸å¯¹çš„æç¤ºè¯ (Requirement 1.2)

        Args:
            rubric: è¯„åˆ†æ ‡å‡†
            student_answer: å­¦ç”Ÿç­”æ¡ˆæè¿°

        Returns:
            str: å¾—åˆ†ç‚¹æ ¸å¯¹æç¤ºè¯
        """
        # æž„å»ºå¾—åˆ†ç‚¹åˆ—è¡¨
        scoring_points_text = ""
        for i, sp in enumerate(rubric.scoring_points, 1):
            required_mark = "ã€å¿…é¡»ã€‘" if sp.is_required else "ã€å¯é€‰ã€‘"
            point_id = sp.point_id or f"{rubric.question_id}.{i}"
            scoring_points_text += (
                f"{i}. [{point_id}] {required_mark} {sp.description} ({sp.score}åˆ†)\n"
            )

        # æž„å»ºå¦ç±»è§£æ³•åˆ—è¡¨ (Requirement 1.3)
        alternative_text = ""
        if rubric.alternative_solutions:
            alternative_text = "\n## å¦ç±»è§£æ³•ï¼ˆåŒæ ·æœ‰æ•ˆï¼‰\n"
            for i, alt in enumerate(rubric.alternative_solutions, 1):
                alternative_text += f"{i}. {alt.description}\n"
                alternative_text += f"   è¯„åˆ†æ¡ä»¶: {alt.scoring_conditions}\n"
                alternative_text += f"   æœ€é«˜åˆ†: {alt.max_score}åˆ†\n"

        notes_block = reviewer_notes.strip() if reviewer_notes else ""

        return f"""è¯·å¯¹ä»¥ä¸‹å­¦ç”Ÿç­”æ¡ˆè¿›è¡Œå¾—åˆ†ç‚¹é€ä¸€æ ¸å¯¹è¯„åˆ†ã€‚

## é¢˜ç›®ä¿¡æ¯
- é¢˜å·: {rubric.question_id}
- æ»¡åˆ†: {rubric.max_score}åˆ†
- é¢˜ç›®: {rubric.question_text}

## æ ‡å‡†ç­”æ¡ˆ
{rubric.standard_answer}

## å¾—åˆ†ç‚¹åˆ—è¡¨
{scoring_points_text}
{alternative_text}
## æ‰¹æ”¹æ³¨æ„äº‹é¡¹
{rubric.grading_notes if rubric.grading_notes else "æ— ç‰¹æ®Šæ³¨æ„äº‹é¡¹"}

## æ•™å¸ˆå¤‡æ³¨
{notes_block or "æ— "}

## å­¦ç”Ÿç­”æ¡ˆ
{student_answer}

## è¯„åˆ†ä»»åŠ¡
è¯·é€ä¸€æ ¸å¯¹æ¯ä¸ªå¾—åˆ†ç‚¹ï¼Œåˆ¤æ–­å­¦ç”Ÿæ˜¯å¦èŽ·å¾—è¯¥å¾—åˆ†ç‚¹çš„åˆ†æ•°ã€‚

æ³¨æ„ï¼š
1. å¦‚æžœå­¦ç”Ÿä½¿ç”¨äº†å¦ç±»è§£æ³•ï¼Œåªè¦ç¬¦åˆè¯„åˆ†æ¡ä»¶ï¼ŒåŒæ ·ç»™åˆ†
2. éƒ¨åˆ†æ­£ç¡®çš„å¾—åˆ†ç‚¹å¯ä»¥ç»™éƒ¨åˆ†åˆ†æ•°
3. å¿…é¡»ä¸ºæ¯ä¸ªå¾—åˆ†ç‚¹æä¾›è¯æ®è¯´æ˜Ž

## è¾“å‡ºæ ¼å¼ï¼ˆJSONï¼‰
```json
{{
    "question_id": "{rubric.question_id}",
    "total_score": å­¦ç”Ÿæ€»å¾—åˆ†,
    "max_score": {rubric.max_score},
    "confidence": è¯„åˆ†ç½®ä¿¡åº¦ï¼ˆ0.0-1.0ï¼‰,
    "used_alternative_solution": false,
    "alternative_solution_index": null,
    "scoring_point_results": [
        {{
            "point_index": 1,
            "description": "å¾—åˆ†ç‚¹æè¿°",
            "max_score": è¯¥å¾—åˆ†ç‚¹æ»¡åˆ†,
            "awarded": èŽ·å¾—çš„åˆ†æ•°,
            "evidence": "åœ¨å­¦ç”Ÿç­”æ¡ˆä¸­æ‰¾åˆ°çš„è¯æ®æˆ–æœªæ‰¾åˆ°çš„è¯´æ˜Ž"
        }}
    ],
    "feedback": "ç»¼åˆè¯„ä»·å’Œæ”¹è¿›å»ºè®®"
}}
```"""

    async def grade_question_with_scoring_points(
        self,
        question_id: str,
        student_answer: str,
        image: Optional[bytes] = None,
    ) -> QuestionResult:
        """
        ä½¿ç”¨å¾—åˆ†ç‚¹é€ä¸€æ ¸å¯¹æ–¹å¼è¯„åˆ†å•é“é¢˜ç›® (Requirement 1.2)

        åŠ¨æ€èŽ·å–è¯„åˆ†æ ‡å‡†ï¼Œé€ä¸€æ ¸å¯¹æ¯ä¸ªå¾—åˆ†ç‚¹ï¼Œæ”¯æŒå¦ç±»è§£æ³•ã€‚

        Args:
            question_id: é¢˜ç›®ç¼–å·
            student_answer: å­¦ç”Ÿç­”æ¡ˆæè¿°ï¼ˆä»Žè§†è§‰åˆ†æžèŽ·å¾—ï¼‰
            image: å¯é€‰çš„é¢˜ç›®å›¾åƒï¼ˆç”¨äºŽè§†è§‰éªŒè¯ï¼‰

        Returns:
            QuestionResult: åŒ…å«å¾—åˆ†ç‚¹æ˜Žç»†çš„è¯„åˆ†ç»“æžœ
        """
        # 1. åŠ¨æ€èŽ·å–è¯„åˆ†æ ‡å‡† (Requirement 1.1)
        rubric = await self.get_rubric_for_question(question_id)

        if rubric is None:
            logger.error(f"æ— æ³•èŽ·å–é¢˜ç›® {question_id} çš„è¯„åˆ†æ ‡å‡†")
            return QuestionResult(
                question_id=question_id,
                score=0.0,
                max_score=0.0,
                confidence=0.0,
                feedback="æ— æ³•èŽ·å–è¯„åˆ†æ ‡å‡†",
                scoring_point_results=[],
                page_indices=[],
                is_cross_page=False,
                student_answer=student_answer,
            )

        # 2. æž„å»ºå¾—åˆ†ç‚¹æ ¸å¯¹æç¤ºè¯
        prompt = self._build_scoring_point_prompt(rubric, student_answer)

        # 3. è°ƒç”¨ LLM è¿›è¡Œè¯„åˆ†
        try:
            if image:
                # å¦‚æžœæœ‰å›¾åƒï¼Œä½¿ç”¨è§†è§‰ API
                img_b64 = (
                    base64.b64encode(image).decode("utf-8") if isinstance(image, bytes) else image
                )
                response_text = await self._call_vision_api(img_b64, prompt)
            else:
                # çº¯æ–‡æœ¬è¯„åˆ†
                response = await self.llm.ainvoke([HumanMessage(content=prompt)])
                response_text = self._extract_text_from_response(response.content)

            # 4. è§£æžå“åº”
            json_text = self._extract_json_from_text(response_text)
            result = json.loads(json_text)

            # 5. æž„å»º QuestionResult
            scoring_point_results = []
            for spr_data in result.get("scoring_point_results", []):
                point_index = spr_data.get("point_index", 1) - 1
                if 0 <= point_index < len(rubric.scoring_points):
                    sp = rubric.scoring_points[point_index]
                else:
                    # åˆ›å»ºä¸´æ—¶å¾—åˆ†ç‚¹
                    sp = ScoringPoint(
                        description=spr_data.get("description", ""),
                        score=spr_data.get("max_score", 0),
                        is_required=True,
                    )

                scoring_point_results.append(
                    ScoringPointResult(
                        scoring_point=sp,
                        awarded=spr_data.get("awarded", 0),
                        evidence=spr_data.get("evidence", ""),
                    )
                )

            question_result = QuestionResult(
                question_id=question_id,
                score=result.get("total_score", 0),
                max_score=result.get("max_score", rubric.max_score),
                confidence=result.get("confidence", 0.8),
                feedback=result.get("feedback", ""),
                scoring_point_results=scoring_point_results,
                page_indices=[],
                is_cross_page=False,
                student_answer=student_answer,
            )

            logger.info(
                f"é¢˜ç›® {question_id} è¯„åˆ†å®Œæˆ: "
                f"{question_result.score}/{question_result.max_score}, "
                f"ç½®ä¿¡åº¦: {question_result.confidence}"
            )

            return question_result

        except json.JSONDecodeError as e:
            logger.error(f"å¾—åˆ†ç‚¹è¯„åˆ† JSON è§£æžå¤±è´¥: {e}")
            return QuestionResult(
                question_id=question_id,
                score=0.0,
                max_score=rubric.max_score,
                confidence=0.0,
                feedback=f"è¯„åˆ†è§£æžå¤±è´¥: {str(e)}",
                scoring_point_results=[],
                page_indices=[],
                is_cross_page=False,
                student_answer=student_answer,
            )
        except Exception as e:
            logger.error(f"å¾—åˆ†ç‚¹è¯„åˆ†å¤±è´¥: {e}", exc_info=True)
            return QuestionResult(
                question_id=question_id,
                score=0.0,
                max_score=rubric.max_score,
                confidence=0.0,
                feedback=f"è¯„åˆ†å¤±è´¥: {str(e)}",
                scoring_point_results=[],
                page_indices=[],
                is_cross_page=False,
                student_answer=student_answer,
            )

    async def grade_page_with_dynamic_rubric(
        self,
        image: bytes,
        page_index: int = 0,
        parsed_rubric: Optional[Dict[str, Any]] = None,
    ) -> PageGradingResult:
        """
        ä½¿ç”¨åŠ¨æ€è¯„åˆ†æ ‡å‡†æ‰¹æ”¹å•é¡µ (Requirements 1.1, 1.2, 1.3)

        é›†æˆ RubricRegistry å’Œ GradingSkillsï¼Œå®žçŽ°ï¼š
        1. è¯†åˆ«é¡µé¢ä¸­çš„é¢˜ç›®ç¼–å·
        2. ä¸ºæ¯é“é¢˜åŠ¨æ€èŽ·å–è¯„åˆ†æ ‡å‡†
        3. é€ä¸€æ ¸å¯¹å¾—åˆ†ç‚¹
        4. æ”¯æŒå¦ç±»è§£æ³•

        Args:
            image: å›¾åƒå­—èŠ‚
            page_index: é¡µç ç´¢å¼•
            parsed_rubric: è§£æžåŽçš„è¯„åˆ†æ ‡å‡†ï¼ˆå¯é€‰ï¼Œç”¨äºŽå…¼å®¹æ—§æŽ¥å£ï¼‰

        Returns:
            PageGradingResult: åŒ…å«è¯¦ç»†å¾—åˆ†ç‚¹æ˜Žç»†çš„é¡µé¢æ‰¹æ”¹ç»“æžœ
        """
        logger.info(f"å¼€å§‹æ‰¹æ”¹ç¬¬ {page_index + 1} é¡µï¼ˆä½¿ç”¨åŠ¨æ€è¯„åˆ†æ ‡å‡†ï¼‰")

        # 1. é¦–å…ˆè¿›è¡ŒåŸºç¡€æ‰¹æ”¹ï¼ŒèŽ·å–é¢˜ç›®ç¼–å·å’Œå­¦ç”Ÿç­”æ¡ˆ
        basic_result = await self.grade_page(
            image=image,
            rubric="",  # å…ˆä¸ä¼ è¯„åˆ†æ ‡å‡†ï¼Œåªåšè¯†åˆ«
            max_score=100.0,
            parsed_rubric=parsed_rubric,
        )

        # 2. å¦‚æžœæ˜¯ç©ºç™½é¡µï¼Œç›´æŽ¥è¿”å›ž
        if basic_result.get("is_blank_page", False):
            return PageGradingResult(
                page_index=page_index,
                question_results=[],
                student_info=None,
                is_blank_page=True,
                raw_response=json.dumps(basic_result, ensure_ascii=False),
            )

        # 3. æå–å­¦ç”Ÿä¿¡æ¯
        student_info = None
        if basic_result.get("student_info"):
            si = basic_result["student_info"]
            student_info = StudentInfo(
                student_id=si.get("student_id"),
                student_name=si.get("name"),
                confidence=0.8,
            )

        # 4. å¯¹æ¯é“é¢˜è¿›è¡Œå¾—åˆ†ç‚¹é€ä¸€æ ¸å¯¹è¯„åˆ†
        question_results = []
        for q_detail in basic_result.get("question_details", []):
            question_id = q_detail.get("question_id", "")
            student_answer = q_detail.get("student_answer", "")

            if not question_id:
                continue

            # ä½¿ç”¨å¾—åˆ†ç‚¹æ ¸å¯¹æ–¹å¼è¯„åˆ†
            if self._rubric_registry:
                q_result = await self.grade_question_with_scoring_points(
                    question_id=question_id,
                    student_answer=student_answer,
                    image=image,
                )
                q_result.page_indices = [page_index]
            else:
                # å¦‚æžœæ²¡æœ‰ RubricRegistryï¼Œä½¿ç”¨åŸºç¡€ç»“æžœ
                q_result = QuestionResult(
                    question_id=question_id,
                    score=q_detail.get("score", 0),
                    max_score=q_detail.get("max_score", 0),
                    confidence=basic_result.get("confidence", 0.8),
                    feedback=q_detail.get("feedback", ""),
                    scoring_point_results=[],
                    page_indices=[page_index],
                    is_cross_page=False,
                    student_answer=student_answer,
                )

            question_results.append(q_result)

        # 5. æž„å»ºé¡µé¢æ‰¹æ”¹ç»“æžœ
        page_result = PageGradingResult(
            page_index=page_index,
            question_results=question_results,
            student_info=student_info,
            is_blank_page=False,
            raw_response=json.dumps(basic_result, ensure_ascii=False),
        )

        total_score = sum(qr.score for qr in question_results)
        total_max = sum(qr.max_score for qr in question_results)

        logger.info(
            f"ç¬¬ {page_index + 1} é¡µæ‰¹æ”¹å®Œæˆ: "
            f"{total_score}/{total_max}, "
            f"å…± {len(question_results)} é“é¢˜"
        )

        return page_result

    def _format_rubric_for_prompt(
        self,
        rubric: QuestionRubric,
    ) -> str:
        """
        å°† QuestionRubric æ ¼å¼åŒ–ä¸ºæç¤ºè¯ä¸­ä½¿ç”¨çš„æ–‡æœ¬

        Args:
            rubric: è¯„åˆ†æ ‡å‡†å¯¹è±¡

        Returns:
            str: æ ¼å¼åŒ–çš„è¯„åˆ†æ ‡å‡†æ–‡æœ¬
        """
        lines = [
            f"ç¬¬{rubric.question_id}é¢˜ (æ»¡åˆ†{rubric.max_score}åˆ†):",
            (
                f"  é¢˜ç›®: {rubric.question_text[:200]}..."
                if len(rubric.question_text) > 200
                else f"  é¢˜ç›®: {rubric.question_text}"
            ),
        ]

        # æ·»åŠ å¾—åˆ†ç‚¹
        if rubric.scoring_points:
            lines.append("  å¾—åˆ†ç‚¹:")
            for sp in rubric.scoring_points:
                required = "ã€å¿…é¡»ã€‘" if sp.is_required else "ã€å¯é€‰ã€‘"
                lines.append(f"    - {required} {sp.description} ({sp.score}åˆ†)")

        # æ·»åŠ æ ‡å‡†ç­”æ¡ˆ
        if rubric.standard_answer:
            answer_preview = (
                rubric.standard_answer[:150] + "..."
                if len(rubric.standard_answer) > 150
                else rubric.standard_answer
            )
            lines.append(f"  æ ‡å‡†ç­”æ¡ˆ: {answer_preview}")

        # æ·»åŠ å¦ç±»è§£æ³• (Requirement 1.3)
        if rubric.alternative_solutions:
            lines.append("  å¦ç±»è§£æ³•:")
            for alt in rubric.alternative_solutions:
                lines.append(f"    - {alt.description} (æœ€é«˜{alt.max_score}åˆ†)")
                lines.append(f"      æ¡ä»¶: {alt.scoring_conditions}")

        return "\n".join(lines)

    async def build_dynamic_rubric_context(
        self,
        question_ids: List[str],
    ) -> str:
        """
        ä¸ºæŒ‡å®šé¢˜ç›®åˆ—è¡¨æž„å»ºåŠ¨æ€è¯„åˆ†æ ‡å‡†ä¸Šä¸‹æ–‡

        Args:
            question_ids: é¢˜ç›®ç¼–å·åˆ—è¡¨

        Returns:
            str: æ ¼å¼åŒ–çš„è¯„åˆ†æ ‡å‡†ä¸Šä¸‹æ–‡æ–‡æœ¬
        """
        if not self._rubric_registry:
            return ""

        rubric_texts = []
        for qid in question_ids:
            rubric = await self.get_rubric_for_question(qid)
            if rubric:
                rubric_texts.append(self._format_rubric_for_prompt(rubric))

        if not rubric_texts:
            return ""

        total_score = self._rubric_registry.total_score
        return f"è¯„åˆ†æ ‡å‡†ï¼ˆæ€»åˆ†{total_score}åˆ†ï¼‰ï¼š\n\n" + "\n\n".join(rubric_texts)

    # ==================== å¾—åˆ†ç‚¹æ˜Žç»†ç”Ÿæˆ (Requirement 1.2) ====================

    def _create_scoring_point_results_from_response(
        self,
        response_data: Dict[str, Any],
        rubric: QuestionRubric,
    ) -> List[ScoringPointResult]:
        """
        ä»Ž LLM å“åº”åˆ›å»ºå¾—åˆ†ç‚¹æ˜Žç»†åˆ—è¡¨ (Requirement 1.2)

        ä¸ºæ¯ä¸ªå¾—åˆ†ç‚¹è®°å½•å¾—åˆ†æƒ…å†µï¼Œç”Ÿæˆè¯¦ç»†çš„å¾—åˆ†ç‚¹æ˜Žç»†ã€‚

        Args:
            response_data: LLM å“åº”æ•°æ®
            rubric: è¯„åˆ†æ ‡å‡†

        Returns:
            List[ScoringPointResult]: å¾—åˆ†ç‚¹æ˜Žç»†åˆ—è¡¨
        """
        scoring_point_results = []
        response_points = response_data.get("scoring_point_results", [])

        # ç¡®ä¿æ¯ä¸ªè¯„åˆ†æ ‡å‡†ä¸­çš„å¾—åˆ†ç‚¹éƒ½æœ‰å¯¹åº”çš„ç»“æžœ
        for i, sp in enumerate(rubric.scoring_points):
            # æŸ¥æ‰¾å¯¹åº”çš„å“åº”æ•°æ®
            matched_response = None
            for rp in response_points:
                # é€šè¿‡ç´¢å¼•æˆ–æè¿°åŒ¹é…
                if rp.get("point_index") == i + 1:
                    matched_response = rp
                    break
                if rp.get("description", "").strip() == sp.description.strip():
                    matched_response = rp
                    break

            if matched_response:
                awarded = matched_response.get("awarded", 0)
                evidence = matched_response.get("evidence", "")
            else:
                # å¦‚æžœæ²¡æœ‰åŒ¹é…çš„å“åº”ï¼Œæ ‡è®°ä¸ºæœªè¯„ä¼°
                awarded = 0
                evidence = "æœªè¯„ä¼°"

            scoring_point_results.append(
                ScoringPointResult(
                    scoring_point=sp,
                    awarded=awarded,
                    evidence=evidence,
                )
            )

        return scoring_point_results

    def generate_scoring_point_summary(
        self,
        scoring_point_results: List[ScoringPointResult],
    ) -> str:
        """
        ç”Ÿæˆå¾—åˆ†ç‚¹æ˜Žç»†æ‘˜è¦ (Requirement 1.2)

        Args:
            scoring_point_results: å¾—åˆ†ç‚¹æ˜Žç»†åˆ—è¡¨

        Returns:
            str: å¾—åˆ†ç‚¹æ˜Žç»†æ‘˜è¦æ–‡æœ¬
        """
        if not scoring_point_results:
            return "æ— å¾—åˆ†ç‚¹æ˜Žç»†"

        lines = ["å¾—åˆ†ç‚¹æ˜Žç»†:"]
        total_awarded = 0
        total_max = 0

        for i, spr in enumerate(scoring_point_results, 1):
            sp = spr.scoring_point
            status = "âœ“" if spr.awarded >= sp.score else ("â–³" if spr.awarded > 0 else "âœ—")
            required_mark = "ã€å¿…é¡»ã€‘" if sp.is_required else "ã€å¯é€‰ã€‘"

            lines.append(
                f"  {i}. {status} {required_mark} {sp.description}: {spr.awarded}/{sp.score}åˆ†"
            )
            if spr.evidence:
                lines.append(f"      è¯æ®: {spr.evidence[:100]}...")

            total_awarded += spr.awarded
            total_max += sp.score

        lines.append(f"  æ€»è®¡: {total_awarded}/{total_max}åˆ†")

        return "\n".join(lines)

    async def grade_with_detailed_scoring_points(
        self,
        image: bytes,
        question_id: str,
        page_index: int = 0,
        reviewer_notes: Optional[str] = None,
    ) -> QuestionResult:
        """
        ä½¿ç”¨è¯¦ç»†å¾—åˆ†ç‚¹æ ¸å¯¹æ–¹å¼è¯„åˆ† (Requirement 1.2)

        è¿™æ˜¯ä¸€ä¸ªå®Œæ•´çš„è¯„åˆ†æµç¨‹ï¼š
        1. è§†è§‰åˆ†æžæå–å­¦ç”Ÿç­”æ¡ˆ
        2. åŠ¨æ€èŽ·å–è¯„åˆ†æ ‡å‡†
        3. é€ä¸€æ ¸å¯¹æ¯ä¸ªå¾—åˆ†ç‚¹
        4. ç”Ÿæˆè¯¦ç»†çš„å¾—åˆ†ç‚¹æ˜Žç»†

        Args:
            image: é¢˜ç›®å›¾åƒ
            question_id: é¢˜ç›®ç¼–å·
            page_index: é¡µç ç´¢å¼•

        Returns:
            QuestionResult: åŒ…å«è¯¦ç»†å¾—åˆ†ç‚¹æ˜Žç»†çš„è¯„åˆ†ç»“æžœ
        """
        # 1. èŽ·å–è¯„åˆ†æ ‡å‡†
        rubric = await self.get_rubric_for_question(question_id)
        if rubric is None:
            return QuestionResult(
                question_id=question_id,
                score=0.0,
                max_score=0.0,
                confidence=0.0,
                feedback="æ— æ³•èŽ·å–è¯„åˆ†æ ‡å‡†",
                scoring_point_results=[],
                page_indices=[page_index],
                is_cross_page=False,
                student_answer="",
            )

        # 2. è§†è§‰åˆ†æžæå–å­¦ç”Ÿç­”æ¡ˆ
        img_b64 = base64.b64encode(image).decode("utf-8") if isinstance(image, bytes) else image

        extraction_prompt = f"""è¯·åˆ†æžè¿™å¼ å­¦ç”Ÿç­”é¢˜å›¾åƒï¼Œæå–ç¬¬{question_id}é¢˜çš„å­¦ç”Ÿç­”æ¡ˆã€‚

ä»»åŠ¡ï¼š
1. æ‰¾åˆ°ç¬¬{question_id}é¢˜çš„å­¦ç”Ÿä½œç­”å†…å®¹
2. è¯¦ç»†æè¿°å­¦ç”Ÿå†™äº†ä»€ä¹ˆï¼ˆå…¬å¼ã€æ–‡å­—ã€å›¾è¡¨ã€è®¡ç®—è¿‡ç¨‹ç­‰ï¼‰
3. å®¢è§‚æè¿°ï¼Œä¸è¦è¯„åˆ†

è¾“å‡ºæ ¼å¼ï¼ˆJSONï¼‰ï¼š
```json
{{
    "question_id": "{question_id}",
    "student_answer": "å­¦ç”Ÿç­”æ¡ˆçš„è¯¦ç»†æè¿°",
    "has_content": true,
    "content_type": "è®¡ç®—/æ–‡å­—/å›¾è¡¨/æ··åˆ"
}}
```"""

        try:
            extraction_response = await self._call_vision_api(img_b64, extraction_prompt)
            extraction_json = self._extract_json_from_text(extraction_response)
            extraction_data = json.loads(extraction_json)
            student_answer = extraction_data.get("student_answer", "")
        except Exception as e:
            logger.warning(f"å­¦ç”Ÿç­”æ¡ˆæå–å¤±è´¥: {e}")
            student_answer = "æ— æ³•æå–å­¦ç”Ÿç­”æ¡ˆ"

        # 3. æž„å»ºå¾—åˆ†ç‚¹æ ¸å¯¹æç¤ºè¯
        prompt = self._build_scoring_point_prompt(
            rubric, student_answer, reviewer_notes=reviewer_notes
        )

        # 4. è°ƒç”¨ LLM è¿›è¡Œå¾—åˆ†ç‚¹æ ¸å¯¹
        try:
            response_text = await self._call_vision_api(img_b64, prompt)
            json_text = self._extract_json_from_text(response_text)
            result_data = json.loads(json_text)

            # 5. åˆ›å»ºå¾—åˆ†ç‚¹æ˜Žç»†
            scoring_point_results = self._create_scoring_point_results_from_response(
                result_data, rubric
            )

            # 6. ç”Ÿæˆåé¦ˆ
            feedback = result_data.get("feedback", "")
            scoring_summary = self.generate_scoring_point_summary(scoring_point_results)
            full_feedback = f"{feedback}\n\n{scoring_summary}"

            return QuestionResult(
                question_id=question_id,
                score=result_data.get("total_score", 0),
                max_score=result_data.get("max_score", rubric.max_score),
                confidence=result_data.get("confidence", 0.8),
                feedback=full_feedback,
                scoring_point_results=scoring_point_results,
                page_indices=[page_index],
                is_cross_page=False,
                student_answer=student_answer,
            )

        except Exception as e:
            logger.error(f"è¯¦ç»†å¾—åˆ†ç‚¹è¯„åˆ†å¤±è´¥: {e}", exc_info=True)
            return QuestionResult(
                question_id=question_id,
                score=0.0,
                max_score=rubric.max_score,
                confidence=0.0,
                feedback=f"è¯„åˆ†å¤±è´¥: {str(e)}",
                scoring_point_results=[],
                page_indices=[page_index],
                is_cross_page=False,
                student_answer=student_answer,
            )

    # ==================== å¦ç±»è§£æ³•æ”¯æŒ (Requirement 1.3) ====================

    def _build_alternative_solution_prompt(
        self,
        rubric: QuestionRubric,
        student_answer: str,
    ) -> str:
        """
        æž„å»ºå¦ç±»è§£æ³•æ£€æµ‹æç¤ºè¯ (Requirement 1.3)

        Args:
            rubric: è¯„åˆ†æ ‡å‡†
            student_answer: å­¦ç”Ÿç­”æ¡ˆæè¿°

        Returns:
            str: å¦ç±»è§£æ³•æ£€æµ‹æç¤ºè¯
        """
        if not rubric.alternative_solutions:
            return ""

        alt_solutions_text = ""
        for i, alt in enumerate(rubric.alternative_solutions, 1):
            alt_solutions_text += f"""
### å¦ç±»è§£æ³• {i}
- æè¿°: {alt.description}
- è¯„åˆ†æ¡ä»¶: {alt.scoring_conditions}
- æœ€é«˜åˆ†: {alt.max_score}åˆ†
"""

        return f"""è¯·åˆ¤æ–­å­¦ç”Ÿæ˜¯å¦ä½¿ç”¨äº†å¦ç±»è§£æ³•ã€‚

## é¢˜ç›®ä¿¡æ¯
- é¢˜å·: {rubric.question_id}
- æ»¡åˆ†: {rubric.max_score}åˆ†

## æ ‡å‡†ç­”æ¡ˆ
{rubric.standard_answer}

## å¯æŽ¥å—çš„å¦ç±»è§£æ³•
{alt_solutions_text}

## å­¦ç”Ÿç­”æ¡ˆ
{student_answer}

## ä»»åŠ¡
1. åˆ¤æ–­å­¦ç”Ÿæ˜¯å¦ä½¿ç”¨äº†æ ‡å‡†è§£æ³•
2. å¦‚æžœä¸æ˜¯æ ‡å‡†è§£æ³•ï¼Œåˆ¤æ–­æ˜¯å¦ä½¿ç”¨äº†æŸä¸ªå¦ç±»è§£æ³•
3. å¦‚æžœä½¿ç”¨äº†å¦ç±»è§£æ³•ï¼Œåˆ¤æ–­æ˜¯å¦æ»¡è¶³è¯„åˆ†æ¡ä»¶

## è¾“å‡ºæ ¼å¼ï¼ˆJSONï¼‰
```json
{{
    "uses_standard_solution": true/false,
    "uses_alternative_solution": true/false,
    "alternative_solution_index": null æˆ– 1/2/3...,
    "alternative_solution_description": "ä½¿ç”¨çš„å¦ç±»è§£æ³•æè¿°",
    "meets_scoring_conditions": true/false,
    "condition_analysis": "è¯„åˆ†æ¡ä»¶åˆ†æž",
    "recommended_max_score": å»ºè®®çš„æœ€é«˜åˆ†
}}
```"""

    async def detect_alternative_solution(
        self,
        question_id: str,
        student_answer: str,
        image: Optional[bytes] = None,
    ) -> Dict[str, Any]:
        """
        æ£€æµ‹å­¦ç”Ÿæ˜¯å¦ä½¿ç”¨äº†å¦ç±»è§£æ³• (Requirement 1.3)

        Args:
            question_id: é¢˜ç›®ç¼–å·
            student_answer: å­¦ç”Ÿç­”æ¡ˆæè¿°
            image: å¯é€‰çš„é¢˜ç›®å›¾åƒ

        Returns:
            Dict: å¦ç±»è§£æ³•æ£€æµ‹ç»“æžœ
        """
        # èŽ·å–è¯„åˆ†æ ‡å‡†
        rubric = await self.get_rubric_for_question(question_id)
        if rubric is None or not rubric.alternative_solutions:
            return {
                "uses_standard_solution": True,
                "uses_alternative_solution": False,
                "alternative_solution_index": None,
                "alternative_solution_description": None,
                "meets_scoring_conditions": False,
                "condition_analysis": "æ— å¦ç±»è§£æ³•å¯æ£€æµ‹",
                "recommended_max_score": rubric.max_score if rubric else 0,
            }

        # æž„å»ºæ£€æµ‹æç¤ºè¯
        prompt = self._build_alternative_solution_prompt(rubric, student_answer)

        try:
            if image:
                img_b64 = (
                    base64.b64encode(image).decode("utf-8") if isinstance(image, bytes) else image
                )
                response_text = await self._call_vision_api(img_b64, prompt)
            else:
                response = await self.llm.ainvoke([HumanMessage(content=prompt)])
                response_text = self._extract_text_from_response(response.content)

            json_text = self._extract_json_from_text(response_text)
            result = json.loads(json_text)

            logger.info(
                f"é¢˜ç›® {question_id} å¦ç±»è§£æ³•æ£€æµ‹: "
                f"æ ‡å‡†è§£æ³•={result.get('uses_standard_solution')}, "
                f"å¦ç±»è§£æ³•={result.get('uses_alternative_solution')}"
            )

            return result

        except Exception as e:
            logger.error(f"å¦ç±»è§£æ³•æ£€æµ‹å¤±è´¥: {e}")
            return {
                "uses_standard_solution": True,
                "uses_alternative_solution": False,
                "alternative_solution_index": None,
                "alternative_solution_description": None,
                "meets_scoring_conditions": False,
                "condition_analysis": f"æ£€æµ‹å¤±è´¥: {str(e)}",
                "recommended_max_score": rubric.max_score,
            }

    async def grade_with_alternative_solution_support(
        self,
        image: bytes,
        question_id: str,
        page_index: int = 0,
    ) -> QuestionResult:
        """
        æ”¯æŒå¦ç±»è§£æ³•çš„å®Œæ•´è¯„åˆ†æµç¨‹ (Requirement 1.3)

        è¿™æ˜¯ä¸€ä¸ªå¢žå¼ºçš„è¯„åˆ†æµç¨‹ï¼š
        1. è§†è§‰åˆ†æžæå–å­¦ç”Ÿç­”æ¡ˆ
        2. æ£€æµ‹æ˜¯å¦ä½¿ç”¨å¦ç±»è§£æ³•
        3. æ ¹æ®è§£æ³•ç±»åž‹é€‰æ‹©è¯„åˆ†æ ‡å‡†
        4. é€ä¸€æ ¸å¯¹å¾—åˆ†ç‚¹
        5. ç”Ÿæˆè¯¦ç»†çš„è¯„åˆ†ç»“æžœ

        Args:
            image: é¢˜ç›®å›¾åƒ
            question_id: é¢˜ç›®ç¼–å·
            page_index: é¡µç ç´¢å¼•

        Returns:
            QuestionResult: åŒ…å«å¦ç±»è§£æ³•ä¿¡æ¯çš„è¯„åˆ†ç»“æžœ
        """
        # 1. èŽ·å–è¯„åˆ†æ ‡å‡†
        rubric = await self.get_rubric_for_question(question_id)
        if rubric is None:
            return QuestionResult(
                question_id=question_id,
                score=0.0,
                max_score=0.0,
                confidence=0.0,
                feedback="æ— æ³•èŽ·å–è¯„åˆ†æ ‡å‡†",
                scoring_point_results=[],
                page_indices=[page_index],
                is_cross_page=False,
                student_answer="",
            )

        # 2. è§†è§‰åˆ†æžæå–å­¦ç”Ÿç­”æ¡ˆ
        img_b64 = base64.b64encode(image).decode("utf-8") if isinstance(image, bytes) else image

        extraction_prompt = f"""è¯·åˆ†æžè¿™å¼ å­¦ç”Ÿç­”é¢˜å›¾åƒï¼Œæå–ç¬¬{question_id}é¢˜çš„å­¦ç”Ÿç­”æ¡ˆã€‚

ä»»åŠ¡ï¼š
1. æ‰¾åˆ°ç¬¬{question_id}é¢˜çš„å­¦ç”Ÿä½œç­”å†…å®¹
2. è¯¦ç»†æè¿°å­¦ç”Ÿçš„è§£é¢˜æ–¹æ³•å’Œæ­¥éª¤
3. å®¢è§‚æè¿°ï¼Œä¸è¦è¯„åˆ†

è¾“å‡ºæ ¼å¼ï¼ˆJSONï¼‰ï¼š
```json
{{
    "question_id": "{question_id}",
    "student_answer": "å­¦ç”Ÿç­”æ¡ˆçš„è¯¦ç»†æè¿°",
    "solution_method": "å­¦ç”Ÿä½¿ç”¨çš„è§£é¢˜æ–¹æ³•æè¿°",
    "has_content": true
}}
```"""

        try:
            extraction_response = await self._call_vision_api(img_b64, extraction_prompt)
            extraction_json = self._extract_json_from_text(extraction_response)
            extraction_data = json.loads(extraction_json)
            student_answer = extraction_data.get("student_answer", "")
            solution_method = extraction_data.get("solution_method", "")
        except Exception as e:
            logger.warning(f"å­¦ç”Ÿç­”æ¡ˆæå–å¤±è´¥: {e}")
            student_answer = "æ— æ³•æå–å­¦ç”Ÿç­”æ¡ˆ"
            solution_method = ""

        # 3. æ£€æµ‹å¦ç±»è§£æ³•
        alt_detection = await self.detect_alternative_solution(
            question_id=question_id,
            student_answer=f"{student_answer}\nè§£é¢˜æ–¹æ³•: {solution_method}",
            image=image,
        )

        # 4. æ ¹æ®è§£æ³•ç±»åž‹æž„å»ºè¯„åˆ†æç¤ºè¯
        if alt_detection.get("uses_alternative_solution") and alt_detection.get(
            "meets_scoring_conditions"
        ):
            # ä½¿ç”¨å¦ç±»è§£æ³•çš„è¯„åˆ†æ ‡å‡†
            alt_index = alt_detection.get("alternative_solution_index", 1) - 1
            if 0 <= alt_index < len(rubric.alternative_solutions):
                alt_solution = rubric.alternative_solutions[alt_index]
                scoring_context = f"""
## å­¦ç”Ÿä½¿ç”¨äº†å¦ç±»è§£æ³•
- è§£æ³•æè¿°: {alt_solution.description}
- è¯„åˆ†æ¡ä»¶: {alt_solution.scoring_conditions}
- æœ€é«˜åˆ†: {alt_solution.max_score}åˆ†

è¯·æ ¹æ®å¦ç±»è§£æ³•çš„è¯„åˆ†æ¡ä»¶è¿›è¡Œè¯„åˆ†ã€‚
"""
                effective_max_score = alt_solution.max_score
            else:
                scoring_context = ""
                effective_max_score = rubric.max_score
        else:
            scoring_context = ""
            effective_max_score = rubric.max_score

        # 5. æž„å»ºå®Œæ•´çš„è¯„åˆ†æç¤ºè¯
        prompt = self._build_scoring_point_prompt(rubric, student_answer)
        if scoring_context:
            prompt = prompt.replace("## å­¦ç”Ÿç­”æ¡ˆ", f"{scoring_context}\n## å­¦ç”Ÿç­”æ¡ˆ")

        # 6. è°ƒç”¨ LLM è¿›è¡Œè¯„åˆ†
        try:
            response_text = await self._call_vision_api(img_b64, prompt)
            json_text = self._extract_json_from_text(response_text)
            result_data = json.loads(json_text)

            # 7. åˆ›å»ºå¾—åˆ†ç‚¹æ˜Žç»†
            scoring_point_results = self._create_scoring_point_results_from_response(
                result_data, rubric
            )

            # 8. ç”Ÿæˆåé¦ˆï¼ˆåŒ…å«å¦ç±»è§£æ³•ä¿¡æ¯ï¼‰
            feedback_parts = [result_data.get("feedback", "")]

            if alt_detection.get("uses_alternative_solution"):
                if alt_detection.get("meets_scoring_conditions"):
                    feedback_parts.append(
                        f"\nã€å¦ç±»è§£æ³•ã€‘å­¦ç”Ÿä½¿ç”¨äº†æœ‰æ•ˆçš„å¦ç±»è§£æ³•: "
                        f"{alt_detection.get('alternative_solution_description', '')}"
                    )
                else:
                    feedback_parts.append(
                        f"\nã€å¦ç±»è§£æ³•ã€‘å­¦ç”Ÿå°è¯•ä½¿ç”¨å¦ç±»è§£æ³•ï¼Œä½†æœªæ»¡è¶³è¯„åˆ†æ¡ä»¶: "
                        f"{alt_detection.get('condition_analysis', '')}"
                    )

            scoring_summary = self.generate_scoring_point_summary(scoring_point_results)
            feedback_parts.append(f"\n{scoring_summary}")

            full_feedback = "\n".join(feedback_parts)

            # 9. ç¡®ä¿åˆ†æ•°ä¸è¶…è¿‡æœ‰æ•ˆæœ€é«˜åˆ†
            final_score = min(result_data.get("total_score", 0), effective_max_score)

            return QuestionResult(
                question_id=question_id,
                score=final_score,
                max_score=effective_max_score,
                confidence=result_data.get("confidence", 0.8),
                feedback=full_feedback,
                scoring_point_results=scoring_point_results,
                page_indices=[page_index],
                is_cross_page=False,
                student_answer=student_answer,
            )

        except Exception as e:
            logger.error(f"å¦ç±»è§£æ³•è¯„åˆ†å¤±è´¥: {e}", exc_info=True)
            return QuestionResult(
                question_id=question_id,
                score=0.0,
                max_score=rubric.max_score,
                confidence=0.0,
                feedback=f"è¯„åˆ†å¤±è´¥: {str(e)}",
                scoring_point_results=[],
                page_indices=[page_index],
                is_cross_page=False,
                student_answer=student_answer,
            )

    # ==================== grade_student æ–¹æ³• ====================

    async def grade_student(
        self,
        images: List[bytes],
        student_key: str,
        parsed_rubric: Dict[str, Any],
        page_indices: Optional[List[int]] = None,
        page_contexts: Optional[Dict[int, Dict[str, Any]]] = None,
        stream_callback: Optional[Callable[[str, str], Awaitable[None]]] = None,
    ) -> Dict[str, Any]:
        """
        ä¸€æ¬¡æ€§æ‰¹æ”¹æ•´ä¸ªå­¦ç”Ÿçš„æ‰€æœ‰é¡µé¢

        å°†å­¦ç”Ÿçš„æ‰€æœ‰ç­”é¢˜é¡µé¢ä½œä¸ºä¸€ä¸ªæ•´ä½“å‘é€ç»™ LLMï¼ŒèŽ·å–å®Œæ•´çš„æ‰¹æ”¹ç»“æžœã€‚
        è¿™ç§æ–¹å¼å¯ä»¥æ›´å¥½åœ°å¤„ç†è·¨é¡µé¢˜ç›®ï¼Œå¹¶å‡å°‘ API è°ƒç”¨æ¬¡æ•°ã€‚

        Args:
            images: å­¦ç”Ÿæ‰€æœ‰ç­”é¢˜é¡µé¢çš„å›¾åƒå­—èŠ‚åˆ—è¡¨
            student_key: å­¦ç”Ÿæ ‡è¯†ï¼ˆå¦‚å§“åã€å­¦å·ï¼‰
            parsed_rubric: è§£æžåŽçš„è¯„åˆ†æ ‡å‡†
            page_indices: é¡µé¢ç´¢å¼•åˆ—è¡¨ï¼ˆå¯é€‰ï¼‰
            page_contexts: é¡µé¢ç´¢å¼•ä¸Šä¸‹æ–‡ï¼ˆå¯é€‰ï¼‰
            stream_callback: æµå¼å›žè°ƒå‡½æ•° (stream_type, chunk) -> None

        Returns:
            Dict: åŒ…å«å­¦ç”Ÿå®Œæ•´æ‰¹æ”¹ç»“æžœ
                - status: "completed" | "failed"
                - total_score: æ€»å¾—åˆ†
                - max_score: æ»¡åˆ†
                - confidence: ç½®ä¿¡åº¦
                - question_details: é¢˜ç›®è¯¦æƒ…åˆ—è¡¨
                - overall_feedback: æ€»ä½“åé¦ˆ
                - student_info: å­¦ç”Ÿä¿¡æ¯ï¼ˆå¦‚æžœè¯†åˆ«åˆ°ï¼‰
        """
        if not images:
            return {
                "status": "failed",
                "error": "æ²¡æœ‰æä¾›ç­”é¢˜å›¾åƒ",
                "total_score": 0,
                "max_score": 0,
                "confidence": 0,
                "question_details": [],
            }

        logger.info(f"[grade_student] å¼€å§‹æ‰¹æ”¹å­¦ç”Ÿ {student_key}ï¼Œå…± {len(images)} é¡µ")

        # æž„å»ºè¯„åˆ†æ ‡å‡†ä¸Šä¸‹æ–‡
        rubric_context = ""
        total_score = 0
        questions_count = 0

        if parsed_rubric:
            rubric_context = parsed_rubric.get("rubric_context", "")
            total_score = parsed_rubric.get("total_score", 0)
            questions_count = len(parsed_rubric.get("questions", []))

            if not rubric_context and parsed_rubric.get("questions"):
                # ä»Žé¢˜ç›®ä¿¡æ¯æž„å»ºè¯„åˆ†æ ‡å‡†ä¸Šä¸‹æ–‡
                rubric_lines = [f"è¯„åˆ†æ ‡å‡†ï¼ˆæ€»åˆ† {total_score} åˆ†ï¼Œå…± {questions_count} é“é¢˜ï¼‰ï¼š\n"]
                for q in parsed_rubric.get("questions", []):
                    qid = q.get("question_id", "?")
                    max_q_score = q.get("max_score", 0)
                    rubric_lines.append(f"\nç¬¬{qid}é¢˜ï¼ˆæ»¡åˆ† {max_q_score} åˆ†ï¼‰ï¼š")

                    # æ·»åŠ å¾—åˆ†ç‚¹
                    for sp in q.get("scoring_points", []):
                        point_id = sp.get("point_id", "")
                        desc = sp.get("description", "")
                        score = sp.get("score", 0)
                        rubric_lines.append(f"  - [{point_id}] {desc}ï¼ˆ{score}åˆ†ï¼‰")

                    # æ·»åŠ æ ‡å‡†ç­”æ¡ˆæ‘˜è¦
                    std_answer = q.get("standard_answer", "")
                    if std_answer:
                        preview = std_answer[:100] + "..." if len(std_answer) > 100 else std_answer
                        rubric_lines.append(f"  æ ‡å‡†ç­”æ¡ˆï¼š{preview}")

                rubric_context = "\n".join(rubric_lines)

        # æž„å»ºé¡µé¢ä¸Šä¸‹æ–‡ä¿¡æ¯
        page_context_info = ""
        if page_contexts:
            context_lines = ["é¡µé¢ç´¢å¼•ä¿¡æ¯ï¼š"]
            for idx, ctx in sorted(page_contexts.items()):
                q_nums = ctx.get("question_numbers", [])
                student_info = ctx.get("student_info")
                is_first = ctx.get("is_first_page", False)
                context_lines.append(f"  - é¡µé¢ {idx}: é¢˜ç›®={q_nums}, é¦–é¡µ={is_first}")
                if student_info:
                    context_lines.append(
                        f"    å­¦ç”Ÿ: {student_info.get('name', 'æœªçŸ¥')}, "
                        f"å­¦å·: {student_info.get('student_id', 'æœªçŸ¥')}"
                    )
            page_context_info = "\n".join(context_lines)

        # æž„å»ºæ‰¹æ”¹æç¤ºè¯
        prompt = f"""ä½ æ˜¯ä¸€ä½ä¸“ä¸šçš„é˜…å·æ•™å¸ˆï¼Œè¯·ä»”ç»†åˆ†æžä»¥ä¸‹å­¦ç”Ÿçš„ç­”é¢˜å›¾åƒå¹¶è¿›è¡Œç²¾ç¡®è¯„åˆ†ï¼ŒåŒæ—¶è¾“å‡º**é€æ­¥éª¤**çš„æ‰¹æ³¨åæ ‡ä¿¡æ¯ã€‚

## å­¦ç”Ÿä¿¡æ¯
- å­¦ç”Ÿæ ‡è¯†ï¼š{student_key}
- ç­”é¢˜é¡µæ•°ï¼š{len(images)} é¡µ

## è¯„åˆ†æ ‡å‡†
{rubric_context}

{page_context_info}

## æ‰¹æ”¹è¦æ±‚
1. **é€é¢˜è¯„åˆ†**ï¼šå¯¹æ¯é“é¢˜ç›®è¿›è¡Œç‹¬ç«‹è¯„åˆ†
2. **å¾—åˆ†ç‚¹æ ¸å¯¹**ï¼šä¸¥æ ¼æŒ‰ç…§è¯„åˆ†æ ‡å‡†çš„å¾—åˆ†ç‚¹ç»™åˆ†
3. **è·¨é¡µå¤„ç†**ï¼šå¦‚æžœä¸€é“é¢˜è·¨è¶Šå¤šé¡µï¼Œéœ€è¦ç»¼åˆæ‰€æœ‰é¡µé¢çš„å†…å®¹è¯„åˆ†
4. **å¦ç±»è§£æ³•**ï¼šå¦‚æžœå­¦ç”Ÿä½¿ç”¨äº†æœ‰æ•ˆçš„å¦ç±»è§£æ³•ï¼ŒåŒæ ·ç»™åˆ†
5. **è¯¦ç»†åé¦ˆ**ï¼šä¸ºæ¯é“é¢˜æä¾›å…·ä½“çš„è¯„åˆ†è¯´æ˜Ž
6. **å®Œæ•´è®°å½•å­¦ç”Ÿä½œç­”**ï¼šstudent_answer å­—æ®µå¿…é¡»å®Œæ•´è®°å½•å­¦ç”Ÿçš„åŽŸå§‹ä½œç­”å†…å®¹ï¼Œä¸è¦çœç•¥
7. **è‡ªç™½ä¸Žç½®ä¿¡åº¦**ï¼šæ¯é“é¢˜å¿…é¡»è¾“å‡º self_critiqueï¼ˆè‡ªæˆ‘åæ€ï¼‰å’Œ self_critique_confidenceï¼ˆç½®ä¿¡åº¦ï¼‰
   - è‡ªç™½éœ€è¯šå®žæŒ‡å‡ºä¸ç¡®å®šä¹‹å¤„ã€è¯æ®ä¸è¶³çš„åœ°æ–¹
   - å¦‚æžœå¯¹æŸé“é¢˜çš„è¯„åˆ†ä¸ç¡®å®šï¼Œå¿…é¡»åœ¨ self_critique ä¸­è¯´æ˜Ž
8. **é€æ­¥éª¤æ‰¹æ³¨**ï¼šè¯†åˆ«å­¦ç”Ÿä½œç­”çš„æ¯ä¸€ä¸ªæ­¥éª¤ï¼Œæ ‡æ³¨åæ ‡å’Œå¾—åˆ†æƒ…å†µ
9. **åŒºåˆ† A mark å’Œ M mark**ï¼š
   - **A markï¼ˆAnswer markï¼‰**ï¼šç­”æ¡ˆåˆ†ï¼Œåªçœ‹æœ€ç»ˆç­”æ¡ˆæ˜¯å¦æ­£ç¡®
   - **M markï¼ˆMethod markï¼‰**ï¼šæ–¹æ³•åˆ†ï¼Œçœ‹è§£é¢˜æ­¥éª¤/æ–¹æ³•æ˜¯å¦æ­£ç¡®

## åæ ‡ç³»ç»Ÿè¯´æ˜Ž
- åæ ‡åŽŸç‚¹åœ¨å›¾ç‰‡**å·¦ä¸Šè§’**
- x è½´å‘å³å¢žåŠ  (0.0 = æœ€å·¦, 1.0 = æœ€å³)
- y è½´å‘ä¸‹å¢žåŠ  (0.0 = æœ€ä¸Š, 1.0 = æœ€ä¸‹)
- ä½¿ç”¨ bounding_box è¡¨ç¤ºåŒºåŸŸ: {{"x_min", "y_min", "x_max", "y_max"}}
- æ‰€æœ‰åæ ‡å€¼ä¸ºå½’ä¸€åŒ–åæ ‡ (0.0-1.0)

## æ‰¹æ³¨ç±»åž‹è¯´æ˜Ž
- `score`: æ€»åˆ†æ ‡æ³¨ï¼Œæ”¾åœ¨é¢˜ç›®ç­”æ¡ˆæ—è¾¹
- `a_mark`: A mark æ ‡æ³¨ï¼ˆç­”æ¡ˆåˆ†ï¼‰ï¼Œæ˜¾ç¤º "A1"ï¼ˆå¾—åˆ†ï¼‰æˆ– "A0"ï¼ˆä¸å¾—åˆ†ï¼‰
- `m_mark`: M mark æ ‡æ³¨ï¼ˆæ–¹æ³•åˆ†ï¼‰ï¼Œæ˜¾ç¤º "M1"ï¼ˆå¾—åˆ†ï¼‰æˆ– "M0"ï¼ˆä¸å¾—åˆ†ï¼‰
- `step_check`: æ­¥éª¤æ­£ç¡®å‹¾é€‰ âœ“
- `step_cross`: æ­¥éª¤é”™è¯¯å‰ âœ—
- `error_circle`: é”™è¯¯åœˆé€‰ï¼Œåœˆå‡ºé”™è¯¯çš„åœ°æ–¹
- `comment`: æ–‡å­—æ‰¹æ³¨/é”™è¯¯è®²è§£
- `correct_check`: æ­£ç¡®å‹¾é€‰ âœ“ï¼ˆç”¨äºŽæ•´é¢˜ï¼‰
- `wrong_cross`: é”™è¯¯å‰ âœ—ï¼ˆç”¨äºŽæ•´é¢˜ï¼‰

## è¾“å‡ºæ ¼å¼ï¼ˆJSONï¼‰
```json
{{
    "student_key": "{student_key}",
    "status": "completed",
    "total_score": æ€»å¾—åˆ†,
    "max_score": {total_score},
    "confidence": è¯„åˆ†ç½®ä¿¡åº¦ï¼ˆ0.0-1.0ï¼‰,
    "student_info": {{
        "name": "è¯†åˆ«åˆ°çš„å­¦ç”Ÿå§“åï¼ˆå¦‚æœ‰ï¼‰",
        "student_id": "è¯†åˆ«åˆ°çš„å­¦å·ï¼ˆå¦‚æœ‰ï¼‰",
        "class_name": "è¯†åˆ«åˆ°çš„ç­çº§ï¼ˆå¦‚æœ‰ï¼‰"
    }},
    "question_details": [
        {{
            "question_id": "é¢˜å·",
            "score": å¾—åˆ†,
            "max_score": æ»¡åˆ†,
            "student_answer": "ã€å¿…é¡»å®Œæ•´ã€‘å­¦ç”Ÿçš„åŽŸå§‹ä½œç­”å†…å®¹ï¼ŒåŒ…æ‹¬æ‰€æœ‰æ–‡å­—ã€å…¬å¼ã€æ­¥éª¤ï¼Œä¸è¦çœç•¥",
            "is_correct": true/false,
            "feedback": "è¯„åˆ†è¯´æ˜Ž",
            "confidence": ç½®ä¿¡åº¦,
            "self_critique": "ã€å¿…é¡»å¡«å†™ã€‘è‡ªæˆ‘åæ€ï¼šå¯¹æœ¬é¢˜è¯„åˆ†çš„ä¸ç¡®å®šä¹‹å¤„ã€å¯èƒ½çš„é—æ¼ã€è¯æ®æ˜¯å¦å……åˆ†ç­‰",
            "self_critique_confidence": è‡ªè¯„ç½®ä¿¡åº¦ï¼ˆ0.0-1.0ï¼Œè¶Šä½Žè¡¨ç¤ºè¶Šä¸ç¡®å®šï¼‰,
            "source_pages": [é¡µç åˆ—è¡¨],
            "answer_region": {{
                "x_min": 0.1, "y_min": 0.2, "x_max": 0.9, "y_max": 0.4
            }},
            "steps": [
                {{
                    "step_id": "1.1",
                    "step_content": "å­¦ç”Ÿå†™çš„ç¬¬ä¸€æ­¥å†…å®¹",
                    "step_region": {{"x_min": 0.1, "y_min": 0.2, "x_max": 0.8, "y_max": 0.25}},
                    "is_correct": true,
                    "mark_type": "M",
                    "mark_value": 1,
                    "feedback": "æ–¹æ³•æ­£ç¡®"
                }},
                {{
                    "step_id": "1.2",
                    "step_content": "å­¦ç”Ÿå†™çš„ç¬¬äºŒæ­¥å†…å®¹",
                    "step_region": {{"x_min": 0.1, "y_min": 0.26, "x_max": 0.8, "y_max": 0.31}},
                    "is_correct": false,
                    "mark_type": "M",
                    "mark_value": 0,
                    "feedback": "è®¡ç®—é”™è¯¯ï¼š3+5 åº”è¯¥ç­‰äºŽ 8ï¼Œä¸æ˜¯ 9",
                    "error_detail": "3+5=9 å†™é”™äº†"
                }},
                {{
                    "step_id": "1.3",
                    "step_content": "æœ€ç»ˆç­”æ¡ˆ",
                    "step_region": {{"x_min": 0.1, "y_min": 0.32, "x_max": 0.5, "y_max": 0.36}},
                    "is_correct": false,
                    "mark_type": "A",
                    "mark_value": 0,
                    "feedback": "ç­”æ¡ˆé”™è¯¯ï¼Œå› ä¸ºå‰é¢è®¡ç®—æœ‰è¯¯"
                }}
            ],
            "scoring_point_results": [
                {{
                    "point_id": "å¾—åˆ†ç‚¹ID",
                    "description": "å¾—åˆ†ç‚¹æè¿°",
                    "mark_type": "M æˆ– A",
                    "max_score": è¯¥å¾—åˆ†ç‚¹æ»¡åˆ†,
                    "awarded": èŽ·å¾—çš„åˆ†æ•°,
                    "evidence": "ã€å¿…é¡»å¼•ç”¨åŽŸæ–‡ã€‘è¯„åˆ†ä¾æ®ï¼Œå¼•ç”¨å­¦ç”Ÿç­”æ¡ˆä¸­çš„å…·ä½“å†…å®¹",
                    "error_region": {{"x_min": 0.3, "y_min": 0.26, "x_max": 0.5, "y_max": 0.31}}
                }}
            ],
            "annotations": [
                {{
                    "type": "score",
                    "page_index": 0,
                    "bounding_box": {{"x_min": 0.85, "y_min": 0.2, "x_max": 0.95, "y_max": 0.25}},
                    "text": "2/3",
                    "color": "#FF8800"
                }},
                {{
                    "type": "m_mark",
                    "page_index": 0,
                    "bounding_box": {{"x_min": 0.82, "y_min": 0.21, "x_max": 0.88, "y_max": 0.24}},
                    "text": "M1",
                    "color": "#00AA00"
                }},
                {{
                    "type": "m_mark",
                    "page_index": 0,
                    "bounding_box": {{"x_min": 0.82, "y_min": 0.27, "x_max": 0.88, "y_max": 0.30}},
                    "text": "M0",
                    "color": "#FF0000"
                }},
                {{
                    "type": "a_mark",
                    "page_index": 0,
                    "bounding_box": {{"x_min": 0.82, "y_min": 0.33, "x_max": 0.88, "y_max": 0.36}},
                    "text": "A0",
                    "color": "#FF0000"
                }},
                {{
                    "type": "step_check",
                    "page_index": 0,
                    "bounding_box": {{"x_min": 0.78, "y_min": 0.21, "x_max": 0.81, "y_max": 0.24}},
                    "text": "",
                    "color": "#00AA00"
                }},
                {{
                    "type": "step_cross",
                    "page_index": 0,
                    "bounding_box": {{"x_min": 0.78, "y_min": 0.27, "x_max": 0.81, "y_max": 0.30}},
                    "text": "",
                    "color": "#FF0000"
                }},
                {{
                    "type": "error_circle",
                    "page_index": 0,
                    "bounding_box": {{"x_min": 0.3, "y_min": 0.26, "x_max": 0.5, "y_max": 0.31}},
                    "text": "3+5â‰ 9",
                    "color": "#FF0000"
                }},
                {{
                    "type": "comment",
                    "page_index": 0,
                    "bounding_box": {{"x_min": 0.55, "y_min": 0.27, "x_max": 0.78, "y_max": 0.30}},
                    "text": "åº”ä¸º 3+5=8",
                    "color": "#0066FF"
                }}
            ]
        }}
    ],
    "overall_feedback": "æ€»ä½“è¯„ä»·å’Œå»ºè®®",
    "page_summaries": [
        {{
            "page_index": é¡µç ,
            "question_numbers": ["è¯¥é¡µåŒ…å«çš„é¢˜å·"],
            "summary": "è¯¥é¡µå†…å®¹æ‘˜è¦"
        }}
    ]
}}
```

## æ‰¹æ³¨åæ ‡è¦æ±‚
1. **æ¯ä¸ªæ­¥éª¤éƒ½è¦æ ‡æ³¨**ï¼šè¯†åˆ«å­¦ç”Ÿå†™çš„æ¯ä¸€è¡Œ/æ¯ä¸€æ­¥ï¼Œç»™å‡ºåæ ‡
2. **åŒºåˆ† A/M mark**ï¼š
   - M mark æ ‡æ³¨åœ¨æ–¹æ³•æ­¥éª¤æ—è¾¹ï¼Œæ˜¾ç¤º "M1"ï¼ˆå¾—åˆ†ï¼‰æˆ– "M0"ï¼ˆä¸å¾—åˆ†ï¼‰
   - A mark æ ‡æ³¨åœ¨æœ€ç»ˆç­”æ¡ˆæ—è¾¹ï¼Œæ˜¾ç¤º "A1"ï¼ˆå¾—åˆ†ï¼‰æˆ– "A0"ï¼ˆä¸å¾—åˆ†ï¼‰
3. **é”™è¯¯å¿…é¡»åœˆå‡º**ï¼šç”¨ error_circle åœˆå‡ºå…·ä½“é”™è¯¯ä½ç½®ï¼Œå¹¶ç”¨ comment è¯´æ˜ŽåŽŸå› 
4. **åæ ‡å¿…é¡»å‡†ç¡®**ï¼šä»”ç»†è§‚å¯Ÿå›¾ç‰‡ï¼Œç»™å‡ºç²¾ç¡®çš„åæ ‡ä½ç½®
5. **é¢œè‰²è§„èŒƒ**ï¼š
   - ç»¿è‰² #00AA00ï¼šæ­£ç¡®ï¼ˆM1, A1, âœ“ï¼‰
   - çº¢è‰² #FF0000ï¼šé”™è¯¯ï¼ˆM0, A0, âœ—, é”™è¯¯åœˆé€‰ï¼‰
   - æ©™è‰² #FF8800ï¼šéƒ¨åˆ†æ­£ç¡®/æ€»åˆ†
   - è“è‰² #0066FFï¼šè®²è§£/æ‰¹æ³¨
6. **æ¯é“é¢˜è‡³å°‘è¾“å‡º**ï¼š
   - ä¸€ä¸ª score ç±»åž‹çš„æ‰¹æ³¨ï¼ˆæ€»åˆ†ï¼‰
   - æ¯ä¸ªæ­¥éª¤å¯¹åº”çš„ m_mark æˆ– a_mark
   - æ¯ä¸ªæ­¥éª¤å¯¹åº”çš„ step_check æˆ– step_cross

## é‡è¦æé†’
- å¿…é¡»æ‰¹æ”¹å…¨éƒ¨ {questions_count} é“é¢˜
- æ¯é“é¢˜çš„ score å¿…é¡»ç­‰äºŽå„å¾—åˆ†ç‚¹ awarded ä¹‹å’Œ
- total_score å¿…é¡»ç­‰äºŽå„é¢˜ score ä¹‹å’Œ
- student_answer å¿…é¡»å®Œæ•´è®°å½•å­¦ç”Ÿçš„åŽŸå§‹ä½œç­”ï¼Œä¸è¦ç”¨"..."çœç•¥
- self_critique å¿…é¡»è¯šå®žåæ˜ è¯„åˆ†çš„ä¸ç¡®å®šæ€§
- å¦‚æžœæ— æ³•è¯†åˆ«æŸé“é¢˜çš„ç­”æ¡ˆï¼Œconfidence å’Œ self_critique_confidence è®¾ä¸ºè¾ƒä½Žå€¼å¹¶åœ¨ self_critique ä¸­è¯´æ˜ŽåŽŸå› 
- **æ‰¹æ³¨åæ ‡å¿…é¡»å‡†ç¡®**ï¼šä»”ç»†è§‚å¯Ÿå›¾ç‰‡ï¼Œç»™å‡ºç²¾ç¡®çš„åæ ‡ä½ç½®
- **æ¯ä¸ªæ­¥éª¤éƒ½è¦æ ‡æ³¨**ï¼šä¸è¦é—æ¼ä»»ä½•æ­¥éª¤çš„åæ ‡
"""

        try:
            # å°†å›¾åƒè½¬ä¸º base64
            content = [{"type": "text", "text": prompt}]
            for idx, img_bytes in enumerate(images):
                if isinstance(img_bytes, bytes):
                    img_b64 = base64.b64encode(img_bytes).decode("utf-8")
                else:
                    img_b64 = img_bytes
                content.append(
                    {"type": "image_url", "image_url": f"data:image/jpeg;base64,{img_b64}"}
                )

            message = HumanMessage(content=content)

            # æµå¼è°ƒç”¨ LLM
            full_response = ""
            thinking_content = ""

            async for chunk in self.llm.astream([message]):
                chunk_content = chunk.content
                if chunk_content:
                    if isinstance(chunk_content, str):
                        full_response += chunk_content
                        if stream_callback:
                            await stream_callback("output", chunk_content)
                    elif isinstance(chunk_content, list):
                        for part in chunk_content:
                            if isinstance(part, str):
                                full_response += part
                                if stream_callback:
                                    await stream_callback("output", part)
                            elif isinstance(part, dict):
                                if part.get("type") == "thinking":
                                    thinking_content += part.get("thinking", "")
                                    if stream_callback:
                                        await stream_callback("thinking", part.get("thinking", ""))
                                elif "text" in part:
                                    full_response += part["text"]
                                    if stream_callback:
                                        await stream_callback("output", part["text"])

            # åˆ†ç¦»æ€è€ƒå†…å®¹å’Œè¾“å‡ºå†…å®¹
            output_text, extracted_thinking = split_thinking_content(full_response)
            if extracted_thinking:
                thinking_content = extracted_thinking

            # è§£æž JSON å“åº”
            json_text = self._extract_json_from_text(output_text)

            # å°è¯•å¤šç§æ–¹å¼è§£æž JSON
            result = None
            try:
                result = self._load_json_with_repair(json_text)
            except json.JSONDecodeError:
                # å°è¯•æå– JSON å—
                json_block = self._extract_json_block(json_text)
                if json_block:
                    try:
                        result = self._load_json_with_repair(json_block)
                    except json.JSONDecodeError:
                        pass

            # å°è¯•è§£æžåˆ†é¡µè¾“å‡ºæ ¼å¼
            if result is None:
                result = self._parse_page_break_output(output_text, student_key)

            if result is None:
                logger.error(f"[grade_student] JSON è§£æžå¤±è´¥: {json_text[:500]}")
                return {
                    "status": "failed",
                    "error": "æ— æ³•è§£æžæ‰¹æ”¹ç»“æžœ",
                    "total_score": 0,
                    "max_score": total_score,
                    "confidence": 0,
                    "question_details": [],
                    "raw_response": output_text[:1000],
                }

            # è§„èŒƒåŒ–ç»“æžœ
            result["status"] = "completed"
            result["student_key"] = student_key

            # ç¡®ä¿å¿…è¦å­—æ®µå­˜åœ¨
            if "total_score" not in result:
                result["total_score"] = sum(
                    q.get("score", 0) for q in result.get("question_details", [])
                )
            if "max_score" not in result:
                result["max_score"] = total_score
            if "confidence" not in result:
                result["confidence"] = 0.8
            if "question_details" not in result:
                result["question_details"] = []

            # è§„èŒƒåŒ– question_details
            normalized_details = []
            for detail in result.get("question_details", []):
                normalized = self._normalize_question_detail(
                    detail, page_indices[0] if page_indices else 0
                )
                normalized_details.append(normalized)
            result["question_details"] = normalized_details

            logger.info(
                f"[grade_student] æ‰¹æ”¹å®Œæˆ: student={student_key}, "
                f"score={result.get('total_score')}/{result.get('max_score')}, "
                f"questions={len(result.get('question_details', []))}"
            )

            return result

        except Exception as e:
            logger.error(f"[grade_student] æ‰¹æ”¹å¤±è´¥: {e}", exc_info=True)
            return {
                "status": "failed",
                "error": str(e),
                "total_score": 0,
                "max_score": total_score,
                "confidence": 0,
                "question_details": [],
            }
