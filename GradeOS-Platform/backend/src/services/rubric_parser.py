"""æ‰¹æ”¹æ ‡å‡†è§£ææœåŠ¡

è§£ææ‰¹æ”¹æ ‡å‡† PDFï¼Œæå–ï¼š
1. æ¯é“é¢˜çš„é¢˜å·å’Œåˆ†å€¼
2. å„ä¸ªå¾—åˆ†ç‚¹åŠå…¶åˆ†å€¼
3. å¦ç±»è§£æ³•ï¼ˆä¸è®¡å…¥æ€»åˆ†ï¼‰
4. æ”¯æŒ"é¢˜ç›®+ç­”æ¡ˆ"æ··åˆæ ¼å¼çš„è§£æ

æ”¯æŒ OpenRouter API å’Œç›´è¿ LLM APIã€‚
"""

import base64
import json
import logging
import os
from typing import List, Optional, Dict, Any
from dataclasses import dataclass, field

# ä½¿ç”¨ LLMReasoningClientï¼ˆä¸æ‰¹æ”¹æµç¨‹ä¸€è‡´ï¼‰
from src.config.models import get_default_model
from src.services.llm_reasoning import LLMReasoningClient


logger = logging.getLogger(__name__)


def _escape_invalid_backslashes(text: str) -> str:
    """Escape invalid backslashes in JSON strings to improve parse resilience."""
    result = []
    i = 0
    hexdigits = "0123456789abcdefABCDEF"
    while i < len(text):
        ch = text[i]
        if ch != "\\":
            result.append(ch)
            i += 1
            continue
        if i + 1 >= len(text):
            result.append("\\\\")
            i += 1
            continue
        nxt = text[i + 1]
        if nxt in ('"', "\\", "/"):
            result.append("\\")
            result.append(nxt)
            i += 2
            continue
        if nxt == "u":
            seq = text[i + 2 : i + 6]
            if len(seq) == 4 and all(c in hexdigits for c in seq):
                result.append("\\u")
                result.append(seq)
                i += 6
                continue
            result.append("\\\\")
            i += 1
            continue
        result.append("\\\\")
        i += 1
    return "".join(result)


def _strip_control_chars(text: str) -> str:
    """Remove control characters that commonly break JSON parsing."""
    cleaned = []
    for ch in text:
        if ord(ch) < 0x20 and ch not in ("\t", "\n", "\r"):
            cleaned.append(" ")
        else:
            cleaned.append(ch)
    return "".join(cleaned)


def _extract_json_block(text: str) -> Optional[str]:
    """Extract the outermost JSON object from a text blob."""
    start = text.find("{")
    end = text.rfind("}")
    if start == -1 or end <= start:
        return None
    return text[start : end + 1]


def _load_json_with_repair(text: str) -> Optional[Dict[str, Any]]:
    """Best-effort JSON loading with multiple repair passes."""
    if not text:
        return None
    candidates = [text]
    repaired = _strip_control_chars(_escape_invalid_backslashes(text))
    if repaired != text:
        candidates.append(repaired)
    block = _extract_json_block(repaired)
    if block and block not in candidates:
        candidates.append(block)

    for candidate in candidates:
        try:
            return json.loads(candidate, strict=False)
        except json.JSONDecodeError:
            continue
    return None


@dataclass
class ScoringPoint:
    """å¾—åˆ†ç‚¹"""

    description: str  # å¾—åˆ†ç‚¹æè¿°
    score: float  # è¯¥å¾—åˆ†ç‚¹çš„åˆ†å€¼
    is_required: bool = True  # æ˜¯å¦å¿…é¡»ï¼ˆéƒ¨åˆ†åˆ†æ•°å¯èƒ½æ˜¯å¯é€‰çš„ï¼‰
    point_id: str = ""  # å¾—åˆ†ç‚¹ç¼–å·
    keywords: List[str] = field(default_factory=list)  # å…³é”®è¯
    expected_value: str = ""  # æœŸæœ›å€¼


@dataclass
class AlternativeSolution:
    """å¦ç±»è§£æ³•"""

    description: str  # è§£æ³•æè¿°
    scoring_criteria: str  # å¾—åˆ†æ¡ä»¶
    note: str = ""  # å¤‡æ³¨


@dataclass
class DeductionRule:
    """æ‰£åˆ†è§„åˆ™"""

    description: str  # å…¸å‹é”™è¯¯/æ‰£åˆ†æ¡ä»¶æè¿°
    deduction: float  # æ‰£åˆ†åˆ†å€¼
    conditions: str = ""  # æ‰£åˆ†æ¡ä»¶è¡¨è¾¾
    rule_id: str = ""  # æ‰£åˆ†è§„åˆ™ç¼–å·


@dataclass
class QuestionConfession:
    """å•é¢˜è§£æè‡ªç™½ï¼ˆæçŸ­ï¼‰"""

    risk: str = ""  # è¯¥é¢˜é£é™©ï¼ˆâ‰¤10å­—ï¼‰
    uncertainty: str = ""  # ä¸ç¡®å®šç‚¹ï¼ˆâ‰¤10å­—ï¼‰

    def to_dict(self) -> Dict[str, Any]:
        return {"risk": self.risk, "uncertainty": self.uncertainty}


@dataclass
class RubricConfession:
    """è¯„åˆ†æ ‡å‡†è§£æè‡ªç™½ï¼ˆLLM ç›´æ¥ç”Ÿæˆï¼ŒæçŸ­ï¼‰"""

    risks: List[str] = field(default_factory=list)  # é£é™©åˆ—è¡¨ï¼ˆæ¯æ¡â‰¤15å­—ï¼‰
    uncertainties: List[str] = field(default_factory=list)  # ä¸ç¡®å®šç‚¹åˆ—è¡¨
    blind_spots: List[str] = field(default_factory=list)  # å¯èƒ½é—æ¼çš„å†…å®¹
    needs_review: List[str] = field(default_factory=list)  # å»ºè®®äººå·¥å¤æ ¸çš„é¡¹
    confidence: float = 1.0  # æ•´ä½“ç½®ä¿¡åº¦ (0.0-1.0)

    def to_dict(self) -> Dict[str, Any]:
        return {
            "risks": self.risks,
            "uncertainties": self.uncertainties,
            "blindSpots": self.blind_spots,
            "needsReview": self.needs_review,
            "confidence": self.confidence,
        }

    @classmethod
    def from_dict(cls, data: Dict[str, Any]) -> "RubricConfession":
        return cls(
            risks=data.get("risks") or [],
            uncertainties=data.get("uncertainties") or [],
            blind_spots=data.get("blind_spots") or data.get("blindSpots") or [],
            needs_review=data.get("needs_review") or data.get("needsReview") or [],
            confidence=float(data.get("confidence", 1.0) or 1.0),
        )


@dataclass
class QuestionRubric:
    """å•é¢˜è¯„åˆ†æ ‡å‡†"""

    question_id: str  # é¢˜å·
    max_score: float  # æ»¡åˆ†
    question_text: str = ""  # é¢˜ç›®å†…å®¹ï¼ˆå¦‚æœæœ‰ï¼‰
    standard_answer: str = ""  # æ ‡å‡†ç­”æ¡ˆ
    scoring_points: List[ScoringPoint] = field(default_factory=list)  # å¾—åˆ†ç‚¹åˆ—è¡¨
    alternative_solutions: List[AlternativeSolution] = field(default_factory=list)  # å¦ç±»è§£æ³•
    deduction_rules: List[DeductionRule] = field(default_factory=list)  # æ‰£åˆ†è§„åˆ™
    grading_notes: str = ""  # æ‰¹æ”¹æ³¨æ„äº‹é¡¹
    # LLM ç›´æ¥ç”Ÿæˆçš„è‡ªç™½ï¼ˆæçŸ­ï¼‰
    confession: QuestionConfession = field(default_factory=QuestionConfession)
    # è§£æè‡ªç™½å­—æ®µï¼ˆå…¼å®¹æ—§ç‰ˆï¼‰
    parse_confidence: float = 1.0  # è§£æç½®ä¿¡åº¦ (0.0-1.0)
    parse_uncertainties: List[str] = field(default_factory=list)  # ä¸ç¡®å®šæ€§åˆ—è¡¨
    parse_quality_issues: List[str] = field(default_factory=list)  # è´¨é‡é—®é¢˜


@dataclass
class ParsedRubric:
    """è§£æåçš„å®Œæ•´è¯„åˆ†æ ‡å‡†"""

    total_questions: int  # æ€»é¢˜æ•°
    total_score: float  # æ€»åˆ†
    questions: List[QuestionRubric]  # å„é¢˜è¯„åˆ†æ ‡å‡†
    general_notes: str = ""  # é€šç”¨æ‰¹æ”¹è¯´æ˜
    rubric_format: str = "standard"  # æ ¼å¼ç±»å‹: standard/embedded
    # LLM ç›´æ¥ç”Ÿæˆçš„è‡ªç™½ï¼ˆæçŸ­ï¼‰
    confession: RubricConfession = field(default_factory=RubricConfession)
    # è§£æè‡ªç™½å­—æ®µï¼ˆå…¼å®¹æ—§ç‰ˆï¼Œç”±è§„åˆ™æ£€æŸ¥ç”Ÿæˆï¼‰
    overall_parse_confidence: float = 1.0  # æ•´ä½“è§£æç½®ä¿¡åº¦ (0.0-1.0)
    parse_confession: Dict[str, Any] = field(default_factory=dict)  # å®Œæ•´è‡ªç™½æŠ¥å‘Š


class RubricParserService:
    """
    æ‰¹æ”¹æ ‡å‡†è§£ææœåŠ¡

    æ”¯æŒä¸¤ç§æ ¼å¼ï¼š
    1. æ ‡å‡†æ ¼å¼ï¼šç‹¬ç«‹çš„è¯„åˆ†æ ‡å‡†æ–‡æ¡£
    2. åµŒå…¥æ ¼å¼ï¼šé¢˜ç›®ä¸Šç›´æ¥æ ‡æ³¨ç­”æ¡ˆçš„æ ¼å¼

    æ”¯æŒ OpenRouter API å’Œç›´è¿ LLM APIã€‚
    """

    def __init__(self, api_key: str = None, model_name: Optional[str] = None):
        """
        åˆå§‹åŒ–æœåŠ¡

        Args:
            api_key: API å¯†é’¥ï¼ˆå¯é€‰ï¼Œé»˜è®¤ä»ç¯å¢ƒå˜é‡è·å–ï¼‰
            model_name: æ¨¡å‹åç§°ï¼ˆå¯é€‰ï¼Œä½¿ç”¨ç¯å¢ƒå˜é‡é…ç½®ï¼‰
        """
        # ä½¿ç”¨ LLMReasoningClientï¼ˆä¸æ‰¹æ”¹æµç¨‹ä¸€è‡´ï¼‰
        self.api_key = api_key or os.getenv("LLM_API_KEY") or os.getenv("OPENROUTER_API_KEY", "")
        self.model_name = model_name or get_default_model()

        # ç§»é™¤ token é™åˆ¶ï¼šè®¾ç½®ä¸º 0 è¡¨ç¤ºä¸é™åˆ¶è¾“å‡ºé•¿åº¦
        # è¿™æ ·å¯ä»¥ç¡®ä¿ LLM èƒ½å®Œæ•´è¾“å‡ºæ‰€æœ‰é¢˜ç›®çš„è§£æç»“æœ
        os.environ["GRADING_MAX_OUTPUT_TOKENS"] = "0"

        self.reasoning_client = LLMReasoningClient(api_key=self.api_key, model_name=self.model_name)

    async def parse_rubric(
        self, rubric_images: List[bytes], progress_callback=None, stream_callback=None
    ) -> ParsedRubric:
        """
        è§£ææ‰¹æ”¹æ ‡å‡†

        Args:
            rubric_images: æ‰¹æ”¹æ ‡å‡†é¡µé¢å›¾åƒåˆ—è¡¨
            progress_callback: è¿›åº¦å›è°ƒ (batch_index, total_batches, status, message)
            stream_callback: æµå¼è¾“å‡ºå›è°ƒ (stream_type, chunk)

        Returns:
            ParsedRubric: è§£æåçš„è¯„åˆ†æ ‡å‡†
        """
        logger.info(f"[rubric_parse] received {len(rubric_images)} pages")

        # Max images per LLM call for rubric parsing.
        MAX_PAGES_PER_BATCH = max(1, int(os.getenv("RUBRIC_PARSE_MAX_PAGES", "14")))
        all_questions = []
        general_notes = ""
        rubric_format = "standard"
        total_batches = 0

        for batch_start in range(0, len(rubric_images), MAX_PAGES_PER_BATCH):
            batch_end = min(batch_start + MAX_PAGES_PER_BATCH, len(rubric_images))
            batch_images = rubric_images[batch_start:batch_end]
            batch_num = batch_start // MAX_PAGES_PER_BATCH + 1
            total_batches = (len(rubric_images) + MAX_PAGES_PER_BATCH - 1) // MAX_PAGES_PER_BATCH

            logger.info(
                f"[rubric_parse] batch {batch_num}/{total_batches} pages {batch_start+1}-{batch_end}"
            )

            # è¿›åº¦å›è°ƒ
            if progress_callback:
                try:
                    import asyncio

                    if asyncio.iscoroutinefunction(progress_callback):
                        await progress_callback(
                            batch_num - 1,
                            total_batches,
                            "parsing",
                            f"Parsing batch {batch_num}/{total_batches}",
                        )
                    else:
                        progress_callback(
                            batch_num - 1,
                            total_batches,
                            "parsing",
                            f"Parsing batch {batch_num}/{total_batches}",
                        )
                except Exception as e:
                    logger.debug(f"[rubric_parse] progress_callback error: {e}")

            batch_result = await self._parse_rubric_batch(
                batch_images,
                batch_num,
                total_batches,
                stream_callback,
            )

            all_questions.extend(batch_result.questions)
            if batch_result.general_notes:
                general_notes = batch_result.general_notes
            if batch_result.rubric_format != "standard":
                rubric_format = batch_result.rubric_format

        # è®¡ç®—è§£æå‡ºçš„æ€»åˆ†
        calculated_total = sum(q.max_score for q in all_questions)

        # åˆå¹¶ç»“æœ
        parsed = ParsedRubric(
            total_questions=len(all_questions),
            total_score=calculated_total,
            questions=all_questions,
            general_notes=general_notes,
            rubric_format=rubric_format,
        )

        if progress_callback and total_batches > 0:
            try:
                import asyncio

                if asyncio.iscoroutinefunction(progress_callback):
                    await progress_callback(
                        total_batches - 1, total_batches, "completed", "Parsing completed"
                    )
                else:
                    progress_callback(
                        total_batches - 1, total_batches, "completed", "Parsing completed"
                    )
            except Exception as e:
                logger.debug(f"[rubric_parse] progress_callback error: {e}")

        logger.info(
            f"æ‰¹æ”¹æ ‡å‡†è§£æå®Œæˆ: " f"{parsed.total_questions} é¢˜, " f"æ€»åˆ† {parsed.total_score}"
        )

        return parsed

    async def _parse_rubric_batch(
        self,
        rubric_images: List[bytes],
        batch_num: int,
        total_batches: int,
        stream_callback=None,
    ) -> ParsedRubric:
        """è§£æå•æ‰¹è¯„åˆ†æ ‡å‡†é¡µé¢"""
        batch_info = f"ï¼ˆç¬¬ {batch_num}/{total_batches} æ‰¹ï¼‰" if total_batches > 1 else ""

        prompt_template = """ä½ æ˜¯ä¸€ä½ä¸“ä¸šçš„è¯„åˆ†æ ‡å‡†åˆ†æä¸“å®¶ã€‚è¯·ä»”ç»†åˆ†æè¿™äº›è¯„åˆ†æ ‡å‡†/ç­”æ¡ˆé¡µé¢{batch_info}ã€‚

## âš ï¸ é‡è¦ï¼šä¸¥æ ¼æŒ‰æä¾›çš„è¯„åˆ†æ ‡å‡†æ„å»º rubric

ä½ å¿…é¡»**ä¸¥æ ¼éµå®ˆ**ä»¥ä¸‹åŸåˆ™ï¼š
1. **ä¸èƒ½åˆå¹¶å¾—åˆ†ç‚¹**ï¼šæ¯ä¸ªå¾—åˆ†ç‚¹å¿…é¡»å•ç‹¬åˆ—å‡ºï¼Œä¸èƒ½å°†å¤šä¸ªå¾—åˆ†ç‚¹åˆå¹¶
2. **ä¸èƒ½æ‹†åˆ†å¾—åˆ†ç‚¹**ï¼šå¦‚æœè¯„åˆ†æ ‡å‡†ä¸­æŸä¸ªå¾—åˆ†ç‚¹æ˜¯å®Œæ•´çš„ï¼Œä¸èƒ½æ‹†åˆ†æˆå¤šä¸ª
3. **é»˜è®¤æ¯ä¸ªå¾—åˆ†ç‚¹ 1 åˆ†**ï¼šé™¤éè¯„åˆ†æ ‡å‡†æ˜ç¡®æ ‡æ³¨æŸä¸ªå¾—åˆ†ç‚¹å€¼ 2 åˆ†æˆ–ä»¥ä¸Šï¼Œå¦åˆ™é»˜è®¤ä¸º 1 åˆ†
4. **ä¸¥æ ¼æŒ‰åŸæ–‡**ï¼šå¾—åˆ†ç‚¹çš„æè¿°å¿…é¡»ä¸åŸæ–‡ä¸€è‡´ï¼Œä¸èƒ½æ”¹å†™æˆ–ç®€åŒ–

## é‡è¦ï¼šä½ æ­£åœ¨åˆ†æçš„æ˜¯ä¸€ä»½å®Œæ•´çš„è¯„åˆ†æ ‡å‡†æ–‡æ¡£
- è¿™ä»½æ–‡æ¡£åŒ…å« **å¤šé“é¢˜ç›®**ï¼ˆå¯èƒ½æœ‰ 10-20 é“æˆ–æ›´å¤šï¼‰
- ä½ å¿…é¡» **é€é¡µä»”ç»†é˜…è¯»**ï¼Œç¡®ä¿è¯†åˆ«å‡º **æ¯ä¸€é“é¢˜ç›®**
- **ä¸è¦é—æ¼ä»»ä½•é¢˜ç›®**ï¼Œå³ä½¿å®ƒä»¬åˆ†å¸ƒåœ¨ä¸åŒçš„é¡µé¢ä¸Š

## å…³é”®ï¼šé¢˜ç›®è¯†åˆ«è§„åˆ™
**åªè®¡æ•°ä¸»é¢˜å·ï¼Œä¸è¦æŠŠå­é¢˜å½“ä½œç‹¬ç«‹é¢˜ç›®ï¼**

ä¾‹å¦‚ï¼š
- âœ… æ­£ç¡®ï¼šé¢˜ç›® "7" åŒ…å«å­é¢˜ 7(1), 7(2), 7(3) â†’ è¿™æ˜¯ **1é“é¢˜**ï¼Œæœ‰3ä¸ªå¾—åˆ†ç‚¹
- âŒ é”™è¯¯ï¼šæŠŠ 7(1), 7(2), 7(3) å½“ä½œ 3é“ç‹¬ç«‹é¢˜ç›®

**ä¸»é¢˜å·è¯†åˆ«**ï¼š
- ä¸»é¢˜å·æ ¼å¼ï¼š1ã€2ã€3... æˆ– ä¸€ã€äºŒã€ä¸‰... æˆ– ç¬¬1é¢˜ã€ç¬¬2é¢˜...
- å­é¢˜æ ¼å¼ï¼š(1)ã€(2)ã€(3)... æˆ– â‘ ã€â‘¡ã€â‘¢... æˆ– a)ã€b)ã€c)...
- **å­é¢˜åº”è¯¥ä½œä¸ºä¸»é¢˜çš„ scoring_pointsï¼Œè€Œä¸æ˜¯ç‹¬ç«‹çš„ question**

## å¾—åˆ†ç‚¹å¤„ç†è§„åˆ™ï¼ˆéå¸¸é‡è¦ï¼‰
1. **æ¯ä¸ªå¾—åˆ†ç‚¹é»˜è®¤ 1 åˆ†**ï¼š
   - å¦‚æœè¯„åˆ†æ ‡å‡†å†™ "å†™å‡ºå…¬å¼"ï¼Œè¿™æ˜¯ 1 ä¸ªå¾—åˆ†ç‚¹ï¼Œå€¼ 1 åˆ†
   - å¦‚æœè¯„åˆ†æ ‡å‡†å†™ "å†™å‡ºå…¬å¼ (2åˆ†)"ï¼Œè¿™æ˜¯ 1 ä¸ªå¾—åˆ†ç‚¹ï¼Œå€¼ 2 åˆ†
   
2. **ä¸èƒ½åˆå¹¶å¾—åˆ†ç‚¹**ï¼š
   - âŒ é”™è¯¯ï¼šå°† "å†™å‡ºå…¬å¼" å’Œ "ä»£å…¥æ•°å€¼" åˆå¹¶ä¸º "å†™å‡ºå…¬å¼å¹¶ä»£å…¥æ•°å€¼"
   - âœ… æ­£ç¡®ï¼šåˆ†åˆ«åˆ—å‡º "å†™å‡ºå…¬å¼" (1åˆ†) å’Œ "ä»£å…¥æ•°å€¼" (1åˆ†)
   
3. **ä¸èƒ½æ‹†åˆ†å¾—åˆ†ç‚¹**ï¼š
   - âŒ é”™è¯¯ï¼šå°† "å†™å‡ºå®Œæ•´è§£é¢˜è¿‡ç¨‹" æ‹†åˆ†ä¸º "å†™å‡ºå…¬å¼"ã€"ä»£å…¥æ•°å€¼"ã€"è®¡ç®—ç»“æœ"
   - âœ… æ­£ç¡®ï¼šä¿æŒåŸæ · "å†™å‡ºå®Œæ•´è§£é¢˜è¿‡ç¨‹" (1åˆ†)
   
4. **ä¸¥æ ¼æŒ‰åŸæ–‡æè¿°**ï¼š
   - å¾—åˆ†ç‚¹çš„ description å¿…é¡»ä¸è¯„åˆ†æ ‡å‡†åŸæ–‡ä¸€è‡´
   - ä¸èƒ½æ”¹å†™ã€ç®€åŒ–æˆ–æ‰©å±•

## ä»»åŠ¡
1. **è¯†åˆ«æ‰€æœ‰ä¸»é¢˜**ï¼šä»”ç»†æŸ¥æ‰¾æ¯ä¸€é“ä¸»é¢˜ï¼ˆä¸åŒ…æ‹¬å­é¢˜ï¼‰
   - åªè®¡æ•°ä¸»é¢˜å·ï¼ˆå¦‚ 1, 2, 3...ï¼‰
   - å­é¢˜ä½œä¸ºè¯¥ä¸»é¢˜çš„å¾—åˆ†ç‚¹
2. **æå–åˆ†å€¼**ï¼šæ¯é“ä¸»é¢˜çš„æ»¡åˆ†åˆ†å€¼ï¼ˆæ‰€æœ‰å­é¢˜åˆ†å€¼ä¹‹å’Œï¼‰
3. **æå–å¾—åˆ†ç‚¹**ï¼šæ¯é“ä¸»é¢˜çš„è¯„åˆ†è¦ç‚¹å’Œå¯¹åº”åˆ†å€¼
   - å¦‚æœæœ‰å­é¢˜ï¼Œæ¯ä¸ªå­é¢˜ä½œä¸ºä¸€ä¸ª scoring_point
   - point_id æ ¼å¼ï¼šä¸»é¢˜å·.å­é¢˜å·ï¼ˆå¦‚ "7.1", "7.2", "7.3"ï¼‰
   - **æ¯ä¸ªå¾—åˆ†ç‚¹é»˜è®¤ 1 åˆ†ï¼Œé™¤éæ˜ç¡®æ ‡æ³¨å…¶ä»–åˆ†å€¼**
4. **æå–æ ‡å‡†ç­”æ¡ˆ**ï¼šå¦‚æœæœ‰æ ‡å‡†ç­”æ¡ˆï¼Œå®Œæ•´æå–
5. **æå–æ‰£åˆ†è§„åˆ™**ï¼šå¦‚æœæœ‰æ‰£åˆ†è¯´æ˜ï¼Œæå–æ‰£åˆ†æ¡ä»¶å’Œåˆ†å€¼

## è¾“å‡ºæ ¼å¼ï¼ˆä»…è¿”å› JSONï¼Œä¸è¦ markdown ä»£ç å—ï¼‰
{{
  "rubric_format": "standard",
  "general_notes": "é€šç”¨æ‰¹æ”¹è¯´æ˜ï¼ˆå¦‚æœ‰ï¼‰",
  "questions": [
    {{
      "question_id": "1",
      "max_score": 5,
      "question_text": "é¢˜ç›®å†…å®¹ï¼ˆå¦‚æœ‰ï¼‰",
      "standard_answer": "æ ‡å‡†ç­”æ¡ˆï¼ˆå®Œæ•´æå–ï¼‰",
      "scoring_points": [
        {{"point_id": "1.1", "description": "å¾—åˆ†ç‚¹æè¿°ï¼ˆå¿…é¡»ä¸åŸæ–‡ä¸€è‡´ï¼‰", "score": 1, "is_required": true}}
      ],
      "deduction_rules": [
        {{"rule_id": "1.d1", "description": "æ‰£åˆ†æ¡ä»¶", "deduction": 1, "conditions": "è§¦å‘æ¡ä»¶"}}
      ],
      "alternative_solutions": [
        {{"description": "å¦ç±»è§£æ³•æè¿°", "scoring_criteria": "å¾—åˆ†æ¡ä»¶", "note": "å¤‡æ³¨"}}
      ],
      "grading_notes": "æ‰¹æ”¹æ³¨æ„äº‹é¡¹"
    }}
  ]
}}

## ç¤ºä¾‹
å¦‚æœè¯„åˆ†æ ‡å‡†æ˜¯ï¼š
```
7. (15åˆ†)
  (1) è®¡ç®—ç»“æœ (5åˆ†)
  (2) å†™å‡ºè¿‡ç¨‹ (5åˆ†)  
  (3) ç”»å‡ºå›¾å½¢ (5åˆ†)
```

åº”è¯¥è¾“å‡ºï¼š
```json
{{
  "questions": [
    {{
      "question_id": "7",
      "max_score": 15,
      "scoring_points": [
        {{"point_id": "7.1", "description": "è®¡ç®—ç»“æœ", "score": 5}},
        {{"point_id": "7.2", "description": "å†™å‡ºè¿‡ç¨‹", "score": 5}},
        {{"point_id": "7.3", "description": "ç”»å‡ºå›¾å½¢", "score": 5}}
      ]
    }}
  ]
}}
```
**æ³¨æ„ï¼šè¿™æ˜¯1é“é¢˜ï¼Œä¸æ˜¯3é“é¢˜ï¼**

## ä¸¥æ ¼è§„åˆ™
- **å¿…é¡»è¿”å›æœ‰æ•ˆçš„ JSON**ï¼ˆä¸è¦ markdown ä»£ç å—ï¼Œä¸è¦ ```jsonï¼‰
- **åªè®¡æ•°ä¸»é¢˜å·**ï¼Œä¸è¦æŠŠå­é¢˜å½“ä½œç‹¬ç«‹é¢˜ç›®
- **é€é¡µæ£€æŸ¥**ï¼šç¡®ä¿æ¯ä¸€é¡µçš„å†…å®¹éƒ½è¢«åˆ†æ
- **å­é¢˜å¤„ç†**ï¼šå¦‚æœä¸€é“å¤§é¢˜åŒ…å«å¤šä¸ªå­é¢˜ï¼ˆå¦‚ 7(1), 7(2), 7(3)ï¼‰ï¼Œå°†å®ƒä»¬ä½œä¸ºè¯¥é¢˜çš„ scoring_pointsï¼Œè€Œä¸æ˜¯ç‹¬ç«‹çš„ questions
- **ä¸èƒ½åˆå¹¶å¾—åˆ†ç‚¹**ï¼šæ¯ä¸ªå¾—åˆ†ç‚¹å¿…é¡»å•ç‹¬åˆ—å‡º
- **ä¸èƒ½æ‹†åˆ†å¾—åˆ†ç‚¹**ï¼šä¿æŒè¯„åˆ†æ ‡å‡†åŸæ–‡çš„å®Œæ•´æ€§
- **é»˜è®¤ 1 åˆ†**ï¼šæ¯ä¸ªå¾—åˆ†ç‚¹é»˜è®¤ 1 åˆ†ï¼Œé™¤éæ˜ç¡®æ ‡æ³¨å…¶ä»–åˆ†å€¼
- max_score å¿…é¡»æ˜¯æ•°å­—ç±»å‹
- ä¸è¦ç¼–é€ ä¸å­˜åœ¨çš„é¢˜ç›®
"""
        prompt = prompt_template.format(batch_info=batch_info)

        try:
            # ä½¿ç”¨ LLMReasoningClient è°ƒç”¨è§†è§‰æ¨¡å‹ï¼ˆå¸¦é‡è¯•ï¼‰
            max_retries = 3
            retry_delay = 5  # ç§’
            last_error = None

            for attempt in range(max_retries):
                try:
                    # ä½¿ç”¨ LLMReasoningClient çš„ analyze_with_vision æ–¹æ³•
                    response = await self.reasoning_client.analyze_with_vision(
                        images=rubric_images,
                        prompt=prompt,
                        stream_callback=stream_callback,
                    )
                    result_text = response.get("response", "")
                    break
                except Exception as e:
                    last_error = e
                    error_str = str(e)
                    if (
                        "503" in error_str
                        or "overloaded" in error_str.lower()
                        or "429" in error_str
                    ):
                        if attempt < max_retries - 1:
                            logger.warning(
                                f"API è¿‡è½½ï¼Œ{retry_delay}ç§’åé‡è¯• ({attempt + 1}/{max_retries})"
                            )
                            import asyncio

                            await asyncio.sleep(retry_delay)
                            retry_delay *= 2  # æŒ‡æ•°é€€é¿
                            continue
                    raise
            else:
                raise last_error

            # æ£€æŸ¥å“åº”æ˜¯å¦ä¸ºç©º
            if not result_text or not result_text.strip():
                logger.warning(f"LLM è¿”å›ç©ºå“åº”ï¼Œä½¿ç”¨ç©ºç»“æœ")
                return ParsedRubric(
                    total_questions=0,
                    total_score=0,
                    questions=[],
                    general_notes="",
                    rubric_format="standard",
                )

            logger.debug(f"LLM åŸå§‹å“åº”: {result_text[:500]}...")

            # ğŸ” ç®€è¦æ—¥å¿—ï¼šåªè®°å½•å“åº”é•¿åº¦
            logger.info(f"[rubric_parse] LLM å“åº”é•¿åº¦: {len(result_text)} å­—ç¬¦")
            # è¯¦ç»†å“åº”å†…å®¹æ”¹ä¸º DEBUG çº§åˆ«
            if len(result_text) < 2000:
                logger.debug(f"[rubric_parse] LLM å®Œæ•´å“åº”: {result_text}")
            else:
                logger.debug(f"[rubric_parse] LLM å“åº”å‰ 2000 å­—ç¬¦: {result_text[:2000]}...")

            # æå– JSON
            json_text = result_text
            if "```json" in result_text:
                json_start = result_text.find("```json") + 7
                json_end = result_text.find("```", json_start)
                if json_end > json_start:
                    json_text = result_text[json_start:json_end].strip()
            elif "```" in result_text:
                json_start = result_text.find("```") + 3
                json_end = result_text.find("```", json_start)
                if json_end > json_start:
                    json_text = result_text[json_start:json_end].strip()

            # å°è¯•æ‰¾åˆ° JSON å¯¹è±¡
            if not json_text.startswith("{"):
                # å°è¯•æ‰¾åˆ°ç¬¬ä¸€ä¸ª {
                brace_start = json_text.find("{")
                if brace_start >= 0:
                    json_text = json_text[brace_start:]

            if not json_text or not json_text.strip().startswith("{"):
                logger.warning(f"æ— æ³•ä»å“åº”ä¸­æå– JSON: {result_text[:200]}...")
                return ParsedRubric(
                    total_questions=0,
                    total_score=0,
                    questions=[],
                    general_notes="",
                    rubric_format="standard",
                )

            data = _load_json_with_repair(json_text)
            if data is None:
                logger.warning(
                    f"[rubric_parse] JSON decode failed after repair attempts. Raw: {json_text[:200]}..."
                )
                return ParsedRubric(
                    total_questions=0,
                    total_score=0,
                    questions=[],
                    general_notes="",
                    rubric_format="standard",
                )

            def ensure_string(value, default=""):
                """ç¡®ä¿å€¼æ˜¯å­—ç¬¦ä¸²ç±»å‹"""
                if value is None:
                    return default
                if isinstance(value, list):
                    return "\n".join(str(item) for item in value)
                if not isinstance(value, str):
                    return str(value)
                return value

            def normalize_question_id(qid: str) -> str:
                """æ ‡å‡†åŒ–é¢˜ç›®ç¼–å·, å°†å­é¢˜åˆå¹¶åˆ°ä¸»é¢˜"""
                if not qid:
                    return qid

                # ç§»é™¤æ‹¬å·å†…å®¹ï¼Œå¦‚ "7(a)" -> "7", "15(1)" -> "15"
                import re

                main_id = re.sub(r"\([^)]*\)", "", str(qid)).strip()
                return main_id

            def _assign_point_ids(question_id: str, scoring_points: List[ScoringPoint]) -> None:
                seen = set()
                for idx, sp in enumerate(scoring_points):
                    point_id = sp.point_id or f"{question_id}.{idx + 1}"
                    while point_id in seen:
                        point_id = f"{question_id}.{len(seen) + 1}"
                    sp.point_id = point_id
                    seen.add(point_id)

            def _assign_rule_ids(question_id: str, deduction_rules: List[DeductionRule]) -> None:
                seen = set()
                for idx, rule in enumerate(deduction_rules):
                    rule_id = rule.rule_id or f"{question_id}.d{idx + 1}"
                    while rule_id in seen:
                        rule_id = f"{question_id}.d{len(seen) + 1}"
                    rule.rule_id = rule_id
                    seen.add(rule_id)

            def _dedupe_deduction_rules(
                deduction_rules: List[DeductionRule],
            ) -> List[DeductionRule]:
                unique = []
                seen = set()
                for rule in deduction_rules:
                    key = (rule.description, rule.deduction, rule.conditions)
                    if key in seen:
                        continue
                    seen.add(key)
                    unique.append(rule)
                return unique

            # å…ˆæ”¶é›†æ‰€æœ‰é¢˜ç›®ï¼Œç„¶åæŒ‰ä¸»é¢˜ç¼–å·åˆå¹¶
            raw_questions = []
            for q in data.get("questions", []):
                # å¤„ç† scoring_pointsï¼Œå¯èƒ½æ˜¯å­—å…¸åˆ—è¡¨æˆ–å­—ç¬¦ä¸²åˆ—è¡¨
                raw_scoring_points = q.get("scoring_points", [])
                scoring_points = []
                for sp in raw_scoring_points:
                    if isinstance(sp, dict):
                        scoring_points.append(
                            ScoringPoint(
                                description=ensure_string(sp.get("description", "")),
                                score=float(sp.get("score", 0)),
                                is_required=sp.get("is_required", True),
                                point_id=ensure_string(
                                    sp.get("point_id") or sp.get("pointId") or sp.get("id") or ""
                                ),
                                keywords=(
                                    [str(item) for item in (sp.get("keywords") or [])]
                                    if isinstance(sp.get("keywords"), list)
                                    else ([str(sp.get("keywords"))] if sp.get("keywords") else [])
                                ),
                                expected_value=ensure_string(
                                    sp.get("expected_value") or sp.get("expectedValue") or ""
                                ),
                            )
                        )
                    elif isinstance(sp, str):
                        # å¦‚æœæ˜¯å­—ç¬¦ä¸²ï¼Œå°†å…¶ä½œä¸ºæè¿°ï¼Œåˆ†æ•°è®¾ä¸º 0
                        scoring_points.append(
                            ScoringPoint(
                                description=sp,
                                score=0,
                                is_required=True,
                                point_id="",
                                keywords=[],
                                expected_value="",
                            )
                        )

                # å¤„ç† alternative_solutionsï¼Œå¯èƒ½æ˜¯å­—å…¸åˆ—è¡¨æˆ–å­—ç¬¦ä¸²åˆ—è¡¨
                raw_alt_solutions = q.get("alternative_solutions", [])
                alternative_solutions = []
                for alt in raw_alt_solutions:
                    if isinstance(alt, dict):
                        alternative_solutions.append(
                            AlternativeSolution(
                                description=ensure_string(alt.get("description", "")),
                                scoring_criteria=ensure_string(alt.get("scoring_criteria", "")),
                                note=ensure_string(alt.get("note", "")),
                            )
                        )
                    elif isinstance(alt, str):
                        # å¦‚æœæ˜¯å­—ç¬¦ä¸²ï¼Œå°†å…¶ä½œä¸ºæè¿°
                        alternative_solutions.append(
                            AlternativeSolution(description=alt, scoring_criteria="", note="")
                        )

                raw_deductions = q.get("deduction_rules") or q.get("deductionRules") or []
                deduction_rules = []
                for dr in raw_deductions:
                    if isinstance(dr, dict):
                        deduction_rules.append(
                            DeductionRule(
                                description=ensure_string(
                                    dr.get("description") or dr.get("rule") or ""
                                ),
                                deduction=float(dr.get("deduction", dr.get("score", 0)) or 0),
                                conditions=ensure_string(
                                    dr.get("conditions") or dr.get("when") or ""
                                ),
                                rule_id=ensure_string(
                                    dr.get("rule_id") or dr.get("ruleId") or dr.get("id") or ""
                                ),
                            )
                        )
                    elif isinstance(dr, str):
                        deduction_rules.append(
                            DeductionRule(
                                description=dr,
                                deduction=0.0,
                                conditions="",
                                rule_id="",
                            )
                        )

                # æå– LLM ç”Ÿæˆçš„é¢˜ç›®çº§ confession
                q_confession_raw = q.get("confession") or {}
                q_confession = QuestionConfession(
                    risk=ensure_string(q_confession_raw.get("risk", "")),
                    uncertainty=ensure_string(q_confession_raw.get("uncertainty", "")),
                )

                raw_questions.append(
                    {
                        "original_id": str(q.get("question_id", "")),
                        "normalized_id": normalize_question_id(str(q.get("question_id", ""))),
                        "max_score": float(q.get("max_score", 0)),
                        "question_text": ensure_string(q.get("question_text", "")),
                        "standard_answer": ensure_string(q.get("standard_answer", "")),
                        "scoring_points": scoring_points,
                        "alternative_solutions": alternative_solutions,
                        "deduction_rules": deduction_rules,
                        "grading_notes": ensure_string(q.get("grading_notes", "")),
                        # LLM ç›´æ¥ç”Ÿæˆçš„è‡ªç™½ï¼ˆæçŸ­ï¼‰
                        "confession": q_confession,
                        # LLM è¾“å‡ºçš„ç½®ä¿¡åº¦å­—æ®µï¼ˆå…¼å®¹æ—§ç‰ˆï¼‰
                        "parse_confidence": float(q.get("parse_confidence", 1.0) or 1.0),
                        "parse_uncertainties": q.get("parse_uncertainties") or [],
                        "parse_quality_issues": q.get("parse_quality_issues") or [],
                    }
                )

            # æŒ‰æ ‡å‡†åŒ–é¢˜ç›®ç¼–å·åˆå¹¶å­é¢˜
            merged_questions = {}
            for q in raw_questions:
                norm_id = q["normalized_id"]
                if norm_id in merged_questions:
                    # åˆå¹¶åˆ°ç°æœ‰é¢˜ç›®
                    existing = merged_questions[norm_id]
                    existing["max_score"] += q["max_score"]
                    existing["scoring_points"].extend(q["scoring_points"])
                    existing["alternative_solutions"].extend(q["alternative_solutions"])
                    existing["deduction_rules"].extend(q["deduction_rules"])

                    # åˆå¹¶æ–‡æœ¬å†…å®¹
                    if q["question_text"] and q["question_text"] not in existing["question_text"]:
                        existing["question_text"] += f"\nå­é¢˜: {q['question_text']}"
                    if (
                        q["standard_answer"]
                        and q["standard_answer"] not in existing["standard_answer"]
                    ):
                        existing["standard_answer"] += f"\nå­é¢˜ç­”æ¡ˆ: {q['standard_answer']}"
                    if q["grading_notes"] and q["grading_notes"] not in existing["grading_notes"]:
                        existing["grading_notes"] += f"\n{q['grading_notes']}"

                    # åˆå¹¶ç½®ä¿¡åº¦å­—æ®µï¼ˆå–æœ€å°ç½®ä¿¡åº¦ï¼Œåˆå¹¶ä¸ç¡®å®šæ€§å’Œè´¨é‡é—®é¢˜ï¼‰
                    existing["parse_confidence"] = min(
                        existing.get("parse_confidence", 1.0), q.get("parse_confidence", 1.0)
                    )
                    existing["parse_uncertainties"].extend(q.get("parse_uncertainties", []))
                    existing["parse_quality_issues"].extend(q.get("parse_quality_issues", []))
                    # åˆå¹¶ confessionï¼ˆåˆå¹¶é£é™©å’Œä¸ç¡®å®šç‚¹ï¼‰
                    existing_conf = existing.get("confession", QuestionConfession())
                    new_conf = q.get("confession", QuestionConfession())
                    if new_conf.risk and not existing_conf.risk:
                        existing_conf.risk = new_conf.risk
                    elif new_conf.risk and existing_conf.risk:
                        existing_conf.risk = f"{existing_conf.risk}; {new_conf.risk}"
                    if new_conf.uncertainty and not existing_conf.uncertainty:
                        existing_conf.uncertainty = new_conf.uncertainty
                    elif new_conf.uncertainty and existing_conf.uncertainty:
                        existing_conf.uncertainty = f"{existing_conf.uncertainty}; {new_conf.uncertainty}"
                    existing["confession"] = existing_conf
                else:
                    # æ–°é¢˜ç›®
                    merged_questions[norm_id] = q.copy()

            # è½¬æ¢ä¸º QuestionRubric å¯¹è±¡
            questions = []
            for norm_id, q in merged_questions.items():
                _assign_point_ids(norm_id, q["scoring_points"])
                _assign_rule_ids(norm_id, q["deduction_rules"])
                q["deduction_rules"] = _dedupe_deduction_rules(q["deduction_rules"])
                questions.append(
                    QuestionRubric(
                        question_id=norm_id,
                        max_score=q["max_score"],
                        question_text=q["question_text"],
                        standard_answer=q["standard_answer"],
                        scoring_points=q["scoring_points"],
                        alternative_solutions=q["alternative_solutions"],
                        deduction_rules=q["deduction_rules"],
                        grading_notes=q["grading_notes"],
                        # LLM ç›´æ¥ç”Ÿæˆçš„è‡ªç™½ï¼ˆæçŸ­ï¼‰
                        confession=q.get("confession", QuestionConfession()),
                        # LLM è§£æç½®ä¿¡åº¦å­—æ®µï¼ˆå…¼å®¹æ—§ç‰ˆï¼‰
                        parse_confidence=q.get("parse_confidence", 1.0),
                        parse_uncertainties=q.get("parse_uncertainties", []),
                        parse_quality_issues=q.get("parse_quality_issues", []),
                    )
                )

            # æå– LLM ç›´æ¥ç”Ÿæˆçš„æ•´ä½“ confession
            confession_raw = data.get("confession") or {}
            llm_confession = RubricConfession(
                risks=confession_raw.get("risks") or [],
                uncertainties=confession_raw.get("uncertainties") or [],
                blind_spots=confession_raw.get("blind_spots") or confession_raw.get("blindSpots") or [],
                needs_review=confession_raw.get("needs_review") or confession_raw.get("needsReview") or [],
                confidence=float(confession_raw.get("confidence", 1.0) or 1.0),
            )

            # è¿”å›æ‰¹æ¬¡ç»“æœï¼ˆåŒ…å« LLM è¾“å‡ºçš„æ•´ä½“ç½®ä¿¡åº¦ï¼‰
            llm_overall_confidence = llm_confession.confidence
            # å¦‚æœ LLM æ²¡æœ‰è¾“å‡ºæ•´ä½“ç½®ä¿¡åº¦ï¼Œä»å„é¢˜ç½®ä¿¡åº¦è®¡ç®—
            if llm_overall_confidence >= 1.0 and questions:
                question_confidences = [
                    q.parse_confidence for q in questions if q.parse_confidence < 1.0
                ]
                if question_confidences:
                    llm_overall_confidence = sum(question_confidences) / len(question_confidences)

            batch_result = ParsedRubric(
                total_questions=len(questions),
                total_score=sum(q.max_score for q in questions),
                questions=questions,
                general_notes=ensure_string(data.get("general_notes", "")),
                rubric_format=ensure_string(data.get("rubric_format", "standard")),
                # LLM ç›´æ¥ç”Ÿæˆçš„è‡ªç™½
                confession=llm_confession,
                # LLM è§£æç½®ä¿¡åº¦
                overall_parse_confidence=llm_overall_confidence,
            )

            logger.info(
                f"æ‰¹æ¬¡è§£æå®Œæˆ: " f"{len(questions)} é¢˜, " f"åˆ†å€¼ {batch_result.total_score}"
            )

            return batch_result

        except Exception as e:
            logger.error(f"æ‰¹æ”¹æ ‡å‡†è§£æå¤±è´¥: {str(e)}")
            raise

    def format_rubric_context(self, rubric: ParsedRubric) -> str:
        """
        å°†è§£æåçš„è¯„åˆ†æ ‡å‡†æ ¼å¼åŒ–ä¸ºæ‰¹æ”¹ Agent å¯ç”¨çš„ä¸Šä¸‹æ–‡
        """

        def ensure_str(value):
            """ç¡®ä¿å€¼æ˜¯å­—ç¬¦ä¸²"""
            if value is None:
                return ""
            if isinstance(value, list):
                return " ".join(str(item) for item in value)
            return str(value)

        lines = [
            "=" * 60,
            "è¯„åˆ†æ ‡å‡†ï¼ˆè¯·ä¸¥æ ¼éµå¾ªï¼‰",
            "=" * 60,
            f"æ€»é¢˜æ•°: {rubric.total_questions}",
            f"æ€»åˆ†: {rubric.total_score}",
            f"æ ¼å¼: {ensure_str(rubric.rubric_format)}",
            "",
        ]

        if rubric.general_notes:
            lines.append(f"é€šç”¨è¯´æ˜: {ensure_str(rubric.general_notes)}")
            lines.append("")

        for q in rubric.questions:
            lines.append("-" * 40)
            lines.append(f"ã€ç¬¬ {ensure_str(q.question_id)} é¢˜ã€‘æ»¡åˆ†: {q.max_score} åˆ†")

            question_text = ensure_str(q.question_text)
            if question_text:
                text_preview = question_text[:100] if len(question_text) > 100 else question_text
                lines.append(f"é¢˜ç›®: {text_preview}...")

            standard_answer = ensure_str(q.standard_answer)
            if standard_answer:
                answer_preview = (
                    standard_answer[:200] if len(standard_answer) > 200 else standard_answer
                )
                lines.append(f"æ ‡å‡†ç­”æ¡ˆ: {answer_preview}...")

            lines.append("å¾—åˆ†ç‚¹:")
            for i, sp in enumerate(q.scoring_points, 1):
                required = "å¿…é¡»" if sp.is_required else "å¯é€‰"
                description = ensure_str(sp.description)
                point_id = ensure_str(sp.point_id) or f"{ensure_str(q.question_id)}.{i}"
                lines.append(f"  [{point_id}] [{sp.score}åˆ†/{required}] {description}")

            if q.deduction_rules:
                lines.append("æ‰£åˆ†è§„åˆ™ï¼ˆå¤‡æ³¨ï¼‰")
                for idx, dr in enumerate(q.deduction_rules, 1):
                    rule_id = ensure_str(dr.rule_id) or f"{ensure_str(q.question_id)}.d{idx}"
                    deduction = dr.deduction
                    conditions = ensure_str(dr.conditions)
                    condition_text = f"ï¼Œæ¡ä»¶: {conditions}" if conditions else ""
                    lines.append(
                        f"  [{rule_id}] -{deduction}åˆ† {ensure_str(dr.description)}{condition_text}"
                    )

            if q.alternative_solutions:
                lines.append("å¦ç±»è§£æ³•ï¼ˆåŒæ ·å¯å¾—åˆ†ï¼‰:")
                for alt in q.alternative_solutions:
                    lines.append(f"  - {ensure_str(alt.description)}")
                    lines.append(f"    å¾—åˆ†æ¡ä»¶: {ensure_str(alt.scoring_criteria)}")

            grading_notes = ensure_str(q.grading_notes)
            if grading_notes:
                lines.append(f"æ‰¹æ”¹æ³¨æ„: {grading_notes}")

            lines.append("")

        return "\n".join(lines)

    def _generate_parse_confession(
        self,
        rubric: ParsedRubric,
        expected_question_count: Optional[int] = None,
        expected_total_score: Optional[float] = None,
    ) -> Dict[str, Any]:
        """
        ç”Ÿæˆè¯„åˆ†æ ‡å‡†è§£æçš„è‡ªç™½æŠ¥å‘Š

        æ‰§è¡Œå¤šç»´åº¦è´¨é‡æ£€æŸ¥:
        - é¢˜ç›®æ•°é‡åˆç†æ€§æ£€æŸ¥
        - åˆ†å€¼ä¸€è‡´æ€§æ£€æŸ¥
        - å¾—åˆ†ç‚¹å®Œæ•´æ€§æ£€æŸ¥
        - å…³é”®ä¿¡æ¯ç¼ºå¤±æ£€æŸ¥

        Args:
            rubric: è§£æåçš„è¯„åˆ†æ ‡å‡†
            expected_question_count: æœŸæœ›çš„é¢˜ç›®æ•°é‡(å¦‚æœå·²çŸ¥)
            expected_total_score: æœŸæœ›çš„æ€»åˆ†(å¦‚æœå·²çŸ¥)

        Returns:
            è‡ªç™½æŠ¥å‘Šå­—å…¸
        """
        from datetime import datetime

        issues = []
        uncertainties = []
        quality_checks = []
        overall_status = "ok"

        # 1. é¢˜ç›®æ•°é‡åˆç†æ€§æ£€æŸ¥
        if rubric.total_questions == 0:
            issues.append(
                {"type": "no_questions", "message": "æœªè¯†åˆ«åˆ°ä»»ä½•é¢˜ç›®", "severity": "high"}
            )
            overall_status = "error"
            quality_checks.append(
                {"check": "é¢˜ç›®æ•°é‡æ£€æŸ¥", "passed": False, "detail": "æœªè¯†åˆ«åˆ°ä»»ä½•é¢˜ç›®"}
            )
        elif rubric.total_questions < 3:
            issues.append(
                {
                    "type": "few_questions",
                    "message": f"é¢˜ç›®æ•°é‡è¾ƒå°‘ï¼ˆ{rubric.total_questions}é¢˜ï¼‰ï¼Œå¯èƒ½å­˜åœ¨é—æ¼",
                    "severity": "medium",
                }
            )
            if overall_status == "ok":
                overall_status = "caution"
            quality_checks.append(
                {
                    "check": "é¢˜ç›®æ•°é‡æ£€æŸ¥",
                    "passed": False,
                    "detail": f"ä»…è¯†åˆ«åˆ° {rubric.total_questions} é¢˜",
                }
            )
        else:
            quality_checks.append(
                {
                    "check": "é¢˜ç›®æ•°é‡æ£€æŸ¥",
                    "passed": True,
                    "detail": f"è¯†åˆ«åˆ° {rubric.total_questions} é¢˜",
                }
            )

        # å¦‚æœæœ‰æœŸæœ›é¢˜ç›®æ•°é‡ï¼Œè¿›è¡Œæ¯”å¯¹
        if expected_question_count and rubric.total_questions != expected_question_count:
            issues.append(
                {
                    "type": "question_count_mismatch",
                    "message": f"è¯†åˆ«åˆ° {rubric.total_questions} é¢˜ï¼Œä½†æœŸæœ› {expected_question_count} é¢˜",
                    "severity": "high",
                }
            )
            overall_status = "error"

        # 2. åˆ†å€¼ä¸€è‡´æ€§æ£€æŸ¥
        calculated_total = sum(q.max_score for q in rubric.questions)
        if abs(calculated_total - rubric.total_score) > 0.1:
            issues.append(
                {
                    "type": "score_mismatch",
                    "message": f"é¢˜ç›®åˆ†å€¼ä¹‹å’Œï¼ˆ{calculated_total}ï¼‰ä¸æ€»åˆ†ï¼ˆ{rubric.total_score}ï¼‰ä¸ä¸€è‡´",
                    "severity": "medium",
                }
            )
            if overall_status == "ok":
                overall_status = "caution"
            quality_checks.append(
                {
                    "check": "åˆ†å€¼ä¸€è‡´æ€§æ£€æŸ¥",
                    "passed": False,
                    "detail": f"åˆ†å€¼å·®å¼‚ {abs(calculated_total - rubric.total_score):.1f} åˆ†",
                }
            )
        else:
            quality_checks.append({"check": "åˆ†å€¼ä¸€è‡´æ€§æ£€æŸ¥", "passed": True, "detail": "åˆ†å€¼ä¸€è‡´"})

        # å¦‚æœæœ‰æœŸæœ›æ€»åˆ†ï¼Œè¿›è¡Œæ¯”å¯¹
        if expected_total_score and abs(rubric.total_score - expected_total_score) > 0.1:
            issues.append(
                {
                    "type": "total_score_mismatch",
                    "message": f"æ€»åˆ†ä¸º {rubric.total_score}ï¼Œä½†æœŸæœ› {expected_total_score}",
                    "severity": "high",
                }
            )
            overall_status = "error"

        # 3. é¢˜ç›®çº§åˆ«æ£€æŸ¥
        questions_with_issues = []
        for q in rubric.questions:
            q_issues = []

            # æ£€æŸ¥å¾—åˆ†ç‚¹
            if not q.scoring_points:
                q_issues.append("ç¼ºå°‘å¾—åˆ†ç‚¹")
                issues.append(
                    {
                        "type": "missing_scoring_points",
                        "message": f"é¢˜ç›® {q.question_id} ç¼ºå°‘å¾—åˆ†ç‚¹",
                        "questionId": q.question_id,
                        "severity": "high",
                    }
                )

            # æ£€æŸ¥åˆ†å€¼åˆç†æ€§
            if q.max_score <= 0:
                q_issues.append("åˆ†å€¼å¼‚å¸¸")
                issues.append(
                    {
                        "type": "invalid_score",
                        "message": f"é¢˜ç›® {q.question_id} åˆ†å€¼å¼‚å¸¸ï¼ˆ{q.max_score}ï¼‰",
                        "questionId": q.question_id,
                        "severity": "high",
                    }
                )
            elif q.max_score > 30:
                uncertainties.append(f"é¢˜ç›® {q.question_id} åˆ†å€¼è¾ƒé«˜ï¼ˆ{q.max_score}åˆ†ï¼‰ï¼Œè¯·ç¡®è®¤")

            # æ£€æŸ¥å¾—åˆ†ç‚¹åˆ†å€¼ä¹‹å’Œ
            if q.scoring_points:
                sp_total = sum(sp.score for sp in q.scoring_points)
                if abs(sp_total - q.max_score) > 0.1:
                    q_issues.append("å¾—åˆ†ç‚¹åˆ†å€¼ä¹‹å’Œä¸é¢˜ç›®æ»¡åˆ†ä¸ä¸€è‡´")
                    issues.append(
                        {
                            "type": "scoring_points_mismatch",
                            "message": f"é¢˜ç›® {q.question_id} å¾—åˆ†ç‚¹åˆ†å€¼ä¹‹å’Œï¼ˆ{sp_total}ï¼‰ä¸æ»¡åˆ†ï¼ˆ{q.max_score}ï¼‰ä¸ä¸€è‡´",
                            "questionId": q.question_id,
                            "severity": "medium",
                        }
                    )

            # æ£€æŸ¥æ ‡å‡†ç­”æ¡ˆ
            if not q.standard_answer:
                uncertainties.append(f"é¢˜ç›® {q.question_id} ç¼ºå°‘æ ‡å‡†ç­”æ¡ˆ")

            # æ£€æŸ¥é¢˜ç›®ç½®ä¿¡åº¦ï¼ˆå¦‚æœæœ‰ï¼‰
            if q.parse_confidence < 0.7:
                q_issues.append(f"è§£æç½®ä¿¡åº¦è¾ƒä½ï¼ˆ{q.parse_confidence:.2f}ï¼‰")
                issues.append(
                    {
                        "type": "low_confidence",
                        "message": f"é¢˜ç›® {q.question_id} è§£æç½®ä¿¡åº¦è¾ƒä½ï¼ˆ{q.parse_confidence:.2f}ï¼‰",
                        "questionId": q.question_id,
                        "severity": "medium",
                    }
                )

            # æ”¶é›†é¢˜ç›®ä¸ç¡®å®šæ€§
            if q.parse_uncertainties:
                for unc in q.parse_uncertainties:
                    uncertainties.append(f"é¢˜ç›® {q.question_id}: {unc}")

            if q_issues:
                questions_with_issues.append(q.question_id)

        # 4. å¾—åˆ†ç‚¹å®Œæ•´æ€§æ£€æŸ¥
        questions_without_points = [q.question_id for q in rubric.questions if not q.scoring_points]
        if questions_without_points:
            quality_checks.append(
                {
                    "check": "å¾—åˆ†ç‚¹å®Œæ•´æ€§æ£€æŸ¥",
                    "passed": False,
                    "detail": f"{len(questions_without_points)} é¢˜ç¼ºå°‘å¾—åˆ†ç‚¹: {', '.join(questions_without_points)}",
                }
            )
            if overall_status == "ok":
                overall_status = "caution"
        else:
            quality_checks.append(
                {"check": "å¾—åˆ†ç‚¹å®Œæ•´æ€§æ£€æŸ¥", "passed": True, "detail": "æ‰€æœ‰é¢˜ç›®éƒ½æœ‰å¾—åˆ†ç‚¹"}
            )

        # 5. æ ‡å‡†ç­”æ¡ˆæ£€æŸ¥
        questions_without_answer = [
            q.question_id for q in rubric.questions if not q.standard_answer
        ]
        if questions_without_answer:
            quality_checks.append(
                {
                    "check": "æ ‡å‡†ç­”æ¡ˆå®Œæ•´æ€§æ£€æŸ¥",
                    "passed": False,
                    "detail": f"{len(questions_without_answer)} é¢˜ç¼ºå°‘æ ‡å‡†ç­”æ¡ˆ",
                }
            )
        else:
            quality_checks.append(
                {"check": "æ ‡å‡†ç­”æ¡ˆå®Œæ•´æ€§æ£€æŸ¥", "passed": True, "detail": "æ‰€æœ‰é¢˜ç›®éƒ½æœ‰æ ‡å‡†ç­”æ¡ˆ"}
            )

        # 6. è®¡ç®—æ•´ä½“ç½®ä¿¡åº¦
        if rubric.overall_parse_confidence < 1.0:
            # ä½¿ç”¨ LLM æä¾›çš„ç½®ä¿¡åº¦
            overall_confidence = rubric.overall_parse_confidence
        else:
            # åŸºäºè´¨é‡æ£€æŸ¥è®¡ç®—ç½®ä¿¡åº¦
            confidence_factors = []

            # é¢˜ç›®æ•°é‡å› ç´ 
            if rubric.total_questions == 0:
                confidence_factors.append(0.0)
            elif rubric.total_questions < 3:
                confidence_factors.append(0.6)
            else:
                confidence_factors.append(0.9)

            # åˆ†å€¼ä¸€è‡´æ€§å› ç´ 
            if abs(calculated_total - rubric.total_score) > 0.1:
                confidence_factors.append(0.7)
            else:
                confidence_factors.append(1.0)

            # å¾—åˆ†ç‚¹å®Œæ•´æ€§å› ç´ 
            if questions_without_points:
                confidence_factors.append(0.5)
            else:
                confidence_factors.append(0.95)

            # é¢˜ç›®ç½®ä¿¡åº¦å¹³å‡å€¼
            if rubric.questions:
                avg_q_confidence = sum(q.parse_confidence for q in rubric.questions) / len(
                    rubric.questions
                )
                confidence_factors.append(avg_q_confidence)

            overall_confidence = (
                sum(confidence_factors) / len(confidence_factors) if confidence_factors else 0.5
            )

        # 7. ç”Ÿæˆæ‘˜è¦
        if overall_status == "ok":
            summary = (
                f"æˆåŠŸè§£æ {rubric.total_questions} é¢˜ï¼Œæ€»åˆ† {rubric.total_score}ï¼Œæ•´ä½“è´¨é‡è‰¯å¥½"
            )
        elif overall_status == "caution":
            summary = f"è§£æ {rubric.total_questions} é¢˜ï¼Œæ€»åˆ† {rubric.total_score}ï¼Œå­˜åœ¨ {len(issues)} ä¸ªé—®é¢˜éœ€è¦æ³¨æ„"
        else:
            summary = f"è§£æå­˜åœ¨ä¸¥é‡é—®é¢˜ï¼Œè¯†åˆ«åˆ° {rubric.total_questions} é¢˜ï¼Œæœ‰ {len([i for i in issues if i['severity'] == 'high'])} ä¸ªé«˜ä¸¥é‡æ€§é—®é¢˜"

        # 8. æ·»åŠ æ•´ä½“ä¸ç¡®å®šæ€§
        if rubric.parse_confession.get("parse_uncertainties"):
            uncertainties.extend(rubric.parse_confession["parse_uncertainties"])

        return {
            "overallStatus": overall_status,
            "overallConfidence": round(overall_confidence, 3),
            "summary": summary,
            "issues": issues,
            "uncertainties": uncertainties,
            "qualityChecks": quality_checks,
            "questionsWithIssues": questions_with_issues,
            "generatedAt": datetime.now().isoformat(),
            "parseMethod": "llm_vision",
        }
