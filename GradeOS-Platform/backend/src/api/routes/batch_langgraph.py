"""æ‰¹é‡æäº¤ API è·¯ç”± - ä½¿ç”¨ LangGraph Orchestrator

æ­£ç¡®çš„æ¶æ„ï¼š
1. ä½¿ç”¨ LangGraph Orchestrator å¯åŠ¨æ‰¹æ”¹æµç¨‹
2. é€šè¿‡ LangGraph çš„æµå¼ API å®æ—¶æ¨é€è¿›åº¦
3. åˆ©ç”¨ PostgreSQL Checkpointer å®ç°æŒä¹…åŒ–å’Œæ–­ç‚¹æ¢å¤
"""

import uuid
import logging
import tempfile
import asyncio
import base64
import json
from datetime import datetime
from pathlib import Path
from typing import List, Optional, Dict, Any

from fastapi import APIRouter, HTTPException, UploadFile, File, Form, WebSocket, WebSocketDisconnect, Depends
from starlette.websockets import WebSocketState
from pydantic import BaseModel, Field
import fitz
from PIL import Image
import os
import redis.asyncio as redis
from redis.exceptions import RedisError

from src.models.enums import SubmissionStatus
from src.orchestration.base import Orchestrator
from src.api.dependencies import get_orchestrator
from src.utils.image import to_jpeg_bytes, pil_to_jpeg_bytes
from src.utils.pool_manager import UnifiedPoolManager, PoolNotInitializedError

# PostgreSQL ä½œä¸ºä¸»å­˜å‚¨
from src.db import (
    GradingHistory,
    StudentGradingResult,
    save_grading_history,
    save_student_result,
    upsert_homework_submission_grade,
    list_class_students,
)


logger = logging.getLogger(__name__)
router = APIRouter(prefix="/batch", tags=["æ‰¹é‡æäº¤"])

# å­˜å‚¨æ´»è·ƒçš„ WebSocket è¿æ¥
active_connections: Dict[str, List[WebSocket]] = {}
# ç¼“å­˜å›¾ç‰‡ï¼Œé¿å… images_ready æ—©äº WebSocket è¿æ¥å¯¼è‡´å‰ç«¯ä¸¢å¤±
batch_image_cache: Dict[str, Dict[str, dict]] = {}
DEBUG_LOG_PATH = os.getenv("GRADEOS_DEBUG_LOG_PATH")
TEACHER_MAX_ACTIVE_RUNS = int(os.getenv("TEACHER_MAX_ACTIVE_RUNS", "3"))
_TEACHER_SEMAPHORE_LOCK = asyncio.Lock()
_TEACHER_SEMAPHORES: Dict[str, asyncio.Semaphore] = {}
REDIS_PROGRESS_TTL_SECONDS = int(os.getenv("REDIS_PROGRESS_TTL_SECONDS", "86400"))
REDIS_PROGRESS_KEY_PREFIX = os.getenv("REDIS_PROGRESS_KEY_PREFIX", "batch_progress")
_REDIS_CACHE_SKIP_TYPES = {"images_ready", "rubric_images_ready", "llm_stream_chunk"}
_REDIS_CLIENT: Optional[redis.Redis] = None
_REDIS_CLIENT_CHECKED: bool = False


def _is_ws_connected(websocket: WebSocket) -> bool:
    return (
        websocket.client_state == WebSocketState.CONNECTED
        and websocket.application_state == WebSocketState.CONNECTED
    )


def _discard_connection(batch_id: str, websocket: WebSocket) -> None:
    connections = active_connections.get(batch_id)
    if not connections:
        return
    try:
        connections.remove(websocket)
    except ValueError:
        return
    if not connections:
        active_connections.pop(batch_id, None)


def _write_debug_log(payload: Dict[str, Any]) -> None:
    if not DEBUG_LOG_PATH:
        return
    try:
        Path(DEBUG_LOG_PATH).parent.mkdir(parents=True, exist_ok=True)
        with open(DEBUG_LOG_PATH, "a", encoding="utf-8") as log_file:
            log_file.write(json.dumps(payload, ensure_ascii=False) + "\n")
    except Exception as exc:
        logger.debug(f"Failed to write debug log: {exc}")


def _progress_cache_key(batch_id: str) -> str:
    return f"{REDIS_PROGRESS_KEY_PREFIX}:{batch_id}"


def _decode_redis_value(value: Any) -> str:
    if isinstance(value, (bytes, bytearray)):
        return value.decode("utf-8", errors="ignore")
    return str(value)


async def _get_redis_client() -> Optional[redis.Redis]:
    global _REDIS_CLIENT, _REDIS_CLIENT_CHECKED
    if _REDIS_CLIENT_CHECKED:
        return _REDIS_CLIENT
    _REDIS_CLIENT_CHECKED = True
    try:
        pool_manager = await UnifiedPoolManager.get_instance()
        if pool_manager.is_initialized:
            _REDIS_CLIENT = pool_manager.get_redis_client()
    except PoolNotInitializedError:
        _REDIS_CLIENT = None
    except Exception as exc:
        logger.debug(f"Redis client unavailable: {exc}")
        _REDIS_CLIENT = None
    return _REDIS_CLIENT


async def _clear_progress_fields(
    redis_client: redis.Redis,
    cache_key: str,
    prefixes: Optional[List[str]] = None,
    fields: Optional[List[str]] = None,
) -> None:
    to_delete: List[Any] = []
    if fields:
        to_delete.extend(fields)
    if prefixes:
        try:
            existing_fields = await redis_client.hkeys(cache_key)
        except RedisError as exc:
            logger.debug(f"Failed to fetch Redis fields for cleanup: {exc}")
            existing_fields = []
        for field in existing_fields:
            field_name = _decode_redis_value(field)
            if any(field_name.startswith(prefix) for prefix in prefixes):
                to_delete.append(field)
    if not to_delete:
        return
    try:
        await redis_client.hdel(cache_key, *to_delete)
    except RedisError as exc:
        logger.debug(f"Failed to cleanup Redis progress cache: {exc}")


async def _cache_progress_message(batch_id: str, message: dict) -> None:
    msg_type = message.get("type", "unknown")
    if msg_type in _REDIS_CACHE_SKIP_TYPES:
        return

    redis_client = await _get_redis_client()
    if not redis_client:
        return

    cache_key = _progress_cache_key(batch_id)
    field = msg_type
    if msg_type == "workflow_update":
        node_id = message.get("nodeId")
        if node_id:
            field = f"{msg_type}:{node_id}"

    try:
        payload = json.dumps(message, ensure_ascii=False)
        await redis_client.hset(cache_key, field, payload)
        await redis_client.expire(cache_key, REDIS_PROGRESS_TTL_SECONDS)
        if msg_type in ("review_completed", "workflow_completed"):
            await _clear_progress_fields(
                redis_client,
                cache_key,
                fields=["review_required"],
            )
    except RedisError as exc:
        logger.debug(f"Failed to cache progress message in Redis: {exc}")


async def _load_cached_progress_messages(batch_id: str) -> List[dict]:
    redis_client = await _get_redis_client()
    if not redis_client:
        return []
    cache_key = _progress_cache_key(batch_id)
    try:
        entries = await redis_client.hgetall(cache_key)
    except RedisError as exc:
        logger.debug(f"Failed to fetch cached progress from Redis: {exc}")
        return []

    messages: List[dict] = []
    for field in sorted(entries.keys(), key=_decode_redis_value):
        payload = entries.get(field)
        if payload is None:
            continue
        raw = _decode_redis_value(payload)
        try:
            message = json.loads(raw)
        except json.JSONDecodeError:
            continue
        if isinstance(message, dict):
            messages.append(message)
    return messages


def _safe_to_jpeg_bytes(image_bytes: bytes, label: str) -> bytes:
    try:
        return to_jpeg_bytes(image_bytes)
    except Exception as exc:
        logger.warning(f"Failed to convert image to JPEG ({label}): {exc}")
        return image_bytes



def _normalize_teacher_key(teacher_id: Optional[str]) -> str:
    if teacher_id and teacher_id.strip():
        return teacher_id.strip()
    return "anonymous"


async def _get_teacher_semaphore(teacher_key: str) -> asyncio.Semaphore:
    async with _TEACHER_SEMAPHORE_LOCK:
        semaphore = _TEACHER_SEMAPHORES.get(teacher_key)
        if not semaphore:
            semaphore = asyncio.Semaphore(max(1, TEACHER_MAX_ACTIVE_RUNS))
            _TEACHER_SEMAPHORES[teacher_key] = semaphore
    return semaphore


class BatchSubmissionResponse(BaseModel):
    """æ‰¹é‡æäº¤å“åº”"""
    batch_id: str = Field(..., description="æ‰¹æ¬¡ ID")
    status: SubmissionStatus = Field(..., description="çŠ¶æ€")
    total_pages: int = Field(..., description="æ€»é¡µæ•°")
    estimated_completion_time: int = Field(..., description="é¢„è®¡å®Œæˆæ—¶é—´ï¼ˆç§’ï¼‰")


class BatchStatusResponse(BaseModel):
    """æ‰¹é‡çŠ¶æ€æŸ¥è¯¢å“åº”"""
    batch_id: str
    exam_id: str
    status: str
    total_students: int = Field(0, description="è¯†åˆ«åˆ°çš„å­¦ç”Ÿæ•°")
    completed_students: int = Field(0, description="å·²å®Œæˆæ‰¹æ”¹çš„å­¦ç”Ÿæ•°")
    unidentified_pages: int = Field(0, description="æœªè¯†åˆ«å­¦ç”Ÿçš„é¡µæ•°")
    results: Optional[List[dict]] = Field(None, description="æ‰¹æ”¹ç»“æœ")


class RubricReviewContextResponse(BaseModel):
    """å‰ç«¯ rubric review ä¸Šä¸‹æ–‡"""
    batch_id: str
    status: Optional[str] = None
    current_stage: Optional[str] = None
    parsed_rubric: Optional[dict] = None
    rubric_images: List[str] = []


class ResultsReviewContextResponse(BaseModel):
    """å‰ç«¯ results review ä¸Šä¸‹æ–‡"""
    batch_id: str
    status: Optional[str] = None
    current_stage: Optional[str] = None
    student_results: List[dict] = []
    answer_images: List[str] = []


def _pdf_to_images(pdf_path: str, dpi: int = 150) -> List[bytes]:
    """å°† PDF è½¬æ¢ä¸ºå›¾åƒåˆ—è¡¨"""
    pdf_doc = fitz.open(pdf_path)
    images = []
    
    for page_num in range(len(pdf_doc)):
        page = pdf_doc[page_num]
        mat = fitz.Matrix(dpi/72, dpi/72)
        pix = page.get_pixmap(matrix=mat)
        img = Image.frombytes("RGB", [pix.width, pix.height], pix.samples)
        
        images.append(pil_to_jpeg_bytes(img))
    
    pdf_doc.close()
    return images


async def broadcast_progress(batch_id: str, message: dict):
    """å‘æ‰€æœ‰è¿æ¥çš„ WebSocket å®¢æˆ·ç«¯å¹¿æ’­è¿›åº¦"""
    # #region agent log - å‡è®¾J: broadcast_progress è¢«è°ƒç”¨
    msg_type = message.get("type", "unknown")
    if msg_type in ("images_ready", "rubric_images_ready", "review_required"):
        cached = batch_image_cache.setdefault(batch_id, {})
        cached[msg_type] = message
    if msg_type == "llm_stream_chunk":
        node_id = message.get("nodeId") or ""
        if node_id in ("rubric_parse", "rubric_review"):
            cached = batch_image_cache.setdefault(batch_id, {})
            stream_cache = cached.setdefault("llm_stream_cache", {})
            cache_key = f"{node_id}:{message.get('agentId') or 'all'}:{message.get('streamType') or 'output'}"
            existing = stream_cache.get(cache_key, {})
            chunk_data = message.get("chunk", "") or ""
            if isinstance(chunk_data, list):
                chunk_data = "".join([str(c) for c in chunk_data])
            else:
                chunk_data = str(chunk_data)
            
            existing_chunk = existing.get("chunk", "") or ""
            combined = existing_chunk + chunk_data
            max_chars = 12000
            if len(combined) > max_chars:
                combined = combined[-max_chars:]
            stream_cache[cache_key] = {
                **message,
                "chunk": combined,
            }
    if msg_type in ("review_completed", "workflow_completed"):
        cached = batch_image_cache.get(batch_id)
        if cached and "review_required" in cached:
            cached.pop("review_required", None)
        if cached and "llm_stream_cache" in cached:
            cached.pop("llm_stream_cache", None)
    if msg_type == "workflow_completed":
        import traceback as _tb_j
        stack = ''.join(_tb_j.format_stack()[-5:-1])  # è·å–è°ƒç”¨æ ˆ
        _write_debug_log({
            "hypothesisId": "J",
            "location": "batch_langgraph.py:broadcast_progress",
            "message": "broadcast_progresså‘é€workflow_completed",
            "data": {
                "batch_id": batch_id,
                "results_count": len(message.get("results", [])),
                "stack_trace": stack[:500],
            },
            "timestamp": int(datetime.now().timestamp() * 1000),
            "sessionId": "debug-session",
        })
    # #endregion
    await _cache_progress_message(batch_id, message)
    if batch_id in active_connections:
        disconnected = []
        for ws in active_connections[batch_id]:
            if not _is_ws_connected(ws):
                disconnected.append(ws)
                continue
            try:
                await ws.send_json(message)
            except Exception as e:
                logger.error(f"WebSocket å‘é€å¤±è´¥: {e}")
                disconnected.append(ws)
        
        # ç§»é™¤æ–­å¼€çš„è¿æ¥
        for ws in disconnected:
            _discard_connection(batch_id, ws)



async def _start_run_with_teacher_limit(
    *,
    teacher_key: str,
    batch_id: str,
    payload: Dict[str, Any],
    orchestrator: Orchestrator,
    class_id: Optional[str],
    homework_id: Optional[str],
    student_mapping: List[dict],
) -> Optional[str]:
    semaphore = await _get_teacher_semaphore(teacher_key)
    await semaphore.acquire()
    run_id: Optional[str] = None
    try:
        run_id = await orchestrator.start_run(
            graph_name="batch_grading",
            payload=payload,
            idempotency_key=batch_id
        )
        logger.info(
            f"LangGraph ??????? "
            f"batch_id={batch_id}, "
            f"run_id={run_id}"
        )
        asyncio.create_task(
            stream_langgraph_progress(
                batch_id=batch_id,
                run_id=run_id,
                orchestrator=orchestrator,
                class_id=class_id,
                homework_id=homework_id,
                student_mapping=student_mapping,
                teacher_key=teacher_key,
                teacher_semaphore=semaphore,
            )
        )
        return run_id
    except Exception as exc:
        logger.error(f"????????: {exc}", exc_info=True)
        semaphore.release()
        await broadcast_progress(batch_id, {
            "type": "workflow_update",
            "nodeId": "rubric_parse",
            "status": "failed",
            "message": "Queued run failed to start"
        })
        return None


@router.post("/submit", response_model=BatchSubmissionResponse)
async def submit_batch(
    exam_id: Optional[str] = Form(None, description="è€ƒè¯• ID"),
    rubrics: List[UploadFile] = File(default=[], description="è¯„åˆ†æ ‡å‡† PDFï¼ˆå¯é€‰ï¼‰"),
    files: List[UploadFile] = File(..., description="å­¦ç”Ÿä½œç­” PDF"),
    api_key: Optional[str] = Form(None, description="LLM API Key"),
    teacher_id: Optional[str] = Form(None, description="?? ID"),
    auto_identify: bool = Form(True, description="æ˜¯å¦è‡ªåŠ¨è¯†åˆ«å­¦ç”Ÿèº«ä»½"),
    student_boundaries: Optional[str] = Form(None, description="æ‰‹åŠ¨è®¾ç½®çš„å­¦ç”Ÿè¾¹ç•Œ (JSON List of page indices)"),
    expected_students: Optional[int] = Form(None, description="é¢„æœŸå­¦ç”Ÿæ•°é‡ï¼ˆå¼ºçƒˆå»ºè®®æä¾›ï¼Œç”¨äºæ›´å‡†ç¡®çš„åˆ†å‰²ï¼‰"),
    # æ–°å¢ï¼šç­çº§æ‰¹æ”¹ä¸Šä¸‹æ–‡
    class_id: Optional[str] = Form(None, description="ç­çº§ IDï¼ˆç”¨äºæˆç»©å†™å›ï¼‰"),
    homework_id: Optional[str] = Form(None, description="ä½œä¸š IDï¼ˆç”¨äºæˆç»©å†™å›ï¼‰"),
    student_mapping_json: Optional[str] = Form(None, description="å­¦ç”Ÿæ˜ å°„ JSON [{studentId, studentName, startIndex, endIndex}]"),
    enable_review: bool = Form(True, description="æ˜¯å¦å¯ç”¨äººå·¥äº¤äº’"),
    grading_mode: Optional[str] = Form(None, description="grading mode: standard/assist_teacher/assist_student/auto"),
    orchestrator: Orchestrator = Depends(get_orchestrator)
):
    """
    æ‰¹é‡æäº¤è¯•å·å¹¶è¿›è¡Œæ‰¹æ”¹ï¼ˆä½¿ç”¨ LangGraph Orchestratorï¼‰
    
    æ­£ç¡®çš„æ¶æ„ï¼š
    1. ä½¿ç”¨ LangGraph Orchestrator å¯åŠ¨ batch_grading Graph
    2. Graph è‡ªåŠ¨å¤„ç†ï¼šè¾¹ç•Œæ£€æµ‹ â†’ å¹¶è¡Œæ‰¹æ”¹ â†’ èšåˆ â†’ æŒä¹…åŒ– â†’ é€šçŸ¥
    3. é€šè¿‡ WebSocket å®æ—¶æ¨é€ LangGraph çš„æ‰§è¡Œè¿›åº¦
    
    Args:
        exam_id: è€ƒè¯• ID
        rubrics: è¯„åˆ†æ ‡å‡† PDF æ–‡ä»¶åˆ—è¡¨
        files: å­¦ç”Ÿä½œç­” PDF æ–‡ä»¶åˆ—è¡¨
        api_key: LLM API Key
        auto_identify: æ˜¯å¦å¯ç”¨è‡ªåŠ¨å­¦ç”Ÿè¯†åˆ«
        orchestrator: LangGraph Orchestratorï¼ˆä¾èµ–æ³¨å…¥ï¼‰
        
    Returns:
        BatchSubmissionResponse: æ‰¹æ¬¡ä¿¡æ¯
    """
    # #region agent log - å‡è®¾K: submit_batch è¢«è°ƒç”¨
    _write_debug_log({
        "hypothesisId": "K",
        "location": "batch_langgraph.py:submit_batch:entry",
        "message": "submit_batchç«¯ç‚¹è¢«è°ƒç”¨",
        "data": {"files_count": len(files), "rubrics_count": len(rubrics)},
        "timestamp": int(datetime.now().timestamp() * 1000),
        "sessionId": "debug-session",
    })
    # #endregion
    # æ£€æŸ¥ orchestrator æ˜¯å¦å¯ç”¨
    if not orchestrator:
        raise HTTPException(
            status_code=503, 
            detail="æ‰¹æ”¹æœåŠ¡æœªåˆå§‹åŒ–ï¼Œè¯·ç¨åé‡è¯•æˆ–æ£€æŸ¥æœåŠ¡é…ç½®"
        )
    
    if not api_key:
        api_key = os.getenv("LLM_API_KEY") or os.getenv("OPENROUTER_API_KEY")
    
    if not api_key:
        raise HTTPException(
            status_code=400,
            detail="æœªæä¾› API Keyï¼Œè¯·åœ¨è¯·æ±‚ä¸­æä¾›æˆ–é…ç½®ç¯å¢ƒå˜é‡ LLM_API_KEY/OPENROUTER_API_KEY"
        )


    
    # è§£æå­¦ç”Ÿè¾¹ç•Œ
    parsed_boundaries = []
    if student_boundaries:
        try:
            logger.info(f"æ¥æ”¶åˆ°åŸå§‹ student_boundaries: {student_boundaries} (type: {type(student_boundaries)})")
            import json
            parsed_boundaries = json.loads(student_boundaries)
            logger.info(f"è§£æåçš„ manual_boundaries: {parsed_boundaries}")
        except Exception as e:
            logger.warning(f"è§£ææ‰‹åŠ¨å­¦ç”Ÿè¾¹ç•Œå¤±è´¥: {e}")

    if not exam_id:
        exam_id = str(uuid.uuid4())

    batch_id = str(uuid.uuid4())
    
    logger.info(
        f"æ”¶åˆ°æ‰¹é‡æäº¤ï¼ˆLangGraphï¼‰: "
        f"batch_id={batch_id}, "
        f"exam_id={exam_id}, "
        f"auto_identify={auto_identify}"
    )
    
    temp_dir = None
    try:
        # åˆ›å»ºä¸´æ—¶ç›®å½•
        temp_dir = tempfile.mkdtemp()
        temp_path = Path(temp_dir)
        
        # === å¤„ç†ç­”é¢˜æ–‡ä»¶ï¼ˆæ”¯æŒå›¾ç‰‡åˆ—è¡¨æˆ–å•ä¸ª PDFï¼‰===
        answer_images = []
        
        for idx, file in enumerate(files):
            file_name = file.filename or f"file_{idx}"
            content = await file.read()
            
            # æ£€æŸ¥æ–‡ä»¶ç±»å‹
            if file_name.lower().endswith(('.png', '.jpg', '.jpeg', '.webp', '.gif')):
                # å›¾ç‰‡æ–‡ä»¶ï¼šç›´æ¥ä½¿ç”¨å†…å®¹
                answer_images.append(_safe_to_jpeg_bytes(content, file_name))
                logger.debug(f"è¯»å–å›¾ç‰‡æ–‡ä»¶: {file_name}, å¤§å°: {len(content)} bytes")
            elif file_name.lower().endswith('.pdf'):
                # PDF æ–‡ä»¶ï¼šè½¬æ¢ä¸ºå›¾åƒ
                pdf_path = temp_path / f"answer_{idx}.pdf"
                with open(pdf_path, "wb") as f:
                    f.write(content)
                loop = asyncio.get_event_loop()
                pdf_images = await loop.run_in_executor(None, _pdf_to_images, str(pdf_path), 150)
                answer_images.extend(pdf_images)
                logger.info(f"PDF æ–‡ä»¶ {file_name} è½¬æ¢ä¸º {len(pdf_images)} é¡µå›¾ç‰‡")
            elif file_name.lower().endswith('.txt'):
                # æ–‡æœ¬æ–‡ä»¶ï¼šç›´æ¥ä½¿ç”¨å†…å®¹
                answer_images.append(content)
                logger.info(f"æ–‡æœ¬æ–‡ä»¶å¤„ç†å®Œæˆ: {file_name}, å†…å®¹é•¿åº¦={len(content)}")
            else:
                # å°è¯•ä½œä¸ºå›¾ç‰‡å¤„ç†ï¼ˆå¯èƒ½æ²¡æœ‰æ‰©å±•åï¼‰
                answer_images.append(_safe_to_jpeg_bytes(content, file_name))
                logger.warning(f"æœªçŸ¥æ–‡ä»¶ç±»å‹ {file_name}ï¼Œå°è¯•ä½œä¸ºå›¾ç‰‡å¤„ç†")
        
        total_pages = len(answer_images)
        logger.info(f"ç­”é¢˜æ–‡ä»¶å¤„ç†å®Œæˆ: batch_id={batch_id}, æ€»é¡µæ•°={total_pages}")
        
        # === å¤„ç†è¯„åˆ†æ ‡å‡†ï¼ˆå¯é€‰ï¼‰===
        # Convert images to base64 and cache them immediately
        # (Fix: Rubric images not displaying on frontend)
        if answer_images:
            try:
                base64_images = [base64.b64encode(img).decode('utf-8') for img in answer_images]
                
                # Cache for direct WebSocket connection
                batch_image_cache.setdefault(batch_id, {})["images_ready"] = {
                    "type": "images_ready",
                    "images": base64_images
                }
                
                # Broadcast (though no clients connected yet usually)
                await broadcast_progress(batch_id, {
                    "type": "images_ready",
                    "images": base64_images
                })
                logger.info(f"å·²ç¼“å­˜ {len(base64_images)} å¼ å›¾ç‰‡ç”¨äºå‰ç«¯æ˜¾ç¤º")
            except Exception as e:
                logger.error(f"å›¾ç‰‡ Base64 è½¬æ¢å¤±è´¥: {e}")

        # === å¤„ç†è¯„åˆ†æ ‡å‡†ï¼ˆå¯é€‰ï¼‰===
        rubric_images = []
        if rubrics and len(rubrics) > 0:
            for idx, rubric_file in enumerate(rubrics):
                rubric_name = rubric_file.filename or f"rubric_{idx}"
                rubric_content = await rubric_file.read()
                
                if rubric_name.lower().endswith(('.png', '.jpg', '.jpeg', '.webp', '.gif')):
                    rubric_images.append(_safe_to_jpeg_bytes(rubric_content, rubric_name))
                elif rubric_name.lower().endswith('.pdf'):
                    rubric_path = temp_path / f"rubric_{idx}.pdf"
                    with open(rubric_path, "wb") as f:
                        f.write(rubric_content)
                    loop = asyncio.get_event_loop()
                    pdf_rubric_images = await loop.run_in_executor(None, _pdf_to_images, str(rubric_path), 150)
                    rubric_images.extend(pdf_rubric_images)
                else:
                    rubric_images.append(_safe_to_jpeg_bytes(rubric_content, rubric_name))
            
            logger.info(f"è¯„åˆ†æ ‡å‡†å¤„ç†å®Œæˆ: batch_id={batch_id}, æ€»é¡µæ•°={len(rubric_images)}")
            if rubric_images:
                try:
                    base64_rubric_images = [base64.b64encode(img).decode("utf-8") for img in rubric_images]
                    batch_image_cache.setdefault(batch_id, {})["rubric_images_ready"] = {
                        "type": "rubric_images_ready",
                        "images": base64_rubric_images
                    }
                    await broadcast_progress(batch_id, {
                        "type": "rubric_images_ready",
                        "images": base64_rubric_images
                    })
                    logger.info(f"å·²ç¼“å­˜ {len(base64_rubric_images)} å¼ è¯„åˆ†æ ‡å‡†å›¾ç‰‡ç”¨äºå‰ç«¯æ˜¾ç¤º")
                except Exception as e:
                    logger.error(f"è¯„åˆ†æ ‡å‡† Base64 è½¬æ¢å¤±è´¥: {e}")
        else:
            logger.info(f"æœªæä¾›è¯„åˆ†æ ‡å‡†ï¼Œå°†ä½¿ç”¨é»˜è®¤è¯„åˆ†: batch_id={batch_id}")
        
        logger.info(
            f"æ–‡ä»¶å¤„ç†å®Œæˆ: "
            f"batch_id={batch_id}, "
            f"rubric_pages={len(rubric_images)}, "
            f"answer_pages={total_pages}"
        )
        
        # ğŸš€ ä½¿ç”¨ LangGraph Orchestrator å¯åŠ¨æ‰¹æ”¹æµç¨‹
        
        # è§£æå­¦ç”Ÿæ˜ å°„ï¼ˆç­çº§æ‰¹æ”¹æ¨¡å¼ï¼‰
        student_mapping = []
        if student_mapping_json:
            try:
                import json
                student_mapping = json.loads(student_mapping_json)
                logger.info(f"ç­çº§æ‰¹æ”¹æ¨¡å¼: class_id={class_id}, homework_id={homework_id}, å­¦ç”Ÿæ•°={len(student_mapping)}")
            except Exception as e:
                logger.warning(f"è§£æå­¦ç”Ÿæ˜ å°„å¤±è´¥: {e}")
        
        payload = {
            "batch_id": batch_id,
            "exam_id": exam_id,
            "temp_dir": str(temp_path),  # ä¸´æ—¶ç›®å½•ï¼ˆç”¨äºæ¸…ç†ï¼‰
            "rubric_images": rubric_images,
            "answer_images": answer_images,
            "api_key": api_key,
            # ç­çº§æ‰¹æ”¹ä¸Šä¸‹æ–‡ï¼ˆå¯é€‰ï¼‰
            "class_id": class_id,
            "homework_id": homework_id,
            "student_mapping": student_mapping,
            "inputs": {
                "rubric": "rubric_content",  # TODO: è§£æ rubric
                "auto_identify": auto_identify,
                "manual_boundaries": parsed_boundaries,  # ä¼ é€’äººå·¥è¾¹ç•Œ
                "expected_students": expected_students if expected_students else 2,  # ğŸ”¥ é»˜è®¤ 2 åå­¦ç”Ÿ
                "enable_review": enable_review,
                "grading_mode": grading_mode or "auto",
            }
        }
        
        # å¯åŠ¨ LangGraph batch_grading Graph
        teacher_key = _normalize_teacher_key(teacher_id)
        teacher_semaphore = await _get_teacher_semaphore(teacher_key)
        if teacher_semaphore.locked():
            await broadcast_progress(batch_id, {
                "type": "workflow_update",
                "nodeId": "rubric_parse",
                "status": "pending",
                "message": "Queued: waiting for teacher slot"
            })
        asyncio.create_task(
            _start_run_with_teacher_limit(
                teacher_key=teacher_key,
                batch_id=batch_id,
                payload=payload,
                orchestrator=orchestrator,
                class_id=class_id,
                homework_id=homework_id,
                student_mapping=student_mapping,
            )
        )

        return BatchSubmissionResponse(
            batch_id=batch_id,
            status=SubmissionStatus.UPLOADED,
            total_pages=total_pages,
            estimated_completion_time=total_pages * 3  # Estimated: 3s per page
        )
        
    except Exception as e:
        logger.error(f"æ‰¹é‡æäº¤å¤±è´¥: {str(e)}", exc_info=True)
        raise HTTPException(status_code=500, detail=f"æ‰¹é‡æäº¤å¤±è´¥: {str(e)}")


async def stream_langgraph_progress(
    batch_id: str,
    run_id: str,
    orchestrator: Orchestrator,
    class_id: Optional[str] = None,
    homework_id: Optional[str] = None,
    student_mapping: Optional[List[dict]] = None,
    teacher_key: Optional[str] = None,
    teacher_semaphore: Optional[asyncio.Semaphore] = None
):
    """
    æµå¼ç›‘å¬ LangGraph æ‰§è¡Œè¿›åº¦å¹¶æ¨é€åˆ° WebSocket
    
    è¿™æ˜¯å®ç°å®æ—¶è¿›åº¦æ¨é€çš„å…³é”®å‡½æ•°ï¼
    
    Args:
        batch_id: æ‰¹æ¬¡ ID
        run_id: LangGraph è¿è¡Œ ID
        orchestrator: LangGraph Orchestrator
    """
    # #region agent log - å‡è®¾G: stream_langgraph_progress å…¥å£
    _write_debug_log({
        "hypothesisId": "G",
        "location": "batch_langgraph.py:stream_langgraph_progress:entry",
        "message": "stream_langgraph_progresså‡½æ•°è¢«è°ƒç”¨",
        "data": {"batch_id": batch_id, "run_id": run_id},
        "timestamp": int(datetime.now().timestamp() * 1000),
        "sessionId": "debug-session",
    })
    # #endregion
    logger.info(f"å¼€å§‹æµå¼ç›‘å¬ LangGraph è¿›åº¦: batch_id={batch_id}, run_id={run_id}")
    
    try:
        # ğŸ”¥ ä½¿ç”¨ LangGraph çš„æµå¼ API
        async for event in orchestrator.stream_run(run_id):
            event_type = event.get("type")
            node_name = event.get("node")
            data = event.get("data", {})
            
            logger.debug(
                f"LangGraph äº‹ä»¶: "
                f"batch_id={batch_id}, "
                f"type={event_type}, "
                f"node={node_name}"
            )
            
            # å°† LangGraph äº‹ä»¶è½¬æ¢ä¸ºå‰ç«¯ WebSocket æ¶ˆæ¯
            if event_type == "node_start":
                await broadcast_progress(batch_id, {
                    "type": "workflow_update",
                    "nodeId": _map_node_to_frontend(node_name),
                    "status": "running",
                    "message": f"Running {_get_node_display_name(node_name)}..."
                })
            
            elif event_type == "node_end":
                await broadcast_progress(batch_id, {
                    "type": "workflow_update",
                    "nodeId": _map_node_to_frontend(node_name),
                    "status": "completed",
                    "message": f"{_get_node_display_name(node_name)} completed"
                })
                
                # å¤„ç†èŠ‚ç‚¹è¾“å‡º
                output = data.get("output", {})
                if isinstance(output, dict):
                    interrupt_payload = output.get("__interrupt__")
                    if interrupt_payload:
                        review_type = interrupt_payload.get("type") if isinstance(interrupt_payload, dict) else "review_required"
                        await broadcast_progress(batch_id, {
                            "type": "review_required",
                            "reviewType": review_type,
                            "payload": interrupt_payload,
                            "nodeId": _map_node_to_frontend("rubric_review") if "rubric" in review_type else _map_node_to_frontend("review"),
                        })
                    # è¯„åˆ†æ ‡å‡†è§£æå®Œæˆ
                    if node_name == "rubric_parse" and output.get("parsed_rubric"):
                        parsed = output["parsed_rubric"]
                        await broadcast_progress(batch_id, {
                            "type": "rubric_parsed",
                            "totalQuestions": parsed.get("total_questions", 0),
                            "totalScore": parsed.get("total_score", 0),
                            "generalNotes": parsed.get("general_notes", ""),
                            "rubricFormat": parsed.get("rubric_format", ""),
                            "questions": [
                                {
                                    "questionId": q.get("question_id", ""),
                                    "maxScore": q.get("max_score", 0),
                                    "questionText": q.get("question_text", ""),
                                    "standardAnswer": q.get("standard_answer", ""),
                                    "gradingNotes": q.get("grading_notes", ""),
                                    "sourcePages": q.get("source_pages") or q.get("sourcePages") or [],
                                    "scoringPoints": [
                                        {
                                            "pointId": sp.get("point_id") or sp.get("pointId") or f"{q.get('question_id')}.{idx + 1}",
                                            "description": sp.get("description", ""),
                                            "expectedValue": sp.get("expected_value") or sp.get("expectedValue", ""),
                                            "keywords": sp.get("keywords") or [],
                                            "score": sp.get("score", 0),
                                            "isRequired": sp.get("is_required", True),
                                        }
                                        for idx, sp in enumerate(q.get("scoring_points", []))
                                    ],
                                    "deductionRules": [
                                        {
                                            "ruleId": dr.get("rule_id") or dr.get("ruleId") or f"{q.get('question_id')}.d{idx + 1}",
                                            "description": dr.get("description", ""),
                                            "deduction": dr.get("deduction", dr.get("score", 0)),
                                            "conditions": dr.get("conditions") or dr.get("when") or "",
                                        }
                                        for idx, dr in enumerate(q.get("deduction_rules") or q.get("deductionRules") or [])
                                    ],
                                    "alternativeSolutions": [
                                        {
                                            "description": alt.get("description", ""),
                                            "scoringCriteria": alt.get("scoring_criteria", ""),
                                            "note": alt.get("note", ""),
                                        }
                                        for alt in q.get("alternative_solutions", [])
                                    ]
                                }
                                for q in parsed.get("questions", [])
                            ]
                        })
                    
                    # æ‰¹æ”¹æ‰¹æ¬¡å®Œæˆ
                    if node_name == "grade_batch" and output.get("grading_results"):
                        results = output["grading_results"]
                        completed = sum(1 for r in results if r.get("status") == "completed")
                        
                        await broadcast_progress(batch_id, {
                            "type": "batch_complete",
                            "batchSize": len(results),
                            "successCount": completed,
                            "totalScore": sum(r.get("score", 0) for r in results if r.get("status") == "completed"),
                            "pages": [r.get("page_index") for r in results]
                        })
                    
                    # ç´¢å¼•å®Œæˆï¼ˆå­¦ç”Ÿè¯†åˆ«ï¼‰
                    if node_name == "index" and output.get("student_boundaries"):
                        boundaries = output["student_boundaries"]
                        await broadcast_progress(batch_id, {
                            "type": "students_identified",
                            "studentCount": len(boundaries),
                            "students": [
                                {
                                    "studentKey": b.get("student_key", ""),
                                    "startPage": b.get("start_page", 0),
                                    "endPage": b.get("end_page", 0),
                                    "confidence": b.get("confidence", 0),
                                    "needsConfirmation": b.get("needs_confirmation", False)
                                }
                                for b in boundaries
                            ]
                        })
                    
                    # å®¡æ ¸å®Œæˆ
                    if node_name == "review" and output.get("review_summary"):
                        await broadcast_progress(batch_id, {
                            "type": "review_completed",
                            "summary": output["review_summary"]
                        })
                    
                    # è·¨é¡µé¢˜ç›®åˆå¹¶å®Œæˆ
                    if node_name == "cross_page_merge":
                        cross_page_questions = output.get("cross_page_questions", [])
                        merged_questions = output.get("merged_questions", [])
                        if cross_page_questions:
                            await broadcast_progress(batch_id, {
                                "type": "cross_page_detected",
                                "questions": cross_page_questions,
                                "mergedCount": len(merged_questions),
                                "crossPageCount": len(cross_page_questions)
                            })
            
            elif event_type == "paused":
                # å¤„ç† Graph ä¸­æ–­/æš‚åœï¼ˆé€šå¸¸æ˜¯éœ€è¦äººå·¥å®¡æ ¸ï¼‰
                data = event.get("data", {})
                interrupt_value = data.get("interrupt_value")
                
                logger.info(f"LangGraph æš‚åœ: batch_id={batch_id}, interrupt_value={interrupt_value}")
                
                if interrupt_value:
                    # å¦‚æœæœ‰ä¸­æ–­ payloadï¼Œå¹¿æ’­ review_required
                    review_type = interrupt_value.get("type") if isinstance(interrupt_value, dict) else "review_required"
                    await broadcast_progress(batch_id, {
                        "type": "review_required",
                        "reviewType": review_type,
                        "payload": interrupt_value,
                        "nodeId": _map_node_to_frontend("rubric_review") if "rubric" in review_type else _map_node_to_frontend("review"),
                    })
                else:
                    # å¦‚æœæ²¡æœ‰ payloadï¼Œè‡³å°‘é€šçŸ¥çŠ¶æ€å˜æ›´
                    await broadcast_progress(batch_id, {
                        "type": "workflow_update",
                        "status": "paused",
                        "message": "Workflow paused (awaiting input)"
                    })

            elif event_type == "state_update":
                # æ¨é€çŠ¶æ€æ›´æ–°
                state = data.get("state", {})
                
                # æ‰¹æ¬¡è¿›åº¦æ›´æ–°
                if state.get("progress"):
                    progress = state["progress"]
                    await broadcast_progress(batch_id, {
                        "type": "batch_progress",
                        "batchIndex": progress.get("current_batch", 0),
                        "totalBatches": progress.get("total_batches", 1),
                        "successCount": progress.get("success_count", 0),
                        "failureCount": progress.get("failure_count", 0)
                    })
                
                # ç™¾åˆ†æ¯”è¿›åº¦
                if state.get("percentage"):
                    await broadcast_progress(batch_id, {
                        "type": "grading_progress",
                        "percentage": state["percentage"],
                        "currentStage": state.get("current_stage", "")
                    })
            
            elif event_type == "llm_stream":
                # Real-time LLM token streaming
                node_name = event.get("node") or data.get("node", "")
                chunk = data.get("chunk") or data.get("content") or ""
                await broadcast_progress(batch_id, {
                    "type": "llm_stream_chunk",
                    "nodeId": _map_node_to_frontend(node_name) if node_name else None,
                    "nodeName": _get_node_display_name(node_name) if node_name else None,
                    "chunk": chunk,
                })

            elif event_type == "error":
                await broadcast_progress(batch_id, {
                    "type": "workflow_error",
                    "message": data.get("error", "Unknown error")
                })
            
            elif event_type == "completed":
                # #region agent log - å‡è®¾H: completed äº‹ä»¶
                _write_debug_log({
                    "hypothesisId": "H",
                    "location": "batch_langgraph.py:event_completed",
                    "message": "æ”¶åˆ°completedäº‹ä»¶",
                    "data": {
                        "event_type": event_type,
                        "data_keys": list(data.keys()) if isinstance(data, dict) else str(type(data)),
                    },
                    "timestamp": int(datetime.now().timestamp() * 1000),
                    "sessionId": "debug-session",
                })
                # #endregion
                # å·¥ä½œæµå®Œæˆ - è·å–å®Œæ•´çš„æœ€ç»ˆçŠ¶æ€
                final_state = data.get("state", {})
                
                # ä» student_results è·å–ç»“æœ
                student_results = final_state.get("student_results", [])
                
                # #region agent log - å‡è®¾I: student_results åŸå§‹æ•°æ®
                _write_debug_log({
                    "hypothesisId": "I",
                    "location": "batch_langgraph.py:student_results_raw",
                    "message": "student_resultsåŸå§‹æ•°æ®",
                    "data": {
                        "count": len(student_results),
                        "students": [{"key": r.get("student_key"), "score": r.get("total_score")} for r in student_results],
                    },
                    "timestamp": int(datetime.now().timestamp() * 1000),
                    "sessionId": "debug-session",
                })
                # #endregion
                
                # å¦‚æœæ²¡æœ‰ student_resultsï¼Œå°è¯•ä» orchestrator è·å–æœ€ç»ˆè¾“å‡º
                if not student_results:
                    try:
                        final_output = await orchestrator.get_final_output(run_id)
                        if final_output:
                            student_results = final_output.get("student_results", [])
                            logger.info(f"ä» orchestrator è·å–åˆ° {len(student_results)} ä¸ªå­¦ç”Ÿç»“æœ")
                    except Exception as e:
                        logger.warning(f"è·å–æœ€ç»ˆè¾“å‡ºå¤±è´¥: {e}")
                
                formatted_results = _format_results_for_frontend(student_results)
                class_report = final_state.get("class_report")
                if not class_report and final_state.get("export_data"):
                    class_report = final_state.get("export_data", {}).get("class_report")
                
                # ä¿å­˜æ‰¹æ”¹å†å²ä¸å­¦ç”Ÿç»“æœï¼ˆæ”¯æŒç­çº§/éç­çº§æ¨¡å¼ï¼‰
                try:
                    logger.info(
                        f"ä¿å­˜æ‰¹æ”¹ç»“æœ: batch_id={batch_id}, class_id={class_id}, homework_id={homework_id}"
                    )

                    now = datetime.now().isoformat()
                    scores = [
                        s.get("score") for s in formatted_results
                        if isinstance(s.get("score"), (int, float))
                    ]
                    average_score = None
                    if isinstance(class_report, dict):
                        average_score = class_report.get("average_score")
                    if average_score is None and scores:
                        average_score = round(sum(scores) / len(scores), 2)

                    history_id = str(uuid.uuid4())
                    history = GradingHistory(
                        id=history_id,
                        batch_id=batch_id,
                        status="completed",
                        class_ids=[class_id] if class_id else None,
                        created_at=now,
                        completed_at=now,
                        total_students=len(formatted_results),
                        average_score=average_score,
                        result_data={
                            "summary": class_report,
                            "class_id": class_id,
                            "homework_id": homework_id,
                        } if class_report or class_id or homework_id else None,
                    )
                    save_grading_history(history)

                    student_map_by_index = {}
                    student_map_by_name = {}
                    if student_mapping:
                        for idx, mapping in enumerate(student_mapping):
                            student_map_by_index[idx] = mapping
                            name_key = (mapping.get("studentName") or "").strip().lower()
                            if name_key:
                                student_map_by_name[name_key] = mapping

                    roster = list_class_students(class_id) if class_id else []
                    roster_by_name = {
                        (student.name or student.username or "").strip().lower(): student
                        for student in roster
                        if (student.name or student.username)
                    }

                    for idx, result in enumerate(formatted_results):
                        student_name = (
                            result.get("studentName")
                            or result.get("student_name")
                            or f"Student {idx + 1}"
                        )
                        student_id = result.get("studentId") or result.get("student_id")

                        if not student_id and student_map_by_index.get(idx):
                            mapping = student_map_by_index[idx]
                            student_id = mapping.get("studentId")
                            student_name = mapping.get("studentName") or student_name
                        if not student_id and student_name:
                            mapping = student_map_by_name.get(student_name.strip().lower())
                            if mapping:
                                student_id = mapping.get("studentId")
                                student_name = mapping.get("studentName") or student_name
                        if not student_id and student_name:
                            roster_hit = roster_by_name.get(student_name.strip().lower())
                            if roster_hit:
                                student_id = roster_hit.id
                                student_name = roster_hit.name or roster_hit.username or student_name
                        if not student_id and class_id and idx < len(roster):
                            roster_hit = roster[idx]
                            student_id = roster_hit.id
                            student_name = roster_hit.name or roster_hit.username or student_name
                        if not student_id and class_id:
                            student_id = f"auto-{idx + 1}"

                        student_summary = result.get("studentSummary") or result.get("student_summary") or {}
                        self_audit = result.get("selfAudit") or result.get("self_audit") or {}
                        student_result = StudentGradingResult(
                            id=str(uuid.uuid4()),
                            grading_history_id=history_id,
                            student_key=student_name,
                            score=result.get("score"),
                            max_score=result.get("maxScore") or result.get("max_score"),
                            class_id=class_id,
                            student_id=student_id if class_id else None,
                            summary=student_summary.get("overall") if isinstance(student_summary, dict) else None,
                            self_report=self_audit.get("summary") if isinstance(self_audit, dict) else None,
                            result_data=result,
                        )
                        save_student_result(student_result)

                        if class_id and homework_id and student_id:
                            feedback = None
                            if isinstance(student_summary, dict):
                                feedback = student_summary.get("overall")
                            upsert_homework_submission_grade(
                                class_id=class_id,
                                homework_id=homework_id,
                                student_id=student_id,
                                student_name=student_name,
                                score=result.get("score"),
                                feedback=feedback,
                                grading_batch_id=batch_id,
                            )

                    logger.info(f"æ‰¹æ”¹ç»“æœå·²ä¿å­˜: history_id={history_id}")
                except Exception as e:
                    logger.error(f"ä¿å­˜æ‰¹æ”¹ç»“æœå¤±è´¥: {e}", exc_info=True)

                # #region agent log - å‡è®¾E: WebSocket æ¶ˆæ¯å‘é€
                _write_debug_log({
                    "hypothesisId": "E",
                    "location": "batch_langgraph.py:workflow_completed",
                    "message": "å‘é€workflow_completed",
                    "data": {
                        "student_count": len(formatted_results),
                        "students": [{"name": f.get("studentName"), "score": f.get("score")} for f in formatted_results],
                    },
                    "timestamp": int(datetime.now().timestamp() * 1000),
                    "sessionId": "debug-session",
                })
                # #endregion
                
                await broadcast_progress(batch_id, {
                    "type": "workflow_completed",
                    "message": f"Grading completed, processed {len(formatted_results)} students",
                    "results": formatted_results,
                    "classReport": class_report
                })
        
        logger.info(f"LangGraph è¿›åº¦æµå¼ä¼ è¾“å®Œæˆ: batch_id={batch_id}")
        
    except Exception as e:
        logger.error(
            f"æµå¼ä¼ è¾“å¤±è´¥: batch_id={batch_id}, error={str(e)}",
            exc_info=True
        )
        await broadcast_progress(batch_id, {
            "type": "workflow_error",
            "message": f"æµå¼ä¼ è¾“å¤±è´¥: {str(e)}"
        })

    finally:
        if teacher_semaphore:
            try:
                run_info = await orchestrator.get_status(run_id)
                status_value = run_info.status.value if hasattr(run_info.status, "value") else str(run_info.status)
                if status_value in ("completed", "failed", "cancelled"):
                    teacher_semaphore.release()
            except Exception:
                teacher_semaphore.release()


def _map_node_to_frontend(node_name: str) -> str:
    """å°† LangGraph èŠ‚ç‚¹åç§°æ˜ å°„åˆ°å‰ç«¯èŠ‚ç‚¹ ID
    
    å‰ç«¯å·¥ä½œæµèŠ‚ç‚¹ï¼ˆconsoleStore.ts initialNodesï¼‰ï¼š
    - intake: æ¥æ”¶æ–‡ä»¶
    - rubric_parse: è§£æè¯„åˆ†æ ‡å‡†
    - grade_batch: åˆ†æ‰¹å¹¶è¡Œæ‰¹æ”¹ï¼ˆisParallelContainerï¼‰
    - cross_page_merge: è·¨é¡µé¢˜ç›®åˆå¹¶
    - index: æ‰¹æ”¹å‰ç´¢å¼•
    - index_merge: ç´¢å¼•èšåˆ
    - export: å¯¼å‡ºç»“æœ
    """
    mapping = {
        # ä¸»è¦èŠ‚ç‚¹ï¼ˆä¸åç«¯ batch_grading.py å®Œå…¨å¯¹åº”ï¼‰
        "intake": "intake",
        "rubric_parse": "rubric_parse",
        "rubric_review": "rubric_review",
        "grade_batch": "grade_batch",
        "cross_page_merge": "cross_page_merge",
        "index": "index",
        "index_merge": "index_merge",
        "segment": "index_merge",
        "review": "review",
        "logic_review": "logic_review",
        "export": "export",
        # å…¼å®¹æ—§åç§°
        "detect_boundaries": "index",
        "grade_student": "grade_batch",
        "grading": "grade_batch",
        "aggregate": "review",
        "batch_persist": "export",
        "batch_notify": "export"
    }
    return mapping.get(node_name, node_name)


def _get_node_display_name(node_name: str) -> str:
    """è·å–èŠ‚ç‚¹çš„æ˜¾ç¤ºåç§°ï¼ˆä¸­æ–‡ï¼‰"""
    display_names = {
        "intake": "Ingest",
        "preprocess": "Preprocess",
        "index": "Index",
        "rubric_parse": "Rubric Parse",
        "rubric_review": "Rubric Review",
        "grading_fanout": "Batch Fanout",
        "grade_batch": "Batch Grading",
        "cross_page_merge": "Cross-Page Merge",
        "logic_review": "Logic Review",
        "index_merge": "Index Merge",
        "segment": "Index Merge",
        "review": "Final Review",
        "export": "Export"
    }
    return display_names.get(node_name, node_name)


def _safe_float(value: Any, default: float = 0.0) -> float:
    try:
        if value is None:
            return default
        return float(value)
    except (TypeError, ValueError):
        return default


def _format_results_for_frontend(results: List[Dict]) -> List[Dict]:
    """æ ¼å¼åŒ–æ‰¹æ”¹ç»“æœä¸ºå‰ç«¯æ ¼å¼"""
    # #region agent log - å‡è®¾D: _format_results_for_frontend è¾“å…¥
    _write_debug_log({
        "hypothesisId": "D",
        "location": "batch_langgraph.py:_format_results_for_frontend:input",
        "message": "è¾“å…¥çš„results",
        "data": {
            "count": len(results),
            "students": [{"key": r.get("student_key"), "score": r.get("total_score")} for r in results],
        },
        "timestamp": int(datetime.now().timestamp() * 1000),
        "sessionId": "debug-session",
    })
    # #endregion
    formatted = []
    for r in results:
        # å¤„ç† question_details æ ¼å¼
        question_results = []
        
        # ä¼˜å…ˆä½¿ç”¨ question_details
        if r.get("question_details"):
            for q in r.get("question_details", []):
                scoring_results = q.get("scoring_point_results") or q.get("scoring_results") or []
                question_results.append({
                    "questionId": str(q.get("question_id", "")),
                    "score": q.get("score", 0),
                    "maxScore": q.get("max_score", 0),
                    "feedback": q.get("feedback", ""),
                    "confidence": q.get("confidence", 0),
                    "confidence_reason": q.get("confidence_reason") or q.get("confidenceReason"),
                    "self_critique": q.get("self_critique") or q.get("selfCritique"),
                    "self_critique_confidence": q.get("self_critique_confidence") or q.get("selfCritiqueConfidence"),
                    "rubric_refs": q.get("rubric_refs") or q.get("rubricRefs"),
                    "review_summary": q.get("review_summary") or q.get("reviewSummary"),
                    "review_corrections": q.get("review_corrections") or q.get("reviewCorrections"),
                    "needsReview": (
                        q.get("needs_review")
                        if q.get("needs_review") is not None
                        else q.get("needsReview", False)
                    ),
                    "reviewReasons": (
                        q.get("review_reasons")
                        if q.get("review_reasons") is not None
                        else q.get("reviewReasons") or []
                    ),
                    "auditFlags": (
                        q.get("audit_flags")
                        if q.get("audit_flags") is not None
                        else q.get("auditFlags") or []
                    ),
                    "honesty_note": q.get("honesty_note") or q.get("honestyNote"),
                    "typo_notes": q.get("typo_notes") or q.get("typoNotes"),
                    "studentAnswer": q.get("student_answer", ""),
                    "question_type": q.get("question_type") or q.get("questionType"),
                    "isCorrect": q.get("is_correct", False),
                    "scoring_point_results": scoring_results,
                    "page_indices": q.get("page_indices", []),
                    "is_cross_page": q.get("is_cross_page", False),
                    "merge_source": q.get("merge_source"),
                    # ğŸ”¥ æ‰¹æ³¨åæ ‡å­—æ®µ
                    "annotations": q.get("annotations") or [],
                    "steps": q.get("steps") or [],
                    "answerRegion": q.get("answer_region") or q.get("answerRegion"),
                })
        # å…¼å®¹æ—§æ ¼å¼ grading_results
        elif r.get("grading_results"):
            for q in r.get("grading_results", []):
                scoring_results = q.get("scoring_point_results") or q.get("scoring_results") or []
                question_results.append({
                    "questionId": str(q.get("question_id", "")),
                    "score": q.get("score", 0),
                    "maxScore": q.get("max_score", 0),
                    "feedback": q.get("feedback", ""),
                    "confidence": q.get("confidence", 0),
                    "confidence_reason": q.get("confidence_reason") or q.get("confidenceReason"),
                    "self_critique": q.get("self_critique") or q.get("selfCritique"),
                    "self_critique_confidence": q.get("self_critique_confidence") or q.get("selfCritiqueConfidence"),
                    "rubric_refs": q.get("rubric_refs") or q.get("rubricRefs"),
                    "review_summary": q.get("review_summary") or q.get("reviewSummary"),
                    "review_corrections": q.get("review_corrections") or q.get("reviewCorrections"),
                    "needsReview": (
                        q.get("needs_review")
                        if q.get("needs_review") is not None
                        else q.get("needsReview", False)
                    ),
                    "reviewReasons": (
                        q.get("review_reasons")
                        if q.get("review_reasons") is not None
                        else q.get("reviewReasons") or []
                    ),
                    "auditFlags": (
                        q.get("audit_flags")
                        if q.get("audit_flags") is not None
                        else q.get("auditFlags") or []
                    ),
                    "honesty_note": q.get("honesty_note") or q.get("honestyNote"),
                    "typo_notes": q.get("typo_notes") or q.get("typoNotes"),
                    "studentAnswer": q.get("student_answer", ""),
                    "question_type": q.get("question_type") or q.get("questionType"),
                    "scoring_point_results": scoring_results,
                    "page_indices": q.get("page_indices", []),
                    "is_cross_page": q.get("is_cross_page", False),
                    "merge_source": q.get("merge_source"),
                    # ğŸ”¥ æ‰¹æ³¨åæ ‡å­—æ®µ
                    "annotations": q.get("annotations") or [],
                    "steps": q.get("steps") or [],
                    "answerRegion": q.get("answer_region") or q.get("answerRegion"),
                })
        # Ã¥â€¦Â¼Ã¥Â®Â¹ export_data Ã§Å¡â€ question_results
        elif r.get("question_results"):
            for q in r.get("question_results", []):
                scoring_results = q.get("scoring_point_results") or q.get("scoring_results") or []
                question_results.append({
                    "questionId": str(q.get("question_id", "")),
                    "score": q.get("score", 0),
                    "maxScore": q.get("max_score", 0),
                    "feedback": q.get("feedback", ""),
                    "confidence": q.get("confidence", 0),
                    "confidence_reason": q.get("confidence_reason") or q.get("confidenceReason"),
                    "self_critique": q.get("self_critique") or q.get("selfCritique"),
                    "self_critique_confidence": q.get("self_critique_confidence") or q.get("selfCritiqueConfidence"),
                    "rubric_refs": q.get("rubric_refs") or q.get("rubricRefs"),
                    "review_summary": q.get("review_summary") or q.get("reviewSummary"),
                    "review_corrections": q.get("review_corrections") or q.get("reviewCorrections"),
                    "needsReview": (
                        q.get("needs_review")
                        if q.get("needs_review") is not None
                        else q.get("needsReview", False)
                    ),
                    "reviewReasons": (
                        q.get("review_reasons")
                        if q.get("review_reasons") is not None
                        else q.get("reviewReasons") or []
                    ),
                    "auditFlags": (
                        q.get("audit_flags")
                        if q.get("audit_flags") is not None
                        else q.get("auditFlags") or []
                    ),
                    "honesty_note": q.get("honesty_note") or q.get("honestyNote"),
                    "typo_notes": q.get("typo_notes") or q.get("typoNotes"),
                    "studentAnswer": q.get("student_answer", ""),
                    "question_type": q.get("question_type") or q.get("questionType"),
                    "isCorrect": q.get("is_correct", False),
                    "scoring_point_results": scoring_results,
                    "page_indices": q.get("page_indices", []),
                    "is_cross_page": q.get("is_cross_page", False),
                    "merge_source": q.get("merge_source"),
                    # ğŸ”¥ æ‰¹æ³¨åæ ‡å­—æ®µ
                    "annotations": q.get("annotations") or [],
                    "steps": q.get("steps") or [],
                    "answerRegion": q.get("answer_region") or q.get("answerRegion"),
                })
        # ä» page_results æå–
        elif r.get("page_results"):
            for page in r.get("page_results", []):
                if page.get("status") == "completed":
                    # ä»é¡µé¢ç»“æœä¸­æå–é¢˜ç›®è¯¦æƒ…
                    for q in page.get("question_details", []):
                        scoring_results = q.get("scoring_point_results") or q.get("scoring_results") or []
                        page_indices = q.get("page_indices")
                        if not page_indices and page.get("page_index") is not None:
                            page_indices = [page.get("page_index")]
                        question_results.append({
                            "questionId": str(q.get("question_id", "")),
                            "score": q.get("score", 0),
                            "maxScore": q.get("max_score", 0),
                            "feedback": q.get("feedback", ""),
                            "confidence": q.get("confidence", 0),
                            "confidence_reason": q.get("confidence_reason") or q.get("confidenceReason"),
                            "self_critique": q.get("self_critique") or q.get("selfCritique"),
                            "self_critique_confidence": q.get("self_critique_confidence") or q.get("selfCritiqueConfidence"),
                            "rubric_refs": q.get("rubric_refs") or q.get("rubricRefs"),
                            "review_summary": q.get("review_summary") or q.get("reviewSummary"),
                            "review_corrections": q.get("review_corrections") or q.get("reviewCorrections"),
                            "needsReview": (
                                q.get("needs_review")
                                if q.get("needs_review") is not None
                                else q.get("needsReview", False)
                            ),
                            "reviewReasons": (
                                q.get("review_reasons")
                                if q.get("review_reasons") is not None
                                else q.get("reviewReasons") or []
                            ),
                            "auditFlags": (
                                q.get("audit_flags")
                                if q.get("audit_flags") is not None
                                else q.get("auditFlags") or []
                            ),
                            "honesty_note": q.get("honesty_note") or q.get("honestyNote"),
                            "typo_notes": q.get("typo_notes") or q.get("typoNotes"),
                            "studentAnswer": q.get("student_answer", ""),
                            "question_type": q.get("question_type") or q.get("questionType"),
                            "isCorrect": q.get("is_correct", False),
                            "scoring_point_results": scoring_results,
                            "page_indices": page_indices or [],
                            "is_cross_page": q.get("is_cross_page", False),
                            "merge_source": q.get("merge_source"),
                            # ğŸ”¥ æ‰¹æ³¨åæ ‡å­—æ®µ
                            "annotations": q.get("annotations") or [],
                            "steps": q.get("steps") or [],
                            "answerRegion": q.get("answer_region") or q.get("answerRegion"),
                        })
        
        computed_score = sum(_safe_float(q.get("score", 0)) for q in question_results)
        computed_max = sum(_safe_float(q.get("maxScore", 0)) for q in question_results)
        raw_score = _safe_float(r.get("total_score", r.get("score", 0)))
        raw_max = _safe_float(r.get("max_total_score", r.get("max_score", 0)))
        final_score = raw_score if raw_score > 0 or computed_score <= 0 else computed_score
        final_max = raw_max if raw_max > 0 or computed_max <= 0 else computed_max

        student_summary = r.get("student_summary") or r.get("studentSummary")
        self_audit = r.get("self_audit") or r.get("selfAudit")
        self_report_raw = r.get("self_report") or r.get("selfReport") or r.get("confession")
        
        # æ ‡å‡†åŒ– selfReport æ ¼å¼ï¼Œç¡®ä¿å‰ç«¯èƒ½æ­£ç¡®æ˜¾ç¤º
        self_report = None
        if self_report_raw and isinstance(self_report_raw, dict):
            self_report = {}
            # å¤åˆ¶æ‰€æœ‰åŸå§‹å­—æ®µ
            self_report.update(self_report_raw)
            # ç¡®ä¿ overallStatus å­˜åœ¨
            if "overallStatus" not in self_report and "overall_status" in self_report_raw:
                self_report["overallStatus"] = self_report_raw["overall_status"]
            elif "overallStatus" not in self_report and "overall_confidence" in self_report_raw:
                conf = self_report_raw.get("overall_confidence", 0)
                if conf >= 0.8:
                    self_report["overallStatus"] = "ok"
                elif conf >= 0.5:
                    self_report["overallStatus"] = "caution"
                else:
                    self_report["overallStatus"] = "needs_review"
            # ç¡®ä¿ overallConfidence å­˜åœ¨
            if "overallConfidence" not in self_report and "overall_confidence" in self_report_raw:
                self_report["overallConfidence"] = self_report_raw["overall_confidence"]
            # ç¡®ä¿ highRiskQuestions æ ¼å¼æ­£ç¡®
            hrq = self_report_raw.get("highRiskQuestions") or self_report_raw.get("high_risk_questions")
            if hrq:
                if isinstance(hrq, list) and hrq and isinstance(hrq[0], str):
                    self_report["highRiskQuestions"] = [
                        {"questionId": q, "description": ""} for q in hrq
                    ]
                else:
                    self_report["highRiskQuestions"] = hrq
            # ç¡®ä¿ issues å­˜åœ¨
            if "issues" not in self_report:
                # ä» potential_errors æˆ– uncertainties æ„å»º issues
                issues = []
                for err in self_report_raw.get("potential_errors", []):
                    if isinstance(err, dict):
                        issues.append({
                            "questionId": err.get("question_id", ""),
                            "message": err.get("description", "")
                        })
                for unc in self_report_raw.get("uncertainties", []):
                    if isinstance(unc, dict):
                        issues.append({
                            "questionId": unc.get("question_id", ""),
                            "message": unc.get("uncertainty", "")
                        })
                if issues:
                    self_report["issues"] = issues
        
        # ğŸ”¥ ç¬¬ä¸€æ¬¡æ‰¹æ”¹è®°å½•ï¼ˆé€»è¾‘å¤æ ¸å‰çš„åŸå§‹ç»“æœï¼‰
        draft_question_details = r.get("draft_question_details") or r.get("draftQuestionDetails")
        draft_question_results = []
        if draft_question_details:
            for dq in draft_question_details:
                draft_scoring_results = dq.get("scoring_point_results") or dq.get("scoring_results") or []
                draft_question_results.append({
                    "questionId": str(dq.get("question_id", "")),
                    "score": dq.get("score", 0),
                    "maxScore": dq.get("max_score", 0),
                    "feedback": dq.get("feedback", ""),
                    "confidence": dq.get("confidence", 0),
                    "self_critique": dq.get("self_critique") or dq.get("selfCritique"),
                    "self_critique_confidence": dq.get("self_critique_confidence") or dq.get("selfCritiqueConfidence"),
                    "studentAnswer": dq.get("student_answer", ""),
                    "question_type": dq.get("question_type") or dq.get("questionType"),
                    "scoring_point_results": draft_scoring_results,
                    "page_indices": dq.get("page_indices", []),
                })
        
        # è®¡ç®—é¡µé¢èŒƒå›´æ˜¾ç¤ºå­—ç¬¦ä¸²
        start_page = r.get("start_page")
        end_page = r.get("end_page")
        page_range = ""
        if start_page is not None:
            if end_page is not None and end_page != start_page:
                page_range = f"{start_page + 1}-{end_page + 1}"
            else:
                page_range = str(start_page + 1)
        
        formatted.append({
            "studentName": r.get("student_key") or r.get("student_name") or r.get("student_id", "Unknown"),
            "score": final_score,
            "maxScore": final_max if final_max > 0 else 0,
            "startPage": start_page,
            "endPage": end_page,
            "pageRange": page_range,
            "questionResults": question_results,
            "confidence": r.get("confidence", 0),
            "needsConfirmation": r.get("needs_confirmation", False),
            "gradingMode": r.get("grading_mode") or r.get("gradingMode"),
            "studentSummary": student_summary,
            "selfAudit": self_audit,
            # ğŸ”¥ æ–°å¢ï¼šæ‰¹æ”¹é€æ˜åº¦å­—æ®µ
            "selfReport": self_report,
            "draftQuestionDetails": draft_question_results if draft_question_results else None,
            "draftTotalScore": r.get("draft_total_score") or r.get("draftTotalScore"),
            "draftMaxScore": r.get("draft_max_score") or r.get("draftMaxScore"),
            "logicReviewedAt": r.get("logic_reviewed_at") or r.get("logicReviewedAt"),
        })
    # #region agent log - å‡è®¾D: _format_results_for_frontend è¾“å‡º
    _write_debug_log({
        "hypothesisId": "D",
        "location": "batch_langgraph.py:_format_results_for_frontend:output",
        "message": "è¾“å‡ºçš„formatted",
        "data": {
            "count": len(formatted),
            "students": [{"name": f.get("studentName"), "score": f.get("score")} for f in formatted],
        },
        "timestamp": int(datetime.now().timestamp() * 1000),
        "sessionId": "debug-session",
    })
    # #endregion
    return formatted


@router.websocket("/ws/{batch_id}")
async def websocket_endpoint(websocket: WebSocket, batch_id: str):
    """
    WebSocket ç«¯ç‚¹ï¼Œç”¨äºå®æ—¶æ¨é€æ‰¹æ”¹è¿›åº¦
    
    å‰ç«¯é€šè¿‡æ­¤ç«¯ç‚¹æ¥æ”¶ LangGraph çš„å®æ—¶æ‰§è¡Œè¿›åº¦
    """
    await websocket.accept()

    redis_client = await _get_redis_client()
    use_redis_cache = redis_client is not None

    cached_images = batch_image_cache.get(batch_id, {})
    if cached_images:
        try:
            for key, message in cached_images.items():
                if key == "llm_stream_cache":
                    continue
                if use_redis_cache and key not in ("images_ready", "rubric_images_ready"):
                    continue
                await websocket.send_json(message)
        except Exception as e:
            logger.warning(f"å‘é€ç¼“å­˜å›¾ç‰‡å¤±è´¥: {e}")

    if use_redis_cache:
        try:
            cached_progress = await _load_cached_progress_messages(batch_id)
            for message in cached_progress:
                await websocket.send_json(message)
        except Exception as e:
            logger.warning(f"å‘é€ç¼“å­˜è¿›åº¦å¤±è´¥: {e}")

    if cached_images:
        try:
            stream_cache = cached_images.get("llm_stream_cache")
            if isinstance(stream_cache, dict):
                for stream_message in stream_cache.values():
                    await websocket.send_json({
                        "type": "llm_stream_chunk",
                        **stream_message,
                    })
        except Exception as e:
            logger.warning(f"å‘é€æµå¼ç¼“å­˜å¤±è´¥: {e}")
    
    # æ³¨å†Œè¿æ¥
    if batch_id not in active_connections:
        active_connections[batch_id] = []
    active_connections[batch_id].append(websocket)
    
    logger.info(f"WebSocket è¿æ¥å»ºç«‹: batch_id={batch_id}")

    # è¿æ¥å»ºç«‹åå°è¯•å‘é€å½“å‰çŠ¶æ€å¿«ç…§ï¼Œé¿å…å‰ç«¯é”™è¿‡æ—©æœŸäº‹ä»¶å¯¼è‡´å¡ä½
    try:
        orchestrator = await get_orchestrator()
        if orchestrator:
            run_id = f"batch_grading_{batch_id}"
            run_info = await orchestrator.get_run_info(run_id)
            if run_info and run_info.state:
                state = run_info.state or {}
                current_stage = state.get("current_stage", "")
                percentage = state.get("percentage", 0)
                if current_stage or percentage:
                    await websocket.send_json({
                        "type": "grading_progress",
                        "percentage": percentage or 0,
                        "currentStage": current_stage
                    })
                if state.get("student_boundaries"):
                    boundaries = state.get("student_boundaries", [])
                    await websocket.send_json({
                        "type": "students_identified",
                        "studentCount": len(boundaries),
                        "students": [
                            {
                                "studentKey": b.get("student_key", ""),
                                "startPage": b.get("start_page", 0),
                                "endPage": b.get("end_page", 0),
                                "confidence": b.get("confidence", 0),
                                "needsConfirmation": b.get("needs_confirmation", False)
                            }
                            for b in boundaries
                        ]
                    })
                if state.get("parsed_rubric"):
                    parsed = state.get("parsed_rubric", {})
                    await websocket.send_json({
                        "type": "rubric_parsed",
                        "totalQuestions": parsed.get("total_questions", 0),
                        "totalScore": parsed.get("total_score", 0),
                        "generalNotes": parsed.get("general_notes", ""),
                        "rubricFormat": parsed.get("rubric_format", ""),
                        "questions": [
                            {
                                "questionId": q.get("question_id", ""),
                                "maxScore": q.get("max_score", 0),
                                "questionText": q.get("question_text", ""),
                                "standardAnswer": q.get("standard_answer", ""),
                                "gradingNotes": q.get("grading_notes", ""),
                          "scoringPoints": [
                              {
                                  "pointId": sp.get("point_id") or sp.get("pointId") or f"{q.get('question_id')}.{idx + 1}",
                                  "description": sp.get("description", ""),
                                  "expectedValue": sp.get("expected_value") or sp.get("expectedValue", ""),
                                  "keywords": sp.get("keywords") or [],
                                  "score": sp.get("score", 0),
                                  "isRequired": sp.get("is_required", True),
                              }
                              for idx, sp in enumerate(q.get("scoring_points", []))
                          ],
                          "deductionRules": [
                              {
                                  "ruleId": dr.get("rule_id") or dr.get("ruleId") or f"{q.get('question_id')}.d{idx + 1}",
                                  "description": dr.get("description", ""),
                                  "deduction": dr.get("deduction", dr.get("score", 0)),
                                  "conditions": dr.get("conditions") or dr.get("when") or "",
                              }
                              for idx, dr in enumerate(q.get("deduction_rules") or q.get("deductionRules") or [])
                          ],
                                "alternativeSolutions": [
                                    {
                                        "description": alt.get("description", ""),
                                        "scoringCriteria": alt.get("scoring_criteria", ""),
                                        "note": alt.get("note", ""),
                                    }
                                    for alt in q.get("alternative_solutions", [])
                                ]
                            }
                            for q in parsed.get("questions", [])
                        ]
                    })
                if run_info.status and run_info.status.value == "completed":
                    student_results = state.get("student_results", [])
                    formatted_results = _format_results_for_frontend(student_results)
                    class_report = state.get("class_report")
                    if not class_report and state.get("export_data"):
                        class_report = state.get("export_data", {}).get("class_report")
                    await websocket.send_json({
                        "type": "workflow_completed",
                        "message": f"Grading completed, processed {len(formatted_results)} students",
                        "results": formatted_results,
                        "cross_page_questions": state.get("cross_page_questions", []),
                        "classReport": class_report
                    })
    except Exception as e:
        logger.warning(f"å‘é€çŠ¶æ€å¿«ç…§å¤±è´¥: {e}")
    
    try:
        # ä¿æŒè¿æ¥ï¼Œç­‰å¾…å®¢æˆ·ç«¯æ¶ˆæ¯æˆ–æ–­å¼€
        while True:
            if not _is_ws_connected(websocket):
                break
            data = await websocket.receive_text()
            logger.debug(f"æ”¶åˆ° WebSocket æ¶ˆæ¯: batch_id={batch_id}, data={data}")
            
    except (WebSocketDisconnect, RuntimeError) as exc:
        logger.info(f"WebSocket è¿æ¥æ–­å¼€: batch_id={batch_id}, reason={exc}")
        _discard_connection(batch_id, websocket)
        return
    except Exception as exc:
        logger.warning(f"WebSocket æ¥æ”¶å¼‚å¸¸: batch_id={batch_id}, error={exc}")
        logger.info(f"WebSocket è¿æ¥æ–­å¼€: batch_id={batch_id}")
        _discard_connection(batch_id, websocket)


@router.get("/status/{batch_id}", response_model=BatchStatusResponse)
async def get_batch_status(
    batch_id: str,
    orchestrator: Orchestrator = Depends(get_orchestrator)
):
    """
    æŸ¥è¯¢æ‰¹æ¬¡çŠ¶æ€ï¼ˆä» LangGraph Orchestratorï¼‰
    
    Args:
        batch_id: æ‰¹æ¬¡ ID
        orchestrator: LangGraph Orchestrator
        
    Returns:
        BatchStatusResponse: æ‰¹æ¬¡çŠ¶æ€
    """
    try:
        if not orchestrator:
            raise HTTPException(status_code=503, detail="ç¼–æ’å™¨æœªåˆå§‹åŒ–")
        
        # æ„å»º run_idï¼ˆä¸ start_run ä¸­çš„æ ¼å¼ä¸€è‡´ï¼‰
        run_id = f"batch_grading_{batch_id}"
        
        # ä» LangGraph Orchestrator æŸ¥è¯¢çŠ¶æ€
        run_info = await orchestrator.get_run_info(run_id)
        
        if not run_info:
            raise HTTPException(status_code=404, detail="æ‰¹æ¬¡ä¸å­˜åœ¨")
        
        state = run_info.state or {}
        
        return BatchStatusResponse(
            batch_id=batch_id,
            exam_id=state.get("exam_id", ""),
            status=run_info.status.value,
            total_students=len(state.get("student_boundaries", [])),
            completed_students=len(state.get("student_results", [])),
            unidentified_pages=0,
            results=state.get("student_results")
        )
        
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"æŸ¥è¯¢æ‰¹æ¬¡çŠ¶æ€å¤±è´¥: {str(e)}", exc_info=True)
        raise HTTPException(status_code=500, detail=f"æŸ¥è¯¢å¤±è´¥: {str(e)}")


@router.get("/rubric/{batch_id}", response_model=RubricReviewContextResponse)
async def get_rubric_review_context(
    batch_id: str,
    orchestrator: Orchestrator = Depends(get_orchestrator)
):
    """è·å– rubric review é¡µé¢ä¸Šä¸‹æ–‡"""
    try:
        if not orchestrator:
            raise HTTPException(status_code=503, detail="ç¼–æ’å™¨æœªåˆå§‹åŒ–")

        run_id = f"batch_grading_{batch_id}"
        run_info = await orchestrator.get_run_info(run_id)
        if not run_info:
            raise HTTPException(status_code=404, detail="æ‰¹æ¬¡ä¸å­˜åœ¨")

        state = run_info.state or {}
        parsed_rubric = state.get("parsed_rubric")

        cached = batch_image_cache.get(batch_id, {})
        cached_images = cached.get("rubric_images_ready", {}).get("images") if cached else None
        rubric_images: List[str] = cached_images or []

        if not rubric_images and state.get("rubric_images"):
            try:
                rubric_images = []
                for img in state.get("rubric_images", []):
                    if isinstance(img, (bytes, bytearray)):
                        rubric_images.append(base64.b64encode(img).decode("utf-8"))
                    elif isinstance(img, str) and img:
                        rubric_images.append(img)
            except Exception as exc:
                logger.warning(f"Failed to convert rubric images: {exc}")
        return RubricReviewContextResponse(
            batch_id=batch_id,
            status=run_info.status.value if run_info.status else None,
            current_stage=state.get("current_stage"),
            parsed_rubric=parsed_rubric,
            rubric_images=rubric_images,
        )
    except HTTPException:
        raise
    except Exception as exc:
        logger.error(f"è·å– rubric ä¸Šä¸‹æ–‡å¤±è´¥: {exc}", exc_info=True)
        raise HTTPException(status_code=500, detail=f"è·å–å¤±è´¥: {str(exc)}")


@router.get("/results-review/{batch_id}", response_model=ResultsReviewContextResponse)
async def get_results_review_context(
    batch_id: str,
    orchestrator: Orchestrator = Depends(get_orchestrator)
):
    """è·å– results review é¡µé¢ä¸Šä¸‹æ–‡"""
    try:
        if not orchestrator:
            raise HTTPException(status_code=503, detail="ç¼–æ’å™¨æœªåˆå§‹åŒ–")

        run_id = f"batch_grading_{batch_id}"
        run_info = await orchestrator.get_run_info(run_id)
        if not run_info:
            raise HTTPException(status_code=404, detail="æ‰¹æ¬¡ä¸å­˜åœ¨")

        state = run_info.state or {}
        student_results = state.get("student_results", [])
        if not student_results:
            try:
                final_output = await orchestrator.get_final_output(run_id)
                if final_output:
                    student_results = final_output.get("student_results", [])
            except Exception as exc:
                logger.warning(f"è·å–æœ€ç»ˆè¾“å‡ºå¤±è´¥: {exc}")

        if not student_results:
            export_students = (state.get("export_data") or {}).get("students", [])
            if export_students:
                student_results = export_students

        cached = batch_image_cache.get(batch_id, {})
        cached_images = cached.get("images_ready", {}).get("images") if cached else None
        answer_images: List[str] = cached_images or []

        if not answer_images:
            raw_images = state.get("processed_images") or state.get("answer_images") or []
            try:
                answer_images = []
                for img in raw_images:
                    if isinstance(img, (bytes, bytearray)):
                        answer_images.append(base64.b64encode(img).decode("utf-8"))
                    elif isinstance(img, str) and img:
                        answer_images.append(img)
            except Exception as exc:
                logger.warning(f"Failed to convert answer images: {exc}")
        return ResultsReviewContextResponse(
            batch_id=batch_id,
            status=run_info.status.value if run_info.status else None,
            current_stage=state.get("current_stage"),
            student_results=_format_results_for_frontend(student_results),
            answer_images=answer_images,
        )
    except HTTPException:
        raise
    except Exception as exc:
        logger.error(f"è·å– results ä¸Šä¸‹æ–‡å¤±è´¥: {exc}", exc_info=True)
        raise HTTPException(status_code=500, detail=f"è·å–å¤±è´¥: {str(exc)}")


@router.get("/results/{batch_id}")
async def get_batch_results(
    batch_id: str,
    orchestrator: Orchestrator = Depends(get_orchestrator)
):
    """
    è·å–æ‰¹æ¬¡æ‰¹æ”¹ç»“æœï¼ˆä» LangGraph Orchestratorï¼‰
    
    Args:
        batch_id: æ‰¹æ¬¡ ID
        orchestrator: LangGraph Orchestrator
        
    Returns:
        æ‰¹æ”¹ç»“æœ
    """
    try:
        if not orchestrator:
            raise HTTPException(status_code=503, detail="ç¼–æ’å™¨æœªåˆå§‹åŒ–")
        
        # æ„å»º run_idï¼ˆä¸ start_run ä¸­çš„æ ¼å¼ä¸€è‡´ï¼‰
        run_id = f"batch_grading_{batch_id}"
        
        run_info = await orchestrator.get_run_info(run_id)
        
        if not run_info:
            raise HTTPException(status_code=404, detail="æ‰¹æ¬¡ä¸å­˜åœ¨")
        
        state = run_info.state or {}
        
        # ä¼˜å…ˆä» student_results è·å–ç»“æœ
        student_results = state.get("student_results", [])
        
        # å¦‚æœæ²¡æœ‰ student_resultsï¼Œå°è¯•ä» orchestrator è·å–æœ€ç»ˆè¾“å‡º
        if not student_results:
            try:
                final_output = await orchestrator.get_final_output(run_id)
                if final_output:
                    student_results = final_output.get("student_results", [])
            except Exception as e:
                logger.warning(f"è·å–æœ€ç»ˆè¾“å‡ºå¤±è´¥: {e}")
        
        return {
            "batch_id": batch_id,
            "status": run_info.status.value,
            "results": _format_results_for_frontend(student_results),
            "class_report": state.get("class_report")
        }
        
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"è·å–æ‰¹æ”¹ç»“æœå¤±è´¥: {str(e)}", exc_info=True)
        raise HTTPException(status_code=500, detail=f"è·å–å¤±è´¥: {str(e)}")


@router.get("/full-results/{batch_id}")
async def get_full_batch_results(
    batch_id: str,
    orchestrator: Orchestrator = Depends(get_orchestrator)
):
    """
    è·å–æ‰¹æ¬¡å®Œæ•´æ‰¹æ”¹ç»“æœï¼ˆåŒ…å«è·¨é¡µé¢˜ç›®ä¿¡æ¯ï¼‰
    
    Args:
        batch_id: æ‰¹æ¬¡ ID
        orchestrator: LangGraph Orchestrator
        
    Returns:
        å®Œæ•´æ‰¹æ”¹ç»“æœï¼ˆåŒ…å«è·¨é¡µé¢˜ç›®ä¿¡æ¯ï¼‰
    """
    try:
        if not orchestrator:
            raise HTTPException(status_code=503, detail="ç¼–æ’å™¨æœªåˆå§‹åŒ–")
        
        run_id = f"batch_grading_{batch_id}"
        run_info = await orchestrator.get_run_info(run_id)
        
        if not run_info:
            raise HTTPException(status_code=404, detail="æ‰¹æ¬¡ä¸å­˜åœ¨")
        
        state = run_info.state or {}
        student_results = state.get("student_results", [])
        cross_page_questions = state.get("cross_page_questions", [])
        parsed_rubric = state.get("parsed_rubric", {})
        class_report = state.get("class_report") or state.get("export_data", {}).get("class_report")
        
        return {
            "batch_id": batch_id,
            "status": run_info.status.value,
            "results": _format_results_for_frontend(student_results),
            "cross_page_questions": cross_page_questions,
            "parsed_rubric": parsed_rubric,
            "class_report": class_report,
            "total_students": len(student_results),
            "total_score": parsed_rubric.get("total_score", 100) if parsed_rubric else 100
        }
        
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"è·å–å®Œæ•´æ‰¹æ”¹ç»“æœå¤±è´¥: {str(e)}", exc_info=True)
        raise HTTPException(status_code=500, detail=f"è·å–å¤±è´¥: {str(e)}")


@router.get("/cross-page-questions/{batch_id}")
async def get_cross_page_questions(
    batch_id: str,
    orchestrator: Orchestrator = Depends(get_orchestrator)
):
    """
    è·å–è·¨é¡µé¢˜ç›®ä¿¡æ¯
    
    Args:
        batch_id: æ‰¹æ¬¡ ID
        orchestrator: LangGraph Orchestrator
        
    Returns:
        è·¨é¡µé¢˜ç›®ä¿¡æ¯åˆ—è¡¨
    """
    try:
        if not orchestrator:
            raise HTTPException(status_code=503, detail="ç¼–æ’å™¨æœªåˆå§‹åŒ–")
        
        run_id = f"batch_grading_{batch_id}"
        run_info = await orchestrator.get_run_info(run_id)
        
        if not run_info:
            raise HTTPException(status_code=404, detail="æ‰¹æ¬¡ä¸å­˜åœ¨")
        
        state = run_info.state or {}
        cross_page_questions = state.get("cross_page_questions", [])
        
        return cross_page_questions
        
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"è·å–è·¨é¡µé¢˜ç›®ä¿¡æ¯å¤±è´¥: {str(e)}", exc_info=True)
        raise HTTPException(status_code=500, detail=f"è·å–å¤±è´¥: {str(e)}")


class ConfirmBoundaryRequest(BaseModel):
    """ç¡®è®¤å­¦ç”Ÿè¾¹ç•Œè¯·æ±‚"""
    batch_id: str = Field(..., description="æ‰¹æ¬¡ ID")
    student_key: str = Field(..., description="å­¦ç”Ÿæ ‡è¯†")
    confirmed_pages: List[int] = Field(..., description="ç¡®è®¤çš„é¡µé¢ç´¢å¼•åˆ—è¡¨")


class RubricReviewRequest(BaseModel):
    """æäº¤è¯„åˆ†æ ‡å‡†äººå·¥ç¡®è®¤ç»“æœ"""
    batch_id: str = Field(..., description="æ‰¹æ¬¡ ID")
    action: str = Field(..., description="approve/update/reparse")
    parsed_rubric: Optional[Dict[str, Any]] = Field(None, description="ä¿®æ­£åçš„è¯„åˆ†æ ‡å‡†")
    selected_question_ids: Optional[List[str]] = Field(None, description="ä»…é‡ä¿®æ­£çš„é—®é¢˜ ID åˆ—è¡¨")
    notes: Optional[str] = Field(None, description="è¡¥å……è¯´æ˜")


class ResultsReviewRequest(BaseModel):
    """æäº¤æ‰¹æ”¹ç»“æœäººå·¥ç¡®è®¤ç»“æœ"""
    batch_id: str = Field(..., description="æ‰¹æ¬¡ ID")
    action: str = Field(..., description="approve/update/regrade")
    results: Optional[List[Dict[str, Any]]] = Field(None, description="ä¿®æ­£åçš„ç»“æœ")
    regrade_items: Optional[List[Dict[str, Any]]] = Field(
        None,
        description="éœ€è¦é‡æ–°æ‰¹æ”¹çš„é¢˜ç›®é¡¹",
    )
    notes: Optional[str] = Field(None, description="è¡¥å……è¯´æ˜")


@router.post("/review/rubric")
async def submit_rubric_review(
    request: RubricReviewRequest,
    orchestrator: Orchestrator = Depends(get_orchestrator)
):
    """æäº¤è¯„åˆ†æ ‡å‡†å¤æ ¸ç»“æœï¼Œæ¢å¤ workflow"""
    try:
        if not orchestrator:
            raise HTTPException(status_code=503, detail="ç¼–æ’å™¨æœªåˆå§‹åŒ–")

        action = request.action.lower().strip()
        if action not in ("approve", "update", "override", "reparse"):
            raise HTTPException(status_code=400, detail="æ— æ•ˆçš„ review action")

        run_id = f"batch_grading_{request.batch_id}"
        run_info = await orchestrator.get_run_info(run_id)
        if not run_info:
            raise HTTPException(status_code=404, detail="æ‰¹æ¬¡ä¸å­˜åœ¨")

        payload: Dict[str, Any] = {
            "action": action,
        }
        if request.parsed_rubric is not None:
            payload["parsed_rubric"] = request.parsed_rubric
        if request.selected_question_ids:
            payload["selected_question_ids"] = request.selected_question_ids
        if request.notes:
            payload["notes"] = request.notes

        success = await orchestrator.send_event(run_id, "review_signal", payload)
        if not success:
            raise HTTPException(status_code=409, detail="æ‰¹æ¬¡æœªå¤„äºå¯å¤æ ¸çŠ¶æ€")

        cached = batch_image_cache.get(request.batch_id)
        if cached and "review_required" in cached:
            cached.pop("review_required", None)

        return {"success": True, "message": "è¯„åˆ†æ ‡å‡†å¤æ ¸å·²æäº¤"}
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"æäº¤è¯„åˆ†æ ‡å‡†å¤æ ¸å¤±è´¥: {str(e)}", exc_info=True)
        raise HTTPException(status_code=500, detail=f"æäº¤å¤±è´¥: {str(e)}")


@router.post("/review/results")
async def submit_results_review(
    request: ResultsReviewRequest,
    orchestrator: Orchestrator = Depends(get_orchestrator)
):
    """æäº¤æ‰¹æ”¹ç»“æœå¤æ ¸ï¼Œæ¢å¤ workflow"""
    try:
        if not orchestrator:
            raise HTTPException(status_code=503, detail="ç¼–æ’å™¨æœªåˆå§‹åŒ–")

        action = request.action.lower().strip()
        if action not in ("approve", "update", "override", "regrade"):
            raise HTTPException(status_code=400, detail="æ— æ•ˆçš„ review action")

        run_id = f"batch_grading_{request.batch_id}"
        run_info = await orchestrator.get_run_info(run_id)
        if not run_info:
            raise HTTPException(status_code=404, detail="æ‰¹æ¬¡ä¸å­˜åœ¨")

        payload: Dict[str, Any] = {
            "action": action,
        }
        if request.results is not None:
            payload["results"] = request.results
        if request.regrade_items is not None:
            payload["regrade_items"] = request.regrade_items
        if request.notes:
            payload["notes"] = request.notes

        success = await orchestrator.send_event(run_id, "review_signal", payload)
        if not success:
            raise HTTPException(status_code=409, detail="æ‰¹æ¬¡æœªå¤„äºå¯å¤æ ¸çŠ¶æ€")

        cached = batch_image_cache.get(request.batch_id)
        if cached and "review_required" in cached:
            cached.pop("review_required", None)

        return {"success": True, "message": "æ‰¹æ”¹ç»“æœå¤æ ¸å·²æäº¤"}
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"æäº¤æ‰¹æ”¹å¤æ ¸å¤±è´¥: {str(e)}", exc_info=True)
        raise HTTPException(status_code=500, detail=f"æäº¤å¤±è´¥: {str(e)}")


@router.post("/confirm-boundary")
async def confirm_student_boundary(
    request: ConfirmBoundaryRequest,
    orchestrator: Orchestrator = Depends(get_orchestrator)
):
    """
    ç¡®è®¤å­¦ç”Ÿè¾¹ç•Œ
    
    å½“ AI è¯†åˆ«çš„å­¦ç”Ÿè¾¹ç•Œä¸å‡†ç¡®æ—¶ï¼Œå…è®¸ç”¨æˆ·æ‰‹åŠ¨ç¡®è®¤
    
    Args:
        request: ç¡®è®¤è¾¹ç•Œè¯·æ±‚
        orchestrator: LangGraph Orchestrator
        
    Returns:
        ç¡®è®¤ç»“æœ
    """
    try:
        if not orchestrator:
            raise HTTPException(status_code=503, detail="ç¼–æ’å™¨æœªåˆå§‹åŒ–")
        
        run_id = f"batch_grading_{request.batch_id}"
        run_info = await orchestrator.get_run_info(run_id)
        
        if not run_info:
            raise HTTPException(status_code=404, detail="æ‰¹æ¬¡ä¸å­˜åœ¨")
        
        # æ›´æ–°çŠ¶æ€ä¸­çš„å­¦ç”Ÿè¾¹ç•Œ
        state = run_info.state or {}
        student_boundaries = state.get("student_boundaries", [])
        
        # æŸ¥æ‰¾å¹¶æ›´æ–°å¯¹åº”å­¦ç”Ÿçš„è¾¹ç•Œ
        updated = False
        for boundary in student_boundaries:
            if boundary.get("student_key") == request.student_key:
                boundary["pages"] = request.confirmed_pages
                boundary["confirmed"] = True
                updated = True
                break
        
        if not updated:
            # å¦‚æœæ²¡æœ‰æ‰¾åˆ°ï¼Œæ·»åŠ æ–°çš„è¾¹ç•Œ
            student_boundaries.append({
                "student_key": request.student_key,
                "pages": request.confirmed_pages,
                "confirmed": True
            })
        
        logger.info(f"å­¦ç”Ÿè¾¹ç•Œå·²ç¡®è®¤: batch_id={request.batch_id}, student_key={request.student_key}")
        
        return {
            "success": True,
            "message": f"å­¦ç”Ÿ {request.student_key} çš„è¾¹ç•Œå·²ç¡®è®¤"
        }
        
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"ç¡®è®¤å­¦ç”Ÿè¾¹ç•Œå¤±è´¥: {str(e)}", exc_info=True)
        raise HTTPException(status_code=500, detail=f"ç¡®è®¤å¤±è´¥: {str(e)}")


# ==================== å¯¼å‡º API ====================

class ExportAnnotatedImagesRequest(BaseModel):
    """å¯¼å‡ºå¸¦æ‰¹æ³¨å›¾ç‰‡è¯·æ±‚"""
    include_original: bool = Field(default=False, description="æ˜¯å¦åŒ…å«åŸå§‹å›¾ç‰‡")


class ExportExcelRequest(BaseModel):
    """å¯¼å‡º Excel è¯·æ±‚"""
    columns: Optional[List[Dict[str, Any]]] = Field(None, description="è‡ªå®šä¹‰åˆ—é…ç½®")


class SmartExcelRequest(BaseModel):
    """æ™ºèƒ½ Excel ç”Ÿæˆè¯·æ±‚"""
    prompt: str = Field(..., description="ç”¨æˆ·æè¿°çš„æ ¼å¼éœ€æ±‚")
    template_base64: Optional[str] = Field(None, description="æ¨¡æ¿ Excel Base64")


@router.post("/export/annotated-images/{batch_id}")
async def export_annotated_images(
    batch_id: str,
    request: ExportAnnotatedImagesRequest = ExportAnnotatedImagesRequest(),
    orchestrator: Orchestrator = Depends(get_orchestrator),
):
    """
    å¯¼å‡ºå¸¦æ‰¹æ³¨çš„å­¦ç”Ÿä½œç­”å›¾ç‰‡ (ZIP)
    
    å°†æ‰€æœ‰å­¦ç”Ÿçš„ä½œç­”å›¾ç‰‡æ¸²æŸ“æ‰¹æ³¨åæ‰“åŒ…ä¸º ZIP ä¸‹è½½
    """
    from fastapi.responses import Response
    from src.services.export_service import AnnotatedImageExporter, ExportConfig
    
    try:
        if not orchestrator:
            raise HTTPException(status_code=503, detail="ç¼–æ’å™¨æœªåˆå§‹åŒ–")
        
        run_id = f"batch_grading_{batch_id}"
        run_info = await orchestrator.get_run_info(run_id)
        
        if not run_info:
            raise HTTPException(status_code=404, detail="æ‰¹æ¬¡ä¸å­˜åœ¨")
        
        state = run_info.state or {}
        student_results = state.get("student_results", [])
        
        if not student_results:
            raise HTTPException(status_code=404, detail="æ— æ‰¹æ”¹ç»“æœ")
        
        # è·å–å›¾ç‰‡
        cached = batch_image_cache.get(batch_id, {})
        images_ready = cached.get("images_ready", {})
        images_b64 = images_ready.get("images", [])
        
        if not images_b64:
            raise HTTPException(status_code=404, detail="æ— å›¾ç‰‡æ•°æ®ï¼Œè¯·é‡æ–°ä¸Šä¼ ")
        
        # è§£ç å›¾ç‰‡
        import base64
        images = []
        for img_b64 in images_b64:
            if img_b64.startswith("data:"):
                img_b64 = img_b64.split(",", 1)[1]
            images.append(base64.b64decode(img_b64))
        
        # æ ¼å¼åŒ–ç»“æœ
        formatted_results = _format_results_for_frontend(student_results)
        
        # å¯¼å‡º
        config = ExportConfig(include_original=request.include_original)
        exporter = AnnotatedImageExporter(config)
        zip_bytes = exporter.export_to_zip(formatted_results, images, batch_id)
        
        filename = f"grading_annotated_{batch_id}_{datetime.now().strftime('%Y%m%d_%H%M%S')}.zip"
        
        return Response(
            content=zip_bytes,
            media_type="application/zip",
            headers={
                "Content-Disposition": f'attachment; filename="{filename}"',
            },
        )
        
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"å¯¼å‡ºå¸¦æ‰¹æ³¨å›¾ç‰‡å¤±è´¥: {str(e)}", exc_info=True)
        raise HTTPException(status_code=500, detail=f"å¯¼å‡ºå¤±è´¥: {str(e)}")


@router.post("/export/excel/{batch_id}")
async def export_excel(
    batch_id: str,
    request: ExportExcelRequest = ExportExcelRequest(),
    orchestrator: Orchestrator = Depends(get_orchestrator),
):
    """
    å¯¼å‡º Excel ç»Ÿè®¡æ•°æ®
    
    åŒ…å«å­¦ç”Ÿæˆç»©ã€é¢˜ç›®ç»Ÿè®¡ã€ç­çº§æŠ¥å‘Šç­‰å¤šä¸ª Sheet
    """
    from fastapi.responses import Response
    from src.services.export_service import ExcelExporter
    
    try:
        if not orchestrator:
            raise HTTPException(status_code=503, detail="ç¼–æ’å™¨æœªåˆå§‹åŒ–")
        
        run_id = f"batch_grading_{batch_id}"
        run_info = await orchestrator.get_run_info(run_id)
        
        if not run_info:
            raise HTTPException(status_code=404, detail="æ‰¹æ¬¡ä¸å­˜åœ¨")
        
        state = run_info.state or {}
        student_results = state.get("student_results", [])
        class_report = state.get("class_report") or state.get("export_data", {}).get("class_report")
        
        if not student_results:
            raise HTTPException(status_code=404, detail="æ— æ‰¹æ”¹ç»“æœ")
        
        # æ ¼å¼åŒ–ç»“æœ
        formatted_results = _format_results_for_frontend(student_results)
        
        # å¯¼å‡º
        exporter = ExcelExporter()
        excel_bytes = exporter.export_basic(formatted_results, class_report, request.columns)
        
        filename = f"grading_report_{batch_id}_{datetime.now().strftime('%Y%m%d_%H%M%S')}.xlsx"
        
        return Response(
            content=excel_bytes,
            media_type="application/vnd.openxmlformats-officedocument.spreadsheetml.sheet",
            headers={
                "Content-Disposition": f'attachment; filename="{filename}"',
            },
        )
        
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"å¯¼å‡º Excel å¤±è´¥: {str(e)}", exc_info=True)
        raise HTTPException(status_code=500, detail=f"å¯¼å‡ºå¤±è´¥: {str(e)}")


@router.post("/export/smart-excel/{batch_id}")
async def export_smart_excel(
    batch_id: str,
    request: SmartExcelRequest,
    orchestrator: Orchestrator = Depends(get_orchestrator),
):
    """
    LLM æ™ºèƒ½ Excel ç”Ÿæˆ
    
    æ”¯æŒï¼š
    - ç”¨æˆ·å¯¹è¯æè¿°æ ¼å¼éœ€æ±‚
    - å¯¼å…¥å·²æœ‰ Excel æ¨¡æ¿å¹¶å¡«å……æ•°æ®
    """
    from fastapi.responses import Response
    from src.services.export_service import SmartExcelGenerator
    from src.services.llm_client import get_llm_client
    
    try:
        if not orchestrator:
            raise HTTPException(status_code=503, detail="ç¼–æ’å™¨æœªåˆå§‹åŒ–")
        
        run_id = f"batch_grading_{batch_id}"
        run_info = await orchestrator.get_run_info(run_id)
        
        if not run_info:
            raise HTTPException(status_code=404, detail="æ‰¹æ¬¡ä¸å­˜åœ¨")
        
        state = run_info.state or {}
        student_results = state.get("student_results", [])
        class_report = state.get("class_report") or state.get("export_data", {}).get("class_report")
        
        if not student_results:
            raise HTTPException(status_code=404, detail="æ— æ‰¹æ”¹ç»“æœ")
        
        # æ ¼å¼åŒ–ç»“æœ
        formatted_results = _format_results_for_frontend(student_results)
        
        # è§£ç æ¨¡æ¿
        template_bytes = None
        if request.template_base64:
            import base64
            try:
                if request.template_base64.startswith("data:"):
                    request.template_base64 = request.template_base64.split(",", 1)[1]
                template_bytes = base64.b64decode(request.template_base64)
            except Exception as e:
                raise HTTPException(status_code=400, detail=f"æ¨¡æ¿è§£ç å¤±è´¥: {e}")
        
        # è·å– LLM å®¢æˆ·ç«¯
        llm_client = None
        try:
            llm_client = get_llm_client()
        except Exception as e:
            logger.warning(f"è·å– LLM å®¢æˆ·ç«¯å¤±è´¥: {e}")
        
        # ç”Ÿæˆ Excel
        generator = SmartExcelGenerator(llm_client)
        excel_bytes, explanation = await generator.generate_from_prompt(
            formatted_results,
            class_report,
            request.prompt,
            template_bytes,
        )
        
        filename = f"grading_smart_{batch_id}_{datetime.now().strftime('%Y%m%d_%H%M%S')}.xlsx"
        
        return Response(
            content=excel_bytes,
            media_type="application/vnd.openxmlformats-officedocument.spreadsheetml.sheet",
            headers={
                "Content-Disposition": f'attachment; filename="{filename}"',
                "X-LLM-Explanation": explanation.encode('utf-8').decode('latin-1'),
            },
        )
        
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"æ™ºèƒ½ Excel ç”Ÿæˆå¤±è´¥: {str(e)}", exc_info=True)
        raise HTTPException(status_code=500, detail=f"ç”Ÿæˆå¤±è´¥: {str(e)}")


@router.post("/render/batch/{batch_id}")
async def render_batch_annotations(
    batch_id: str,
    page_indices: Optional[List[int]] = None,
    orchestrator: Orchestrator = Depends(get_orchestrator),
):
    """
    æ‰¹é‡æ¸²æŸ“æ‰¹æ³¨åˆ°å›¾ç‰‡
    
    è¿”å›æŒ‡å®šé¡µé¢çš„å¸¦æ‰¹æ³¨å›¾ç‰‡ Base64 åˆ—è¡¨
    """
    from src.services.export_service import AnnotatedImageExporter
    
    try:
        if not orchestrator:
            raise HTTPException(status_code=503, detail="ç¼–æ’å™¨æœªåˆå§‹åŒ–")
        
        run_id = f"batch_grading_{batch_id}"
        run_info = await orchestrator.get_run_info(run_id)
        
        if not run_info:
            raise HTTPException(status_code=404, detail="æ‰¹æ¬¡ä¸å­˜åœ¨")
        
        state = run_info.state or {}
        student_results = state.get("student_results", [])
        
        # è·å–å›¾ç‰‡
        cached = batch_image_cache.get(batch_id, {})
        images_ready = cached.get("images_ready", {})
        images_b64 = images_ready.get("images", [])
        
        if not images_b64:
            raise HTTPException(status_code=404, detail="æ— å›¾ç‰‡æ•°æ®")
        
        # è§£ç å›¾ç‰‡
        import base64
        images = []
        for img_b64 in images_b64:
            if img_b64.startswith("data:"):
                img_b64 = img_b64.split(",", 1)[1]
            images.append(base64.b64decode(img_b64))
        
        # æ ¼å¼åŒ–ç»“æœ
        formatted_results = _format_results_for_frontend(student_results)
        
        # æ¸²æŸ“
        exporter = AnnotatedImageExporter()
        rendered_images = {}
        
        # ç¡®å®šè¦æ¸²æŸ“çš„é¡µé¢
        target_pages = page_indices if page_indices else list(range(len(images)))
        
        for student in formatted_results:
            start_page = student.get("startPage") or 0
            end_page = student.get("endPage") or len(images) - 1
            
            for page_idx, rendered_bytes in exporter.render_student_pages(
                student, images, start_page, end_page
            ):
                if page_idx in target_pages:
                    rendered_images[page_idx] = base64.b64encode(rendered_bytes).decode('utf-8')
        
        return {
            "success": True,
            "rendered_images": rendered_images,
        }
        
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"æ‰¹é‡æ¸²æŸ“æ‰¹æ³¨å¤±è´¥: {str(e)}", exc_info=True)
        raise HTTPException(status_code=500, detail=f"æ¸²æŸ“å¤±è´¥: {str(e)}")
