"""æ‰¹é‡æäº¤ API è·¯ç”± - ä½¿ç”¨ LangGraph Orchestrator

æ­£ç¡®çš„æ¶æ„ï¼š
1. ä½¿ç”¨ LangGraph Orchestrator å¯åŠ¨æ‰¹æ”¹æµç¨‹
2. é€šè¿‡ LangGraph çš„æµå¼ API å®æ—¶æ¨é€è¿›åº¦
3. åˆ©ç”¨ PostgreSQL Checkpointer å®ç°æŒä¹…åŒ–å’Œæ–­ç‚¹æ¢å¤
"""

import uuid
import logging
import tempfile
import asyncio
import inspect
import base64
import json
from datetime import datetime
from pathlib import Path
from typing import List, Optional, Dict, Any

from fastapi import (
    APIRouter,
    HTTPException,
    UploadFile,
    File,
    Form,
    WebSocket,
    WebSocketDisconnect,
    Depends,
)
from starlette.websockets import WebSocketState
from pydantic import BaseModel, Field
import fitz
from PIL import Image
import os
import redis.asyncio as redis
from redis.exceptions import RedisError

from src.models.enums import SubmissionStatus
from src.orchestration.base import Orchestrator, RunStatus
from src.api.dependencies import get_orchestrator
from src.utils.image import to_jpeg_bytes, pil_to_jpeg_bytes
from src.utils.pool_manager import UnifiedPoolManager, PoolNotInitializedError
from src.services.grading_run_control import GradingRunSnapshot, get_run_controller
from src.services.file_storage import get_file_storage_service, StoredFile

# PostgreSQL ä½œä¸ºä¸»å­˜å‚¨
from src.db import (
    GradingHistory,
    StudentGradingResult,
    save_grading_history,
    save_student_result,
    upsert_homework_submission_grade,
    list_class_students,
    get_grading_history,
    get_student_results,
    get_page_images,
)


logger = logging.getLogger(__name__)


async def _maybe_await(value):
    return await value if inspect.isawaitable(value) else value
router = APIRouter(prefix="/batch", tags=["æ‰¹é‡æäº¤"])

# å­˜å‚¨æ´»è·ƒçš„ WebSocket è¿æ¥
active_connections: Dict[str, List[WebSocket]] = {}
# ç¼“å­˜å›¾ç‰‡ï¼Œé¿å… images_ready æ—©äº WebSocket è¿æ¥å¯¼è‡´å‰ç«¯ä¸¢å¤±
batch_image_cache: Dict[str, Dict[str, dict]] = {}
_active_stream_tasks: Dict[str, asyncio.Task] = {}
DEBUG_LOG_PATH = os.getenv("GRADEOS_DEBUG_LOG_PATH")
TEACHER_MAX_ACTIVE_RUNS = int(os.getenv("TEACHER_MAX_ACTIVE_RUNS", "3"))
RUN_QUEUE_POLL_SECONDS = float(os.getenv("GRADING_RUN_POLL_SECONDS", "2.0"))
RUN_QUEUE_TIMEOUT_SECONDS = float(os.getenv("GRADING_RUN_WAIT_TIMEOUT_SECONDS", "0"))
REDIS_PROGRESS_TTL_SECONDS = int(os.getenv("REDIS_PROGRESS_TTL_SECONDS", "86400"))
REDIS_PROGRESS_KEY_PREFIX = os.getenv("REDIS_PROGRESS_KEY_PREFIX", "batch_progress")
_REDIS_CACHE_SKIP_TYPES = {"images_ready", "rubric_images_ready", "llm_stream_chunk"}
_REDIS_CLIENT: Optional[redis.Redis] = None
_REDIS_CLIENT_CHECKED: bool = False


def _is_ws_connected(websocket: WebSocket) -> bool:
    return (
        websocket.client_state == WebSocketState.CONNECTED
        and websocket.application_state == WebSocketState.CONNECTED
    )


def _discard_connection(batch_id: str, websocket: WebSocket) -> None:
    connections = active_connections.get(batch_id)
    if not connections:
        return
    try:
        connections.remove(websocket)
    except ValueError:
        return
    if not connections:
        active_connections.pop(batch_id, None)


def _write_debug_log(payload: Dict[str, Any]) -> None:
    if not DEBUG_LOG_PATH:
        return
    try:
        Path(DEBUG_LOG_PATH).parent.mkdir(parents=True, exist_ok=True)
        with open(DEBUG_LOG_PATH, "a", encoding="utf-8") as log_file:
            log_file.write(json.dumps(payload, ensure_ascii=False) + "\n")
    except Exception as exc:
        logger.debug(f"Failed to write debug log: {exc}")


def _progress_cache_key(batch_id: str) -> str:
    return f"{REDIS_PROGRESS_KEY_PREFIX}:{batch_id}"


def _decode_redis_value(value: Any) -> str:
    if isinstance(value, (bytes, bytearray)):
        return value.decode("utf-8", errors="ignore")
    return str(value)


async def _get_redis_client() -> Optional[redis.Redis]:
    global _REDIS_CLIENT, _REDIS_CLIENT_CHECKED
    if _REDIS_CLIENT_CHECKED:
        return _REDIS_CLIENT
    _REDIS_CLIENT_CHECKED = True
    try:
        pool_manager = await UnifiedPoolManager.get_instance()
        if pool_manager.is_initialized:
            _REDIS_CLIENT = pool_manager.get_redis_client()
    except PoolNotInitializedError:
        _REDIS_CLIENT = None
    except Exception as exc:
        logger.debug(f"Redis client unavailable: {exc}")
        _REDIS_CLIENT = None
    return _REDIS_CLIENT


async def _clear_progress_fields(
    redis_client: redis.Redis,
    cache_key: str,
    prefixes: Optional[List[str]] = None,
    fields: Optional[List[str]] = None,
) -> None:
    to_delete: List[Any] = []
    if fields:
        to_delete.extend(fields)
    if prefixes:
        try:
            existing_fields = await redis_client.hkeys(cache_key)
        except RedisError as exc:
            logger.debug(f"Failed to fetch Redis fields for cleanup: {exc}")
            existing_fields = []
        for field in existing_fields:
            field_name = _decode_redis_value(field)
            if any(field_name.startswith(prefix) for prefix in prefixes):
                to_delete.append(field)
    if not to_delete:
        return
    try:
        await redis_client.hdel(cache_key, *to_delete)
    except RedisError as exc:
        logger.debug(f"Failed to cleanup Redis progress cache: {exc}")


async def _cache_progress_message(batch_id: str, message: dict) -> None:
    msg_type = message.get("type", "unknown")
    if msg_type in _REDIS_CACHE_SKIP_TYPES:
        return

    redis_client = await _get_redis_client()
    if not redis_client:
        return

    cache_key = _progress_cache_key(batch_id)
    field = msg_type
    if msg_type == "workflow_update":
        node_id = message.get("nodeId")
        if node_id:
            field = f"{msg_type}:{node_id}"

    try:
        payload = json.dumps(message, ensure_ascii=False, default=str)
        await redis_client.hset(cache_key, field, payload)
        await redis_client.expire(cache_key, REDIS_PROGRESS_TTL_SECONDS)
        if msg_type in ("review_completed", "workflow_completed"):
            await _clear_progress_fields(
                redis_client,
                cache_key,
                fields=["review_required"],
            )
    except (TypeError, ValueError) as exc:
        logger.debug(f"Failed to serialize progress message: {exc}")
    except RedisError as exc:
        logger.debug(f"Failed to cache progress message in Redis: {exc}")


async def _load_cached_progress_messages(batch_id: str) -> List[dict]:
    redis_client = await _get_redis_client()
    if not redis_client:
        return []
    cache_key = _progress_cache_key(batch_id)
    try:
        entries = await redis_client.hgetall(cache_key)
    except RedisError as exc:
        logger.debug(f"Failed to fetch cached progress from Redis: {exc}")
        return []

    messages: List[dict] = []
    for field in sorted(entries.keys(), key=_decode_redis_value):
        payload = entries.get(field)
        if payload is None:
            continue
        raw = _decode_redis_value(payload)
        try:
            message = json.loads(raw)
        except json.JSONDecodeError:
            continue
        if isinstance(message, dict):
            messages.append(message)
    return messages


def _safe_to_jpeg_bytes(image_bytes: bytes, label: str) -> bytes:
    try:
        return to_jpeg_bytes(image_bytes)
    except Exception as exc:
        logger.debug(f"Failed to convert image to JPEG ({label}): {exc}")
        return image_bytes


def _normalize_teacher_key(teacher_id: Optional[str]) -> str:
    if teacher_id and teacher_id.strip():
        return teacher_id.strip()
    return "anonymous"


class BatchSubmissionResponse(BaseModel):
    """æ‰¹é‡æäº¤å“åº”"""

    batch_id: str = Field(..., description="æ‰¹æ¬¡ ID")
    status: SubmissionStatus = Field(..., description="çŠ¶æ€")
    total_pages: int = Field(..., description="æ€»é¡µæ•°")
    estimated_completion_time: int = Field(..., description="é¢„è®¡å®Œæˆæ—¶é—´ï¼ˆç§’ï¼‰")


class BatchStatusResponse(BaseModel):
    """æ‰¹é‡çŠ¶æ€æŸ¥è¯¢å“åº”"""

    batch_id: str
    exam_id: str
    status: str
    current_stage: Optional[str] = None
    error: Optional[str] = None
    total_students: int = Field(0, description="è¯†åˆ«åˆ°çš„å­¦ç”Ÿæ•°")
    completed_students: int = Field(0, description="å·²å®Œæˆæ‰¹æ”¹çš„å­¦ç”Ÿæ•°")
    unidentified_pages: int = Field(0, description="æœªè¯†åˆ«å­¦ç”Ÿçš„é¡µæ•°")
    results: Optional[List[dict]] = Field(None, description="æ‰¹æ”¹ç»“æœ")


class ActiveRunItem(BaseModel):
    batch_id: str
    status: str
    class_id: Optional[str] = None
    homework_id: Optional[str] = None
    created_at: Optional[str] = None
    updated_at: Optional[str] = None
    started_at: Optional[str] = None
    completed_at: Optional[str] = None
    total_pages: Optional[int] = None
    progress: Optional[float] = None
    current_stage: Optional[str] = None


class ActiveRunsResponse(BaseModel):
    teacher_id: str
    runs: List[ActiveRunItem]


class RubricReviewContextResponse(BaseModel):
    """å‰ç«¯ rubric review ä¸Šä¸‹æ–‡"""

    batch_id: str
    status: Optional[str] = None
    current_stage: Optional[str] = None
    parsed_rubric: Optional[dict] = None
    rubric_images: List[str] = []


class ResultsReviewContextResponse(BaseModel):
    """å‰ç«¯ results review ä¸Šä¸‹æ–‡"""

    batch_id: str
    status: Optional[str] = None
    current_stage: Optional[str] = None
    student_results: List[dict] = []
    answer_images: List[str] = []


def _pdf_to_images(pdf_path: str, dpi: int = 150) -> List[bytes]:
    """å°† PDF è½¬æ¢ä¸ºå›¾åƒåˆ—è¡¨"""
    pdf_doc = fitz.open(pdf_path)
    images = []

    for page_num in range(len(pdf_doc)):
        page = pdf_doc[page_num]
        mat = fitz.Matrix(dpi / 72, dpi / 72)
        pix = page.get_pixmap(matrix=mat)
        img = Image.frombytes("RGB", [pix.width, pix.height], pix.samples)

        images.append(pil_to_jpeg_bytes(img))

    pdf_doc.close()
    return images


async def broadcast_progress(batch_id: str, message: dict):
    """å‘æ‰€æœ‰è¿æ¥çš„ WebSocket å®¢æˆ·ç«¯å¹¿æ’­è¿›åº¦"""
    # #region agent log - å‡è®¾J: broadcast_progress è¢«è°ƒç”¨
    msg_type = message.get("type", "unknown")
    if msg_type in ("images_ready", "rubric_images_ready", "review_required"):
        cached = batch_image_cache.setdefault(batch_id, {})
        cached[msg_type] = message
    if msg_type == "llm_stream_chunk":
        node_id = message.get("nodeId") or ""
        if node_id in ("rubric_parse", "rubric_review"):
            cached = batch_image_cache.setdefault(batch_id, {})
            stream_cache = cached.setdefault("llm_stream_cache", {})
            cache_key = f"{node_id}:{message.get('agentId') or 'all'}:{message.get('streamType') or 'output'}"
            existing = stream_cache.get(cache_key, {})
            chunk_data = message.get("chunk", "") or ""
            if isinstance(chunk_data, list):
                chunk_data = "".join([str(c) for c in chunk_data])
            else:
                chunk_data = str(chunk_data)

            existing_chunk = existing.get("chunk", "") or ""
            combined = existing_chunk + chunk_data
            max_chars = 12000
            if len(combined) > max_chars:
                combined = combined[-max_chars:]
            stream_cache[cache_key] = {
                **message,
                "chunk": combined,
            }
    if msg_type in ("review_completed", "workflow_completed"):
        cached = batch_image_cache.get(batch_id)
        if cached and "review_required" in cached:
            cached.pop("review_required", None)
        if cached and "llm_stream_cache" in cached:
            cached.pop("llm_stream_cache", None)
    if msg_type == "workflow_completed":
        import traceback as _tb_j

        stack = "".join(_tb_j.format_stack()[-5:-1])  # è·å–è°ƒç”¨æ ˆ
        _write_debug_log(
            {
                "hypothesisId": "J",
                "location": "batch_langgraph.py:broadcast_progress",
                "message": "broadcast_progresså‘é€workflow_completed",
                "data": {
                    "batch_id": batch_id,
                    "results_count": len(message.get("results", [])),
                    "stack_trace": stack[:500],
                },
                "timestamp": int(datetime.now().timestamp() * 1000),
                "sessionId": "debug-session",
            }
        )
    # #endregion
    if msg_type in (
        "workflow_update",
        "grading_progress",
        "workflow_completed",
        "workflow_error",
        "batch_error",
    ):
        run_updates: Dict[str, Any] = {"updated_at": datetime.now().isoformat()}
        if msg_type == "workflow_update":
            status = message.get("status")
            if status == "pending":
                run_updates["status"] = "queued"
            elif status in ("running", "paused"):
                run_updates["status"] = "running"
            elif status == "completed":
                run_updates["status"] = "completed"
                run_updates["completed_at"] = run_updates["updated_at"]
            elif status == "failed":
                run_updates["status"] = "failed"
                run_updates["completed_at"] = run_updates["updated_at"]
            node_id = message.get("nodeId")
            if node_id:
                run_updates["current_stage"] = node_id
        elif msg_type == "grading_progress":
            percentage = message.get("percentage")
            if percentage is not None:
                try:
                    progress_value = float(percentage)
                    if progress_value > 1.0:
                        progress_value = progress_value / 100.0
                    run_updates["progress"] = max(0.0, min(progress_value, 1.0))
                except (TypeError, ValueError):
                    pass
            stage = message.get("currentStage")
            if stage:
                run_updates["current_stage"] = stage
        elif msg_type == "workflow_completed":
            run_updates.update(
                {
                    "status": "completed",
                    "completed_at": run_updates["updated_at"],
                    "progress": 1.0,
                }
            )
        else:
            run_updates.update(
                {
                    "status": "failed",
                    "completed_at": run_updates["updated_at"],
                }
            )
        run_controller = await get_run_controller()
        if run_controller:
            await run_controller.update_run(batch_id, run_updates)
    try:
        await _cache_progress_message(batch_id, message)
    except Exception as exc:
        logger.debug(f"Failed to cache progress message: {exc}")
    if batch_id in active_connections:
        disconnected = []
        for ws in active_connections[batch_id]:
            if not _is_ws_connected(ws):
                disconnected.append(ws)
                continue
            try:
                await ws.send_json(message)
            except WebSocketDisconnect:
                disconnected.append(ws)
            except RuntimeError as exc:
                logger.debug(f"WebSocket å‘é€è¢«è·³è¿‡: {exc}")
                disconnected.append(ws)
            except Exception as e:
                logger.error(f"WebSocket å‘é€å¤±è´¥: {e}")
                disconnected.append(ws)

        # ç§»é™¤æ–­å¼€çš„è¿æ¥
        for ws in disconnected:
            _discard_connection(batch_id, ws)


async def _start_run_with_teacher_limit(
    *,
    teacher_key: str,
    batch_id: str,
    payload: Dict[str, Any],
    orchestrator: Orchestrator,
    class_id: Optional[str],
    homework_id: Optional[str],
    student_mapping: List[dict],
) -> Optional[str]:
    logger.info(f"[_start_run_with_teacher_limit] å¼€å§‹æ‰§è¡Œ: batch_id={batch_id}")
    logger.info(f"[_start_run_with_teacher_limit] payload keys: {list(payload.keys())}")
    logger.info(f"[_start_run_with_teacher_limit] answer_images count: {len(payload.get('answer_images', []))}")
    logger.info(f"[_start_run_with_teacher_limit] rubric_images count: {len(payload.get('rubric_images', []))}")
    
    logger.info(f"[_start_run_with_teacher_limit] å‡†å¤‡è°ƒç”¨ get_run_controller()")
    try:
        run_controller = await get_run_controller()
        logger.info(f"[_start_run_with_teacher_limit] get_run_controller() è¿”å›: {run_controller is not None}")
    except Exception as e:
        logger.error(f"[_start_run_with_teacher_limit] get_run_controller() å¼‚å¸¸: {e}", exc_info=True)
        run_controller = None
    
    if run_controller:
        logger.info(f"[_start_run_with_teacher_limit] å‡†å¤‡è·å– slot: teacher_key={teacher_key}")
        try:
            acquired = await run_controller.try_acquire_slot(
                teacher_key,
                batch_id,
                TEACHER_MAX_ACTIVE_RUNS,
            )
            logger.info(f"[_start_run_with_teacher_limit] try_acquire_slot è¿”å›: {acquired}")
        except Exception as e:
            logger.error(f"[_start_run_with_teacher_limit] try_acquire_slot å¼‚å¸¸: {e}", exc_info=True)
            acquired = False
        
        if not acquired:
            logger.info(f"[_start_run_with_teacher_limit] æœªè·å–åˆ° slotï¼Œè¿›å…¥ç­‰å¾…é˜Ÿåˆ—")
            await broadcast_progress(
                batch_id,
                {
                    "type": "workflow_update",
                    "nodeId": "rubric_parse",
                    "status": "pending",
                    "message": "Queued: waiting for grading slot",
                },
            )
            logger.info(f"[_start_run_with_teacher_limit] å¼€å§‹ç­‰å¾… slot")
            max_wait = RUN_QUEUE_TIMEOUT_SECONDS if RUN_QUEUE_TIMEOUT_SECONDS > 0 else None
            acquired = await run_controller.wait_for_slot(
                teacher_key,
                batch_id,
                TEACHER_MAX_ACTIVE_RUNS,
                RUN_QUEUE_POLL_SECONDS,
                max_wait,
            )
            logger.info(f"[_start_run_with_teacher_limit] wait_for_slot è¿”å›: {acquired}")
            if not acquired:
                logger.info(f"[_start_run_with_teacher_limit] ç­‰å¾…è¶…æ—¶ï¼Œä»»åŠ¡å¤±è´¥")
                await run_controller.update_run(
                    batch_id,
                    {"status": "failed", "updated_at": datetime.now().isoformat()},
                )
                await run_controller.remove_from_queue(teacher_key, batch_id)
                await broadcast_progress(
                    batch_id,
                    {
                        "type": "workflow_update",
                        "nodeId": "rubric_parse",
                        "status": "failed",
                        "message": "Queued run timed out",
                    },
                )
                return None
        
        logger.info(f"[_start_run_with_teacher_limit] æˆåŠŸè·å– slotï¼Œæ›´æ–°è¿è¡ŒçŠ¶æ€")
        await run_controller.update_run(
            batch_id,
            {
                "status": "running",
                "started_at": datetime.now().isoformat(),
                "updated_at": datetime.now().isoformat(),
            },
        )
    
    run_id: Optional[str] = None
    try:
        logger.info(f"[_start_run_with_teacher_limit] å‡†å¤‡å¯åŠ¨ LangGraph run")
        run_id = await orchestrator.start_run(
            graph_name="batch_grading", payload=payload, idempotency_key=batch_id
        )
        logger.info(f"[_start_run_with_teacher_limit] LangGraph å¯åŠ¨æˆåŠŸ: batch_id={batch_id}, run_id={run_id}")
        
        stream_task = asyncio.create_task(
            stream_langgraph_progress(
                batch_id=batch_id,
                run_id=run_id,
                orchestrator=orchestrator,
                class_id=class_id,
                homework_id=homework_id,
                student_mapping=student_mapping,
                teacher_key=teacher_key,
            )
        )
        _active_stream_tasks[batch_id] = stream_task
        logger.info(f"[_start_run_with_teacher_limit] æµå¼ä»»åŠ¡å·²åˆ›å»º")
        return run_id
    except Exception as exc:
        logger.error(f"[_start_run_with_teacher_limit] å¯åŠ¨å¤±è´¥: {exc}", exc_info=True)
        if run_controller:
            await run_controller.release_slot(teacher_key, batch_id)
            await run_controller.update_run(
                batch_id,
                {"status": "failed", "updated_at": datetime.now().isoformat()},
            )
        await broadcast_progress(
            batch_id,
            {
                "type": "workflow_update",
                "nodeId": "rubric_parse",
                "status": "failed",
                "message": "Queued run failed to start",
            },
        )
        return None


@router.post("/submit", response_model=BatchSubmissionResponse)
async def submit_batch(
    exam_id: Optional[str] = Form(None, description="è€ƒè¯• ID"),
    rubrics: List[UploadFile] = File(default=[], description="è¯„åˆ†æ ‡å‡† PDFï¼ˆå¯é€‰ï¼‰"),
    files: List[UploadFile] = File(..., description="å­¦ç”Ÿä½œç­” PDF"),
    api_key: Optional[str] = Form(None, description="LLM API Key"),
    teacher_id: Optional[str] = Form(None, description="?? ID"),
    auto_identify: bool = Form(True, description="æ˜¯å¦è‡ªåŠ¨è¯†åˆ«å­¦ç”Ÿèº«ä»½"),
    student_boundaries: Optional[str] = Form(
        None, description="æ‰‹åŠ¨è®¾ç½®çš„å­¦ç”Ÿè¾¹ç•Œ (JSON List of page indices)"
    ),
    expected_students: Optional[int] = Form(
        None, description="é¢„æœŸå­¦ç”Ÿæ•°é‡ï¼ˆå¼ºçƒˆå»ºè®®æä¾›ï¼Œç”¨äºæ›´å‡†ç¡®çš„åˆ†å‰²ï¼‰"
    ),
    expected_total_score: Optional[float] = Form(None, description="Expected total score"),
    # æ–°å¢ï¼šç­çº§æ‰¹æ”¹ä¸Šä¸‹æ–‡
    class_id: Optional[str] = Form(None, description="ç­çº§ IDï¼ˆç”¨äºæˆç»©å†™å›ï¼‰"),
    homework_id: Optional[str] = Form(None, description="ä½œä¸š IDï¼ˆç”¨äºæˆç»©å†™å›ï¼‰"),
    student_mapping_json: Optional[str] = Form(
        None, description="å­¦ç”Ÿæ˜ å°„ JSON [{studentId, studentName, startIndex, endIndex}]"
    ),
    enable_review: bool = Form(True, description="æ˜¯å¦å¯ç”¨äººå·¥äº¤äº’"),
    grading_mode: Optional[str] = Form(
        None, description="grading mode: standard/assist_teacher/assist_student/auto"
    ),
    orchestrator: Orchestrator = Depends(get_orchestrator),
):
    """
    æ‰¹é‡æäº¤è¯•å·å¹¶è¿›è¡Œæ‰¹æ”¹ï¼ˆä½¿ç”¨ LangGraph Orchestratorï¼‰

    æ­£ç¡®çš„æ¶æ„ï¼š
    1. ä½¿ç”¨ LangGraph Orchestrator å¯åŠ¨ batch_grading Graph
    2. Graph è‡ªåŠ¨å¤„ç†ï¼šè¾¹ç•Œæ£€æµ‹ â†’ å¹¶è¡Œæ‰¹æ”¹ â†’ èšåˆ â†’ æŒä¹…åŒ– â†’ é€šçŸ¥
    3. é€šè¿‡ WebSocket å®æ—¶æ¨é€ LangGraph çš„æ‰§è¡Œè¿›åº¦

    Args:
        exam_id: è€ƒè¯• ID
        rubrics: è¯„åˆ†æ ‡å‡† PDF æ–‡ä»¶åˆ—è¡¨
        files: å­¦ç”Ÿä½œç­” PDF æ–‡ä»¶åˆ—è¡¨
        api_key: LLM API Key
        auto_identify: æ˜¯å¦å¯ç”¨è‡ªåŠ¨å­¦ç”Ÿè¯†åˆ«
        orchestrator: LangGraph Orchestratorï¼ˆä¾èµ–æ³¨å…¥ï¼‰

    Returns:
        BatchSubmissionResponse: æ‰¹æ¬¡ä¿¡æ¯
    """
    # #region agent log - å‡è®¾K: submit_batch è¢«è°ƒç”¨
    _write_debug_log(
        {
            "hypothesisId": "K",
            "location": "batch_langgraph.py:submit_batch:entry",
            "message": "submit_batchç«¯ç‚¹è¢«è°ƒç”¨",
            "data": {"files_count": len(files), "rubrics_count": len(rubrics)},
            "timestamp": int(datetime.now().timestamp() * 1000),
            "sessionId": "debug-session",
        }
    )
    # #endregion
    # æ£€æŸ¥ orchestrator æ˜¯å¦å¯ç”¨
    if not orchestrator:
        raise HTTPException(status_code=503, detail="æ‰¹æ”¹æœåŠ¡æœªåˆå§‹åŒ–ï¼Œè¯·ç¨åé‡è¯•æˆ–æ£€æŸ¥æœåŠ¡é…ç½®")

    if not api_key:
        api_key = os.getenv("LLM_API_KEY") or os.getenv("OPENROUTER_API_KEY")

    if not api_key:
        raise HTTPException(
            status_code=400,
            detail="æœªæä¾› API Keyï¼Œè¯·åœ¨è¯·æ±‚ä¸­æä¾›æˆ–é…ç½®ç¯å¢ƒå˜é‡ LLM_API_KEY/OPENROUTER_API_KEY",
        )

    # è§£æå­¦ç”Ÿè¾¹ç•Œ
    parsed_boundaries = []
    if student_boundaries:
        try:
            logger.debug(
                f"æ¥æ”¶åˆ°åŸå§‹ student_boundaries: {student_boundaries} (type: {type(student_boundaries)})"
            )
            import json

            parsed_boundaries = json.loads(student_boundaries)
            logger.debug(f"è§£æåçš„ manual_boundaries: {parsed_boundaries}")
        except Exception as e:
            logger.debug(f"è§£ææ‰‹åŠ¨å­¦ç”Ÿè¾¹ç•Œå¤±è´¥: {e}")

    if not exam_id:
        exam_id = str(uuid.uuid4())

    batch_id = str(uuid.uuid4())

    logger.info(
        f"æ”¶åˆ°æ‰¹é‡æäº¤ï¼ˆLangGraphï¼‰: "
        f"batch_id={batch_id}, "
        f"exam_id={exam_id}, "
        f"auto_identify={auto_identify}"
    )

    temp_dir = None
    try:
        # åˆ›å»ºä¸´æ—¶ç›®å½•
        temp_dir = tempfile.mkdtemp()
        temp_path = Path(temp_dir)

        # === å¤„ç†ç­”é¢˜æ–‡ä»¶ï¼ˆæ”¯æŒå›¾ç‰‡åˆ—è¡¨æˆ–å•ä¸ª PDFï¼‰===
        answer_images = []

        for idx, file in enumerate(files):
            file_name = file.filename or f"file_{idx}"
            content = await file.read()

            # æ£€æŸ¥æ–‡ä»¶ç±»å‹
            if file_name.lower().endswith((".png", ".jpg", ".jpeg", ".webp", ".gif")):
                # å›¾ç‰‡æ–‡ä»¶ï¼šç›´æ¥ä½¿ç”¨å†…å®¹
                answer_images.append(_safe_to_jpeg_bytes(content, file_name))
                logger.debug(f"è¯»å–å›¾ç‰‡æ–‡ä»¶: {file_name}, å¤§å°: {len(content)} bytes")
            elif file_name.lower().endswith(".pdf"):
                # PDF æ–‡ä»¶ï¼šè½¬æ¢ä¸ºå›¾åƒ
                pdf_path = temp_path / f"answer_{idx}.pdf"
                with open(pdf_path, "wb") as f:
                    f.write(content)
                loop = asyncio.get_event_loop()
                pdf_images = await loop.run_in_executor(None, _pdf_to_images, str(pdf_path), 150)
                answer_images.extend(pdf_images)
                logger.debug(f"PDF æ–‡ä»¶ {file_name} è½¬æ¢ä¸º {len(pdf_images)} é¡µå›¾ç‰‡")
            elif file_name.lower().endswith(".txt"):
                # æ–‡æœ¬æ–‡ä»¶ï¼šç›´æ¥ä½¿ç”¨å†…å®¹
                answer_images.append(content)
                logger.debug(f"æ–‡æœ¬æ–‡ä»¶å¤„ç†å®Œæˆ: {file_name}, å†…å®¹é•¿åº¦={len(content)}")
            else:
                # å°è¯•ä½œä¸ºå›¾ç‰‡å¤„ç†ï¼ˆå¯èƒ½æ²¡æœ‰æ‰©å±•åï¼‰
                answer_images.append(_safe_to_jpeg_bytes(content, file_name))
                logger.debug(f"æœªçŸ¥æ–‡ä»¶ç±»å‹ {file_name}ï¼Œå°è¯•ä½œä¸ºå›¾ç‰‡å¤„ç†")

        total_pages = len(answer_images)

        teacher_key = _normalize_teacher_key(teacher_id)
        run_controller = await get_run_controller()
        if run_controller:
            await run_controller.register_run(
                GradingRunSnapshot(
                    batch_id=batch_id,
                    teacher_id=teacher_key,
                    status="queued",
                    class_id=class_id,
                    homework_id=homework_id,
                    created_at=datetime.now().isoformat(),
                    updated_at=datetime.now().isoformat(),
                    total_pages=total_pages,
                )
            )
        logger.info(f"ç­”é¢˜æ–‡ä»¶å¤„ç†å®Œæˆ: batch_id={batch_id}, æ€»é¡µæ•°={total_pages}")

        # === å¤„ç†è¯„åˆ†æ ‡å‡†ï¼ˆå¯é€‰ï¼‰===
        # Convert images to base64 and cache them immediately
        # (Fix: Rubric images not displaying on frontend)
        if answer_images:
            try:
                base64_images = [base64.b64encode(img).decode("utf-8") for img in answer_images]

                # Cache for direct WebSocket connection
                batch_image_cache.setdefault(batch_id, {})["images_ready"] = {
                    "type": "images_ready",
                    "images": base64_images,
                }

                # Broadcast (though no clients connected yet usually)
                await broadcast_progress(
                    batch_id, {"type": "images_ready", "images": base64_images}
                )
                logger.info(f"å·²ç¼“å­˜ {len(base64_images)} å¼ å›¾ç‰‡ç”¨äºå‰ç«¯æ˜¾ç¤º")
            except Exception as e:
                logger.error(f"å›¾ç‰‡ Base64 è½¬æ¢å¤±è´¥: {e}")

        # === å¤„ç†è¯„åˆ†æ ‡å‡†ï¼ˆå¯é€‰ï¼‰===
        rubric_images = []
        if rubrics and len(rubrics) > 0:
            for idx, rubric_file in enumerate(rubrics):
                rubric_name = rubric_file.filename or f"rubric_{idx}"
                rubric_content = await rubric_file.read()

                if rubric_name.lower().endswith((".png", ".jpg", ".jpeg", ".webp", ".gif")):
                    rubric_images.append(_safe_to_jpeg_bytes(rubric_content, rubric_name))
                elif rubric_name.lower().endswith(".pdf"):
                    rubric_path = temp_path / f"rubric_{idx}.pdf"
                    with open(rubric_path, "wb") as f:
                        f.write(rubric_content)
                    loop = asyncio.get_event_loop()
                    pdf_rubric_images = await loop.run_in_executor(
                        None, _pdf_to_images, str(rubric_path), 150
                    )
                    rubric_images.extend(pdf_rubric_images)
                else:
                    rubric_images.append(_safe_to_jpeg_bytes(rubric_content, rubric_name))

            logger.info(f"è¯„åˆ†æ ‡å‡†å¤„ç†å®Œæˆ: batch_id={batch_id}, æ€»é¡µæ•°={len(rubric_images)}")
            if rubric_images:
                try:
                    base64_rubric_images = [
                        base64.b64encode(img).decode("utf-8") for img in rubric_images
                    ]
                    batch_image_cache.setdefault(batch_id, {})["rubric_images_ready"] = {
                        "type": "rubric_images_ready",
                        "images": base64_rubric_images,
                    }
                    await broadcast_progress(
                        batch_id, {"type": "rubric_images_ready", "images": base64_rubric_images}
                    )
                    logger.info(f"å·²ç¼“å­˜ {len(base64_rubric_images)} å¼ è¯„åˆ†æ ‡å‡†å›¾ç‰‡ç”¨äºå‰ç«¯æ˜¾ç¤º")
                except Exception as e:
                    logger.error(f"è¯„åˆ†æ ‡å‡† Base64 è½¬æ¢å¤±è´¥: {e}")
        else:
            logger.info(f"æœªæä¾›è¯„åˆ†æ ‡å‡†ï¼Œå°†ä½¿ç”¨é»˜è®¤è¯„åˆ†: batch_id={batch_id}")

        logger.info(
            f"æ–‡ä»¶å¤„ç†å®Œæˆ: "
            f"batch_id={batch_id}, "
            f"rubric_pages={len(rubric_images)}, "
            f"answer_pages={total_pages}"
        )

        # ğŸ“ æŒä¹…åŒ–å­˜å‚¨åŸå§‹æ–‡ä»¶ï¼ˆå¯é€‰ï¼Œé€šè¿‡ç¯å¢ƒå˜é‡ ENABLE_FILE_STORAGE æ§åˆ¶ï¼‰
        stored_files: List[StoredFile] = []
        # ğŸ“ æŒä¹…åŒ–å­˜å‚¨åŸå§‹æ–‡ä»¶ï¼ˆå¯é€‰ï¼Œé€šè¿‡ç¯å¢ƒå˜é‡ ENABLE_FILE_STORAGE æ§åˆ¶ï¼‰
        stored_files: List[StoredFile] = []
        if os.getenv("ENABLE_FILE_STORAGE", "false").lower() == "true":
            try:
                file_storage = get_file_storage_service()
                
                # ä¿å­˜ç­”é¢˜æ–‡ä»¶ï¼ˆä»¥å¤„ç†åçš„å›¾ç‰‡å½¢å¼ï¼‰
                answer_filenames = [f"answer_page_{i+1}.jpg" for i in range(len(answer_images))]
                stored_answers = await file_storage.save_answer_files(
                    batch_id=batch_id,
                    files=answer_images,
                    filenames=answer_filenames,
                )
                stored_files.extend(stored_answers)
                
                # ä¿å­˜è¯„åˆ†æ ‡å‡†æ–‡ä»¶ï¼ˆå¦‚æœæœ‰ï¼‰
                if rubric_images:
                    rubric_filenames = [f"rubric_page_{i+1}.jpg" for i in range(len(rubric_images))]
                    stored_rubrics = await file_storage.save_rubric_files(
                        batch_id=batch_id,
                        files=rubric_images,
                        filenames=rubric_filenames,
                    )
                    stored_files.extend(stored_rubrics)
                
                logger.info(
                    f"[FileStorage] æ–‡ä»¶å­˜å‚¨å®Œæˆ: batch_id={batch_id}, "
                    f"å…±ä¿å­˜ {len(stored_files)} ä¸ªæ–‡ä»¶"
                )
            except Exception as e:
                logger.warning(f"[FileStorage] æ–‡ä»¶å­˜å‚¨å¤±è´¥ï¼ˆä¸å½±å“æ‰¹æ”¹æµç¨‹ï¼‰: {e}")

        # ğŸš€ ä½¿ç”¨ LangGraph Orchestrator å¯åŠ¨æ‰¹æ”¹æµç¨‹

        # è§£æå­¦ç”Ÿæ˜ å°„ï¼ˆç­çº§æ‰¹æ”¹æ¨¡å¼ï¼‰
        student_mapping = []
        if student_mapping_json:
            try:
                import json

                student_mapping = json.loads(student_mapping_json)
                logger.info(
                    f"ç­çº§æ‰¹æ”¹æ¨¡å¼: class_id={class_id}, homework_id={homework_id}, å­¦ç”Ÿæ•°={len(student_mapping)}"
                )
            except Exception as e:
                logger.debug(f"è§£æå­¦ç”Ÿæ˜ å°„å¤±è´¥: {e}")

        resolved_expected_students = expected_students or 0
        if resolved_expected_students <= 0:
            if student_mapping:
                resolved_expected_students = len(student_mapping)
            elif parsed_boundaries:
                resolved_expected_students = len(parsed_boundaries)
            else:
                resolved_expected_students = 1

        payload = {
            "batch_id": batch_id,
            "exam_id": exam_id,
            "temp_dir": str(temp_path),  # ä¸´æ—¶ç›®å½•ï¼ˆç”¨äºæ¸…ç†ï¼‰
            "rubric_images": rubric_images,
            "answer_images": answer_images,
            "api_key": api_key,
            # ç­çº§æ‰¹æ”¹ä¸Šä¸‹æ–‡ï¼ˆå¯é€‰ï¼‰
            "class_id": class_id,
            "homework_id": homework_id,
            "student_mapping": student_mapping,
            "inputs": {
                "rubric": "rubric_content",  # TODO: è§£æ rubric
                "auto_identify": auto_identify,
                "manual_boundaries": parsed_boundaries,  # ä¼ é€’äººå·¥è¾¹ç•Œ
                "expected_students": resolved_expected_students,
                "expected_total_score": expected_total_score,
                "enable_review": enable_review,
                "grading_mode": grading_mode or "auto",
            },
        }

        # å¯åŠ¨ LangGraph batch_grading Graph
        logger.info(f"å‡†å¤‡å¯åŠ¨æ‰¹æ”¹ä»»åŠ¡: batch_id={batch_id}, answer_images={len(answer_images)}, rubric_images={len(rubric_images)}")
        
        task = asyncio.create_task(
            _start_run_with_teacher_limit(
                teacher_key=teacher_key,
                batch_id=batch_id,
                payload=payload,
                orchestrator=orchestrator,
                class_id=class_id,
                homework_id=homework_id,
                student_mapping=student_mapping,
            )
        )
        
        # æ·»åŠ é”™è¯¯å¤„ç†å›è°ƒ
        def task_done_callback(t):
            try:
                t.result()
            except Exception as e:
                logger.error(f"æ‰¹æ”¹ä»»åŠ¡å¯åŠ¨å¤±è´¥: batch_id={batch_id}, error={e}", exc_info=True)
        
        task.add_done_callback(task_done_callback)
        logger.info(f"æ‰¹æ”¹ä»»åŠ¡å·²æäº¤åˆ°äº‹ä»¶å¾ªç¯: batch_id={batch_id}")

        return BatchSubmissionResponse(
            batch_id=batch_id,
            status=SubmissionStatus.UPLOADED,
            total_pages=total_pages,
            estimated_completion_time=total_pages * 3,  # Estimated: 3s per page
        )

    except Exception as e:
        logger.error(f"æ‰¹é‡æäº¤å¤±è´¥: {str(e)}", exc_info=True)
        raise HTTPException(status_code=500, detail=f"æ‰¹é‡æäº¤å¤±è´¥: {str(e)}")


async def stream_langgraph_progress(
    batch_id: str,
    run_id: str,
    orchestrator: Orchestrator,
    class_id: Optional[str] = None,
    homework_id: Optional[str] = None,
    student_mapping: Optional[List[dict]] = None,
    teacher_key: Optional[str] = None,
):
    """
    æµå¼ç›‘å¬ LangGraph æ‰§è¡Œè¿›åº¦å¹¶æ¨é€åˆ° WebSocket

    è¿™æ˜¯å®ç°å®æ—¶è¿›åº¦æ¨é€çš„å…³é”®å‡½æ•°ï¼

    Args:
        batch_id: æ‰¹æ¬¡ ID
        run_id: LangGraph è¿è¡Œ ID
        orchestrator: LangGraph Orchestrator
    """
    # #region agent log - å‡è®¾G: stream_langgraph_progress å…¥å£
    _write_debug_log(
        {
            "hypothesisId": "G",
            "location": "batch_langgraph.py:stream_langgraph_progress:entry",
            "message": "stream_langgraph_progresså‡½æ•°è¢«è°ƒç”¨",
            "data": {"batch_id": batch_id, "run_id": run_id},
            "timestamp": int(datetime.now().timestamp() * 1000),
            "sessionId": "debug-session",
        }
    )
    # #endregion
    logger.info(f"å¼€å§‹æµå¼ç›‘å¬ LangGraph è¿›åº¦: batch_id={batch_id}, run_id={run_id}")

    try:
        # ğŸ”¥ ä½¿ç”¨ LangGraph çš„æµå¼ API
        async for event in orchestrator.stream_run(run_id):
            event_type = event.get("type")
            node_name = event.get("node")
            data = event.get("data", {})

            logger.debug(
                f"LangGraph äº‹ä»¶: batch_id={batch_id}, type={event_type}, node={node_name}"
            )

            # å°† LangGraph äº‹ä»¶è½¬æ¢ä¸ºå‰ç«¯ WebSocket æ¶ˆæ¯
            if event_type == "node_start":
                await broadcast_progress(
                    batch_id,
                    {
                        "type": "workflow_update",
                        "nodeId": _map_node_to_frontend(node_name),
                        "status": "running",
                        "message": f"Running {_get_node_display_name(node_name)}...",
                    },
                )

            elif event_type == "node_end":
                await broadcast_progress(
                    batch_id,
                    {
                        "type": "workflow_update",
                        "nodeId": _map_node_to_frontend(node_name),
                        "status": "completed",
                        "message": f"{_get_node_display_name(node_name)} completed",
                    },
                )

                # å¤„ç†èŠ‚ç‚¹è¾“å‡º
                output = data.get("output", {})
                if isinstance(output, dict):
                    interrupt_payload = output.get("__interrupt__")
                    if interrupt_payload:
                        review_type = (
                            interrupt_payload.get("type")
                            if isinstance(interrupt_payload, dict)
                            else "review_required"
                        )
                        await broadcast_progress(
                            batch_id,
                            {
                                "type": "review_required",
                                "reviewType": review_type,
                                "payload": interrupt_payload,
                                "nodeId": (
                                    _map_node_to_frontend("rubric_review")
                                    if "rubric" in review_type
                                    else _map_node_to_frontend("review")
                                ),
                            },
                        )
                    # è¯„åˆ†æ ‡å‡†è§£æå®Œæˆ
                    if node_name == "rubric_parse" and output.get("parsed_rubric"):
                        parsed = output["parsed_rubric"]
                        await broadcast_progress(
                            batch_id,
                            {
                                "type": "rubric_parsed",
                                "totalQuestions": parsed.get("total_questions", 0),
                                "totalScore": parsed.get("total_score", 0),
                                "generalNotes": parsed.get("general_notes", ""),
                                "rubricFormat": parsed.get("rubric_format", ""),
                                "questions": [
                                    {
                                        "questionId": q.get("question_id", ""),
                                        "maxScore": q.get("max_score", 0),
                                        "questionText": q.get("question_text", ""),
                                        "standardAnswer": q.get("standard_answer", ""),
                                        "gradingNotes": q.get("grading_notes", ""),
                                        "sourcePages": q.get("source_pages")
                                        or q.get("sourcePages")
                                        or [],
                                        "scoringPoints": [
                                            {
                                                "pointId": sp.get("point_id")
                                                or sp.get("pointId")
                                                or f"{q.get('question_id')}.{idx + 1}",
                                                "description": sp.get("description", ""),
                                                "expectedValue": sp.get("expected_value")
                                                or sp.get("expectedValue", ""),
                                                "keywords": sp.get("keywords") or [],
                                                "score": sp.get("score", 0),
                                                "isRequired": sp.get("is_required", True),
                                            }
                                            for idx, sp in enumerate(q.get("scoring_points", []))
                                        ],
                                        "deductionRules": [
                                            {
                                                "ruleId": dr.get("rule_id")
                                                or dr.get("ruleId")
                                                or f"{q.get('question_id')}.d{idx + 1}",
                                                "description": dr.get("description", ""),
                                                "deduction": dr.get(
                                                    "deduction", dr.get("score", 0)
                                                ),
                                                "conditions": dr.get("conditions")
                                                or dr.get("when")
                                                or "",
                                            }
                                            for idx, dr in enumerate(
                                                q.get("deduction_rules")
                                                or q.get("deductionRules")
                                                or []
                                            )
                                        ],
                                        "alternativeSolutions": [
                                            {
                                                "description": alt.get("description", ""),
                                                "scoringCriteria": alt.get("scoring_criteria", ""),
                                                "note": alt.get("note", ""),
                                            }
                                            for alt in q.get("alternative_solutions", [])
                                        ],
                                    }
                                    for q in parsed.get("questions", [])
                                ],
                            },
                        )

                    # æ‰¹æ”¹æ‰¹æ¬¡å®Œæˆ
                    if node_name == "grade_batch" and output.get("grading_results"):
                        results = output["grading_results"]
                        completed = sum(1 for r in results if r.get("status") == "completed")

                        await broadcast_progress(
                            batch_id,
                            {
                                "type": "batch_complete",
                                "batchSize": len(results),
                                "successCount": completed,
                                "totalScore": sum(
                                    r.get("score", 0)
                                    for r in results
                                    if r.get("status") == "completed"
                                ),
                                "pages": [r.get("page_index") for r in results],
                            },
                        )

                    # å­¦ç”Ÿè¯†åˆ«å®Œæˆ
                    if output.get("student_boundaries"):
                        boundaries = output["student_boundaries"]
                        await broadcast_progress(
                            batch_id,
                            {
                                "type": "students_identified",
                                "studentCount": len(boundaries),
                                "students": [
                                    {
                                        "studentKey": b.get("student_key", ""),
                                        "startPage": b.get("start_page", 0),
                                        "endPage": b.get("end_page", 0),
                                        "confidence": b.get("confidence", 0),
                                        "needsConfirmation": b.get("needs_confirmation", False),
                                    }
                                    for b in boundaries
                                ],
                            },
                        )

                    # å®¡æ ¸å®Œæˆ
                    if node_name == "review" and output.get("review_summary"):
                        await broadcast_progress(
                            batch_id,
                            {"type": "review_completed", "summary": output["review_summary"]},
                        )

                    # è·¨é¡µé¢˜ç›®åˆå¹¶å®Œæˆ
                    if node_name == "cross_page_merge":
                        cross_page_questions = output.get("cross_page_questions", [])
                        merged_questions = output.get("merged_questions", [])
                        if cross_page_questions:
                            await broadcast_progress(
                                batch_id,
                                {
                                    "type": "cross_page_detected",
                                    "questions": cross_page_questions,
                                    "mergedCount": len(merged_questions),
                                    "crossPageCount": len(cross_page_questions),
                                },
                            )

            elif event_type == "paused":
                # å¤„ç† Graph ä¸­æ–­/æš‚åœï¼ˆé€šå¸¸æ˜¯éœ€è¦äººå·¥å®¡æ ¸ï¼‰
                data = event.get("data", {})
                interrupt_value = data.get("interrupt_value")

                logger.info(
                    f"LangGraph æš‚åœ: batch_id={batch_id}, interrupt_value={interrupt_value}"
                )

                if interrupt_value:
                    # å¦‚æœæœ‰ä¸­æ–­ payloadï¼Œå¹¿æ’­ review_required
                    review_type = (
                        interrupt_value.get("type")
                        if isinstance(interrupt_value, dict)
                        else "review_required"
                    )
                    await broadcast_progress(
                        batch_id,
                        {
                            "type": "review_required",
                            "reviewType": review_type,
                            "payload": interrupt_value,
                            "nodeId": (
                                _map_node_to_frontend("rubric_review")
                                if "rubric" in review_type
                                else _map_node_to_frontend("review")
                            ),
                        },
                    )
                else:
                    # å¦‚æœæ²¡æœ‰ payloadï¼Œè‡³å°‘é€šçŸ¥çŠ¶æ€å˜æ›´
                    await broadcast_progress(
                        batch_id,
                        {
                            "type": "workflow_update",
                            "status": "paused",
                            "message": "Workflow paused (awaiting input)",
                        },
                    )

            elif event_type == "state_update":
                # æ¨é€çŠ¶æ€æ›´æ–°
                state = data.get("state", {})

                # æ‰¹æ¬¡è¿›åº¦æ›´æ–°
                if state.get("progress"):
                    progress = state["progress"]
                    await broadcast_progress(
                        batch_id,
                        {
                            "type": "batch_progress",
                            "batchIndex": progress.get("current_batch", 0),
                            "totalBatches": progress.get("total_batches", 1),
                            "successCount": progress.get("success_count", 0),
                            "failureCount": progress.get("failure_count", 0),
                        },
                    )

                # ç™¾åˆ†æ¯”è¿›åº¦
                if state.get("percentage"):
                    await broadcast_progress(
                        batch_id,
                        {
                            "type": "grading_progress",
                            "percentage": state["percentage"],
                            "currentStage": state.get("current_stage", ""),
                        },
                    )

            elif event_type == "llm_stream":
                # Real-time LLM token streaming
                node_name = event.get("node") or data.get("node", "")
                chunk = data.get("chunk") or data.get("content") or ""
                await broadcast_progress(
                    batch_id,
                    {
                        "type": "llm_stream_chunk",
                        "nodeId": _map_node_to_frontend(node_name) if node_name else None,
                        "nodeName": _get_node_display_name(node_name) if node_name else None,
                        "chunk": chunk,
                    },
                )

            elif event_type == "error":
                await broadcast_progress(
                    batch_id,
                    {"type": "workflow_error", "message": data.get("error", "Unknown error")},
                )

            elif event_type == "completed":
                # #region agent log - å‡è®¾H: completed äº‹ä»¶
                _write_debug_log(
                    {
                        "hypothesisId": "H",
                        "location": "batch_langgraph.py:event_completed",
                        "message": "æ”¶åˆ°completedäº‹ä»¶",
                        "data": {
                            "event_type": event_type,
                            "data_keys": (
                                list(data.keys()) if isinstance(data, dict) else str(type(data))
                            ),
                        },
                        "timestamp": int(datetime.now().timestamp() * 1000),
                        "sessionId": "debug-session",
                    }
                )
                # #endregion
                # å·¥ä½œæµå®Œæˆ - è·å–å®Œæ•´çš„æœ€ç»ˆçŠ¶æ€
                final_state = data.get("state", {})

                # ä» student_results è·å–ç»“æœ
                student_results = final_state.get("student_results", [])

                # #region agent log - å‡è®¾I: student_results åŸå§‹æ•°æ®
                _write_debug_log(
                    {
                        "hypothesisId": "I",
                        "location": "batch_langgraph.py:student_results_raw",
                        "message": "student_resultsåŸå§‹æ•°æ®",
                        "data": {
                            "count": len(student_results),
                            "students": [
                                {"key": r.get("student_key"), "score": r.get("total_score")}
                                for r in student_results
                            ],
                        },
                        "timestamp": int(datetime.now().timestamp() * 1000),
                        "sessionId": "debug-session",
                    }
                )
                # #endregion

                # å¦‚æœæ²¡æœ‰ student_resultsï¼Œå°è¯•ä» orchestrator è·å–æœ€ç»ˆè¾“å‡º
                if not student_results:
                    try:
                        final_output = await orchestrator.get_final_output(run_id)
                        if final_output:
                            student_results = final_output.get("student_results", [])
                            logger.info(f"ä» orchestrator è·å–åˆ° {len(student_results)} ä¸ªå­¦ç”Ÿç»“æœ")
                    except Exception as e:
                        logger.debug(f"è·å–æœ€ç»ˆè¾“å‡ºå¤±è´¥: {e}")

                if not student_results:
                    grading_results = final_state.get("grading_results") or []
                    if not grading_results:
                        try:
                            final_output = await orchestrator.get_final_output(run_id)
                            if final_output:
                                grading_results = final_output.get("grading_results") or []
                        except Exception as e:
                            logger.debug(f"é‘¾å³°å½‡ grading_results æ¾¶è¾«è§¦: {e}")
                    if grading_results:
                        student_results = _build_student_results_from_grading_results(grading_results)
                        logger.info(
                            f"æµ ?grading_results é­ãˆ î˜² {len(student_results)} æ¶“î„î„Ÿé¢ç†ºç²¨é‹?"
                        )

                formatted_results = _format_results_for_frontend(student_results)
                class_report = final_state.get("class_report")
                if not class_report and final_state.get("export_data"):
                    class_report = final_state.get("export_data", {}).get("class_report")

                # ä¿å­˜æ‰¹æ”¹å†å²ä¸å­¦ç”Ÿç»“æœï¼ˆæ”¯æŒç­çº§/éç­çº§æ¨¡å¼ï¼‰
                try:
                    logger.info(
                        f"ä¿å­˜æ‰¹æ”¹ç»“æœ: batch_id={batch_id}, class_id={class_id}, homework_id={homework_id}"
                    )

                    now = datetime.now().isoformat()
                    scores = [
                        s.get("score")
                        for s in formatted_results
                        if isinstance(s.get("score"), (int, float))
                    ]
                    average_score = None
                    if isinstance(class_report, dict):
                        average_score = class_report.get("average_score")
                    if average_score is None and scores:
                        average_score = round(sum(scores) / len(scores), 2)

                    # å…ˆæ£€æŸ¥æ˜¯å¦å·²ç»å­˜åœ¨æ‰¹æ”¹å†å²è®°å½•
                    existing_history = await get_grading_history(batch_id)
                    if existing_history:
                        history_id = existing_history.id
                        logger.info(f"ä½¿ç”¨å·²å­˜åœ¨çš„æ‰¹æ”¹å†å²: history_id={history_id}")
                    else:
                        history_id = str(uuid.uuid4())
                        history = GradingHistory(
                            id=history_id,
                            batch_id=batch_id,
                            status="completed",
                            class_ids=[class_id] if class_id else None,
                            created_at=now,
                            completed_at=now,
                            total_students=len(formatted_results),
                            average_score=average_score,
                            result_data=(
                                {
                                    "summary": class_report,
                                    "class_id": class_id,
                                    "homework_id": homework_id,
                                }
                                if class_report or class_id or homework_id
                                else None
                            ),
                        )
                        await _maybe_await(save_grading_history(history))
                        logger.info(f"åˆ›å»ºæ–°çš„æ‰¹æ”¹å†å²: history_id={history_id}")

                    student_map_by_index = {}
                    student_map_by_name = {}
                    if student_mapping:
                        for idx, mapping in enumerate(student_mapping):
                            student_map_by_index[idx] = mapping
                            name_key = (mapping.get("studentName") or "").strip().lower()
                            if name_key:
                                student_map_by_name[name_key] = mapping

                    roster = list_class_students(class_id) if class_id else []
                    roster_by_name = {
                        (student.name or student.username or "").strip().lower(): student
                        for student in roster
                        if (student.name or student.username)
                    }

                    for idx, result in enumerate(formatted_results):
                        student_name = (
                            result.get("studentName")
                            or result.get("student_name")
                            or f"Student {idx + 1}"
                        )
                        student_id = result.get("studentId") or result.get("student_id")

                        if not student_id and student_map_by_index.get(idx):
                            mapping = student_map_by_index[idx]
                            student_id = mapping.get("studentId")
                            student_name = mapping.get("studentName") or student_name
                        if not student_id and student_name:
                            mapping = student_map_by_name.get(student_name.strip().lower())
                            if mapping:
                                student_id = mapping.get("studentId")
                                student_name = mapping.get("studentName") or student_name
                        if not student_id and student_name:
                            roster_hit = roster_by_name.get(student_name.strip().lower())
                            if roster_hit:
                                student_id = roster_hit.id
                                student_name = (
                                    roster_hit.name or roster_hit.username or student_name
                                )
                        if not student_id and class_id and idx < len(roster):
                            roster_hit = roster[idx]
                            student_id = roster_hit.id
                            student_name = roster_hit.name or roster_hit.username or student_name
                        if not student_id and class_id:
                            student_id = f"auto-{idx + 1}"

                        student_summary = (
                            result.get("studentSummary") or result.get("student_summary") or {}
                        )
                        self_audit = result.get("selfAudit") or result.get("self_audit") or {}
                        confession_payload = (
                            result.get("confession")
                            or result.get("confession_data")
                            or result.get("confessionData")
                        )
                        student_id_value = student_id if class_id else None
                        student_result = StudentGradingResult(
                            id=_make_student_result_id(history_id, student_name, student_id_value),
                            grading_history_id=history_id,
                            student_key=student_name,
                            score=result.get("score"),
                            max_score=result.get("maxScore") or result.get("max_score"),
                            class_id=class_id,
                            student_id=student_id_value,
                            summary=(
                                student_summary.get("overall")
                                if isinstance(student_summary, dict)
                                else None
                            ),
                            confession=confession_payload,
                            result_data=result,
                        )
                        await _maybe_await(save_student_result(student_result))

                        if class_id and homework_id and student_id:
                            feedback = None
                            if isinstance(student_summary, dict):
                                feedback = student_summary.get("overall")
                            upsert_homework_submission_grade(
                                class_id=class_id,
                                homework_id=homework_id,
                                student_id=student_id,
                                student_name=student_name,
                                score=result.get("score"),
                                feedback=feedback,
                                grading_batch_id=batch_id,
                            )

                    logger.info(f"æ‰¹æ”¹ç»“æœå·²ä¿å­˜: history_id={history_id}")
                except Exception as e:
                    logger.error(f"ä¿å­˜æ‰¹æ”¹ç»“æœå¤±è´¥: {e}", exc_info=True)

                # #region agent log - å‡è®¾E: WebSocket æ¶ˆæ¯å‘é€
                _write_debug_log(
                    {
                        "hypothesisId": "E",
                        "location": "batch_langgraph.py:workflow_completed",
                        "message": "å‘é€workflow_completed",
                        "data": {
                            "student_count": len(formatted_results),
                            "students": [
                                {"name": f.get("studentName"), "score": f.get("score")}
                                for f in formatted_results
                            ],
                        },
                        "timestamp": int(datetime.now().timestamp() * 1000),
                        "sessionId": "debug-session",
                    }
                )
                # #endregion

                await broadcast_progress(
                    batch_id,
                    {
                        "type": "workflow_completed",
                        "message": f"Grading completed, processed {len(formatted_results)} students",
                        "results": formatted_results,
                        "classReport": class_report,
                    },
                )

        logger.info(f"LangGraph è¿›åº¦æµå¼ä¼ è¾“å®Œæˆ: batch_id={batch_id}")

    except Exception as e:
        logger.error(f"æµå¼ä¼ è¾“å¤±è´¥: batch_id={batch_id}, error={str(e)}", exc_info=True)
        await broadcast_progress(
            batch_id, {"type": "workflow_error", "message": f"æµå¼ä¼ è¾“å¤±è´¥: {str(e)}"}
        )

    finally:
        run_controller = await get_run_controller()
        if run_controller and teacher_key:
            try:
                run_info = await orchestrator.get_status(run_id)
                status_value = (
                    run_info.status.value
                    if hasattr(run_info.status, "value")
                    else str(run_info.status)
                )
                if status_value in ("completed", "failed", "cancelled"):
                    await run_controller.update_run(
                        batch_id,
                        {
                            "status": "completed" if status_value == "completed" else "failed",
                            "completed_at": datetime.now().isoformat(),
                            "updated_at": datetime.now().isoformat(),
                        },
                    )
                    await run_controller.release_slot(teacher_key, batch_id)
            except Exception:
                await run_controller.release_slot(teacher_key, batch_id)
        current_task = asyncio.current_task()
        if current_task and _active_stream_tasks.get(batch_id) is current_task:
            _active_stream_tasks.pop(batch_id, None)


async def _ensure_stream_task(
    *,
    batch_id: str,
    run_id: str,
    orchestrator: Orchestrator,
    class_id: Optional[str] = None,
    homework_id: Optional[str] = None,
    student_mapping: Optional[List[dict]] = None,
    teacher_key: Optional[str] = None,
) -> None:
    existing = _active_stream_tasks.get(batch_id)
    if existing and not existing.done():
        return
    stream_task = asyncio.create_task(
        stream_langgraph_progress(
            batch_id=batch_id,
            run_id=run_id,
            orchestrator=orchestrator,
            class_id=class_id,
            homework_id=homework_id,
            student_mapping=student_mapping,
            teacher_key=teacher_key,
        )
    )
    _active_stream_tasks[batch_id] = stream_task


async def resume_orphaned_streams(orchestrator: Optional[Orchestrator]) -> None:
    if not orchestrator:
        return
    try:
        running_runs = await orchestrator.list_runs(
            graph_name="batch_grading", status=RunStatus.RUNNING, limit=50
        )
        pending_runs = await orchestrator.list_runs(
            graph_name="batch_grading", status=RunStatus.PENDING, limit=50
        )
    except Exception as exc:
        logger.debug("Failed to list runs for stream recovery: %s", exc)
        return

    run_controller = await get_run_controller()
    for run_info in [*running_runs, *pending_runs]:
        run_id = run_info.run_id
        if not run_id.startswith("batch_grading_"):
            continue
        batch_id = run_id.replace("batch_grading_", "", 1)
        teacher_key = None
        class_id = None
        homework_id = None
        if run_controller:
            snapshot = await run_controller.get_run(batch_id)
            if snapshot:
                teacher_key = snapshot.teacher_id
                class_id = snapshot.class_id
                homework_id = snapshot.homework_id
        await _ensure_stream_task(
            batch_id=batch_id,
            run_id=run_id,
            orchestrator=orchestrator,
            class_id=class_id,
            homework_id=homework_id,
            student_mapping=None,
            teacher_key=teacher_key,
        )


def _map_node_to_frontend(node_name: str) -> str:
    """å°† LangGraph èŠ‚ç‚¹åç§°æ˜ å°„åˆ°å‰ç«¯èŠ‚ç‚¹ ID

    å‰ç«¯å·¥ä½œæµèŠ‚ç‚¹ï¼ˆconsoleStore.ts initialNodesï¼‰ï¼š
    - intake: æ¥æ”¶æ–‡ä»¶
    - rubric_parse: è§£æè¯„åˆ†æ ‡å‡†
    - grade_batch: åˆ†æ‰¹å¹¶è¡Œæ‰¹æ”¹ï¼ˆisParallelContainerï¼‰
    - cross_page_merge: è·¨é¡µé¢˜ç›®åˆå¹¶
    - index: æ‰¹æ”¹å‰ç´¢å¼•
    - index_merge: ç´¢å¼•èšåˆ
    - export: å¯¼å‡ºç»“æœ
    """
    mapping = {
        # ä¸»è¦èŠ‚ç‚¹ï¼ˆä¸åç«¯ batch_grading.py å®Œå…¨å¯¹åº”ï¼‰
        "intake": "intake",
        "rubric_parse": "rubric_parse",
        "rubric_review": "rubric_review",
        "grade_batch": "grade_batch",
        "cross_page_merge": "cross_page_merge",
        "index": "index",
        "index_merge": "index_merge",
        "segment": "index_merge",
        "review": "review",
        "logic_review": "logic_review",
        "export": "export",
        # å…¼å®¹æ—§åç§°
        "detect_boundaries": "index",
        "grade_student": "grade_batch",
        "grading": "grade_batch",
        "aggregate": "review",
        "batch_persist": "export",
        "batch_notify": "export",
    }
    if node_name in mapping:
        return mapping[node_name]
    if ":" in node_name:
        base = node_name.split(":", 1)[0]
        if base in mapping:
            return mapping[base]
    if "." in node_name:
        base = node_name.split(".", 1)[0]
        if base in mapping:
            return mapping[base]
    return node_name


def _get_node_display_name(node_name: str) -> str:
    """è·å–èŠ‚ç‚¹çš„æ˜¾ç¤ºåç§°ï¼ˆä¸­æ–‡ï¼‰"""
    display_names = {
        "intake": "Ingest",
        "preprocess": "Preprocess",
        "index": "Index",
        "rubric_parse": "Rubric Parse",
        "rubric_review": "Rubric Review",
        "grading_fanout": "Batch Fanout",
        "grade_batch": "Batch Grading",
        "cross_page_merge": "Cross-Page Merge",
        "logic_review": "Logic Review",
        "index_merge": "Index Merge",
        "segment": "Index Merge",
        "review": "Final Review",
        "export": "Export",
    }
    if node_name in display_names:
        return display_names[node_name]
    if ":" in node_name:
        base = node_name.split(":", 1)[0]
        if base in display_names:
            return display_names[base]
    if "." in node_name:
        base = node_name.split(".", 1)[0]
        if base in display_names:
            return display_names[base]
    return node_name


def _safe_float(value: Any, default: float = 0.0) -> float:
    try:
        if value is None:
            return default
        return float(value)
    except (TypeError, ValueError):
        return default


def _make_student_result_id(
    grading_history_id: str,
    student_key: Optional[str],
    student_id: Optional[str] = None,
) -> str:
    seed = student_id or student_key or "unknown"
    return str(uuid.uuid5(uuid.NAMESPACE_URL, f"{grading_history_id}:{seed}"))


def _resolve_page_indices(question: Dict[str, Any], fallback_page_index: Optional[int] = None) -> List[int]:
    pages = question.get("page_indices") or question.get("pageIndices")
    if not pages:
        page_index = question.get("page_index")
        if page_index is None:
            page_index = question.get("pageIndex")
        if page_index is None:
            page_index = fallback_page_index
        if page_index is not None:
            pages = [page_index]
    if isinstance(pages, (list, tuple)):
        return [int(p) for p in pages if isinstance(p, (int, float, str)) and str(p).strip() != ""]
    if pages is None:
        return []
    try:
        return [int(pages)]
    except (TypeError, ValueError):
        return []


def _resolve_question_max_score(
    question: Dict[str, Any], scoring_results: List[Dict[str, Any]]
) -> float:
    max_score = _safe_float(
        question.get("max_score")
        or question.get("maxScore")
        or question.get("max_points")
        or question.get("maxPoints"),
        default=0.0,
    )
    if max_score > 0:
        return max_score
    total_points = 0.0
    for item in scoring_results or []:
        max_points = item.get("max_points") or item.get("maxPoints")
        if max_points is None:
            scoring_point = item.get("scoring_point") or item.get("scoringPoint") or {}
            max_points = scoring_point.get("score") or scoring_point.get("points")
        total_points += _safe_float(max_points, default=0.0)
    return total_points


def _resolve_question_confidence(
    question: Dict[str, Any],
    scoring_results: List[Dict[str, Any]],
    *,
    score: float,
    max_score: float,
) -> float:
    raw_confidence = question.get("confidence")
    if raw_confidence is None:
        raw_confidence = question.get("confidence_score")
        if raw_confidence is None:
            raw_confidence = question.get("confidenceScore")
        if raw_confidence is None:
            raw_confidence = question.get("self_critique_confidence")
        if raw_confidence is None:
            raw_confidence = question.get("selfCritiqueConfidence")
    confidence = _safe_float(raw_confidence, default=0.0)
    has_signal = bool(
        scoring_results
        or question.get("student_answer")
        or question.get("studentAnswer")
        or question.get("feedback")
    )
    if confidence <= 0 and has_signal:
        if max_score > 0:
            confidence = max(0.1, min(1.0, score / max_score))
        elif scoring_results:
            total_points = 0.0
            awarded_points = 0.0
            for item in scoring_results:
                max_points = item.get("max_points") or item.get("maxPoints")
                if max_points is None:
                    scoring_point = item.get("scoring_point") or item.get("scoringPoint") or {}
                    max_points = scoring_point.get("score")
                total_points += _safe_float(max_points, default=0.0)
                awarded_points += _safe_float(item.get("awarded") or item.get("score"), default=0.0)
            if total_points > 0:
                confidence = max(0.1, min(1.0, awarded_points / total_points))
    if confidence <= 0 and has_signal:
        confidence = 0.6 if score > 0 else 0.35
    return max(0.0, min(1.0, confidence))


def _build_student_results_from_grading_results(
    grading_results: List[Dict[str, Any]],
) -> List[Dict[str, Any]]:
    if not grading_results:
        return []
    try:
        from src.graphs.batch_grading import _build_student_results_from_page_results

        return _build_student_results_from_page_results(grading_results)
    except Exception as exc:
        logger.debug(f"Failed to rebuild student_results from grading_results: {exc}")
        return []


def _merge_question_results(existing: Dict[str, Any], incoming: Dict[str, Any]) -> Dict[str, Any]:
    merged = existing.copy()
    merged["score"] = max(_safe_float(existing.get("score", 0)), _safe_float(incoming.get("score", 0)))
    merged["maxScore"] = max(
        _safe_float(existing.get("maxScore", 0)),
        _safe_float(incoming.get("maxScore", 0)),
    )
    if not merged.get("feedback") and incoming.get("feedback"):
        merged["feedback"] = incoming.get("feedback")
    if not merged.get("studentAnswer") and incoming.get("studentAnswer"):
        merged["studentAnswer"] = incoming.get("studentAnswer")
    merged_conf = _safe_float(existing.get("confidence", 0))
    incoming_conf = _safe_float(incoming.get("confidence", 0))
    merged["confidence"] = max(merged_conf, incoming_conf)
    pages = set(existing.get("page_indices") or existing.get("pageIndices") or [])
    pages.update(incoming.get("page_indices") or incoming.get("pageIndices") or [])
    if pages:
        merged["page_indices"] = sorted(pages)
        merged["pageIndices"] = sorted(pages)
    if not merged.get("scoring_point_results") and incoming.get("scoring_point_results"):
        merged["scoring_point_results"] = incoming.get("scoring_point_results")
    if not merged.get("annotations") and incoming.get("annotations"):
        merged["annotations"] = incoming.get("annotations")
    if not merged.get("steps") and incoming.get("steps"):
        merged["steps"] = incoming.get("steps")
    return merged


def _dedupe_formatted_results(results: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
    grouped: Dict[str, Dict[str, Any]] = {}
    for result in results:
        student_id = result.get("studentId") or result.get("student_id")
        student_name = result.get("studentName") or result.get("student_name") or ""
        student_key = result.get("studentKey") or result.get("student_key")
        start_page = result.get("startPage")
        end_page = result.get("endPage")
        key: Optional[str] = None
        if student_id:
            key = f"id:{student_id}"
        elif student_name and (start_page is not None or end_page is not None):
            key = f"name:{student_name}:{start_page}-{end_page}"
        elif student_name:
            key = f"name:{student_name}"
        elif student_key:
            key = f"key:{student_key}"
        if not key:
            key = str(len(grouped))
        entry = grouped.get(key)
        if not entry:
            grouped[key] = result
            continue

        merged = entry.copy()
        merged_questions: Dict[str, Dict[str, Any]] = {}
        for q in entry.get("questionResults", []) or []:
            qid = str(q.get("questionId") or q.get("question_id") or "")
            if not qid:
                qid = f"idx:{len(merged_questions)}"
            merged_questions[qid] = q
        for q in result.get("questionResults", []) or []:
            qid = str(q.get("questionId") or q.get("question_id") or "")
            if not qid:
                qid = f"idx:{len(merged_questions)}"
            if qid in merged_questions:
                merged_questions[qid] = _merge_question_results(merged_questions[qid], q)
            else:
                merged_questions[qid] = q
        merged["questionResults"] = list(merged_questions.values())

        merged_start = merged.get("startPage")
        merged_end = merged.get("endPage")
        candidate_start = result.get("startPage")
        candidate_end = result.get("endPage")
        if merged_start is not None and candidate_start is not None:
            merged["startPage"] = min(merged_start, candidate_start)
        elif merged_start is None and candidate_start is not None:
            merged["startPage"] = candidate_start
        if merged_end is not None and candidate_end is not None:
            merged["endPage"] = max(merged_end, candidate_end)
        elif merged_end is None and candidate_end is not None:
            merged["endPage"] = candidate_end

        if merged.get("startPage") is not None:
            if merged.get("endPage") is not None and merged["endPage"] != merged["startPage"]:
                merged["pageRange"] = f"{merged['startPage'] + 1}-{merged['endPage'] + 1}"
            else:
                merged["pageRange"] = str(merged["startPage"] + 1)

        question_scores = [
            _safe_float(q.get("score", 0)) for q in merged.get("questionResults", []) or []
        ]
        question_max = [
            _safe_float(q.get("maxScore", 0)) for q in merged.get("questionResults", []) or []
        ]
        if question_scores:
            merged["score"] = sum(question_scores)
        if question_max:
            merged["maxScore"] = sum(question_max)

        grouped[key] = merged
    return list(grouped.values())


def _format_results_for_frontend(results: List[Dict]) -> List[Dict]:
    """æ ¼å¼åŒ–æ‰¹æ”¹ç»“æœä¸ºå‰ç«¯æ ¼å¼"""
    # #region agent log - å‡è®¾D: _format_results_for_frontend è¾“å…¥
    _write_debug_log(
        {
            "hypothesisId": "D",
            "location": "batch_langgraph.py:_format_results_for_frontend:input",
            "message": "è¾“å…¥çš„results",
            "data": {
                "count": len(results),
                "students": [
                    {"key": r.get("student_key"), "score": r.get("total_score")} for r in results
                ],
            },
            "timestamp": int(datetime.now().timestamp() * 1000),
            "sessionId": "debug-session",
        }
    )
    # #endregion
    formatted = []
    for r in results:
        # å¤„ç† question_details æ ¼å¼
        question_results = []

        # ä¼˜å…ˆä½¿ç”¨ question_details
        if r.get("question_details"):
            for q in r.get("question_details", []):
                scoring_results = q.get("scoring_point_results") or q.get("scoring_results") or []
                score_value = _safe_float(q.get("score", 0))
                max_score_value = _resolve_question_max_score(q, scoring_results)
                page_indices = _resolve_page_indices(q)
                confidence = _resolve_question_confidence(
                    q,
                    scoring_results,
                    score=score_value,
                    max_score=max_score_value,
                )
                question_results.append(
                    {
                        "questionId": str(q.get("question_id", "")),
                        "score": score_value,
                        "maxScore": max_score_value,
                        "feedback": q.get("feedback", ""),
                        "confidence": confidence,
                        "confidence_reason": q.get("confidence_reason")
                        or q.get("confidenceReason"),
                        "self_critique": q.get("self_critique") or q.get("selfCritique"),
                        "self_critique_confidence": q.get("self_critique_confidence")
                        or q.get("selfCritiqueConfidence"),
                        "rubric_refs": q.get("rubric_refs") or q.get("rubricRefs"),
                        "review_summary": q.get("review_summary") or q.get("reviewSummary"),
                        "review_corrections": q.get("review_corrections")
                        or q.get("reviewCorrections"),
                        "needsReview": (
                            q.get("needs_review")
                            if q.get("needs_review") is not None
                            else q.get("needsReview", False)
                        ),
                        "reviewReasons": (
                            q.get("review_reasons")
                            if q.get("review_reasons") is not None
                            else q.get("reviewReasons") or []
                        ),
                        "auditFlags": (
                            q.get("audit_flags")
                            if q.get("audit_flags") is not None
                            else q.get("auditFlags") or []
                        ),
                        "honesty_note": q.get("honesty_note") or q.get("honestyNote"),
                        "typo_notes": q.get("typo_notes") or q.get("typoNotes"),
                        "studentAnswer": q.get("student_answer", ""),
                        "question_type": q.get("question_type") or q.get("questionType"),
                        "isCorrect": q.get("is_correct", False),
                        "scoring_point_results": scoring_results,
                        "page_indices": page_indices,
                        "is_cross_page": q.get("is_cross_page", False),
                        "merge_source": q.get("merge_source"),
                        # ğŸ”¥ æ‰¹æ³¨åæ ‡å­—æ®µ
                        "annotations": q.get("annotations") or [],
                        "steps": q.get("steps") or [],
                        "answerRegion": q.get("answer_region") or q.get("answerRegion"),
                    }
                )
        # å…¼å®¹æ—§æ ¼å¼ grading_results
        elif r.get("grading_results"):
            for q in r.get("grading_results", []):
                scoring_results = q.get("scoring_point_results") or q.get("scoring_results") or []
                score_value = _safe_float(q.get("score", 0))
                max_score_value = _resolve_question_max_score(q, scoring_results)
                page_indices = _resolve_page_indices(q)
                confidence = _resolve_question_confidence(
                    q,
                    scoring_results,
                    score=score_value,
                    max_score=max_score_value,
                )
                question_results.append(
                    {
                        "questionId": str(q.get("question_id", "")),
                        "score": score_value,
                        "maxScore": max_score_value,
                        "feedback": q.get("feedback", ""),
                        "confidence": confidence,
                        "confidence_reason": q.get("confidence_reason")
                        or q.get("confidenceReason"),
                        "self_critique": q.get("self_critique") or q.get("selfCritique"),
                        "self_critique_confidence": q.get("self_critique_confidence")
                        or q.get("selfCritiqueConfidence"),
                        "rubric_refs": q.get("rubric_refs") or q.get("rubricRefs"),
                        "review_summary": q.get("review_summary") or q.get("reviewSummary"),
                        "review_corrections": q.get("review_corrections")
                        or q.get("reviewCorrections"),
                        "needsReview": (
                            q.get("needs_review")
                            if q.get("needs_review") is not None
                            else q.get("needsReview", False)
                        ),
                        "reviewReasons": (
                            q.get("review_reasons")
                            if q.get("review_reasons") is not None
                            else q.get("reviewReasons") or []
                        ),
                        "auditFlags": (
                            q.get("audit_flags")
                            if q.get("audit_flags") is not None
                            else q.get("auditFlags") or []
                        ),
                        "honesty_note": q.get("honesty_note") or q.get("honestyNote"),
                        "typo_notes": q.get("typo_notes") or q.get("typoNotes"),
                        "studentAnswer": q.get("student_answer", ""),
                        "question_type": q.get("question_type") or q.get("questionType"),
                        "scoring_point_results": scoring_results,
                        "page_indices": page_indices,
                        "is_cross_page": q.get("is_cross_page", False),
                        "merge_source": q.get("merge_source"),
                        # ğŸ”¥ æ‰¹æ³¨åæ ‡å­—æ®µ
                        "annotations": q.get("annotations") or [],
                        "steps": q.get("steps") or [],
                        "answerRegion": q.get("answer_region") or q.get("answerRegion"),
                    }
                )
        # Ã¥â€¦Â¼Ã¥Â®Â¹ export_data Ã§Å¡â€ question_results
        elif r.get("question_results") or r.get("questionResults"):
            raw_question_results = r.get("question_results") or r.get("questionResults") or []
            for q in raw_question_results:
                scoring_results = (
                    q.get("scoring_point_results")
                    or q.get("scoring_results")
                    or q.get("scoringPointResults")
                    or []
                )
                score_value = _safe_float(q.get("score", 0))
                max_score_value = _resolve_question_max_score(q, scoring_results)
                page_indices = _resolve_page_indices(q)
                confidence = _resolve_question_confidence(
                    q,
                    scoring_results,
                    score=score_value,
                    max_score=max_score_value,
                )
                question_results.append(
                    {
                        "questionId": str(q.get("question_id") or q.get("questionId") or ""),
                        "score": score_value,
                        "maxScore": max_score_value,
                        "feedback": q.get("feedback", ""),
                        "confidence": confidence,
                        "confidence_reason": q.get("confidence_reason")
                        or q.get("confidenceReason"),
                        "self_critique": q.get("self_critique") or q.get("selfCritique"),
                        "self_critique_confidence": q.get("self_critique_confidence")
                        or q.get("selfCritiqueConfidence"),
                        "rubric_refs": q.get("rubric_refs") or q.get("rubricRefs"),
                        "review_summary": q.get("review_summary") or q.get("reviewSummary"),
                        "review_corrections": q.get("review_corrections")
                        or q.get("reviewCorrections"),
                        "needsReview": (
                            q.get("needs_review")
                            if q.get("needs_review") is not None
                            else q.get("needsReview", False)
                        ),
                        "reviewReasons": (
                            q.get("review_reasons")
                            if q.get("review_reasons") is not None
                            else q.get("reviewReasons") or []
                        ),
                        "auditFlags": (
                            q.get("audit_flags")
                            if q.get("audit_flags") is not None
                            else q.get("auditFlags") or []
                        ),
                        "honesty_note": q.get("honesty_note") or q.get("honestyNote"),
                        "typo_notes": q.get("typo_notes") or q.get("typoNotes"),
                        "studentAnswer": q.get("student_answer") or q.get("studentAnswer") or "",
                        "question_type": q.get("question_type") or q.get("questionType"),
                        "isCorrect": q.get("is_correct", False),
                        "scoring_point_results": scoring_results,
                        "page_indices": page_indices,
                        "is_cross_page": q.get("is_cross_page", False),
                        "merge_source": q.get("merge_source"),
                        # ğŸ”¥ æ‰¹æ³¨åæ ‡å­—æ®µ
                        "annotations": q.get("annotations") or [],
                        "steps": q.get("steps") or [],
                        "answerRegion": q.get("answer_region") or q.get("answerRegion"),
                    }
                )
        # ä» page_results æå–
        elif r.get("page_results"):
            for page in r.get("page_results", []):
                if page.get("status") == "completed":
                    # ä»é¡µé¢ç»“æœä¸­æå–é¢˜ç›®è¯¦æƒ…
                    for q in page.get("question_details", []):
                        scoring_results = (
                            q.get("scoring_point_results") or q.get("scoring_results") or []
                        )
                        score_value = _safe_float(q.get("score", 0))
                        max_score_value = _resolve_question_max_score(q, scoring_results)
                        page_indices = _resolve_page_indices(q, page.get("page_index"))
                        confidence = _resolve_question_confidence(
                            q,
                            scoring_results,
                            score=score_value,
                            max_score=max_score_value,
                        )
                        question_results.append(
                            {
                                "questionId": str(q.get("question_id", "")),
                                "score": score_value,
                                "maxScore": max_score_value,
                                "feedback": q.get("feedback", ""),
                                "confidence": confidence,
                                "confidence_reason": q.get("confidence_reason")
                                or q.get("confidenceReason"),
                                "self_critique": q.get("self_critique") or q.get("selfCritique"),
                                "self_critique_confidence": q.get("self_critique_confidence")
                                or q.get("selfCritiqueConfidence"),
                                "rubric_refs": q.get("rubric_refs") or q.get("rubricRefs"),
                                "review_summary": q.get("review_summary") or q.get("reviewSummary"),
                                "review_corrections": q.get("review_corrections")
                                or q.get("reviewCorrections"),
                                "needsReview": (
                                    q.get("needs_review")
                                    if q.get("needs_review") is not None
                                    else q.get("needsReview", False)
                                ),
                                "reviewReasons": (
                                    q.get("review_reasons")
                                    if q.get("review_reasons") is not None
                                    else q.get("reviewReasons") or []
                                ),
                                "auditFlags": (
                                    q.get("audit_flags")
                                    if q.get("audit_flags") is not None
                                    else q.get("auditFlags") or []
                                ),
                                "honesty_note": q.get("honesty_note") or q.get("honestyNote"),
                                "typo_notes": q.get("typo_notes") or q.get("typoNotes"),
                                "studentAnswer": q.get("student_answer", ""),
                                "question_type": q.get("question_type") or q.get("questionType"),
                                "isCorrect": q.get("is_correct", False),
                                "scoring_point_results": scoring_results,
                                "page_indices": page_indices or [],
                                "is_cross_page": q.get("is_cross_page", False),
                                "merge_source": q.get("merge_source"),
                                # ğŸ”¥ æ‰¹æ³¨åæ ‡å­—æ®µ
                                "annotations": q.get("annotations") or [],
                                "steps": q.get("steps") or [],
                                "answerRegion": q.get("answer_region") or q.get("answerRegion"),
                            }
                        )

        computed_score = sum(_safe_float(q.get("score", 0)) for q in question_results)
        computed_max = sum(_safe_float(q.get("maxScore", 0)) for q in question_results)
        raw_score = _safe_float(r.get("total_score", r.get("score", 0)))
        raw_max = _safe_float(r.get("max_total_score", r.get("max_score", 0)))
        final_score = raw_score if raw_score > 0 or computed_score <= 0 else computed_score
        final_max = raw_max if raw_max > 0 or computed_max <= 0 else computed_max

        student_summary = r.get("student_summary") or r.get("studentSummary")
        self_audit = r.get("self_audit") or r.get("selfAudit")
        confession_raw = r.get("confession") or r.get("confession_data") or r.get("confessionData")
        if isinstance(confession_raw, str):
            try:
                confession_raw = json.loads(confession_raw)
            except Exception:
                confession_raw = None

        logic_review_raw = r.get("logic_review") or r.get("logicReview")
        if isinstance(logic_review_raw, str):
            try:
                logic_review_raw = json.loads(logic_review_raw)
            except Exception:
                logic_review_raw = None

        # æ ‡å‡†åŒ– confession æ ¼å¼ï¼Œç¡®ä¿å‰ç«¯èƒ½æ­£ç¡®æ˜¾ç¤º
        confession = None
        if confession_raw and isinstance(confession_raw, dict):
            confession = {}
            # å¤åˆ¶æ‰€æœ‰åŸå§‹å­—æ®µ
            confession.update(confession_raw)
            # ç¡®ä¿ overallStatus å­˜åœ¨
            if "overallStatus" not in confession and "overall_status" in confession_raw:
                confession["overallStatus"] = confession_raw["overall_status"]
            elif "overallStatus" not in confession and "overall_confidence" in confession_raw:
                conf = confession_raw.get("overall_confidence", 0)
                if conf >= 0.8:
                    confession["overallStatus"] = "ok"
                elif conf >= 0.5:
                    confession["overallStatus"] = "caution"
                else:
                    confession["overallStatus"] = "needs_review"
            # ç¡®ä¿ overallConfidence å­˜åœ¨
            if "overallConfidence" not in confession and "overall_confidence" in confession_raw:
                confession["overallConfidence"] = confession_raw["overall_confidence"]
            # ç¡®ä¿ highRiskQuestions æ ¼å¼æ­£ç¡®
            hrq = confession_raw.get("highRiskQuestions") or confession_raw.get(
                "high_risk_questions"
            )
            if hrq:
                if isinstance(hrq, list) and hrq and isinstance(hrq[0], str):
                    confession["highRiskQuestions"] = [
                        {"questionId": q, "description": ""} for q in hrq
                    ]
                else:
                    confession["highRiskQuestions"] = hrq
            # ç¡®ä¿ issues å­˜åœ¨
            if "issues" not in confession:
                # ä» potential_errors æˆ– uncertainties æ„å»º issues
                issues = []
                for err in confession_raw.get("potential_errors", []):
                    if isinstance(err, dict):
                        issues.append(
                            {
                                "questionId": err.get("question_id", ""),
                                "message": err.get("description", ""),
                            }
                        )
                for unc in confession_raw.get("uncertainties", []):
                    if isinstance(unc, dict):
                        issues.append(
                            {
                                "questionId": unc.get("question_id", ""),
                                "message": unc.get("uncertainty", ""),
                            }
                        )
                if issues:
                    confession["issues"] = issues

        # ğŸ”¥ ç¬¬ä¸€æ¬¡æ‰¹æ”¹è®°å½•ï¼ˆé€»è¾‘å¤æ ¸å‰çš„åŸå§‹ç»“æœï¼‰
        draft_question_details = r.get("draft_question_details") or r.get("draftQuestionDetails")
        draft_question_results = []
        if draft_question_details:
            for dq in draft_question_details:
                draft_scoring_results = (
                    dq.get("scoring_point_results") or dq.get("scoring_results") or []
                )
                draft_score_value = _safe_float(dq.get("score", 0))
                draft_max_score_value = _resolve_question_max_score(
                    dq, draft_scoring_results
                )
                draft_page_indices = _resolve_page_indices(dq)
                draft_confidence = _resolve_question_confidence(
                    dq,
                    draft_scoring_results,
                    score=draft_score_value,
                    max_score=draft_max_score_value,
                )
                draft_question_results.append(
                    {
                        "questionId": str(dq.get("question_id", "")),
                        "score": draft_score_value,
                        "maxScore": draft_max_score_value,
                        "feedback": dq.get("feedback", ""),
                        "confidence": draft_confidence,
                        "self_critique": dq.get("self_critique") or dq.get("selfCritique"),
                        "self_critique_confidence": dq.get("self_critique_confidence")
                        or dq.get("selfCritiqueConfidence"),
                        "studentAnswer": dq.get("student_answer", ""),
                        "question_type": dq.get("question_type") or dq.get("questionType"),
                        "scoring_point_results": draft_scoring_results,
                        "page_indices": draft_page_indices,
                    }
                )

        # è®¡ç®—é¡µé¢èŒƒå›´æ˜¾ç¤ºå­—ç¬¦ä¸²
        start_page = r.get("start_page") if r.get("start_page") is not None else r.get("startPage")
        end_page = r.get("end_page") if r.get("end_page") is not None else r.get("endPage")
        page_range = ""
        if start_page is not None:
            if end_page is not None and end_page != start_page:
                page_range = f"{start_page + 1}-{end_page + 1}"
            else:
                page_range = str(start_page + 1)

        student_id = r.get("student_id") or r.get("studentId")
        student_key = r.get("student_key") or r.get("studentKey")
        formatted.append(
            {
                "studentName": r.get("student_name")
                or r.get("studentName")
                or student_key
                or student_id
                or "Unknown",
                "studentId": student_id,
                "studentKey": student_key,
                "score": final_score,
                "maxScore": final_max if final_max > 0 else 0,
                "startPage": start_page,
                "endPage": end_page,
                "pageRange": page_range,
                "questionResults": question_results,
                "confidence": r.get("confidence", 0),
                "needsConfirmation": r.get("needs_confirmation", False),
                "gradingMode": r.get("grading_mode") or r.get("gradingMode"),
                "studentSummary": student_summary,
                "selfAudit": self_audit,
                # ğŸ”¥ æ–°å¢ï¼šæ‰¹æ”¹é€æ˜åº¦å­—æ®µ
                "confession": confession,
                "logicReview": logic_review_raw,
                "draftQuestionDetails": draft_question_results if draft_question_results else None,
                "draftTotalScore": r.get("draft_total_score") or r.get("draftTotalScore"),
                "draftMaxScore": r.get("draft_max_score") or r.get("draftMaxScore"),
                "logicReviewedAt": r.get("logic_reviewed_at") or r.get("logicReviewedAt"),
                
            }
        )
    # #region agent log - å‡è®¾D: _format_results_for_frontend è¾“å‡º
    _write_debug_log(
        {
            "hypothesisId": "D",
            "location": "batch_langgraph.py:_format_results_for_frontend:output",
            "message": "è¾“å‡ºçš„formatted",
            "data": {
                "count": len(formatted),
                "students": [
                    {"name": f.get("studentName"), "score": f.get("score")} for f in formatted
                ],
            },
            "timestamp": int(datetime.now().timestamp() * 1000),
            "sessionId": "debug-session",
        }
    )
    # #endregion
    formatted = _dedupe_formatted_results(formatted)
    return formatted


@router.websocket("/ws/{batch_id}")
async def websocket_endpoint(websocket: WebSocket, batch_id: str):
    """
    WebSocket ç«¯ç‚¹ï¼Œç”¨äºå®æ—¶æ¨é€æ‰¹æ”¹è¿›åº¦

    å‰ç«¯é€šè¿‡æ­¤ç«¯ç‚¹æ¥æ”¶ LangGraph çš„å®æ—¶æ‰§è¡Œè¿›åº¦
    """
    await websocket.accept()

    redis_client = await _get_redis_client()
    use_redis_cache = redis_client is not None

    cached_images = batch_image_cache.get(batch_id, {})
    if cached_images:
        try:
            for key, message in cached_images.items():
                if key == "llm_stream_cache":
                    continue
                if use_redis_cache and key not in ("images_ready", "rubric_images_ready"):
                    continue
                await websocket.send_json(message)
        except Exception as e:
            logger.debug(f"å‘é€ç¼“å­˜å›¾ç‰‡å¤±è´¥: {e}")

    if use_redis_cache:
        try:
            cached_progress = await _load_cached_progress_messages(batch_id)
            for message in cached_progress:
                await websocket.send_json(message)
        except Exception as e:
            logger.debug(f"å‘é€ç¼“å­˜è¿›åº¦å¤±è´¥: {e}")

    if cached_images:
        try:
            stream_cache = cached_images.get("llm_stream_cache")
            if isinstance(stream_cache, dict):
                for stream_message in stream_cache.values():
                    await websocket.send_json(
                        {
                            "type": "llm_stream_chunk",
                            **stream_message,
                        }
                    )
        except Exception as e:
            logger.debug(f"å‘é€æµå¼ç¼“å­˜å¤±è´¥: {e}")

    # æ³¨å†Œè¿æ¥
    if batch_id not in active_connections:
        active_connections[batch_id] = []
    active_connections[batch_id].append(websocket)

    logger.info(f"WebSocket è¿æ¥å»ºç«‹: batch_id={batch_id}")

    # è¿æ¥å»ºç«‹åå°è¯•å‘é€å½“å‰çŠ¶æ€å¿«ç…§ï¼Œé¿å…å‰ç«¯é”™è¿‡æ—©æœŸäº‹ä»¶å¯¼è‡´å¡ä½
    try:
        orchestrator = await get_orchestrator()
        if orchestrator:
            run_id = f"batch_grading_{batch_id}"
            run_info = await orchestrator.get_run_info(run_id)
            if run_info and run_info.state:
                state = run_info.state or {}
                current_stage = state.get("current_stage", "")
                percentage = state.get("percentage", 0)
                if current_stage or percentage:
                    await websocket.send_json(
                        {
                            "type": "grading_progress",
                            "percentage": percentage or 0,
                            "currentStage": current_stage,
                        }
                    )
                if state.get("student_boundaries"):
                    boundaries = state.get("student_boundaries", [])
                    await websocket.send_json(
                        {
                            "type": "students_identified",
                            "studentCount": len(boundaries),
                            "students": [
                                {
                                    "studentKey": b.get("student_key", ""),
                                    "startPage": b.get("start_page", 0),
                                    "endPage": b.get("end_page", 0),
                                    "confidence": b.get("confidence", 0),
                                    "needsConfirmation": b.get("needs_confirmation", False),
                                }
                                for b in boundaries
                            ],
                        }
                    )
                if state.get("parsed_rubric"):
                    parsed = state.get("parsed_rubric", {})
                    await websocket.send_json(
                        {
                            "type": "rubric_parsed",
                            "totalQuestions": parsed.get("total_questions", 0),
                            "totalScore": parsed.get("total_score", 0),
                            "generalNotes": parsed.get("general_notes", ""),
                            "rubricFormat": parsed.get("rubric_format", ""),
                            "questions": [
                                {
                                    "questionId": q.get("question_id", ""),
                                    "maxScore": q.get("max_score", 0),
                                    "questionText": q.get("question_text", ""),
                                    "standardAnswer": q.get("standard_answer", ""),
                                    "gradingNotes": q.get("grading_notes", ""),
                                    "scoringPoints": [
                                        {
                                            "pointId": sp.get("point_id")
                                            or sp.get("pointId")
                                            or f"{q.get('question_id')}.{idx + 1}",
                                            "description": sp.get("description", ""),
                                            "expectedValue": sp.get("expected_value")
                                            or sp.get("expectedValue", ""),
                                            "keywords": sp.get("keywords") or [],
                                            "score": sp.get("score", 0),
                                            "isRequired": sp.get("is_required", True),
                                        }
                                        for idx, sp in enumerate(q.get("scoring_points", []))
                                    ],
                                    "deductionRules": [
                                        {
                                            "ruleId": dr.get("rule_id")
                                            or dr.get("ruleId")
                                            or f"{q.get('question_id')}.d{idx + 1}",
                                            "description": dr.get("description", ""),
                                            "deduction": dr.get("deduction", dr.get("score", 0)),
                                            "conditions": dr.get("conditions")
                                            or dr.get("when")
                                            or "",
                                        }
                                        for idx, dr in enumerate(
                                            q.get("deduction_rules")
                                            or q.get("deductionRules")
                                            or []
                                        )
                                    ],
                                    "alternativeSolutions": [
                                        {
                                            "description": alt.get("description", ""),
                                            "scoringCriteria": alt.get("scoring_criteria", ""),
                                            "note": alt.get("note", ""),
                                        }
                                        for alt in q.get("alternative_solutions", [])
                                    ],
                                }
                                for q in parsed.get("questions", [])
                            ],
                        }
                    )
                if run_info.status and run_info.status.value == "completed":
                    student_results = state.get("student_results", [])
                    formatted_results = _format_results_for_frontend(student_results)
                    class_report = state.get("class_report")
                    if not class_report and state.get("export_data"):
                        class_report = state.get("export_data", {}).get("class_report")
                    await websocket.send_json(
                        {
                            "type": "workflow_completed",
                            "message": f"Grading completed, processed {len(formatted_results)} students",
                            "results": formatted_results,
                            "cross_page_questions": state.get("cross_page_questions", []),
                            "classReport": class_report,
                        }
                    )
            if run_info and run_info.status and run_info.status.value in ("running", "pending"):
                run_controller = await get_run_controller()
                teacher_key = None
                class_id = None
                homework_id = None
                if run_controller:
                    snapshot = await run_controller.get_run(batch_id)
                    if snapshot:
                        teacher_key = snapshot.teacher_id
                        class_id = snapshot.class_id
                        homework_id = snapshot.homework_id
                await _ensure_stream_task(
                    batch_id=batch_id,
                    run_id=run_id,
                    orchestrator=orchestrator,
                    class_id=class_id,
                    homework_id=homework_id,
                    student_mapping=None,
                    teacher_key=teacher_key,
                )
    except Exception as e:
        logger.debug(f"å‘é€çŠ¶æ€å¿«ç…§å¤±è´¥: {e}")

    try:
        # ä¿æŒè¿æ¥ï¼Œç­‰å¾…å®¢æˆ·ç«¯æ¶ˆæ¯æˆ–æ–­å¼€
        while True:
            if not _is_ws_connected(websocket):
                break
            data = await websocket.receive_text()
            logger.debug(f"æ”¶åˆ° WebSocket æ¶ˆæ¯: batch_id={batch_id}, data={data}")

    except (WebSocketDisconnect, RuntimeError) as exc:
        logger.info(f"WebSocket è¿æ¥æ–­å¼€: batch_id={batch_id}, reason={exc}")
        _discard_connection(batch_id, websocket)
        return
    except Exception as exc:
        logger.debug(f"WebSocket æ¥æ”¶å¼‚å¸¸: batch_id={batch_id}, error={exc}")
        logger.info(f"WebSocket è¿æ¥æ–­å¼€: batch_id={batch_id}")
        _discard_connection(batch_id, websocket)


@router.get("/active", response_model=ActiveRunsResponse)
async def list_active_runs(teacher_id: Optional[str] = None) -> ActiveRunsResponse:
    teacher_key = _normalize_teacher_key(teacher_id)
    run_controller = await get_run_controller()
    if not run_controller:
        return ActiveRunsResponse(teacher_id=teacher_key, runs=[])
    snapshots = await run_controller.list_runs(teacher_key)
    runs = [
        ActiveRunItem(
            batch_id=item.batch_id,
            status=item.status,
            class_id=item.class_id,
            homework_id=item.homework_id,
            created_at=item.created_at,
            updated_at=item.updated_at,
            started_at=item.started_at,
            completed_at=item.completed_at,
            total_pages=item.total_pages,
            progress=item.progress,
            current_stage=item.current_stage,
        )
        for item in snapshots
    ]
    return ActiveRunsResponse(teacher_id=teacher_key, runs=runs)


@router.get("/status/{batch_id}", response_model=BatchStatusResponse)
async def get_batch_status(batch_id: str, orchestrator: Orchestrator = Depends(get_orchestrator)):
    """
    æŸ¥è¯¢æ‰¹æ¬¡çŠ¶æ€ï¼ˆä» LangGraph Orchestratorï¼‰

    Args:
        batch_id: æ‰¹æ¬¡ ID
        orchestrator: LangGraph Orchestrator

    Returns:
        BatchStatusResponse: æ‰¹æ¬¡çŠ¶æ€
    """
    try:
        if not orchestrator:
            raise HTTPException(status_code=503, detail="ç¼–æ’å™¨æœªåˆå§‹åŒ–")

        # æ„å»º run_idï¼ˆä¸ start_run ä¸­çš„æ ¼å¼ä¸€è‡´ï¼‰
        run_id = f"batch_grading_{batch_id}"

        # ä» LangGraph Orchestrator æŸ¥è¯¢çŠ¶æ€
        run_info = await orchestrator.get_run_info(run_id)

        if not run_info:
            raise HTTPException(status_code=404, detail="æ‰¹æ¬¡ä¸å­˜åœ¨")

        state = run_info.state or {}

        return BatchStatusResponse(
            batch_id=batch_id,
            exam_id=state.get("exam_id", ""),
            status=run_info.status.value,
            current_stage=state.get("current_stage"),
            error=run_info.error,
            total_students=len(state.get("student_boundaries", [])),
            completed_students=len(state.get("student_results", [])),
            unidentified_pages=0,
            results=state.get("student_results"),
        )

    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"æŸ¥è¯¢æ‰¹æ¬¡çŠ¶æ€å¤±è´¥: {str(e)}", exc_info=True)
        raise HTTPException(status_code=500, detail=f"æŸ¥è¯¢å¤±è´¥: {str(e)}")


@router.get("/rubric/{batch_id}", response_model=RubricReviewContextResponse)
async def get_rubric_review_context(
    batch_id: str, orchestrator: Orchestrator = Depends(get_orchestrator)
):
    """è·å– rubric review é¡µé¢ä¸Šä¸‹æ–‡"""
    try:
        if not orchestrator:
            raise HTTPException(status_code=503, detail="ç¼–æ’å™¨æœªåˆå§‹åŒ–")

        run_id = f"batch_grading_{batch_id}"
        run_info = await orchestrator.get_run_info(run_id)
        if not run_info:
            raise HTTPException(status_code=404, detail="æ‰¹æ¬¡ä¸å­˜åœ¨")

        state = run_info.state or {}
        parsed_rubric = state.get("parsed_rubric")

        cached = batch_image_cache.get(batch_id, {})
        cached_images = cached.get("rubric_images_ready", {}).get("images") if cached else None
        rubric_images: List[str] = cached_images or []

        if not rubric_images and state.get("rubric_images"):
            try:
                rubric_images = []
                for img in state.get("rubric_images", []):
                    if isinstance(img, (bytes, bytearray)):
                        rubric_images.append(base64.b64encode(img).decode("utf-8"))
                    elif isinstance(img, str) and img:
                        rubric_images.append(img)
            except Exception as exc:
                logger.debug(f"Failed to convert rubric images: {exc}")
        return RubricReviewContextResponse(
            batch_id=batch_id,
            status=run_info.status.value if run_info.status else None,
            current_stage=state.get("current_stage"),
            parsed_rubric=parsed_rubric,
            rubric_images=rubric_images,
        )
    except HTTPException:
        raise
    except Exception as exc:
        logger.error(f"è·å– rubric ä¸Šä¸‹æ–‡å¤±è´¥: {exc}", exc_info=True)
        raise HTTPException(status_code=500, detail=f"è·å–å¤±è´¥: {str(exc)}")


@router.get("/results-review/{batch_id}", response_model=ResultsReviewContextResponse)
async def get_results_review_context(
    batch_id: str, orchestrator: Orchestrator = Depends(get_orchestrator)
):
    """è·å– results review é¡µé¢ä¸Šä¸‹æ–‡"""

    async def _load_answer_images_from_storage() -> List[str]:
        if os.getenv("ENABLE_FILE_STORAGE", "false").lower() != "true":
            return []
        try:
            file_storage = get_file_storage_service()
            stored_files = await file_storage.list_batch_files(batch_id)
            if not stored_files:
                return []
            answer_files = [
                item
                for item in stored_files
                if item.metadata.get("type") == "answer"
                or item.filename.startswith("answer_page")
            ]
            if not answer_files:
                return []
            answer_files.sort(key=lambda f: f.filename)
            images: List[str] = []
            for item in answer_files:
                data = await file_storage.get_file(item.file_id)
                if not data:
                    continue
                images.append(base64.b64encode(data).decode("utf-8"))
            return images
        except Exception as exc:
            logger.debug(f"Failed to load answer images from storage: {exc}")
            return []

    async def _load_answer_images_from_db(history_id: str) -> List[str]:
        try:
            images = await _maybe_await(get_page_images(history_id))
            if not images:
                return []
            return [img.file_url for img in images if img.file_url]
        except Exception as exc:
            logger.debug(f"Failed to load answer images from DB: {exc}")
            return []

    async def _load_from_db() -> ResultsReviewContextResponse:
        """ä»æ•°æ®åº“åŠ è½½æ‰¹æ”¹ç»“æœ"""
        history = await _maybe_await(get_grading_history(batch_id))
        if not history:
            raise HTTPException(status_code=404, detail="æ‰¹æ¬¡ä¸å­˜åœ¨")

        raw_results: List[Dict[str, Any]] = []
        student_rows = await _maybe_await(get_student_results(history.id))
        for row in student_rows:
            data = row.result_data
            if isinstance(data, str):
                try:
                    data = json.loads(data)
                except Exception:
                    data = {}
            if not isinstance(data, dict):
                data = {}
            if not data:
                data = {
                    "studentName": row.student_key,
                    "score": row.score,
                    "maxScore": row.max_score,
                }
            raw_results.append(data)

        answer_images = await _load_answer_images_from_db(history.id)
        if not answer_images:
            answer_images = await _load_answer_images_from_storage()
        return ResultsReviewContextResponse(
            batch_id=batch_id,
            status=history.status,
            current_stage=None,
            student_results=_format_results_for_frontend(raw_results),
            answer_images=answer_images,
        )

    try:
        if not orchestrator:
            return await _load_from_db()

        run_id = f"batch_grading_{batch_id}"
        run_info = await orchestrator.get_run_info(run_id)
        if not run_info:
            return await _load_from_db()

        state = run_info.state or {}
        student_results = state.get("student_results", [])
        if not student_results:
            try:
                final_output = await orchestrator.get_final_output(run_id)
                if final_output:
                    student_results = final_output.get("student_results", [])
            except Exception as exc:
                logger.debug(f"è·å–æœ€ç»ˆè¾“å‡ºå¤±è´¥: {exc}")

        if not student_results:
            export_students = (state.get("export_data") or {}).get("students", [])
            if export_students:
                student_results = export_students
            else:
                grading_results = state.get("grading_results") or []
                if not grading_results:
                    try:
                        final_output = await orchestrator.get_final_output(run_id)
                        if final_output:
                            grading_results = final_output.get("grading_results") or []
                    except Exception as exc:
                        logger.debug(f"Failed to load grading_results from orchestrator: {exc}")
                if grading_results:
                    student_results = _build_student_results_from_grading_results(grading_results)
                if not student_results:
                    try:
                        return await _load_from_db()
                    except HTTPException:
                        student_results = []

        cached = batch_image_cache.get(batch_id, {})
        cached_images = cached.get("images_ready", {}).get("images") if cached else None
        answer_images: List[str] = cached_images or []

        if not answer_images:
            raw_images = state.get("processed_images") or state.get("answer_images") or []
            try:
                answer_images = []
                for img in raw_images:
                    if isinstance(img, (bytes, bytearray)):
                        answer_images.append(base64.b64encode(img).decode("utf-8"))
                    elif isinstance(img, str) and img:
                        answer_images.append(img)
            except Exception as exc:
                logger.debug(f"Failed to convert answer images: {exc}")
        if not answer_images:
            history = await _maybe_await(get_grading_history(batch_id))
            if history:
                answer_images = await _load_answer_images_from_db(history.id)
        if not answer_images:
            answer_images = await _load_answer_images_from_storage()
        return ResultsReviewContextResponse(
            batch_id=batch_id,
            status=run_info.status.value if run_info.status else None,
            current_stage=state.get("current_stage"),
            student_results=_format_results_for_frontend(student_results),
            answer_images=answer_images,
        )
    except HTTPException:
        raise
    except Exception as exc:
        logger.error(f"è·å– results ä¸Šä¸‹æ–‡å¤±è´¥: {exc}", exc_info=True)
        raise HTTPException(status_code=500, detail=f"è·å–å¤±è´¥: {str(exc)}")


@router.get("/results/{batch_id}")
async def get_batch_results(batch_id: str, orchestrator: Orchestrator = Depends(get_orchestrator)):
    """
    è·å–æ‰¹æ¬¡æ‰¹æ”¹ç»“æœï¼ˆä» LangGraph Orchestratorï¼‰

    Args:
        batch_id: æ‰¹æ¬¡ ID
        orchestrator: LangGraph Orchestrator

    Returns:
        æ‰¹æ”¹ç»“æœ
    """
    try:
        if not orchestrator:
            raise HTTPException(status_code=503, detail="ç¼–æ’å™¨æœªåˆå§‹åŒ–")

        # æ„å»º run_idï¼ˆä¸ start_run ä¸­çš„æ ¼å¼ä¸€è‡´ï¼‰
        run_id = f"batch_grading_{batch_id}"

        run_info = await orchestrator.get_run_info(run_id)

        if not run_info:
            raise HTTPException(status_code=404, detail="æ‰¹æ¬¡ä¸å­˜åœ¨")

        state = run_info.state or {}

        # ä¼˜å…ˆä» student_results è·å–ç»“æœ
        student_results = state.get("student_results", [])

        # å¦‚æœæ²¡æœ‰ student_resultsï¼Œå°è¯•ä» orchestrator è·å–æœ€ç»ˆè¾“å‡º
        if not student_results:
            try:
                final_output = await orchestrator.get_final_output(run_id)
                if final_output:
                    student_results = final_output.get("student_results", [])
            except Exception as e:
                logger.debug(f"è·å–æœ€ç»ˆè¾“å‡ºå¤±è´¥: {e}")

        return {
            "batch_id": batch_id,
            "status": run_info.status.value,
            "results": _format_results_for_frontend(student_results),
            "class_report": state.get("class_report"),
        }

    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"è·å–æ‰¹æ”¹ç»“æœå¤±è´¥: {str(e)}", exc_info=True)
        raise HTTPException(status_code=500, detail=f"è·å–å¤±è´¥: {str(e)}")


@router.get("/full-results/{batch_id}")
async def get_full_batch_results(
    batch_id: str, orchestrator: Orchestrator = Depends(get_orchestrator)
):
    """
    è·å–æ‰¹æ¬¡å®Œæ•´æ‰¹æ”¹ç»“æœï¼ˆåŒ…å«è·¨é¡µé¢˜ç›®ä¿¡æ¯ï¼‰

    Args:
        batch_id: æ‰¹æ¬¡ ID
        orchestrator: LangGraph Orchestrator

    Returns:
        å®Œæ•´æ‰¹æ”¹ç»“æœï¼ˆåŒ…å«è·¨é¡µé¢˜ç›®ä¿¡æ¯ï¼‰
    """

    async def _load_answer_images_from_db(history_id: str) -> List[str]:
        try:
            images = await _maybe_await(get_page_images(history_id))
            if not images:
                return []
            return [img.file_url for img in images if img.file_url]
        except Exception as exc:
            logger.debug(f"Failed to load answer images from DB: {exc}")
            return []

    async def _load_from_db() -> Dict[str, Any]:
        history = await get_grading_history(batch_id)
        if not history:
            raise HTTPException(status_code=404, detail="æ‰¹æ¬¡ä¸å­˜åœ¨")

        raw_results: List[Dict[str, Any]] = []
        for row in await get_student_results(history.id):
            data = row.result_data
            if isinstance(data, str):
                try:
                    data = json.loads(data)
                except Exception:
                    data = {}
            if not isinstance(data, dict):
                data = {}
            if not data:
                data = {
                    "studentName": row.student_key,
                    "score": row.score,
                    "maxScore": row.max_score,
                }
            raw_results.append(data)

        class_report = None
        history_data = history.result_data
        if history_data:
            if isinstance(history_data, str):
                try:
                    history_data = json.loads(history_data)
                except Exception:
                    history_data = {}
            if isinstance(history_data, dict):
                class_report = history_data.get("summary") or history_data.get("class_report")

        formatted_results = _format_results_for_frontend(raw_results)
        total_max = 0.0
        for item in formatted_results:
            try:
                total_max = max(total_max, float(item.get("maxScore") or 0))
            except (TypeError, ValueError):
                continue

        return {
            "batch_id": batch_id,
            "status": history.status or "completed",
            "results": formatted_results,
            "cross_page_questions": [],
            "parsed_rubric": {},
            "class_report": class_report,
            "total_students": len(formatted_results),
            "total_score": total_max or 100,
        }

    try:
        if not orchestrator:
            return await _load_from_db()

        run_id = f"batch_grading_{batch_id}"
        run_info = await orchestrator.get_run_info(run_id)

        if not run_info:
            return await _load_from_db()

        state = run_info.state or {}
        student_results = state.get("student_results", []) or []
        cross_page_questions = state.get("cross_page_questions", []) or []
        parsed_rubric = state.get("parsed_rubric", {}) or {}
        class_report = state.get("class_report") or state.get("export_data", {}).get("class_report")
        final_output: Optional[Dict[str, Any]] = None
        if not student_results or not parsed_rubric:
            final_output = await orchestrator.get_final_output(run_id)
            if final_output:
                student_results = student_results or final_output.get("student_results") or final_output.get(
                    "results"
                ) or []
                parsed_rubric = parsed_rubric or final_output.get("parsed_rubric", {}) or {}
                cross_page_questions = cross_page_questions or final_output.get("cross_page_questions", []) or []

        if not student_results:
            grading_results = state.get("grading_results") or []
            if not grading_results and final_output:
                grading_results = final_output.get("grading_results") or []
            if grading_results:
                student_results = _build_student_results_from_grading_results(grading_results)

        # å¦‚æœ orchestrator è¿”å›ç©ºç»“æœï¼Œå›é€€åˆ°æ•°æ®åº“æŸ¥è¯¢
        if not student_results:
            logger.info(f"Orchestrator è¿”å›ç©ºç»“æœï¼Œå›é€€åˆ°æ•°æ®åº“æŸ¥è¯¢: batch_id={batch_id}")
            return await _load_from_db()

        return {
            "batch_id": batch_id,
            "status": run_info.status.value,
            "results": _format_results_for_frontend(student_results),
            "cross_page_questions": cross_page_questions,
            "parsed_rubric": parsed_rubric,
            "class_report": class_report,
            "total_students": len(student_results),
            "total_score": parsed_rubric.get("total_score", 100) if parsed_rubric else 100,
        }

    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"è·å–å®Œæ•´æ‰¹æ”¹ç»“æœå¤±è´¥: {str(e)}", exc_info=True)
        raise HTTPException(status_code=500, detail=f"è·å–å¤±è´¥: {str(e)}")


@router.get("/cross-page-questions/{batch_id}")
async def get_cross_page_questions(
    batch_id: str, orchestrator: Orchestrator = Depends(get_orchestrator)
):
    """
    è·å–è·¨é¡µé¢˜ç›®ä¿¡æ¯

    Args:
        batch_id: æ‰¹æ¬¡ ID
        orchestrator: LangGraph Orchestrator

    Returns:
        è·¨é¡µé¢˜ç›®ä¿¡æ¯åˆ—è¡¨
    """
    try:
        if not orchestrator:
            raise HTTPException(status_code=503, detail="ç¼–æ’å™¨æœªåˆå§‹åŒ–")

        run_id = f"batch_grading_{batch_id}"
        run_info = await orchestrator.get_run_info(run_id)

        if not run_info:
            raise HTTPException(status_code=404, detail="æ‰¹æ¬¡ä¸å­˜åœ¨")

        state = run_info.state or {}
        cross_page_questions = state.get("cross_page_questions", [])

        return cross_page_questions

    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"è·å–è·¨é¡µé¢˜ç›®ä¿¡æ¯å¤±è´¥: {str(e)}", exc_info=True)
        raise HTTPException(status_code=500, detail=f"è·å–å¤±è´¥: {str(e)}")


class ConfirmBoundaryRequest(BaseModel):
    """ç¡®è®¤å­¦ç”Ÿè¾¹ç•Œè¯·æ±‚"""

    batch_id: str = Field(..., description="æ‰¹æ¬¡ ID")
    student_key: str = Field(..., description="å­¦ç”Ÿæ ‡è¯†")
    confirmed_pages: List[int] = Field(..., description="ç¡®è®¤çš„é¡µé¢ç´¢å¼•åˆ—è¡¨")


class RubricReviewRequest(BaseModel):
    """æäº¤è¯„åˆ†æ ‡å‡†äººå·¥ç¡®è®¤ç»“æœ"""

    batch_id: str = Field(..., description="æ‰¹æ¬¡ ID")
    action: str = Field(..., description="approve/update/reparse")
    parsed_rubric: Optional[Dict[str, Any]] = Field(None, description="ä¿®æ­£åçš„è¯„åˆ†æ ‡å‡†")
    selected_question_ids: Optional[List[str]] = Field(None, description="ä»…é‡ä¿®æ­£çš„é—®é¢˜ ID åˆ—è¡¨")
    notes: Optional[str] = Field(None, description="è¡¥å……è¯´æ˜")


class ResultsReviewRequest(BaseModel):
    """æäº¤æ‰¹æ”¹ç»“æœäººå·¥ç¡®è®¤ç»“æœ"""

    batch_id: str = Field(..., description="æ‰¹æ¬¡ ID")
    action: str = Field(..., description="approve/update/regrade")
    results: Optional[List[Dict[str, Any]]] = Field(None, description="ä¿®æ­£åçš„ç»“æœ")
    regrade_items: Optional[List[Dict[str, Any]]] = Field(
        None,
        description="éœ€è¦é‡æ–°æ‰¹æ”¹çš„é¢˜ç›®é¡¹",
    )
    notes: Optional[str] = Field(None, description="è¡¥å……è¯´æ˜")


@router.post("/review/rubric")
async def submit_rubric_review(
    request: RubricReviewRequest, orchestrator: Orchestrator = Depends(get_orchestrator)
):
    """æäº¤è¯„åˆ†æ ‡å‡†å¤æ ¸ç»“æœï¼Œæ¢å¤ workflow"""
    try:
        if not orchestrator:
            raise HTTPException(status_code=503, detail="ç¼–æ’å™¨æœªåˆå§‹åŒ–")

        action = request.action.lower().strip()
        if action not in ("approve", "update", "override", "reparse"):
            raise HTTPException(status_code=400, detail="æ— æ•ˆçš„ review action")

        run_id = f"batch_grading_{request.batch_id}"
        run_info = await orchestrator.get_run_info(run_id)
        if not run_info:
            raise HTTPException(status_code=404, detail="æ‰¹æ¬¡ä¸å­˜åœ¨")

        payload: Dict[str, Any] = {
            "action": action,
        }
        if request.parsed_rubric is not None:
            payload["parsed_rubric"] = request.parsed_rubric
        if request.selected_question_ids:
            payload["selected_question_ids"] = request.selected_question_ids
        if request.notes:
            payload["notes"] = request.notes

        success = await orchestrator.send_event(run_id, "review_signal", payload)
        if not success:
            raise HTTPException(status_code=409, detail="æ‰¹æ¬¡æœªå¤„äºå¯å¤æ ¸çŠ¶æ€")

        cached = batch_image_cache.get(request.batch_id)
        if cached and "review_required" in cached:
            cached.pop("review_required", None)

        return {"success": True, "message": "è¯„åˆ†æ ‡å‡†å¤æ ¸å·²æäº¤"}
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"æäº¤è¯„åˆ†æ ‡å‡†å¤æ ¸å¤±è´¥: {str(e)}", exc_info=True)
        raise HTTPException(status_code=500, detail=f"æäº¤å¤±è´¥: {str(e)}")


@router.post("/review/results")
async def submit_results_review(
    request: ResultsReviewRequest, orchestrator: Orchestrator = Depends(get_orchestrator)
):
    """æäº¤æ‰¹æ”¹ç»“æœå¤æ ¸ï¼Œæ¢å¤ workflow"""
    try:
        if not orchestrator:
            raise HTTPException(status_code=503, detail="ç¼–æ’å™¨æœªåˆå§‹åŒ–")

        action = request.action.lower().strip()
        if action not in ("approve", "update", "override", "regrade"):
            raise HTTPException(status_code=400, detail="æ— æ•ˆçš„ review action")

        run_id = f"batch_grading_{request.batch_id}"
        run_info = await orchestrator.get_run_info(run_id)
        if not run_info:
            raise HTTPException(status_code=404, detail="æ‰¹æ¬¡ä¸å­˜åœ¨")

        payload: Dict[str, Any] = {
            "action": action,
        }
        if request.results is not None:
            payload["results"] = request.results
        if request.regrade_items is not None:
            payload["regrade_items"] = request.regrade_items
        if request.notes:
            payload["notes"] = request.notes

        success = await orchestrator.send_event(run_id, "review_signal", payload)
        if not success and not request.results:
            raise HTTPException(status_code=409, detail="æ‰¹æ¬¡æœªå¤„äºå¯å¤æ ¸çŠ¶æ€")

        cached = batch_image_cache.get(request.batch_id)
        if cached and "review_required" in cached:
            cached.pop("review_required", None)

        if request.results:
            try:
                history = get_grading_history(request.batch_id)
                if history:
                    raw_history_data = history.result_data
                    history_data: Dict[str, Any] = {}
                    if isinstance(raw_history_data, dict):
                        history_data = raw_history_data
                    elif isinstance(raw_history_data, str):
                        try:
                            parsed_history = json.loads(raw_history_data)
                            if isinstance(parsed_history, dict):
                                history_data = parsed_history
                        except Exception:
                            history_data = {}

                    class_id = history_data.get("class_id") if history_data else None
                    homework_id = history_data.get("homework_id") if history_data else None

                    existing_results = {
                        row.student_key: row for row in get_student_results(history.id)
                    }

                    updated_scores: List[float] = []
                    updated_keys: set[str] = set()

                    for incoming in request.results:
                        student_key = (
                            incoming.get("studentKey")
                            or incoming.get("student_key")
                            or incoming.get("studentName")
                            or incoming.get("student_name")
                        )
                        if not student_key:
                            continue

                        existing_row = existing_results.get(student_key)
                        existing_data = existing_row.result_data if existing_row else None
                        if isinstance(existing_data, str):
                            try:
                                existing_data = json.loads(existing_data)
                            except Exception:
                                existing_data = {}
                        if not isinstance(existing_data, dict):
                            existing_data = {}

                        question_results = (
                            existing_data.get("questionResults")
                            or existing_data.get("question_results")
                            or []
                        )
                        if not isinstance(question_results, list):
                            question_results = []
                        original_scores = {
                            str(q.get("questionId") or q.get("question_id")): _safe_float(
                                q.get("score")
                            )
                            for q in question_results
                            if q.get("questionId") or q.get("question_id")
                        }

                        updates = (
                            incoming.get("questionResults")
                            or incoming.get("question_results")
                            or []
                        )
                        if not isinstance(updates, list):
                            updates = []

                        for update in updates:
                            question_id = str(
                                update.get("questionId") or update.get("question_id") or ""
                            )
                            if not question_id:
                                continue
                            updated_score = update.get("score")
                            updated_feedback = update.get("feedback")
                            target = next(
                                (
                                    q
                                    for q in question_results
                                    if str(q.get("questionId") or q.get("question_id"))
                                    == question_id
                                ),
                                None,
                            )
                            if not target:
                                target = {
                                    "questionId": question_id,
                                    "score": updated_score or 0,
                                    "feedback": updated_feedback or "",
                                }
                                question_results.append(target)
                                continue
                            original_score = original_scores.get(
                                question_id, target.get("score") or 0
                            )
                            target["score"] = (
                                updated_score if updated_score is not None else original_score
                            )
                            if updated_feedback is not None:
                                target["feedback"] = updated_feedback

                        existing_data["questionResults"] = question_results

                        total_score = sum(_safe_float(q.get("score")) for q in question_results)
                        total_max = sum(
                            _safe_float(q.get("maxScore") or q.get("max_score"))
                            for q in question_results
                        )
                        existing_data["score"] = total_score
                        if total_max > 0:
                            existing_data["maxScore"] = total_max
                            existing_data.pop("gradingAnnotations", None)
                            existing_data.pop("grading_annotations", None)

                        updated_scores.append(total_score)
                        updated_keys.add(student_key)

                        student_id_value = existing_row.student_id if existing_row else None
                        student_result = StudentGradingResult(
                            id=existing_row.id if existing_row else _make_student_result_id(history.id, student_key, student_id_value),
                            grading_history_id=history.id,
                            student_key=student_key,
                            score=total_score,
                            max_score=total_max or None,
                            class_id=existing_row.class_id if existing_row else None,
                            student_id=student_id_value,
                            summary=existing_row.summary if existing_row else None,
                            confession=existing_row.confession if existing_row else None,
                            result_data=existing_data,
                        )
                        save_student_result(student_result)

                        if class_id and homework_id and student_result.student_id:
                            upsert_homework_submission_grade(
                                class_id=class_id,
                                homework_id=homework_id,
                                student_id=student_result.student_id,
                                student_name=student_key,
                                score=total_score,
                                feedback=(
                                    existing_data.get("studentSummary", {}).get("overall")
                                    if isinstance(existing_data.get("studentSummary"), dict)
                                    else None
                                ),
                                grading_batch_id=request.batch_id,
                            )

                    if updated_scores:
                        remaining_scores = [
                            row.score or 0
                            for key, row in existing_results.items()
                            if key not in updated_keys
                        ]
                        all_scores = [*updated_scores, *remaining_scores]
                        history.average_score = (
                            round(sum(all_scores) / len(all_scores), 2) if all_scores else None
                        )
                        history.total_students = len(all_scores)
                        if history_data:
                            summary = history_data.get("summary")
                            if isinstance(summary, dict):
                                summary["average_score"] = history.average_score
                            history.result_data = history_data
                        save_grading_history(history)
            except Exception as exc:
                logger.error("ä¿å­˜å¤æ ¸ç»“æœå¤±è´¥: %s", exc, exc_info=True)

        if success:
            return {"success": True, "message": "æ‰¹æ”¹ç»“æœå¤æ ¸å·²æäº¤"}
        return {"success": True, "message": "æ‰¹æ”¹ç»“æœå·²ä¿å­˜ï¼Œæµç¨‹æœªæ¢å¤"}
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"æäº¤æ‰¹æ”¹å¤æ ¸å¤±è´¥: {str(e)}", exc_info=True)
        raise HTTPException(status_code=500, detail=f"æäº¤å¤±è´¥: {str(e)}")


@router.post("/confirm-boundary")
async def confirm_student_boundary(
    request: ConfirmBoundaryRequest, orchestrator: Orchestrator = Depends(get_orchestrator)
):
    """
    ç¡®è®¤å­¦ç”Ÿè¾¹ç•Œ

    å½“ AI è¯†åˆ«çš„å­¦ç”Ÿè¾¹ç•Œä¸å‡†ç¡®æ—¶ï¼Œå…è®¸ç”¨æˆ·æ‰‹åŠ¨ç¡®è®¤

    Args:
        request: ç¡®è®¤è¾¹ç•Œè¯·æ±‚
        orchestrator: LangGraph Orchestrator

    Returns:
        ç¡®è®¤ç»“æœ
    """
    try:
        if not orchestrator:
            raise HTTPException(status_code=503, detail="ç¼–æ’å™¨æœªåˆå§‹åŒ–")

        run_id = f"batch_grading_{request.batch_id}"
        run_info = await orchestrator.get_run_info(run_id)

        if not run_info:
            raise HTTPException(status_code=404, detail="æ‰¹æ¬¡ä¸å­˜åœ¨")

        # æ›´æ–°çŠ¶æ€ä¸­çš„å­¦ç”Ÿè¾¹ç•Œ
        state = run_info.state or {}
        student_boundaries = state.get("student_boundaries", [])

        # æŸ¥æ‰¾å¹¶æ›´æ–°å¯¹åº”å­¦ç”Ÿçš„è¾¹ç•Œ
        updated = False
        for boundary in student_boundaries:
            if boundary.get("student_key") == request.student_key:
                boundary["pages"] = request.confirmed_pages
                boundary["confirmed"] = True
                updated = True
                break

        if not updated:
            # å¦‚æœæ²¡æœ‰æ‰¾åˆ°ï¼Œæ·»åŠ æ–°çš„è¾¹ç•Œ
            student_boundaries.append(
                {
                    "student_key": request.student_key,
                    "pages": request.confirmed_pages,
                    "confirmed": True,
                }
            )

        logger.info(
            f"å­¦ç”Ÿè¾¹ç•Œå·²ç¡®è®¤: batch_id={request.batch_id}, student_key={request.student_key}"
        )

        return {"success": True, "message": f"å­¦ç”Ÿ {request.student_key} çš„è¾¹ç•Œå·²ç¡®è®¤"}

    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"ç¡®è®¤å­¦ç”Ÿè¾¹ç•Œå¤±è´¥: {str(e)}", exc_info=True)
        raise HTTPException(status_code=500, detail=f"ç¡®è®¤å¤±è´¥: {str(e)}")


# ==================== å¯¼å‡º API ====================


class ExportAnnotatedImagesRequest(BaseModel):
    """å¯¼å‡ºå¸¦æ‰¹æ³¨å›¾ç‰‡è¯·æ±‚"""

    include_original: bool = Field(default=False, description="æ˜¯å¦åŒ…å«åŸå§‹å›¾ç‰‡")


class ExportExcelRequest(BaseModel):
    """å¯¼å‡º Excel è¯·æ±‚"""

    columns: Optional[List[Dict[str, Any]]] = Field(None, description="è‡ªå®šä¹‰åˆ—é…ç½®")


class SmartExcelRequest(BaseModel):
    """æ™ºèƒ½ Excel ç”Ÿæˆè¯·æ±‚"""

    prompt: str = Field(..., description="ç”¨æˆ·æè¿°çš„æ ¼å¼éœ€æ±‚")
    template_base64: Optional[str] = Field(None, description="æ¨¡æ¿ Excel Base64")


@router.post("/export/annotated-images/{batch_id}")
async def export_annotated_images(
    batch_id: str,
    request: ExportAnnotatedImagesRequest = ExportAnnotatedImagesRequest(),
    orchestrator: Orchestrator = Depends(get_orchestrator),
):
    """
    å¯¼å‡ºå¸¦æ‰¹æ³¨çš„å­¦ç”Ÿä½œç­”å›¾ç‰‡ (ZIP)

    å°†æ‰€æœ‰å­¦ç”Ÿçš„ä½œç­”å›¾ç‰‡æ¸²æŸ“æ‰¹æ³¨åæ‰“åŒ…ä¸º ZIP ä¸‹è½½
    """
    raise HTTPException(
        status_code=410,
        detail="åç«¯æ‰¹æ³¨æ¸²æŸ“å·²ç¦ç”¨ï¼Œè¯·ä½¿ç”¨å‰ç«¯ Canvas æ¸²æŸ“ä¸å¯¼å‡ºã€‚",
    )


@router.post("/export/excel/{batch_id}")
async def export_excel(
    batch_id: str,
    request: ExportExcelRequest = ExportExcelRequest(),
    orchestrator: Orchestrator = Depends(get_orchestrator),
):
    """
    å¯¼å‡º Excel ç»Ÿè®¡æ•°æ®

    åŒ…å«å­¦ç”Ÿæˆç»©ã€é¢˜ç›®ç»Ÿè®¡ã€ç­çº§æŠ¥å‘Šç­‰å¤šä¸ª Sheet
    """
    from fastapi.responses import Response
    from src.services.export_service import ExcelExporter

    try:
        if not orchestrator:
            raise HTTPException(status_code=503, detail="ç¼–æ’å™¨æœªåˆå§‹åŒ–")

        run_id = f"batch_grading_{batch_id}"
        run_info = await orchestrator.get_run_info(run_id)

        if not run_info:
            raise HTTPException(status_code=404, detail="æ‰¹æ¬¡ä¸å­˜åœ¨")

        state = run_info.state or {}
        student_results = state.get("student_results", [])
        class_report = state.get("class_report") or state.get("export_data", {}).get("class_report")

        if not student_results:
            raise HTTPException(status_code=404, detail="æ— æ‰¹æ”¹ç»“æœ")

        # æ ¼å¼åŒ–ç»“æœ
        formatted_results = _format_results_for_frontend(student_results)

        # å¯¼å‡º
        exporter = ExcelExporter()
        excel_bytes = exporter.export_basic(formatted_results, class_report, request.columns)

        filename = f"grading_report_{batch_id}_{datetime.now().strftime('%Y%m%d_%H%M%S')}.xlsx"

        return Response(
            content=excel_bytes,
            media_type="application/vnd.openxmlformats-officedocument.spreadsheetml.sheet",
            headers={
                "Content-Disposition": f'attachment; filename="{filename}"',
            },
        )

    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"å¯¼å‡º Excel å¤±è´¥: {str(e)}", exc_info=True)
        raise HTTPException(status_code=500, detail=f"å¯¼å‡ºå¤±è´¥: {str(e)}")


@router.post("/export/smart-excel/{batch_id}")
async def export_smart_excel(
    batch_id: str,
    request: SmartExcelRequest,
    orchestrator: Orchestrator = Depends(get_orchestrator),
):
    """
    LLM æ™ºèƒ½ Excel ç”Ÿæˆ

    æ”¯æŒï¼š
    - ç”¨æˆ·å¯¹è¯æè¿°æ ¼å¼éœ€æ±‚
    - å¯¼å…¥å·²æœ‰ Excel æ¨¡æ¿å¹¶å¡«å……æ•°æ®
    """
    from fastapi.responses import Response
    from src.services.export_service import SmartExcelGenerator
    from src.services.llm_client import get_llm_client

    try:
        if not orchestrator:
            raise HTTPException(status_code=503, detail="ç¼–æ’å™¨æœªåˆå§‹åŒ–")

        run_id = f"batch_grading_{batch_id}"
        run_info = await orchestrator.get_run_info(run_id)

        if not run_info:
            raise HTTPException(status_code=404, detail="æ‰¹æ¬¡ä¸å­˜åœ¨")

        state = run_info.state or {}
        student_results = state.get("student_results", [])
        class_report = state.get("class_report") or state.get("export_data", {}).get("class_report")

        if not student_results:
            raise HTTPException(status_code=404, detail="æ— æ‰¹æ”¹ç»“æœ")

        # æ ¼å¼åŒ–ç»“æœ
        formatted_results = _format_results_for_frontend(student_results)

        # è§£ç æ¨¡æ¿
        template_bytes = None
        if request.template_base64:
            import base64

            try:
                if request.template_base64.startswith("data:"):
                    request.template_base64 = request.template_base64.split(",", 1)[1]
                template_bytes = base64.b64decode(request.template_base64)
            except Exception as e:
                raise HTTPException(status_code=400, detail=f"æ¨¡æ¿è§£ç å¤±è´¥: {e}")

        # è·å– LLM å®¢æˆ·ç«¯
        llm_client = None
        try:
            llm_client = get_llm_client()
        except Exception as e:
            logger.debug(f"è·å– LLM å®¢æˆ·ç«¯å¤±è´¥: {e}")

        # ç”Ÿæˆ Excel
        generator = SmartExcelGenerator(llm_client)
        excel_bytes, explanation = await generator.generate_from_prompt(
            formatted_results,
            class_report,
            request.prompt,
            template_bytes,
        )

        filename = f"grading_smart_{batch_id}_{datetime.now().strftime('%Y%m%d_%H%M%S')}.xlsx"

        return Response(
            content=excel_bytes,
            media_type="application/vnd.openxmlformats-officedocument.spreadsheetml.sheet",
            headers={
                "Content-Disposition": f'attachment; filename="{filename}"',
                "X-LLM-Explanation": explanation.encode("utf-8").decode("latin-1"),
            },
        )

    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"æ™ºèƒ½ Excel ç”Ÿæˆå¤±è´¥: {str(e)}", exc_info=True)
        raise HTTPException(status_code=500, detail=f"ç”Ÿæˆå¤±è´¥: {str(e)}")


@router.post("/render/batch/{batch_id}")
async def render_batch_annotations(
    batch_id: str,
    page_indices: Optional[List[int]] = None,
    orchestrator: Orchestrator = Depends(get_orchestrator),
):
    """
    æ‰¹é‡æ¸²æŸ“æ‰¹æ³¨åˆ°å›¾ç‰‡

    è¿”å›æŒ‡å®šé¡µé¢çš„å¸¦æ‰¹æ³¨å›¾ç‰‡ Base64 åˆ—è¡¨
    """
    raise HTTPException(
        status_code=410,
        detail="åç«¯æ‰¹æ³¨æ¸²æŸ“å·²ç¦ç”¨ï¼Œè¯·ä½¿ç”¨å‰ç«¯ Canvas æ¸²æŸ“ã€‚",
    )


# ==================== Confession API (Task 11) ====================


class ConfessionResponse(BaseModel):
    """Confession report response"""

    batch_id: str
    overall_status: str
    overall_confidence: float
    issues: List[Dict[str, Any]]
    warnings: List[str]
    summary: str
    memory_updates: List[Dict[str, Any]]
    generated_at: str


@router.get("/confession/{batch_id}", response_model=ConfessionResponse)
async def get_batch_confession(
    batch_id: str,
    include_memory_updates: bool = True,
    orchestrator: Orchestrator = Depends(get_orchestrator),
):
    """
    Get batch confession report (enhanced).

    Returns:
        Confession report with issues, warnings, and memory updates.
    """
    try:
        if not orchestrator:
            raise HTTPException(status_code=503, detail="Orchestrator not initialized")

        run_id = f"batch_grading_{batch_id}"
        run_info = await orchestrator.get_run_info(run_id)

        if not run_info:
            raise HTTPException(status_code=404, detail="Batch not found")

        state = run_info.state or {}
        student_results = state.get("student_results", [])

        if not student_results:
            raise HTTPException(status_code=404, detail="No grading results")

        all_issues: List[Dict[str, Any]] = []
        all_warnings: List[str] = []
        total_confidence = 0.0
        student_count = 0

        for student in student_results:
            confession = student.get("confession") or {}
            self_audit = student.get("self_audit") or student.get("selfAudit") or {}

            issues = confession.get("issues", [])
            if issues:
                student_key = student.get("student_key") or student.get("studentKey") or "Unknown"
                for issue in issues:
                    issue_copy = dict(issue)
                    issue_copy["student_key"] = student_key
                    all_issues.append(issue_copy)

            warnings = confession.get("warnings", [])
            all_warnings.extend(warnings)

            conf = confession.get("overall_confidence") or self_audit.get("overall_confidence")
            if conf:
                total_confidence += float(conf)
                student_count += 1

        avg_confidence = total_confidence / student_count if student_count > 0 else 0.5

        error_count = sum(1 for i in all_issues if i.get("severity") == "error")
        warning_count = sum(1 for i in all_issues if i.get("severity") == "warning")

        if error_count > 0:
            overall_status = "needs_review"
        elif warning_count > 3:
            overall_status = "caution"
        else:
            overall_status = "ok"

        memory_updates: List[Dict[str, Any]] = []
        if include_memory_updates:
            try:
                from src.services.grading_memory import get_memory_service

                memory_service = get_memory_service()
                batch_memory = memory_service.get_batch_memory(batch_id)

                if batch_memory:
                    for correction in batch_memory.corrections:
                        memory_updates.append(
                            {
                                "type": "correction",
                                "question_id": correction.get("question_id"),
                                "original_score": correction.get("original_score"),
                                "corrected_score": correction.get("corrected_score"),
                                "reason": correction.get("reason"),
                            }
                        )
            except Exception as exc:
                logger.debug(f"Failed to collect memory updates: {exc}")

        return {
            "batch_id": batch_id,
            "overall_status": overall_status,
            "overall_confidence": round(avg_confidence, 3),
            "issues": all_issues,
            "warnings": list(set(all_warnings)),
            "summary": f"Batch {batch_id}: {len(student_results)} students, avg confidence {avg_confidence:.1%}",
            "memory_updates": memory_updates,
            "generated_at": datetime.now().isoformat(),
        }

    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"Failed to get confession report: {str(e)}", exc_info=True)
        raise HTTPException(status_code=500, detail=f"Failed to get confession: {str(e)}")


@router.get("/{batch_id}/files")
async def list_batch_files(batch_id: str):
    """è·å–æ‰¹æ¬¡çš„æ‰€æœ‰å­˜å‚¨æ–‡ä»¶åˆ—è¡¨"""
    try:
        file_storage = get_file_storage_service()
        files = await file_storage.list_batch_files(batch_id)
        
        return {
            "batch_id": batch_id,
            "files": [f.to_dict() for f in files],
            "total_count": len(files),
        }
    except Exception as e:
        logger.error(f"è·å–æ–‡ä»¶åˆ—è¡¨å¤±è´¥: {str(e)}", exc_info=True)
        raise HTTPException(status_code=500, detail=f"è·å–æ–‡ä»¶åˆ—è¡¨å¤±è´¥: {str(e)}")


@router.get("/files/{file_id}")
async def get_file(file_id: str):
    """è·å–æ–‡ä»¶ä¿¡æ¯"""
    try:
        file_storage = get_file_storage_service()
        file_info = await file_storage.get_file_info(file_id)
        
        if not file_info:
            raise HTTPException(status_code=404, detail="æ–‡ä»¶ä¸å­˜åœ¨")
        
        return file_info.to_dict()
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"è·å–æ–‡ä»¶ä¿¡æ¯å¤±è´¥: {str(e)}", exc_info=True)
        raise HTTPException(status_code=500, detail=f"è·å–æ–‡ä»¶ä¿¡æ¯å¤±è´¥: {str(e)}")


@router.get("/files/{file_id}/download")
async def download_file(file_id: str):
    """ä¸‹è½½æ–‡ä»¶"""
    from fastapi.responses import Response
    
    try:
        file_storage = get_file_storage_service()
        file_info = await file_storage.get_file_info(file_id)
        
        if not file_info:
            raise HTTPException(status_code=404, detail="æ–‡ä»¶ä¸å­˜åœ¨")
        
        content = await file_storage.get_file(file_id)
        if not content:
            raise HTTPException(status_code=404, detail="æ–‡ä»¶å†…å®¹ä¸å­˜åœ¨")
        
        return Response(
            content=content,
            media_type=file_info.content_type,
            headers={
                "Content-Disposition": f'attachment; filename="{file_info.filename}"',
            },
        )
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"ä¸‹è½½æ–‡ä»¶å¤±è´¥: {str(e)}", exc_info=True)
        raise HTTPException(status_code=500, detail=f"ä¸‹è½½æ–‡ä»¶å¤±è´¥: {str(e)}")
