"""æ‰¹é‡æäº¤ API è·¯ç”± - ä½¿ç”¨ LangGraph Orchestrator

æ­£ç¡®çš„æ¶æ„ï¼š
1. ä½¿ç”¨ LangGraph Orchestrator å¯åŠ¨æ‰¹æ”¹æµç¨‹
2. é€šè¿‡ LangGraph çš„æµå¼ API å®æ—¶æ¨é€è¿›åº¦
3. åˆ©ç”¨ PostgreSQL Checkpointer å®ç°æŒä¹…åŒ–å’Œæ–­ç‚¹æ¢å¤
"""

import uuid
import logging
import tempfile
import asyncio
import base64
import json
from datetime import datetime
from pathlib import Path
from typing import List, Optional, Dict, Any

from fastapi import APIRouter, HTTPException, UploadFile, File, Form, WebSocket, WebSocketDisconnect, Depends
from starlette.websockets import WebSocketState
from pydantic import BaseModel, Field
import fitz
from PIL import Image
import os

from src.models.enums import SubmissionStatus
from src.orchestration.base import Orchestrator
from src.api.dependencies import get_orchestrator
from src.utils.image import to_jpeg_bytes, pil_to_jpeg_bytes
from src.db.sqlite import (
    save_grading_history, 
    save_student_result, 
    GradingHistory, 
    StudentGradingResult,
    upsert_homework_submission_grade,
    list_class_students,
)


logger = logging.getLogger(__name__)
router = APIRouter(prefix="/batch", tags=["æ‰¹é‡æäº¤"])

# å­˜å‚¨æ´»è·ƒçš„ WebSocket è¿æ¥
active_connections: Dict[str, List[WebSocket]] = {}
# ç¼“å­˜å›¾ç‰‡ï¼Œé¿å… images_ready æ—©äº WebSocket è¿æ¥å¯¼è‡´å‰ç«¯ä¸¢å¤±
batch_image_cache: Dict[str, Dict[str, dict]] = {}
DEBUG_LOG_PATH = os.getenv("GRADEOS_DEBUG_LOG_PATH")
TEACHER_MAX_ACTIVE_RUNS = int(os.getenv("TEACHER_MAX_ACTIVE_RUNS", "3"))
_TEACHER_SEMAPHORE_LOCK = asyncio.Lock()
_TEACHER_SEMAPHORES: Dict[str, asyncio.Semaphore] = {}


def _is_ws_connected(websocket: WebSocket) -> bool:
    return (
        websocket.client_state == WebSocketState.CONNECTED
        and websocket.application_state == WebSocketState.CONNECTED
    )


def _write_debug_log(payload: Dict[str, Any]) -> None:
    if not DEBUG_LOG_PATH:
        return
    try:
        Path(DEBUG_LOG_PATH).parent.mkdir(parents=True, exist_ok=True)
        with open(DEBUG_LOG_PATH, "a", encoding="utf-8") as log_file:
            log_file.write(json.dumps(payload, ensure_ascii=False) + "\n")
    except Exception as exc:
        logger.debug(f"Failed to write debug log: {exc}")


def _safe_to_jpeg_bytes(image_bytes: bytes, label: str) -> bytes:
    try:
        return to_jpeg_bytes(image_bytes)
    except Exception as exc:
        logger.warning(f"Failed to convert image to JPEG ({label}): {exc}")
        return image_bytes



def _normalize_teacher_key(teacher_id: Optional[str]) -> str:
    if teacher_id and teacher_id.strip():
        return teacher_id.strip()
    return "anonymous"


async def _get_teacher_semaphore(teacher_key: str) -> asyncio.Semaphore:
    async with _TEACHER_SEMAPHORE_LOCK:
        semaphore = _TEACHER_SEMAPHORES.get(teacher_key)
        if not semaphore:
            semaphore = asyncio.Semaphore(max(1, TEACHER_MAX_ACTIVE_RUNS))
            _TEACHER_SEMAPHORES[teacher_key] = semaphore
    return semaphore


class BatchSubmissionResponse(BaseModel):
    """æ‰¹é‡æäº¤å“åº”"""
    batch_id: str = Field(..., description="æ‰¹æ¬¡ ID")
    status: SubmissionStatus = Field(..., description="çŠ¶æ€")
    total_pages: int = Field(..., description="æ€»é¡µæ•°")
    estimated_completion_time: int = Field(..., description="é¢„è®¡å®Œæˆæ—¶é—´ï¼ˆç§’ï¼‰")


class BatchStatusResponse(BaseModel):
    """æ‰¹é‡çŠ¶æ€æŸ¥è¯¢å“åº”"""
    batch_id: str
    exam_id: str
    status: str
    total_students: int = Field(0, description="è¯†åˆ«åˆ°çš„å­¦ç”Ÿæ•°")
    completed_students: int = Field(0, description="å·²å®Œæˆæ‰¹æ”¹çš„å­¦ç”Ÿæ•°")
    unidentified_pages: int = Field(0, description="æœªè¯†åˆ«å­¦ç”Ÿçš„é¡µæ•°")
    results: Optional[List[dict]] = Field(None, description="æ‰¹æ”¹ç»“æœ")


class RubricReviewContextResponse(BaseModel):
    """å‰ç«¯ rubric review ä¸Šä¸‹æ–‡"""
    batch_id: str
    status: Optional[str] = None
    current_stage: Optional[str] = None
    parsed_rubric: Optional[dict] = None
    rubric_images: List[str] = []


class ResultsReviewContextResponse(BaseModel):
    """å‰ç«¯ results review ä¸Šä¸‹æ–‡"""
    batch_id: str
    status: Optional[str] = None
    current_stage: Optional[str] = None
    student_results: List[dict] = []
    answer_images: List[str] = []


def _pdf_to_images(pdf_path: str, dpi: int = 150) -> List[bytes]:
    """å°† PDF è½¬æ¢ä¸ºå›¾åƒåˆ—è¡¨"""
    pdf_doc = fitz.open(pdf_path)
    images = []
    
    for page_num in range(len(pdf_doc)):
        page = pdf_doc[page_num]
        mat = fitz.Matrix(dpi/72, dpi/72)
        pix = page.get_pixmap(matrix=mat)
        img = Image.frombytes("RGB", [pix.width, pix.height], pix.samples)
        
        images.append(pil_to_jpeg_bytes(img))
    
    pdf_doc.close()
    return images


async def broadcast_progress(batch_id: str, message: dict):
    """å‘æ‰€æœ‰è¿æ¥çš„ WebSocket å®¢æˆ·ç«¯å¹¿æ’­è¿›åº¦"""
    # #region agent log - å‡è®¾J: broadcast_progress è¢«è°ƒç”¨
    msg_type = message.get("type", "unknown")
    if msg_type in ("images_ready", "rubric_images_ready", "review_required"):
        cached = batch_image_cache.setdefault(batch_id, {})
        cached[msg_type] = message
    if msg_type == "llm_stream_chunk":
        node_id = message.get("nodeId") or ""
        if node_id in ("rubric_parse", "rubric_review"):
            cached = batch_image_cache.setdefault(batch_id, {})
            stream_cache = cached.setdefault("llm_stream_cache", {})
            cache_key = f"{node_id}:{message.get('agentId') or 'all'}:{message.get('streamType') or 'output'}"
            existing = stream_cache.get(cache_key, {})
            chunk_data = message.get("chunk", "") or ""
            if isinstance(chunk_data, list):
                chunk_data = "".join([str(c) for c in chunk_data])
            else:
                chunk_data = str(chunk_data)
            
            existing_chunk = existing.get("chunk", "") or ""
            combined = existing_chunk + chunk_data
            max_chars = 12000
            if len(combined) > max_chars:
                combined = combined[-max_chars:]
            stream_cache[cache_key] = {
                **message,
                "chunk": combined,
            }
    if msg_type in ("review_completed", "workflow_completed"):
        cached = batch_image_cache.get(batch_id)
        if cached and "review_required" in cached:
            cached.pop("review_required", None)
        if cached and "llm_stream_cache" in cached:
            cached.pop("llm_stream_cache", None)
    if msg_type == "workflow_completed":
        import traceback as _tb_j
        stack = ''.join(_tb_j.format_stack()[-5:-1])  # è·å–è°ƒç”¨æ ˆ
        _write_debug_log({
            "hypothesisId": "J",
            "location": "batch_langgraph.py:broadcast_progress",
            "message": "broadcast_progresså‘é€workflow_completed",
            "data": {
                "batch_id": batch_id,
                "results_count": len(message.get("results", [])),
                "stack_trace": stack[:500],
            },
            "timestamp": int(datetime.now().timestamp() * 1000),
            "sessionId": "debug-session",
        })
    # #endregion
    if batch_id in active_connections:
        disconnected = []
        for ws in active_connections[batch_id]:
            if not _is_ws_connected(ws):
                disconnected.append(ws)
                continue
            try:
                await ws.send_json(message)
            except Exception as e:
                logger.error(f"WebSocket å‘é€å¤±è´¥: {e}")
                disconnected.append(ws)
        
        # ç§»é™¤æ–­å¼€çš„è¿æ¥
        for ws in disconnected:
            if ws in active_connections[batch_id]:
                active_connections[batch_id].remove(ws)



async def _start_run_with_teacher_limit(
    *,
    teacher_key: str,
    batch_id: str,
    payload: Dict[str, Any],
    orchestrator: Orchestrator,
    class_id: Optional[str],
    homework_id: Optional[str],
    student_mapping: List[dict],
) -> Optional[str]:
    semaphore = await _get_teacher_semaphore(teacher_key)
    await semaphore.acquire()
    run_id: Optional[str] = None
    try:
        run_id = await orchestrator.start_run(
            graph_name="batch_grading",
            payload=payload,
            idempotency_key=batch_id
        )
        logger.info(
            f"LangGraph ??????? "
            f"batch_id={batch_id}, "
            f"run_id={run_id}"
        )
        asyncio.create_task(
            stream_langgraph_progress(
                batch_id=batch_id,
                run_id=run_id,
                orchestrator=orchestrator,
                class_id=class_id,
                homework_id=homework_id,
                student_mapping=student_mapping,
                teacher_key=teacher_key,
                teacher_semaphore=semaphore,
            )
        )
        return run_id
    except Exception as exc:
        logger.error(f"????????: {exc}", exc_info=True)
        semaphore.release()
        await broadcast_progress(batch_id, {
            "type": "workflow_update",
            "nodeId": "rubric_parse",
            "status": "failed",
            "message": "Queued run failed to start"
        })
        return None


@router.post("/submit", response_model=BatchSubmissionResponse)
async def submit_batch(
    exam_id: Optional[str] = Form(None, description="è€ƒè¯• ID"),
    rubrics: List[UploadFile] = File(default=[], description="è¯„åˆ†æ ‡å‡† PDFï¼ˆå¯é€‰ï¼‰"),
    files: List[UploadFile] = File(..., description="å­¦ç”Ÿä½œç­” PDF"),
    api_key: Optional[str] = Form(None, description="LLM API Key"),
    teacher_id: Optional[str] = Form(None, description="?? ID"),
    auto_identify: bool = Form(True, description="æ˜¯å¦è‡ªåŠ¨è¯†åˆ«å­¦ç”Ÿèº«ä»½"),
    student_boundaries: Optional[str] = Form(None, description="æ‰‹åŠ¨è®¾ç½®çš„å­¦ç”Ÿè¾¹ç•Œ (JSON List of page indices)"),
    expected_students: Optional[int] = Form(None, description="é¢„æœŸå­¦ç”Ÿæ•°é‡ï¼ˆå¼ºçƒˆå»ºè®®æä¾›ï¼Œç”¨äºæ›´å‡†ç¡®çš„åˆ†å‰²ï¼‰"),
    # æ–°å¢ï¼šç­çº§æ‰¹æ”¹ä¸Šä¸‹æ–‡
    class_id: Optional[str] = Form(None, description="ç­çº§ IDï¼ˆç”¨äºæˆç»©å†™å›ï¼‰"),
    homework_id: Optional[str] = Form(None, description="ä½œä¸š IDï¼ˆç”¨äºæˆç»©å†™å›ï¼‰"),
    student_mapping_json: Optional[str] = Form(None, description="å­¦ç”Ÿæ˜ å°„ JSON [{studentId, studentName, startIndex, endIndex}]"),
    enable_review: bool = Form(True, description="æ˜¯å¦å¯ç”¨äººå·¥äº¤äº’"),
    grading_mode: Optional[str] = Form(None, description="grading mode: standard/assist_teacher/assist_student/auto"),
    orchestrator: Orchestrator = Depends(get_orchestrator)
):
    """
    æ‰¹é‡æäº¤è¯•å·å¹¶è¿›è¡Œæ‰¹æ”¹ï¼ˆä½¿ç”¨ LangGraph Orchestratorï¼‰
    
    æ­£ç¡®çš„æ¶æ„ï¼š
    1. ä½¿ç”¨ LangGraph Orchestrator å¯åŠ¨ batch_grading Graph
    2. Graph è‡ªåŠ¨å¤„ç†ï¼šè¾¹ç•Œæ£€æµ‹ â†’ å¹¶è¡Œæ‰¹æ”¹ â†’ èšåˆ â†’ æŒä¹…åŒ– â†’ é€šçŸ¥
    3. é€šè¿‡ WebSocket å®æ—¶æ¨é€ LangGraph çš„æ‰§è¡Œè¿›åº¦
    
    Args:
        exam_id: è€ƒè¯• ID
        rubrics: è¯„åˆ†æ ‡å‡† PDF æ–‡ä»¶åˆ—è¡¨
        files: å­¦ç”Ÿä½œç­” PDF æ–‡ä»¶åˆ—è¡¨
        api_key: LLM API Key
        auto_identify: æ˜¯å¦å¯ç”¨è‡ªåŠ¨å­¦ç”Ÿè¯†åˆ«
        orchestrator: LangGraph Orchestratorï¼ˆä¾èµ–æ³¨å…¥ï¼‰
        
    Returns:
        BatchSubmissionResponse: æ‰¹æ¬¡ä¿¡æ¯
    """
    # #region agent log - å‡è®¾K: submit_batch è¢«è°ƒç”¨
    _write_debug_log({
        "hypothesisId": "K",
        "location": "batch_langgraph.py:submit_batch:entry",
        "message": "submit_batchç«¯ç‚¹è¢«è°ƒç”¨",
        "data": {"files_count": len(files), "rubrics_count": len(rubrics)},
        "timestamp": int(datetime.now().timestamp() * 1000),
        "sessionId": "debug-session",
    })
    # #endregion
    # æ£€æŸ¥ orchestrator æ˜¯å¦å¯ç”¨
    if not orchestrator:
        raise HTTPException(
            status_code=503, 
            detail="æ‰¹æ”¹æœåŠ¡æœªåˆå§‹åŒ–ï¼Œè¯·ç¨åé‡è¯•æˆ–æ£€æŸ¥æœåŠ¡é…ç½®"
        )
    
    if not api_key:
        api_key = os.getenv("LLM_API_KEY") or os.getenv("OPENROUTER_API_KEY")
    
    if not api_key:
        raise HTTPException(
            status_code=400,
            detail="æœªæä¾› API Keyï¼Œè¯·åœ¨è¯·æ±‚ä¸­æä¾›æˆ–é…ç½®ç¯å¢ƒå˜é‡ LLM_API_KEY/OPENROUTER_API_KEY"
        )


    
    # è§£æå­¦ç”Ÿè¾¹ç•Œ
    parsed_boundaries = []
    if student_boundaries:
        try:
            logger.info(f"æ¥æ”¶åˆ°åŸå§‹ student_boundaries: {student_boundaries} (type: {type(student_boundaries)})")
            import json
            parsed_boundaries = json.loads(student_boundaries)
            logger.info(f"è§£æåçš„ manual_boundaries: {parsed_boundaries}")
        except Exception as e:
            logger.warning(f"è§£ææ‰‹åŠ¨å­¦ç”Ÿè¾¹ç•Œå¤±è´¥: {e}")

    if not exam_id:
        exam_id = str(uuid.uuid4())

    batch_id = str(uuid.uuid4())
    
    logger.info(
        f"æ”¶åˆ°æ‰¹é‡æäº¤ï¼ˆLangGraphï¼‰: "
        f"batch_id={batch_id}, "
        f"exam_id={exam_id}, "
        f"auto_identify={auto_identify}"
    )
    
    temp_dir = None
    try:
        # åˆ›å»ºä¸´æ—¶ç›®å½•
        temp_dir = tempfile.mkdtemp()
        temp_path = Path(temp_dir)
        
        # === å¤„ç†ç­”é¢˜æ–‡ä»¶ï¼ˆæ”¯æŒå›¾ç‰‡åˆ—è¡¨æˆ–å•ä¸ª PDFï¼‰===
        answer_images = []
        
        for idx, file in enumerate(files):
            file_name = file.filename or f"file_{idx}"
            content = await file.read()
            
            # æ£€æŸ¥æ–‡ä»¶ç±»å‹
            if file_name.lower().endswith(('.png', '.jpg', '.jpeg', '.webp', '.gif')):
                # å›¾ç‰‡æ–‡ä»¶ï¼šç›´æ¥ä½¿ç”¨å†…å®¹
                answer_images.append(_safe_to_jpeg_bytes(content, file_name))
                logger.debug(f"è¯»å–å›¾ç‰‡æ–‡ä»¶: {file_name}, å¤§å°: {len(content)} bytes")
            elif file_name.lower().endswith('.pdf'):
                # PDF æ–‡ä»¶ï¼šè½¬æ¢ä¸ºå›¾åƒ
                pdf_path = temp_path / f"answer_{idx}.pdf"
                with open(pdf_path, "wb") as f:
                    f.write(content)
                loop = asyncio.get_event_loop()
                pdf_images = await loop.run_in_executor(None, _pdf_to_images, str(pdf_path), 150)
                answer_images.extend(pdf_images)
                logger.info(f"PDF æ–‡ä»¶ {file_name} è½¬æ¢ä¸º {len(pdf_images)} é¡µå›¾ç‰‡")
            elif file_name.lower().endswith('.txt'):
                # æ–‡æœ¬æ–‡ä»¶ï¼šç›´æ¥ä½¿ç”¨å†…å®¹
                answer_images.append(content)
                logger.info(f"æ–‡æœ¬æ–‡ä»¶å¤„ç†å®Œæˆ: {file_name}, å†…å®¹é•¿åº¦={len(content)}")
            else:
                # å°è¯•ä½œä¸ºå›¾ç‰‡å¤„ç†ï¼ˆå¯èƒ½æ²¡æœ‰æ‰©å±•åï¼‰
                answer_images.append(_safe_to_jpeg_bytes(content, file_name))
                logger.warning(f"æœªçŸ¥æ–‡ä»¶ç±»å‹ {file_name}ï¼Œå°è¯•ä½œä¸ºå›¾ç‰‡å¤„ç†")
        
        total_pages = len(answer_images)
        logger.info(f"ç­”é¢˜æ–‡ä»¶å¤„ç†å®Œæˆ: batch_id={batch_id}, æ€»é¡µæ•°={total_pages}")
        
        # === å¤„ç†è¯„åˆ†æ ‡å‡†ï¼ˆå¯é€‰ï¼‰===
        # Convert images to base64 and cache them immediately
        # (Fix: Rubric images not displaying on frontend)
        if answer_images:
            try:
                base64_images = [base64.b64encode(img).decode('utf-8') for img in answer_images]
                
                # Cache for direct WebSocket connection
                batch_image_cache.setdefault(batch_id, {})["images_ready"] = {
                    "type": "images_ready",
                    "images": base64_images
                }
                
                # Broadcast (though no clients connected yet usually)
                await broadcast_progress(batch_id, {
                    "type": "images_ready",
                    "images": base64_images
                })
                logger.info(f"å·²ç¼“å­˜ {len(base64_images)} å¼ å›¾ç‰‡ç”¨äºå‰ç«¯æ˜¾ç¤º")
            except Exception as e:
                logger.error(f"å›¾ç‰‡ Base64 è½¬æ¢å¤±è´¥: {e}")

        # === å¤„ç†è¯„åˆ†æ ‡å‡†ï¼ˆå¯é€‰ï¼‰===
        rubric_images = []
        if rubrics and len(rubrics) > 0:
            for idx, rubric_file in enumerate(rubrics):
                rubric_name = rubric_file.filename or f"rubric_{idx}"
                rubric_content = await rubric_file.read()
                
                if rubric_name.lower().endswith(('.png', '.jpg', '.jpeg', '.webp', '.gif')):
                    rubric_images.append(_safe_to_jpeg_bytes(rubric_content, rubric_name))
                elif rubric_name.lower().endswith('.pdf'):
                    rubric_path = temp_path / f"rubric_{idx}.pdf"
                    with open(rubric_path, "wb") as f:
                        f.write(rubric_content)
                    loop = asyncio.get_event_loop()
                    pdf_rubric_images = await loop.run_in_executor(None, _pdf_to_images, str(rubric_path), 150)
                    rubric_images.extend(pdf_rubric_images)
                else:
                    rubric_images.append(_safe_to_jpeg_bytes(rubric_content, rubric_name))
            
            logger.info(f"è¯„åˆ†æ ‡å‡†å¤„ç†å®Œæˆ: batch_id={batch_id}, æ€»é¡µæ•°={len(rubric_images)}")
            if rubric_images:
                try:
                    base64_rubric_images = [base64.b64encode(img).decode("utf-8") for img in rubric_images]
                    batch_image_cache.setdefault(batch_id, {})["rubric_images_ready"] = {
                        "type": "rubric_images_ready",
                        "images": base64_rubric_images
                    }
                    await broadcast_progress(batch_id, {
                        "type": "rubric_images_ready",
                        "images": base64_rubric_images
                    })
                    logger.info(f"å·²ç¼“å­˜ {len(base64_rubric_images)} å¼ è¯„åˆ†æ ‡å‡†å›¾ç‰‡ç”¨äºå‰ç«¯æ˜¾ç¤º")
                except Exception as e:
                    logger.error(f"è¯„åˆ†æ ‡å‡† Base64 è½¬æ¢å¤±è´¥: {e}")
        else:
            logger.info(f"æœªæä¾›è¯„åˆ†æ ‡å‡†ï¼Œå°†ä½¿ç”¨é»˜è®¤è¯„åˆ†: batch_id={batch_id}")
        
        logger.info(
            f"æ–‡ä»¶å¤„ç†å®Œæˆ: "
            f"batch_id={batch_id}, "
            f"rubric_pages={len(rubric_images)}, "
            f"answer_pages={total_pages}"
        )
        
        # ğŸš€ ä½¿ç”¨ LangGraph Orchestrator å¯åŠ¨æ‰¹æ”¹æµç¨‹
        
        # è§£æå­¦ç”Ÿæ˜ å°„ï¼ˆç­çº§æ‰¹æ”¹æ¨¡å¼ï¼‰
        student_mapping = []
        if student_mapping_json:
            try:
                import json
                student_mapping = json.loads(student_mapping_json)
                logger.info(f"ç­çº§æ‰¹æ”¹æ¨¡å¼: class_id={class_id}, homework_id={homework_id}, å­¦ç”Ÿæ•°={len(student_mapping)}")
            except Exception as e:
                logger.warning(f"è§£æå­¦ç”Ÿæ˜ å°„å¤±è´¥: {e}")
        
        payload = {
            "batch_id": batch_id,
            "exam_id": exam_id,
            "temp_dir": str(temp_path),  # ä¸´æ—¶ç›®å½•ï¼ˆç”¨äºæ¸…ç†ï¼‰
            "rubric_images": rubric_images,
            "answer_images": answer_images,
            "api_key": api_key,
            # ç­çº§æ‰¹æ”¹ä¸Šä¸‹æ–‡ï¼ˆå¯é€‰ï¼‰
            "class_id": class_id,
            "homework_id": homework_id,
            "student_mapping": student_mapping,
            "inputs": {
                "rubric": "rubric_content",  # TODO: è§£æ rubric
                "auto_identify": auto_identify,
                "manual_boundaries": parsed_boundaries,  # ä¼ é€’äººå·¥è¾¹ç•Œ
                "expected_students": expected_students if expected_students else 2,  # ğŸ”¥ é»˜è®¤ 2 åå­¦ç”Ÿ
                "enable_review": enable_review,
                "grading_mode": grading_mode or "auto",
            }
        }
        
        # å¯åŠ¨ LangGraph batch_grading Graph
        teacher_key = _normalize_teacher_key(teacher_id)
        teacher_semaphore = await _get_teacher_semaphore(teacher_key)
        if teacher_semaphore.locked():
            await broadcast_progress(batch_id, {
                "type": "workflow_update",
                "nodeId": "rubric_parse",
                "status": "pending",
                "message": "Queued: waiting for teacher slot"
            })
        asyncio.create_task(
            _start_run_with_teacher_limit(
                teacher_key=teacher_key,
                batch_id=batch_id,
                payload=payload,
                orchestrator=orchestrator,
                class_id=class_id,
                homework_id=homework_id,
                student_mapping=student_mapping,
            )
        )

        return BatchSubmissionResponse(
            batch_id=batch_id,
            status=SubmissionStatus.UPLOADED,
            total_pages=total_pages,
            estimated_completion_time=total_pages * 3  # Estimated: 3s per page
        )
        
    except Exception as e:
        logger.error(f"æ‰¹é‡æäº¤å¤±è´¥: {str(e)}", exc_info=True)
        raise HTTPException(status_code=500, detail=f"æ‰¹é‡æäº¤å¤±è´¥: {str(e)}")


async def stream_langgraph_progress(
    batch_id: str,
    run_id: str,
    orchestrator: Orchestrator,
    class_id: Optional[str] = None,
    homework_id: Optional[str] = None,
    student_mapping: Optional[List[dict]] = None,
    teacher_key: Optional[str] = None,
    teacher_semaphore: Optional[asyncio.Semaphore] = None
):
    """
    æµå¼ç›‘å¬ LangGraph æ‰§è¡Œè¿›åº¦å¹¶æ¨é€åˆ° WebSocket
    
    è¿™æ˜¯å®ç°å®æ—¶è¿›åº¦æ¨é€çš„å…³é”®å‡½æ•°ï¼
    
    Args:
        batch_id: æ‰¹æ¬¡ ID
        run_id: LangGraph è¿è¡Œ ID
        orchestrator: LangGraph Orchestrator
    """
    # #region agent log - å‡è®¾G: stream_langgraph_progress å…¥å£
    _write_debug_log({
        "hypothesisId": "G",
        "location": "batch_langgraph.py:stream_langgraph_progress:entry",
        "message": "stream_langgraph_progresså‡½æ•°è¢«è°ƒç”¨",
        "data": {"batch_id": batch_id, "run_id": run_id},
        "timestamp": int(datetime.now().timestamp() * 1000),
        "sessionId": "debug-session",
    })
    # #endregion
    logger.info(f"å¼€å§‹æµå¼ç›‘å¬ LangGraph è¿›åº¦: batch_id={batch_id}, run_id={run_id}")
    
    try:
        # ğŸ”¥ ä½¿ç”¨ LangGraph çš„æµå¼ API
        async for event in orchestrator.stream_run(run_id):
            event_type = event.get("type")
            node_name = event.get("node")
            data = event.get("data", {})
            
            logger.debug(
                f"LangGraph äº‹ä»¶: "
                f"batch_id={batch_id}, "
                f"type={event_type}, "
                f"node={node_name}"
            )
            
            # å°† LangGraph äº‹ä»¶è½¬æ¢ä¸ºå‰ç«¯ WebSocket æ¶ˆæ¯
            if event_type == "node_start":
                await broadcast_progress(batch_id, {
                    "type": "workflow_update",
                    "nodeId": _map_node_to_frontend(node_name),
                    "status": "running",
                    "message": f"Running {_get_node_display_name(node_name)}..."
                })
            
            elif event_type == "node_end":
                await broadcast_progress(batch_id, {
                    "type": "workflow_update",
                    "nodeId": _map_node_to_frontend(node_name),
                    "status": "completed",
                    "message": f"{_get_node_display_name(node_name)} completed"
                })
                
                # å¤„ç†èŠ‚ç‚¹è¾“å‡º
                output = data.get("output", {})
                if isinstance(output, dict):
                    interrupt_payload = output.get("__interrupt__")
                    if interrupt_payload:
                        review_type = interrupt_payload.get("type") if isinstance(interrupt_payload, dict) else "review_required"
                        await broadcast_progress(batch_id, {
                            "type": "review_required",
                            "reviewType": review_type,
                            "payload": interrupt_payload,
                            "nodeId": _map_node_to_frontend("rubric_review") if "rubric" in review_type else _map_node_to_frontend("review"),
                        })
                    # è¯„åˆ†æ ‡å‡†è§£æå®Œæˆ
                    if node_name == "rubric_parse" and output.get("parsed_rubric"):
                        parsed = output["parsed_rubric"]
                        await broadcast_progress(batch_id, {
                            "type": "rubric_parsed",
                            "totalQuestions": parsed.get("total_questions", 0),
                            "totalScore": parsed.get("total_score", 0),
                            "generalNotes": parsed.get("general_notes", ""),
                            "rubricFormat": parsed.get("rubric_format", ""),
                            "questions": [
                                {
                                    "questionId": q.get("question_id", ""),
                                    "maxScore": q.get("max_score", 0),
                                    "questionText": q.get("question_text", ""),
                                    "standardAnswer": q.get("standard_answer", ""),
                                    "gradingNotes": q.get("grading_notes", ""),
                                    "sourcePages": q.get("source_pages") or q.get("sourcePages") or [],
                                    "scoringPoints": [
                                        {
                                            "pointId": sp.get("point_id") or sp.get("pointId") or f"{q.get('question_id')}.{idx + 1}",
                                            "description": sp.get("description", ""),
                                            "expectedValue": sp.get("expected_value") or sp.get("expectedValue", ""),
                                            "keywords": sp.get("keywords") or [],
                                            "score": sp.get("score", 0),
                                            "isRequired": sp.get("is_required", True),
                                        }
                                        for idx, sp in enumerate(q.get("scoring_points", []))
                                    ],
                                    "deductionRules": [
                                        {
                                            "ruleId": dr.get("rule_id") or dr.get("ruleId") or f"{q.get('question_id')}.d{idx + 1}",
                                            "description": dr.get("description", ""),
                                            "deduction": dr.get("deduction", dr.get("score", 0)),
                                            "conditions": dr.get("conditions") or dr.get("when") or "",
                                        }
                                        for idx, dr in enumerate(q.get("deduction_rules") or q.get("deductionRules") or [])
                                    ],
                                    "alternativeSolutions": [
                                        {
                                            "description": alt.get("description", ""),
                                            "scoringCriteria": alt.get("scoring_criteria", ""),
                                            "note": alt.get("note", ""),
                                        }
                                        for alt in q.get("alternative_solutions", [])
                                    ]
                                }
                                for q in parsed.get("questions", [])
                            ]
                        })
                    
                    # æ‰¹æ”¹æ‰¹æ¬¡å®Œæˆ
                    if node_name == "grade_batch" and output.get("grading_results"):
                        results = output["grading_results"]
                        completed = sum(1 for r in results if r.get("status") == "completed")
                        
                        await broadcast_progress(batch_id, {
                            "type": "batch_complete",
                            "batchSize": len(results),
                            "successCount": completed,
                            "totalScore": sum(r.get("score", 0) for r in results if r.get("status") == "completed"),
                            "pages": [r.get("page_index") for r in results]
                        })
                    
                    # ç´¢å¼•å®Œæˆï¼ˆå­¦ç”Ÿè¯†åˆ«ï¼‰
                    if node_name == "index" and output.get("student_boundaries"):
                        boundaries = output["student_boundaries"]
                        await broadcast_progress(batch_id, {
                            "type": "students_identified",
                            "studentCount": len(boundaries),
                            "students": [
                                {
                                    "studentKey": b.get("student_key", ""),
                                    "startPage": b.get("start_page", 0),
                                    "endPage": b.get("end_page", 0),
                                    "confidence": b.get("confidence", 0),
                                    "needsConfirmation": b.get("needs_confirmation", False)
                                }
                                for b in boundaries
                            ]
                        })
                    
                    # å®¡æ ¸å®Œæˆ
                    if node_name == "review" and output.get("review_summary"):
                        await broadcast_progress(batch_id, {
                            "type": "review_completed",
                            "summary": output["review_summary"]
                        })
                    
                    # è·¨é¡µé¢˜ç›®åˆå¹¶å®Œæˆ
                    if node_name == "cross_page_merge":
                        cross_page_questions = output.get("cross_page_questions", [])
                        merged_questions = output.get("merged_questions", [])
                        if cross_page_questions:
                            await broadcast_progress(batch_id, {
                                "type": "cross_page_detected",
                                "questions": cross_page_questions,
                                "mergedCount": len(merged_questions),
                                "crossPageCount": len(cross_page_questions)
                            })
            
            elif event_type == "paused":
                # å¤„ç† Graph ä¸­æ–­/æš‚åœï¼ˆé€šå¸¸æ˜¯éœ€è¦äººå·¥å®¡æ ¸ï¼‰
                data = event.get("data", {})
                interrupt_value = data.get("interrupt_value")
                
                logger.info(f"LangGraph æš‚åœ: batch_id={batch_id}, interrupt_value={interrupt_value}")
                
                if interrupt_value:
                    # å¦‚æœæœ‰ä¸­æ–­ payloadï¼Œå¹¿æ’­ review_required
                    review_type = interrupt_value.get("type") if isinstance(interrupt_value, dict) else "review_required"
                    await broadcast_progress(batch_id, {
                        "type": "review_required",
                        "reviewType": review_type,
                        "payload": interrupt_value,
                        "nodeId": _map_node_to_frontend("rubric_review") if "rubric" in review_type else _map_node_to_frontend("review"),
                    })
                else:
                    # å¦‚æœæ²¡æœ‰ payloadï¼Œè‡³å°‘é€šçŸ¥çŠ¶æ€å˜æ›´
                    await broadcast_progress(batch_id, {
                        "type": "workflow_update",
                        "status": "paused",
                        "message": "Workflow paused (awaiting input)"
                    })

            elif event_type == "state_update":
                # æ¨é€çŠ¶æ€æ›´æ–°
                state = data.get("state", {})
                
                # æ‰¹æ¬¡è¿›åº¦æ›´æ–°
                if state.get("progress"):
                    progress = state["progress"]
                    await broadcast_progress(batch_id, {
                        "type": "batch_progress",
                        "batchIndex": progress.get("current_batch", 0),
                        "totalBatches": progress.get("total_batches", 1),
                        "successCount": progress.get("success_count", 0),
                        "failureCount": progress.get("failure_count", 0)
                    })
                
                # ç™¾åˆ†æ¯”è¿›åº¦
                if state.get("percentage"):
                    await broadcast_progress(batch_id, {
                        "type": "grading_progress",
                        "percentage": state["percentage"],
                        "currentStage": state.get("current_stage", "")
                    })
            
            elif event_type == "llm_stream":
                # Real-time LLM token streaming
                node_name = event.get("node") or data.get("node", "")
                chunk = data.get("chunk") or data.get("content") or ""
                await broadcast_progress(batch_id, {
                    "type": "llm_stream_chunk",
                    "nodeId": _map_node_to_frontend(node_name) if node_name else None,
                    "nodeName": _get_node_display_name(node_name) if node_name else None,
                    "chunk": chunk,
                })

            elif event_type == "error":
                await broadcast_progress(batch_id, {
                    "type": "workflow_error",
                    "message": data.get("error", "Unknown error")
                })
            
            elif event_type == "completed":
                # #region agent log - å‡è®¾H: completed äº‹ä»¶
                _write_debug_log({
                    "hypothesisId": "H",
                    "location": "batch_langgraph.py:event_completed",
                    "message": "æ”¶åˆ°completedäº‹ä»¶",
                    "data": {
                        "event_type": event_type,
                        "data_keys": list(data.keys()) if isinstance(data, dict) else str(type(data)),
                    },
                    "timestamp": int(datetime.now().timestamp() * 1000),
                    "sessionId": "debug-session",
                })
                # #endregion
                # å·¥ä½œæµå®Œæˆ - è·å–å®Œæ•´çš„æœ€ç»ˆçŠ¶æ€
                final_state = data.get("state", {})
                
                # ä» student_results è·å–ç»“æœ
                student_results = final_state.get("student_results", [])
                
                # #region agent log - å‡è®¾I: student_results åŸå§‹æ•°æ®
                _write_debug_log({
                    "hypothesisId": "I",
                    "location": "batch_langgraph.py:student_results_raw",
                    "message": "student_resultsåŸå§‹æ•°æ®",
                    "data": {
                        "count": len(student_results),
                        "students": [{"key": r.get("student_key"), "score": r.get("total_score")} for r in student_results],
                    },
                    "timestamp": int(datetime.now().timestamp() * 1000),
                    "sessionId": "debug-session",
                })
                # #endregion
                
                # å¦‚æœæ²¡æœ‰ student_resultsï¼Œå°è¯•ä» orchestrator è·å–æœ€ç»ˆè¾“å‡º
                if not student_results:
                    try:
                        final_output = await orchestrator.get_final_output(run_id)
                        if final_output:
                            student_results = final_output.get("student_results", [])
                            logger.info(f"ä» orchestrator è·å–åˆ° {len(student_results)} ä¸ªå­¦ç”Ÿç»“æœ")
                    except Exception as e:
                        logger.warning(f"è·å–æœ€ç»ˆè¾“å‡ºå¤±è´¥: {e}")
                
                formatted_results = _format_results_for_frontend(student_results)
                class_report = final_state.get("class_report")
                if not class_report and final_state.get("export_data"):
                    class_report = final_state.get("export_data", {}).get("class_report")
                
                # ä¿å­˜æ‰¹æ”¹å†å²ä¸å­¦ç”Ÿç»“æœï¼ˆæ”¯æŒç­çº§/éç­çº§æ¨¡å¼ï¼‰
                try:
                    logger.info(
                        f"ä¿å­˜æ‰¹æ”¹ç»“æœ: batch_id={batch_id}, class_id={class_id}, homework_id={homework_id}"
                    )

                    now = datetime.now().isoformat()
                    scores = [
                        s.get("score") for s in formatted_results
                        if isinstance(s.get("score"), (int, float))
                    ]
                    average_score = None
                    if isinstance(class_report, dict):
                        average_score = class_report.get("average_score")
                    if average_score is None and scores:
                        average_score = round(sum(scores) / len(scores), 2)

                    history_id = str(uuid.uuid4())
                    history = GradingHistory(
                        id=history_id,
                        batch_id=batch_id,
                        status="completed",
                        class_ids=[class_id] if class_id else None,
                        created_at=now,
                        completed_at=now,
                        total_students=len(formatted_results),
                        average_score=average_score,
                        result_data={
                            "summary": class_report,
                            "class_id": class_id,
                            "homework_id": homework_id,
                        } if class_report or class_id or homework_id else None,
                    )
                    save_grading_history(history)

                    student_map_by_index = {}
                    student_map_by_name = {}
                    if student_mapping:
                        for idx, mapping in enumerate(student_mapping):
                            student_map_by_index[idx] = mapping
                            name_key = (mapping.get("studentName") or "").strip().lower()
                            if name_key:
                                student_map_by_name[name_key] = mapping

                    roster = list_class_students(class_id) if class_id else []
                    roster_by_name = {
                        (student.name or student.username or "").strip().lower(): student
                        for student in roster
                        if (student.name or student.username)
                    }

                    for idx, result in enumerate(formatted_results):
                        student_name = (
                            result.get("studentName")
                            or result.get("student_name")
                            or f"Student {idx + 1}"
                        )
                        student_id = result.get("studentId") or result.get("student_id")

                        if not student_id and student_map_by_index.get(idx):
                            mapping = student_map_by_index[idx]
                            student_id = mapping.get("studentId")
                            student_name = mapping.get("studentName") or student_name
                        if not student_id and student_name:
                            mapping = student_map_by_name.get(student_name.strip().lower())
                            if mapping:
                                student_id = mapping.get("studentId")
                                student_name = mapping.get("studentName") or student_name
                        if not student_id and student_name:
                            roster_hit = roster_by_name.get(student_name.strip().lower())
                            if roster_hit:
                                student_id = roster_hit.id
                                student_name = roster_hit.name or roster_hit.username or student_name
                        if not student_id and class_id and idx < len(roster):
                            roster_hit = roster[idx]
                            student_id = roster_hit.id
                            student_name = roster_hit.name or roster_hit.username or student_name
                        if not student_id and class_id:
                            student_id = f"auto-{idx + 1}"

                        student_summary = result.get("studentSummary") or result.get("student_summary") or {}
                        self_audit = result.get("selfAudit") or result.get("self_audit") or {}
                        student_result = StudentGradingResult(
                            id=str(uuid.uuid4()),
                            grading_history_id=history_id,
                            student_key=student_name,
                            score=result.get("score"),
                            max_score=result.get("maxScore") or result.get("max_score"),
                            class_id=class_id,
                            student_id=student_id if class_id else None,
                            summary=student_summary.get("overall") if isinstance(student_summary, dict) else None,
                            self_report=self_audit.get("summary") if isinstance(self_audit, dict) else None,
                            result_data=result,
                        )
                        save_student_result(student_result)

                        if class_id and homework_id and student_id:
                            feedback = None
                            if isinstance(student_summary, dict):
                                feedback = student_summary.get("overall")
                            upsert_homework_submission_grade(
                                class_id=class_id,
                                homework_id=homework_id,
                                student_id=student_id,
                                student_name=student_name,
                                score=result.get("score"),
                                feedback=feedback,
                                grading_batch_id=batch_id,
                            )

                    logger.info(f"æ‰¹æ”¹ç»“æœå·²ä¿å­˜: history_id={history_id}")
                except Exception as e:
                    logger.error(f"ä¿å­˜æ‰¹æ”¹ç»“æœå¤±è´¥: {e}", exc_info=True)

                # #region agent log - å‡è®¾E: WebSocket æ¶ˆæ¯å‘é€
                _write_debug_log({
                    "hypothesisId": "E",
                    "location": "batch_langgraph.py:workflow_completed",
                    "message": "å‘é€workflow_completed",
                    "data": {
                        "student_count": len(formatted_results),
                        "students": [{"name": f.get("studentName"), "score": f.get("score")} for f in formatted_results],
                    },
                    "timestamp": int(datetime.now().timestamp() * 1000),
                    "sessionId": "debug-session",
                })
                # #endregion
                
                await broadcast_progress(batch_id, {
                    "type": "workflow_completed",
                    "message": f"Grading completed, processed {len(formatted_results)} students",
                    "results": formatted_results,
                    "classReport": class_report
                })
        
        logger.info(f"LangGraph è¿›åº¦æµå¼ä¼ è¾“å®Œæˆ: batch_id={batch_id}")
        
    except Exception as e:
        logger.error(
            f"æµå¼ä¼ è¾“å¤±è´¥: batch_id={batch_id}, error={str(e)}",
            exc_info=True
        )
        await broadcast_progress(batch_id, {
            "type": "workflow_error",
            "message": f"æµå¼ä¼ è¾“å¤±è´¥: {str(e)}"
        })

    finally:
        if teacher_semaphore:
            try:
                run_info = await orchestrator.get_status(run_id)
                status_value = run_info.status.value if hasattr(run_info.status, "value") else str(run_info.status)
                if status_value in ("completed", "failed", "cancelled"):
                    teacher_semaphore.release()
            except Exception:
                teacher_semaphore.release()


def _map_node_to_frontend(node_name: str) -> str:
    """å°† LangGraph èŠ‚ç‚¹åç§°æ˜ å°„åˆ°å‰ç«¯èŠ‚ç‚¹ ID
    
    å‰ç«¯å·¥ä½œæµèŠ‚ç‚¹ï¼ˆconsoleStore.ts initialNodesï¼‰ï¼š
    - intake: æ¥æ”¶æ–‡ä»¶
    - rubric_parse: è§£æè¯„åˆ†æ ‡å‡†
    - grade_batch: åˆ†æ‰¹å¹¶è¡Œæ‰¹æ”¹ï¼ˆisParallelContainerï¼‰
    - cross_page_merge: è·¨é¡µé¢˜ç›®åˆå¹¶
    - index: æ‰¹æ”¹å‰ç´¢å¼•
    - index_merge: ç´¢å¼•èšåˆ
    - export: å¯¼å‡ºç»“æœ
    """
    mapping = {
        # ä¸»è¦èŠ‚ç‚¹ï¼ˆä¸åç«¯ batch_grading.py å®Œå…¨å¯¹åº”ï¼‰
        "intake": "intake",
        "rubric_parse": "rubric_parse",
        "rubric_review": "rubric_review",
        "grade_batch": "grade_batch",
        "cross_page_merge": "cross_page_merge",
        "index": "index",
        "index_merge": "index_merge",
        "segment": "index_merge",
        "review": "review",
        "logic_review": "logic_review",
        "export": "export",
        # å…¼å®¹æ—§åç§°
        "detect_boundaries": "index",
        "grade_student": "grade_batch",
        "grading": "grade_batch",
        "aggregate": "review",
        "batch_persist": "export",
        "batch_notify": "export"
    }
    return mapping.get(node_name, node_name)


def _get_node_display_name(node_name: str) -> str:
    """è·å–èŠ‚ç‚¹çš„æ˜¾ç¤ºåç§°ï¼ˆä¸­æ–‡ï¼‰"""
    display_names = {
        "intake": "Ingest",
        "preprocess": "Preprocess",
        "index": "Index",
        "rubric_parse": "Rubric Parse",
        "rubric_review": "Rubric Review",
        "grading_fanout": "Batch Fanout",
        "grade_batch": "Batch Grading",
        "cross_page_merge": "Cross-Page Merge",
        "logic_review": "Logic Review",
        "index_merge": "Index Merge",
        "segment": "Index Merge",
        "review": "Final Review",
        "export": "Export"
    }
    return display_names.get(node_name, node_name)


def _safe_float(value: Any, default: float = 0.0) -> float:
    try:
        if value is None:
            return default
        return float(value)
    except (TypeError, ValueError):
        return default


def _format_results_for_frontend(results: List[Dict]) -> List[Dict]:
    """æ ¼å¼åŒ–æ‰¹æ”¹ç»“æœä¸ºå‰ç«¯æ ¼å¼"""
    # #region agent log - å‡è®¾D: _format_results_for_frontend è¾“å…¥
    _write_debug_log({
        "hypothesisId": "D",
        "location": "batch_langgraph.py:_format_results_for_frontend:input",
        "message": "è¾“å…¥çš„results",
        "data": {
            "count": len(results),
            "students": [{"key": r.get("student_key"), "score": r.get("total_score")} for r in results],
        },
        "timestamp": int(datetime.now().timestamp() * 1000),
        "sessionId": "debug-session",
    })
    # #endregion
    formatted = []
    for r in results:
        # å¤„ç† question_details æ ¼å¼
        question_results = []
        
        # ä¼˜å…ˆä½¿ç”¨ question_details
        if r.get("question_details"):
            for q in r.get("question_details", []):
                scoring_results = q.get("scoring_point_results") or q.get("scoring_results") or []
                question_results.append({
                    "questionId": str(q.get("question_id", "")),
                    "score": q.get("score", 0),
                    "maxScore": q.get("max_score", 0),
                    "feedback": q.get("feedback", ""),
                    "confidence": q.get("confidence", 0),
                    "confidence_reason": q.get("confidence_reason") or q.get("confidenceReason"),
                    "self_critique": q.get("self_critique") or q.get("selfCritique"),
                    "self_critique_confidence": q.get("self_critique_confidence") or q.get("selfCritiqueConfidence"),
                    "rubric_refs": q.get("rubric_refs") or q.get("rubricRefs"),
                    "review_summary": q.get("review_summary") or q.get("reviewSummary"),
                    "review_corrections": q.get("review_corrections") or q.get("reviewCorrections"),
                    "needsReview": (
                        q.get("needs_review")
                        if q.get("needs_review") is not None
                        else q.get("needsReview", False)
                    ),
                    "reviewReasons": (
                        q.get("review_reasons")
                        if q.get("review_reasons") is not None
                        else q.get("reviewReasons") or []
                    ),
                    "auditFlags": (
                        q.get("audit_flags")
                        if q.get("audit_flags") is not None
                        else q.get("auditFlags") or []
                    ),
                    "typo_notes": q.get("typo_notes") or q.get("typoNotes"),
                    "studentAnswer": q.get("student_answer", ""),
                    "question_type": q.get("question_type") or q.get("questionType"),
                    "isCorrect": q.get("is_correct", False),
                    "scoring_point_results": scoring_results,
                    "page_indices": q.get("page_indices", []),
                    "is_cross_page": q.get("is_cross_page", False),
                    "merge_source": q.get("merge_source")
                })
        # å…¼å®¹æ—§æ ¼å¼ grading_results
        elif r.get("grading_results"):
            for q in r.get("grading_results", []):
                scoring_results = q.get("scoring_point_results") or q.get("scoring_results") or []
                question_results.append({
                    "questionId": str(q.get("question_id", "")),
                    "score": q.get("score", 0),
                    "maxScore": q.get("max_score", 0),
                    "feedback": q.get("feedback", ""),
                    "confidence": q.get("confidence", 0),
                    "confidence_reason": q.get("confidence_reason") or q.get("confidenceReason"),
                    "self_critique": q.get("self_critique") or q.get("selfCritique"),
                    "self_critique_confidence": q.get("self_critique_confidence") or q.get("selfCritiqueConfidence"),
                    "rubric_refs": q.get("rubric_refs") or q.get("rubricRefs"),
                    "review_summary": q.get("review_summary") or q.get("reviewSummary"),
                    "review_corrections": q.get("review_corrections") or q.get("reviewCorrections"),
                    "needsReview": (
                        q.get("needs_review")
                        if q.get("needs_review") is not None
                        else q.get("needsReview", False)
                    ),
                    "reviewReasons": (
                        q.get("review_reasons")
                        if q.get("review_reasons") is not None
                        else q.get("reviewReasons") or []
                    ),
                    "auditFlags": (
                        q.get("audit_flags")
                        if q.get("audit_flags") is not None
                        else q.get("auditFlags") or []
                    ),
                    "typo_notes": q.get("typo_notes") or q.get("typoNotes"),
                    "studentAnswer": q.get("student_answer", ""),
                    "question_type": q.get("question_type") or q.get("questionType"),
                    "scoring_point_results": scoring_results,
                    "page_indices": q.get("page_indices", []),
                    "is_cross_page": q.get("is_cross_page", False),
                    "merge_source": q.get("merge_source")
                })
        # Ã¥â€¦Â¼Ã¥Â®Â¹ export_data Ã§Å¡â€ question_results
        elif r.get("question_results"):
            for q in r.get("question_results", []):
                scoring_results = q.get("scoring_point_results") or q.get("scoring_results") or []
                question_results.append({
                    "questionId": str(q.get("question_id", "")),
                    "score": q.get("score", 0),
                    "maxScore": q.get("max_score", 0),
                    "feedback": q.get("feedback", ""),
                    "confidence": q.get("confidence", 0),
                    "confidence_reason": q.get("confidence_reason") or q.get("confidenceReason"),
                    "self_critique": q.get("self_critique") or q.get("selfCritique"),
                    "self_critique_confidence": q.get("self_critique_confidence") or q.get("selfCritiqueConfidence"),
                    "rubric_refs": q.get("rubric_refs") or q.get("rubricRefs"),
                    "review_summary": q.get("review_summary") or q.get("reviewSummary"),
                    "review_corrections": q.get("review_corrections") or q.get("reviewCorrections"),
                    "needsReview": (
                        q.get("needs_review")
                        if q.get("needs_review") is not None
                        else q.get("needsReview", False)
                    ),
                    "reviewReasons": (
                        q.get("review_reasons")
                        if q.get("review_reasons") is not None
                        else q.get("reviewReasons") or []
                    ),
                    "auditFlags": (
                        q.get("audit_flags")
                        if q.get("audit_flags") is not None
                        else q.get("auditFlags") or []
                    ),
                    "typo_notes": q.get("typo_notes") or q.get("typoNotes"),
                    "studentAnswer": q.get("student_answer", ""),
                    "question_type": q.get("question_type") or q.get("questionType"),
                    "isCorrect": q.get("is_correct", False),
                    "scoring_point_results": scoring_results,
                    "page_indices": q.get("page_indices", []),
                    "is_cross_page": q.get("is_cross_page", False),
                    "merge_source": q.get("merge_source"),
                })
        # ä» page_results æå–
        elif r.get("page_results"):
            for page in r.get("page_results", []):
                if page.get("status") == "completed":
                    # ä»é¡µé¢ç»“æœä¸­æå–é¢˜ç›®è¯¦æƒ…
                    for q in page.get("question_details", []):
                        scoring_results = q.get("scoring_point_results") or q.get("scoring_results") or []
                        page_indices = q.get("page_indices")
                        if not page_indices and page.get("page_index") is not None:
                            page_indices = [page.get("page_index")]
                        question_results.append({
                            "questionId": str(q.get("question_id", "")),
                            "score": q.get("score", 0),
                            "maxScore": q.get("max_score", 0),
                            "feedback": q.get("feedback", ""),
                            "confidence": q.get("confidence", 0),
                            "confidence_reason": q.get("confidence_reason") or q.get("confidenceReason"),
                            "self_critique": q.get("self_critique") or q.get("selfCritique"),
                            "self_critique_confidence": q.get("self_critique_confidence") or q.get("selfCritiqueConfidence"),
                            "rubric_refs": q.get("rubric_refs") or q.get("rubricRefs"),
                            "review_summary": q.get("review_summary") or q.get("reviewSummary"),
                            "review_corrections": q.get("review_corrections") or q.get("reviewCorrections"),
                            "needsReview": (
                                q.get("needs_review")
                                if q.get("needs_review") is not None
                                else q.get("needsReview", False)
                            ),
                            "reviewReasons": (
                                q.get("review_reasons")
                                if q.get("review_reasons") is not None
                                else q.get("reviewReasons") or []
                            ),
                            "auditFlags": (
                                q.get("audit_flags")
                                if q.get("audit_flags") is not None
                                else q.get("auditFlags") or []
                            ),
                            "typo_notes": q.get("typo_notes") or q.get("typoNotes"),
                            "studentAnswer": q.get("student_answer", ""),
                            "question_type": q.get("question_type") or q.get("questionType"),
                            "isCorrect": q.get("is_correct", False),
                            "scoring_point_results": scoring_results,
                            "page_indices": page_indices or [],
                            "is_cross_page": q.get("is_cross_page", False),
                            "merge_source": q.get("merge_source")
                        })
        
        computed_score = sum(_safe_float(q.get("score", 0)) for q in question_results)
        computed_max = sum(_safe_float(q.get("maxScore", 0)) for q in question_results)
        raw_score = _safe_float(r.get("total_score", r.get("score", 0)))
        raw_max = _safe_float(r.get("max_total_score", r.get("max_score", 0)))
        final_score = raw_score if raw_score > 0 or computed_score <= 0 else computed_score
        final_max = raw_max if raw_max > 0 or computed_max <= 0 else computed_max

        student_summary = r.get("student_summary") or r.get("studentSummary")
        self_audit = r.get("self_audit") or r.get("selfAudit")
        formatted.append({
            "studentName": r.get("student_key") or r.get("student_name") or r.get("student_id", "Unknown"),
            "score": final_score,
            "maxScore": final_max if final_max > 0 else 0,
            "startPage": r.get("start_page"),   # ğŸ”¥ æ–°å¢ï¼šå­¦ç”Ÿé¡µé¢èŒƒå›´
            "endPage": r.get("end_page"),       # ğŸ”¥ æ–°å¢ï¼šå­¦ç”Ÿé¡µé¢èŒƒå›´
            "questionResults": question_results,
            "confidence": r.get("confidence", 0),
            "needsConfirmation": r.get("needs_confirmation", False),
            "gradingMode": r.get("grading_mode") or r.get("gradingMode"),
            "studentSummary": student_summary,
            "selfAudit": self_audit
        })
    # #region agent log - å‡è®¾D: _format_results_for_frontend è¾“å‡º
    _write_debug_log({
        "hypothesisId": "D",
        "location": "batch_langgraph.py:_format_results_for_frontend:output",
        "message": "è¾“å‡ºçš„formatted",
        "data": {
            "count": len(formatted),
            "students": [{"name": f.get("studentName"), "score": f.get("score")} for f in formatted],
        },
        "timestamp": int(datetime.now().timestamp() * 1000),
        "sessionId": "debug-session",
    })
    # #endregion
    return formatted


@router.websocket("/ws/{batch_id}")
async def websocket_endpoint(websocket: WebSocket, batch_id: str):
    """
    WebSocket ç«¯ç‚¹ï¼Œç”¨äºå®æ—¶æ¨é€æ‰¹æ”¹è¿›åº¦
    
    å‰ç«¯é€šè¿‡æ­¤ç«¯ç‚¹æ¥æ”¶ LangGraph çš„å®æ—¶æ‰§è¡Œè¿›åº¦
    """
    await websocket.accept()

    cached_images = batch_image_cache.get(batch_id, {})
    if cached_images:
        try:
            for key, message in cached_images.items():
                if key == "llm_stream_cache":
                    continue
                await websocket.send_json(message)
            stream_cache = cached_images.get("llm_stream_cache")
            if isinstance(stream_cache, dict):
                for stream_message in stream_cache.values():
                    await websocket.send_json({
                        "type": "llm_stream_chunk",
                        **stream_message,
                    })
        except Exception as e:
            logger.warning(f"å‘é€ç¼“å­˜å›¾ç‰‡å¤±è´¥: {e}")
    
    # æ³¨å†Œè¿æ¥
    if batch_id not in active_connections:
        active_connections[batch_id] = []
    active_connections[batch_id].append(websocket)
    
    logger.info(f"WebSocket è¿æ¥å»ºç«‹: batch_id={batch_id}")

    # è¿æ¥å»ºç«‹åå°è¯•å‘é€å½“å‰çŠ¶æ€å¿«ç…§ï¼Œé¿å…å‰ç«¯é”™è¿‡æ—©æœŸäº‹ä»¶å¯¼è‡´å¡ä½
    try:
        orchestrator = await get_orchestrator()
        if orchestrator:
            run_id = f"batch_grading_{batch_id}"
            run_info = await orchestrator.get_run_info(run_id)
            if run_info and run_info.state:
                state = run_info.state or {}
                current_stage = state.get("current_stage", "")
                percentage = state.get("percentage", 0)
                if current_stage or percentage:
                    await websocket.send_json({
                        "type": "grading_progress",
                        "percentage": percentage or 0,
                        "currentStage": current_stage
                    })
                if state.get("student_boundaries"):
                    boundaries = state.get("student_boundaries", [])
                    await websocket.send_json({
                        "type": "students_identified",
                        "studentCount": len(boundaries),
                        "students": [
                            {
                                "studentKey": b.get("student_key", ""),
                                "startPage": b.get("start_page", 0),
                                "endPage": b.get("end_page", 0),
                                "confidence": b.get("confidence", 0),
                                "needsConfirmation": b.get("needs_confirmation", False)
                            }
                            for b in boundaries
                        ]
                    })
                if state.get("parsed_rubric"):
                    parsed = state.get("parsed_rubric", {})
                    await websocket.send_json({
                        "type": "rubric_parsed",
                        "totalQuestions": parsed.get("total_questions", 0),
                        "totalScore": parsed.get("total_score", 0),
                        "generalNotes": parsed.get("general_notes", ""),
                        "rubricFormat": parsed.get("rubric_format", ""),
                        "questions": [
                            {
                                "questionId": q.get("question_id", ""),
                                "maxScore": q.get("max_score", 0),
                                "questionText": q.get("question_text", ""),
                                "standardAnswer": q.get("standard_answer", ""),
                                "gradingNotes": q.get("grading_notes", ""),
                          "scoringPoints": [
                              {
                                  "pointId": sp.get("point_id") or sp.get("pointId") or f"{q.get('question_id')}.{idx + 1}",
                                  "description": sp.get("description", ""),
                                  "expectedValue": sp.get("expected_value") or sp.get("expectedValue", ""),
                                  "keywords": sp.get("keywords") or [],
                                  "score": sp.get("score", 0),
                                  "isRequired": sp.get("is_required", True),
                              }
                              for idx, sp in enumerate(q.get("scoring_points", []))
                          ],
                          "deductionRules": [
                              {
                                  "ruleId": dr.get("rule_id") or dr.get("ruleId") or f"{q.get('question_id')}.d{idx + 1}",
                                  "description": dr.get("description", ""),
                                  "deduction": dr.get("deduction", dr.get("score", 0)),
                                  "conditions": dr.get("conditions") or dr.get("when") or "",
                              }
                              for idx, dr in enumerate(q.get("deduction_rules") or q.get("deductionRules") or [])
                          ],
                                "alternativeSolutions": [
                                    {
                                        "description": alt.get("description", ""),
                                        "scoringCriteria": alt.get("scoring_criteria", ""),
                                        "note": alt.get("note", ""),
                                    }
                                    for alt in q.get("alternative_solutions", [])
                                ]
                            }
                            for q in parsed.get("questions", [])
                        ]
                    })
                if run_info.status and run_info.status.value == "completed":
                    student_results = state.get("student_results", [])
                    formatted_results = _format_results_for_frontend(student_results)
                    class_report = state.get("class_report")
                    if not class_report and state.get("export_data"):
                        class_report = state.get("export_data", {}).get("class_report")
                    await websocket.send_json({
                        "type": "workflow_completed",
                        "message": f"Grading completed, processed {len(formatted_results)} students",
                        "results": formatted_results,
                        "cross_page_questions": state.get("cross_page_questions", []),
                        "classReport": class_report
                    })
    except Exception as e:
        logger.warning(f"å‘é€çŠ¶æ€å¿«ç…§å¤±è´¥: {e}")
    
    try:
        # ä¿æŒè¿æ¥ï¼Œç­‰å¾…å®¢æˆ·ç«¯æ¶ˆæ¯æˆ–æ–­å¼€
        while True:
            if not _is_ws_connected(websocket):
                break
            data = await websocket.receive_text()
            logger.debug(f"æ”¶åˆ° WebSocket æ¶ˆæ¯: batch_id={batch_id}, data={data}")
            
    except (WebSocketDisconnect, RuntimeError) as exc:
        logger.info(f"WebSocket è¿æ¥æ–­å¼€: batch_id={batch_id}, reason={exc}")
        if batch_id in active_connections and websocket in active_connections[batch_id]:
            active_connections[batch_id].remove(websocket)
            if not active_connections[batch_id]:
                del active_connections[batch_id]
        return
    except Exception as exc:
        logger.warning(f"WebSocket æ¥æ”¶å¼‚å¸¸: batch_id={batch_id}, error={exc}")
        logger.info(f"WebSocket è¿æ¥æ–­å¼€: batch_id={batch_id}")
        if batch_id in active_connections and websocket in active_connections[batch_id]:
            active_connections[batch_id].remove(websocket)
            if not active_connections[batch_id]:
                del active_connections[batch_id]


@router.get("/status/{batch_id}", response_model=BatchStatusResponse)
async def get_batch_status(
    batch_id: str,
    orchestrator: Orchestrator = Depends(get_orchestrator)
):
    """
    æŸ¥è¯¢æ‰¹æ¬¡çŠ¶æ€ï¼ˆä» LangGraph Orchestratorï¼‰
    
    Args:
        batch_id: æ‰¹æ¬¡ ID
        orchestrator: LangGraph Orchestrator
        
    Returns:
        BatchStatusResponse: æ‰¹æ¬¡çŠ¶æ€
    """
    try:
        if not orchestrator:
            raise HTTPException(status_code=503, detail="ç¼–æ’å™¨æœªåˆå§‹åŒ–")
        
        # æ„å»º run_idï¼ˆä¸ start_run ä¸­çš„æ ¼å¼ä¸€è‡´ï¼‰
        run_id = f"batch_grading_{batch_id}"
        
        # ä» LangGraph Orchestrator æŸ¥è¯¢çŠ¶æ€
        run_info = await orchestrator.get_run_info(run_id)
        
        if not run_info:
            raise HTTPException(status_code=404, detail="æ‰¹æ¬¡ä¸å­˜åœ¨")
        
        state = run_info.state or {}
        
        return BatchStatusResponse(
            batch_id=batch_id,
            exam_id=state.get("exam_id", ""),
            status=run_info.status.value,
            total_students=len(state.get("student_boundaries", [])),
            completed_students=len(state.get("student_results", [])),
            unidentified_pages=0,
            results=state.get("student_results")
        )
        
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"æŸ¥è¯¢æ‰¹æ¬¡çŠ¶æ€å¤±è´¥: {str(e)}", exc_info=True)
        raise HTTPException(status_code=500, detail=f"æŸ¥è¯¢å¤±è´¥: {str(e)}")


@router.get("/rubric/{batch_id}", response_model=RubricReviewContextResponse)
async def get_rubric_review_context(
    batch_id: str,
    orchestrator: Orchestrator = Depends(get_orchestrator)
):
    """è·å– rubric review é¡µé¢ä¸Šä¸‹æ–‡"""
    try:
        if not orchestrator:
            raise HTTPException(status_code=503, detail="ç¼–æ’å™¨æœªåˆå§‹åŒ–")

        run_id = f"batch_grading_{batch_id}"
        run_info = await orchestrator.get_run_info(run_id)
        if not run_info:
            raise HTTPException(status_code=404, detail="æ‰¹æ¬¡ä¸å­˜åœ¨")

        state = run_info.state or {}
        parsed_rubric = state.get("parsed_rubric")

        cached = batch_image_cache.get(batch_id, {})
        cached_images = cached.get("rubric_images_ready", {}).get("images") if cached else None
        rubric_images: List[str] = cached_images or []

        if not rubric_images and state.get("rubric_images"):
            try:
                rubric_images = []
                for img in state.get("rubric_images", []):
                    if isinstance(img, (bytes, bytearray)):
                        rubric_images.append(base64.b64encode(img).decode("utf-8"))
                    elif isinstance(img, str) and img:
                        rubric_images.append(img)
            except Exception as exc:
                logger.warning(f"Failed to convert rubric images: {exc}")
        return RubricReviewContextResponse(
            batch_id=batch_id,
            status=run_info.status.value if run_info.status else None,
            current_stage=state.get("current_stage"),
            parsed_rubric=parsed_rubric,
            rubric_images=rubric_images,
        )
    except HTTPException:
        raise
    except Exception as exc:
        logger.error(f"è·å– rubric ä¸Šä¸‹æ–‡å¤±è´¥: {exc}", exc_info=True)
        raise HTTPException(status_code=500, detail=f"è·å–å¤±è´¥: {str(exc)}")


@router.get("/results-review/{batch_id}", response_model=ResultsReviewContextResponse)
async def get_results_review_context(
    batch_id: str,
    orchestrator: Orchestrator = Depends(get_orchestrator)
):
    """è·å– results review é¡µé¢ä¸Šä¸‹æ–‡"""
    try:
        if not orchestrator:
            raise HTTPException(status_code=503, detail="ç¼–æ’å™¨æœªåˆå§‹åŒ–")

        run_id = f"batch_grading_{batch_id}"
        run_info = await orchestrator.get_run_info(run_id)
        if not run_info:
            raise HTTPException(status_code=404, detail="æ‰¹æ¬¡ä¸å­˜åœ¨")

        state = run_info.state or {}
        student_results = state.get("student_results", [])
        if not student_results:
            try:
                final_output = await orchestrator.get_final_output(run_id)
                if final_output:
                    student_results = final_output.get("student_results", [])
            except Exception as exc:
                logger.warning(f"è·å–æœ€ç»ˆè¾“å‡ºå¤±è´¥: {exc}")

        if not student_results:
            export_students = (state.get("export_data") or {}).get("students", [])
            if export_students:
                student_results = export_students

        cached = batch_image_cache.get(batch_id, {})
        cached_images = cached.get("images_ready", {}).get("images") if cached else None
        answer_images: List[str] = cached_images or []

        if not answer_images:
            raw_images = state.get("processed_images") or state.get("answer_images") or []
            try:
                answer_images = []
                for img in raw_images:
                    if isinstance(img, (bytes, bytearray)):
                        answer_images.append(base64.b64encode(img).decode("utf-8"))
                    elif isinstance(img, str) and img:
                        answer_images.append(img)
            except Exception as exc:
                logger.warning(f"Failed to convert answer images: {exc}")
        return ResultsReviewContextResponse(
            batch_id=batch_id,
            status=run_info.status.value if run_info.status else None,
            current_stage=state.get("current_stage"),
            student_results=_format_results_for_frontend(student_results),
            answer_images=answer_images,
        )
    except HTTPException:
        raise
    except Exception as exc:
        logger.error(f"è·å– results ä¸Šä¸‹æ–‡å¤±è´¥: {exc}", exc_info=True)
        raise HTTPException(status_code=500, detail=f"è·å–å¤±è´¥: {str(exc)}")


@router.get("/results/{batch_id}")
async def get_batch_results(
    batch_id: str,
    orchestrator: Orchestrator = Depends(get_orchestrator)
):
    """
    è·å–æ‰¹æ¬¡æ‰¹æ”¹ç»“æœï¼ˆä» LangGraph Orchestratorï¼‰
    
    Args:
        batch_id: æ‰¹æ¬¡ ID
        orchestrator: LangGraph Orchestrator
        
    Returns:
        æ‰¹æ”¹ç»“æœ
    """
    try:
        if not orchestrator:
            raise HTTPException(status_code=503, detail="ç¼–æ’å™¨æœªåˆå§‹åŒ–")
        
        # æ„å»º run_idï¼ˆä¸ start_run ä¸­çš„æ ¼å¼ä¸€è‡´ï¼‰
        run_id = f"batch_grading_{batch_id}"
        
        run_info = await orchestrator.get_run_info(run_id)
        
        if not run_info:
            raise HTTPException(status_code=404, detail="æ‰¹æ¬¡ä¸å­˜åœ¨")
        
        state = run_info.state or {}
        
        # ä¼˜å…ˆä» student_results è·å–ç»“æœ
        student_results = state.get("student_results", [])
        
        # å¦‚æœæ²¡æœ‰ student_resultsï¼Œå°è¯•ä» orchestrator è·å–æœ€ç»ˆè¾“å‡º
        if not student_results:
            try:
                final_output = await orchestrator.get_final_output(run_id)
                if final_output:
                    student_results = final_output.get("student_results", [])
            except Exception as e:
                logger.warning(f"è·å–æœ€ç»ˆè¾“å‡ºå¤±è´¥: {e}")
        
        return {
            "batch_id": batch_id,
            "status": run_info.status.value,
            "results": _format_results_for_frontend(student_results),
            "class_report": state.get("class_report")
        }
        
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"è·å–æ‰¹æ”¹ç»“æœå¤±è´¥: {str(e)}", exc_info=True)
        raise HTTPException(status_code=500, detail=f"è·å–å¤±è´¥: {str(e)}")


@router.get("/full-results/{batch_id}")
async def get_full_batch_results(
    batch_id: str,
    orchestrator: Orchestrator = Depends(get_orchestrator)
):
    """
    è·å–æ‰¹æ¬¡å®Œæ•´æ‰¹æ”¹ç»“æœï¼ˆåŒ…å«è·¨é¡µé¢˜ç›®ä¿¡æ¯ï¼‰
    
    Args:
        batch_id: æ‰¹æ¬¡ ID
        orchestrator: LangGraph Orchestrator
        
    Returns:
        å®Œæ•´æ‰¹æ”¹ç»“æœï¼ˆåŒ…å«è·¨é¡µé¢˜ç›®ä¿¡æ¯ï¼‰
    """
    try:
        if not orchestrator:
            raise HTTPException(status_code=503, detail="ç¼–æ’å™¨æœªåˆå§‹åŒ–")
        
        run_id = f"batch_grading_{batch_id}"
        run_info = await orchestrator.get_run_info(run_id)
        
        if not run_info:
            raise HTTPException(status_code=404, detail="æ‰¹æ¬¡ä¸å­˜åœ¨")
        
        state = run_info.state or {}
        student_results = state.get("student_results", [])
        cross_page_questions = state.get("cross_page_questions", [])
        parsed_rubric = state.get("parsed_rubric", {})
        class_report = state.get("class_report") or state.get("export_data", {}).get("class_report")
        
        return {
            "batch_id": batch_id,
            "status": run_info.status.value,
            "results": _format_results_for_frontend(student_results),
            "cross_page_questions": cross_page_questions,
            "parsed_rubric": parsed_rubric,
            "class_report": class_report,
            "total_students": len(student_results),
            "total_score": parsed_rubric.get("total_score", 100) if parsed_rubric else 100
        }
        
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"è·å–å®Œæ•´æ‰¹æ”¹ç»“æœå¤±è´¥: {str(e)}", exc_info=True)
        raise HTTPException(status_code=500, detail=f"è·å–å¤±è´¥: {str(e)}")


@router.get("/cross-page-questions/{batch_id}")
async def get_cross_page_questions(
    batch_id: str,
    orchestrator: Orchestrator = Depends(get_orchestrator)
):
    """
    è·å–è·¨é¡µé¢˜ç›®ä¿¡æ¯
    
    Args:
        batch_id: æ‰¹æ¬¡ ID
        orchestrator: LangGraph Orchestrator
        
    Returns:
        è·¨é¡µé¢˜ç›®ä¿¡æ¯åˆ—è¡¨
    """
    try:
        if not orchestrator:
            raise HTTPException(status_code=503, detail="ç¼–æ’å™¨æœªåˆå§‹åŒ–")
        
        run_id = f"batch_grading_{batch_id}"
        run_info = await orchestrator.get_run_info(run_id)
        
        if not run_info:
            raise HTTPException(status_code=404, detail="æ‰¹æ¬¡ä¸å­˜åœ¨")
        
        state = run_info.state or {}
        cross_page_questions = state.get("cross_page_questions", [])
        
        return cross_page_questions
        
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"è·å–è·¨é¡µé¢˜ç›®ä¿¡æ¯å¤±è´¥: {str(e)}", exc_info=True)
        raise HTTPException(status_code=500, detail=f"è·å–å¤±è´¥: {str(e)}")


class ConfirmBoundaryRequest(BaseModel):
    """ç¡®è®¤å­¦ç”Ÿè¾¹ç•Œè¯·æ±‚"""
    batch_id: str = Field(..., description="æ‰¹æ¬¡ ID")
    student_key: str = Field(..., description="å­¦ç”Ÿæ ‡è¯†")
    confirmed_pages: List[int] = Field(..., description="ç¡®è®¤çš„é¡µé¢ç´¢å¼•åˆ—è¡¨")


class RubricReviewRequest(BaseModel):
    """æäº¤è¯„åˆ†æ ‡å‡†äººå·¥ç¡®è®¤ç»“æœ"""
    batch_id: str = Field(..., description="æ‰¹æ¬¡ ID")
    action: str = Field(..., description="approve/update/reparse")
    parsed_rubric: Optional[Dict[str, Any]] = Field(None, description="ä¿®æ­£åçš„è¯„åˆ†æ ‡å‡†")
    selected_question_ids: Optional[List[str]] = Field(None, description="ä»…é‡ä¿®æ­£çš„é—®é¢˜ ID åˆ—è¡¨")
    notes: Optional[str] = Field(None, description="è¡¥å……è¯´æ˜")


class ResultsReviewRequest(BaseModel):
    """æäº¤æ‰¹æ”¹ç»“æœäººå·¥ç¡®è®¤ç»“æœ"""
    batch_id: str = Field(..., description="æ‰¹æ¬¡ ID")
    action: str = Field(..., description="approve/update/regrade")
    results: Optional[List[Dict[str, Any]]] = Field(None, description="ä¿®æ­£åçš„ç»“æœ")
    regrade_items: Optional[List[Dict[str, Any]]] = Field(
        None,
        description="éœ€è¦é‡æ–°æ‰¹æ”¹çš„é¢˜ç›®é¡¹",
    )
    notes: Optional[str] = Field(None, description="è¡¥å……è¯´æ˜")


@router.post("/review/rubric")
async def submit_rubric_review(
    request: RubricReviewRequest,
    orchestrator: Orchestrator = Depends(get_orchestrator)
):
    """æäº¤è¯„åˆ†æ ‡å‡†å¤æ ¸ç»“æœï¼Œæ¢å¤ workflow"""
    try:
        if not orchestrator:
            raise HTTPException(status_code=503, detail="ç¼–æ’å™¨æœªåˆå§‹åŒ–")

        action = request.action.lower().strip()
        if action not in ("approve", "update", "override", "reparse"):
            raise HTTPException(status_code=400, detail="æ— æ•ˆçš„ review action")

        run_id = f"batch_grading_{request.batch_id}"
        run_info = await orchestrator.get_run_info(run_id)
        if not run_info:
            raise HTTPException(status_code=404, detail="æ‰¹æ¬¡ä¸å­˜åœ¨")

        payload: Dict[str, Any] = {
            "action": action,
        }
        if request.parsed_rubric is not None:
            payload["parsed_rubric"] = request.parsed_rubric
        if request.selected_question_ids:
            payload["selected_question_ids"] = request.selected_question_ids
        if request.notes:
            payload["notes"] = request.notes

        success = await orchestrator.send_event(run_id, "review_signal", payload)
        if not success:
            raise HTTPException(status_code=409, detail="æ‰¹æ¬¡æœªå¤„äºå¯å¤æ ¸çŠ¶æ€")

        cached = batch_image_cache.get(request.batch_id)
        if cached and "review_required" in cached:
            cached.pop("review_required", None)

        return {"success": True, "message": "è¯„åˆ†æ ‡å‡†å¤æ ¸å·²æäº¤"}
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"æäº¤è¯„åˆ†æ ‡å‡†å¤æ ¸å¤±è´¥: {str(e)}", exc_info=True)
        raise HTTPException(status_code=500, detail=f"æäº¤å¤±è´¥: {str(e)}")


@router.post("/review/results")
async def submit_results_review(
    request: ResultsReviewRequest,
    orchestrator: Orchestrator = Depends(get_orchestrator)
):
    """æäº¤æ‰¹æ”¹ç»“æœå¤æ ¸ï¼Œæ¢å¤ workflow"""
    try:
        if not orchestrator:
            raise HTTPException(status_code=503, detail="ç¼–æ’å™¨æœªåˆå§‹åŒ–")

        action = request.action.lower().strip()
        if action not in ("approve", "update", "override", "regrade"):
            raise HTTPException(status_code=400, detail="æ— æ•ˆçš„ review action")

        run_id = f"batch_grading_{request.batch_id}"
        run_info = await orchestrator.get_run_info(run_id)
        if not run_info:
            raise HTTPException(status_code=404, detail="æ‰¹æ¬¡ä¸å­˜åœ¨")

        payload: Dict[str, Any] = {
            "action": action,
        }
        if request.results is not None:
            payload["results"] = request.results
        if request.regrade_items is not None:
            payload["regrade_items"] = request.regrade_items
        if request.notes:
            payload["notes"] = request.notes

        success = await orchestrator.send_event(run_id, "review_signal", payload)
        if not success:
            raise HTTPException(status_code=409, detail="æ‰¹æ¬¡æœªå¤„äºå¯å¤æ ¸çŠ¶æ€")

        cached = batch_image_cache.get(request.batch_id)
        if cached and "review_required" in cached:
            cached.pop("review_required", None)

        return {"success": True, "message": "æ‰¹æ”¹ç»“æœå¤æ ¸å·²æäº¤"}
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"æäº¤æ‰¹æ”¹å¤æ ¸å¤±è´¥: {str(e)}", exc_info=True)
        raise HTTPException(status_code=500, detail=f"æäº¤å¤±è´¥: {str(e)}")


@router.post("/confirm-boundary")
async def confirm_student_boundary(
    request: ConfirmBoundaryRequest,
    orchestrator: Orchestrator = Depends(get_orchestrator)
):
    """
    ç¡®è®¤å­¦ç”Ÿè¾¹ç•Œ
    
    å½“ AI è¯†åˆ«çš„å­¦ç”Ÿè¾¹ç•Œä¸å‡†ç¡®æ—¶ï¼Œå…è®¸ç”¨æˆ·æ‰‹åŠ¨ç¡®è®¤
    
    Args:
        request: ç¡®è®¤è¾¹ç•Œè¯·æ±‚
        orchestrator: LangGraph Orchestrator
        
    Returns:
        ç¡®è®¤ç»“æœ
    """
    try:
        if not orchestrator:
            raise HTTPException(status_code=503, detail="ç¼–æ’å™¨æœªåˆå§‹åŒ–")
        
        run_id = f"batch_grading_{request.batch_id}"
        run_info = await orchestrator.get_run_info(run_id)
        
        if not run_info:
            raise HTTPException(status_code=404, detail="æ‰¹æ¬¡ä¸å­˜åœ¨")
        
        # æ›´æ–°çŠ¶æ€ä¸­çš„å­¦ç”Ÿè¾¹ç•Œ
        state = run_info.state or {}
        student_boundaries = state.get("student_boundaries", [])
        
        # æŸ¥æ‰¾å¹¶æ›´æ–°å¯¹åº”å­¦ç”Ÿçš„è¾¹ç•Œ
        updated = False
        for boundary in student_boundaries:
            if boundary.get("student_key") == request.student_key:
                boundary["pages"] = request.confirmed_pages
                boundary["confirmed"] = True
                updated = True
                break
        
        if not updated:
            # å¦‚æœæ²¡æœ‰æ‰¾åˆ°ï¼Œæ·»åŠ æ–°çš„è¾¹ç•Œ
            student_boundaries.append({
                "student_key": request.student_key,
                "pages": request.confirmed_pages,
                "confirmed": True
            })
        
        logger.info(f"å­¦ç”Ÿè¾¹ç•Œå·²ç¡®è®¤: batch_id={request.batch_id}, student_key={request.student_key}")
        
        return {
            "success": True,
            "message": f"å­¦ç”Ÿ {request.student_key} çš„è¾¹ç•Œå·²ç¡®è®¤"
        }
        
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"ç¡®è®¤å­¦ç”Ÿè¾¹ç•Œå¤±è´¥: {str(e)}", exc_info=True)
        raise HTTPException(status_code=500, detail=f"ç¡®è®¤å¤±è´¥: {str(e)}")
