"""æ‰¹é‡æäº¤ API è·¯ç”± - ä½¿ç”¨ LangGraph Orchestrator

æ­£ç¡®çš„æ¶æ„ï¼š
1. ä½¿ç”¨ LangGraph Orchestrator å¯åŠ¨æ‰¹æ”¹æµç¨‹
2. é€šè¿‡ LangGraph çš„æµå¼ API å®æ—¶æ¨é€è¿›åº¦
3. åˆ©ç”¨ PostgreSQL Checkpointer å®ç°æŒä¹…åŒ–å’Œæ–­ç‚¹æ¢å¤
"""

import uuid
import logging
import tempfile
import asyncio
import inspect
import base64
import json
from datetime import datetime
from pathlib import Path
from typing import List, Optional, Dict, Any

from fastapi import (
    APIRouter,
    HTTPException,
    UploadFile,
    File,
    Form,
    WebSocket,
    WebSocketDisconnect,
    Depends,
)
from starlette.websockets import WebSocketState
from pydantic import BaseModel, Field
import fitz
from PIL import Image
import os
import redis.asyncio as redis
from redis.exceptions import RedisError

from src.config.runtime_controls import get_runtime_controls
from src.models.enums import SubmissionStatus
from src.orchestration.base import Orchestrator, RunStatus
from src.api.dependencies import get_orchestrator
from src.utils.image import to_jpeg_bytes, pil_to_jpeg_bytes
from src.utils.pool_manager import UnifiedPoolManager, PoolNotInitializedError
from src.services.grading_run_control import GradingRunSnapshot, get_run_controller
from src.services.file_storage import get_file_storage_service, StoredFile

# PostgreSQL ä½œä¸ºä¸»å­˜å‚¨
from src.db import (
    GradingHistory,
    StudentGradingResult,
    save_grading_history,
    save_student_result,
    upsert_homework_submission_grade,
    list_class_students,
    get_grading_history,
    get_student_results,
    get_page_images,
    # PostgreSQL å›¾ç‰‡å­˜å‚¨
    save_batch_images_concurrent,
    get_batch_images_as_bytes_list,
)


logger = logging.getLogger(__name__)


async def _maybe_await(value):
    return await value if inspect.isawaitable(value) else value
router = APIRouter(prefix="/batch", tags=["æ‰¹é‡æäº¤"])

# å­˜å‚¨æ´»è·ƒçš„ WebSocket è¿æ¥
active_connections: Dict[str, List[WebSocket]] = {}
# WebSocket é”ï¼šé˜²æ­¢å¹¶å‘å†™å…¥å¯¼è‡´çš„ç«æ€æ¡ä»¶  
ws_locks: Dict[int, asyncio.Lock] = {}
# ç¼“å­˜å›¾ç‰‡ï¼Œé¿å… images_ready æ—©äº WebSocket è¿æ¥å¯¼è‡´å‰ç«¯ä¸¢å¤±
batch_image_cache: Dict[str, Dict[str, dict]] = {}
_active_stream_tasks: Dict[str, asyncio.Task] = {}
_RUNTIME_CONTROLS = get_runtime_controls()
DEBUG_LOG_PATH = os.getenv("GRADEOS_DEBUG_LOG_PATH")
TEACHER_MAX_ACTIVE_RUNS = _RUNTIME_CONTROLS.teacher_max_active_runs
RUN_QUEUE_POLL_SECONDS = _RUNTIME_CONTROLS.run_queue_poll_seconds
# ä¿®å¤ï¼šè®¾ç½®é»˜è®¤è¶…æ—¶ä¸º 60 ç§’ï¼Œé¿å…æ— é™ç­‰å¾…
RUN_QUEUE_TIMEOUT_SECONDS = _RUNTIME_CONTROLS.run_queue_timeout_seconds
RUN_UPLOAD_QUEUE_WATERMARK = _RUNTIME_CONTROLS.upload_queue_watermark
RUN_UPLOAD_ACTIVE_WATERMARK = _RUNTIME_CONTROLS.upload_active_watermark
REDIS_PROGRESS_TTL_SECONDS = int(os.getenv("REDIS_PROGRESS_TTL_SECONDS", "86400"))
REDIS_PROGRESS_KEY_PREFIX = os.getenv("REDIS_PROGRESS_KEY_PREFIX", "batch_progress")
_REDIS_CACHE_SKIP_TYPES = {"images_ready", "rubric_images_ready", "llm_stream_chunk"}
_REDIS_CLIENT: Optional[redis.Redis] = None
_REDIS_CLIENT_CHECKED: bool = False
_BATCH_IMAGE_CACHE_MAX_BATCHES = _RUNTIME_CONTROLS.batch_image_cache_max_batches


def _is_ws_connected(websocket: WebSocket) -> bool:
    return (
        websocket.client_state == WebSocketState.CONNECTED
        and websocket.application_state == WebSocketState.CONNECTED
    )


def _get_batch_cache_bucket(batch_id: str) -> Dict[str, Any]:
    """Get/create a bounded in-memory cache bucket for a batch."""
    if batch_id not in batch_image_cache:
        if _BATCH_IMAGE_CACHE_MAX_BATCHES > 0:
            while len(batch_image_cache) >= _BATCH_IMAGE_CACHE_MAX_BATCHES:
                oldest = next(iter(batch_image_cache), None)
                if oldest is None:
                    break
                batch_image_cache.pop(oldest, None)
        batch_image_cache[batch_id] = {}
    return batch_image_cache[batch_id]


def _discard_connection(batch_id: str, websocket: WebSocket) -> None:
    connections = active_connections.get(batch_id)
    if not connections:
        return
    try:
        connections.remove(websocket)
    except ValueError:
        return
    if not connections:
        active_connections.pop(batch_id, None)
    # æ¸…ç† WebSocket é”
    ws_id = id(websocket)
    ws_locks.pop(ws_id, None)




def _progress_cache_key(batch_id: str) -> str:
    return f"{REDIS_PROGRESS_KEY_PREFIX}:{batch_id}"


def _decode_redis_value(value: Any) -> str:
    if isinstance(value, (bytes, bytearray)):
        return value.decode("utf-8", errors="ignore")
    return str(value)


async def _get_redis_client() -> Optional[redis.Redis]:
    global _REDIS_CLIENT, _REDIS_CLIENT_CHECKED
    if _REDIS_CLIENT_CHECKED:
        return _REDIS_CLIENT
    _REDIS_CLIENT_CHECKED = True
    try:
        pool_manager = await UnifiedPoolManager.get_instance()
        if pool_manager.is_initialized:
            _REDIS_CLIENT = pool_manager.get_redis_client()
    except PoolNotInitializedError:
        _REDIS_CLIENT = None
    except Exception as exc:
        logger.debug(f"Redis client unavailable: {exc}")
        _REDIS_CLIENT = None
    return _REDIS_CLIENT


async def _clear_progress_fields(
    redis_client: redis.Redis,
    cache_key: str,
    prefixes: Optional[List[str]] = None,
    fields: Optional[List[str]] = None,
) -> None:
    to_delete: List[Any] = []
    if fields:
        to_delete.extend(fields)
    if prefixes:
        try:
            existing_fields = await redis_client.hkeys(cache_key)
        except RedisError as exc:
            logger.debug(f"Failed to fetch Redis fields for cleanup: {exc}")
            existing_fields = []
        for field in existing_fields:
            field_name = _decode_redis_value(field)
            if any(field_name.startswith(prefix) for prefix in prefixes):
                to_delete.append(field)
    if not to_delete:
        return
    try:
        await redis_client.hdel(cache_key, *to_delete)
    except RedisError as exc:
        logger.debug(f"Failed to cleanup Redis progress cache: {exc}")


async def _cache_progress_message(batch_id: str, message: dict) -> None:
    msg_type = message.get("type", "unknown")
    if msg_type in _REDIS_CACHE_SKIP_TYPES:
        return

    redis_client = await _get_redis_client()
    if not redis_client:
        return

    cache_key = _progress_cache_key(batch_id)
    field = msg_type
    if msg_type == "workflow_update":
        node_id = message.get("nodeId")
        if node_id:
            field = f"{msg_type}:{node_id}"

    try:
        payload = json.dumps(message, ensure_ascii=False, default=str)
        await redis_client.hset(cache_key, field, payload)
        await redis_client.expire(cache_key, REDIS_PROGRESS_TTL_SECONDS)
        if msg_type in ("review_completed", "workflow_completed"):
            await _clear_progress_fields(
                redis_client,
                cache_key,
                fields=["review_required"],
            )
    except (TypeError, ValueError) as exc:
        logger.debug(f"Failed to serialize progress message: {exc}")
    except RedisError as exc:
        logger.debug(f"Failed to cache progress message in Redis: {exc}")


async def _load_cached_progress_messages(batch_id: str) -> List[dict]:
    redis_client = await _get_redis_client()
    if not redis_client:
        return []
    cache_key = _progress_cache_key(batch_id)
    try:
        entries = await redis_client.hgetall(cache_key)
    except RedisError as exc:
        logger.debug(f"Failed to fetch cached progress from Redis: {exc}")
        return []

    messages: List[dict] = []
    for field in sorted(entries.keys(), key=_decode_redis_value):
        payload = entries.get(field)
        if payload is None:
            continue
        raw = _decode_redis_value(payload)
        try:
            message = json.loads(raw)
        except json.JSONDecodeError:
            continue
        if isinstance(message, dict):
            messages.append(message)
    return messages


def _safe_to_jpeg_bytes(image_bytes: bytes, label: str) -> bytes:
    try:
        return to_jpeg_bytes(image_bytes)
    except Exception as exc:
        logger.debug(f"Failed to convert image to JPEG ({label}): {exc}")
        return image_bytes


def _normalize_teacher_key(teacher_id: Optional[str]) -> str:
    if teacher_id and teacher_id.strip():
        return teacher_id.strip()
    return "anonymous"


async def _enforce_submit_backpressure(run_controller, teacher_key: str) -> None:
    if not run_controller:
        return
    capacity = await run_controller.get_teacher_capacity(teacher_key)
    active_count = int(capacity.get("active_count", 0))
    queued_count = int(capacity.get("queued_count", 0))

    if RUN_UPLOAD_ACTIVE_WATERMARK > 0 and active_count >= RUN_UPLOAD_ACTIVE_WATERMARK:
        raise HTTPException(
            status_code=429,
            detail=(
                f"Too many active runs for teacher={teacher_key}. "
                f"active={active_count}, limit={RUN_UPLOAD_ACTIVE_WATERMARK}"
            ),
        )

    if RUN_UPLOAD_QUEUE_WATERMARK > 0 and queued_count >= RUN_UPLOAD_QUEUE_WATERMARK:
        raise HTTPException(
            status_code=429,
            detail=(
                f"Queue watermark reached for teacher={teacher_key}. "
                f"queued={queued_count}, limit={RUN_UPLOAD_QUEUE_WATERMARK}"
            ),
        )


class BatchSubmissionResponse(BaseModel):
    """æ‰¹é‡æäº¤å“åº”"""

    batch_id: str = Field(..., description="æ‰¹æ¬¡ ID")
    status: SubmissionStatus = Field(..., description="çŠ¶æ€")
    total_pages: int = Field(..., description="æ€»é¡µæ•°")
    estimated_completion_time: int = Field(..., description="é¢„è®¡å®Œæˆæ—¶é—´ï¼ˆç§’ï¼‰")


class BatchStatusResponse(BaseModel):
    """æ‰¹é‡çŠ¶æ€æŸ¥è¯¢å“åº”"""

    batch_id: str
    exam_id: str
    status: str
    current_stage: Optional[str] = None
    error: Optional[str] = None
    total_students: int = Field(0, description="è¯†åˆ«åˆ°çš„å­¦ç”Ÿæ•°")
    completed_students: int = Field(0, description="å·²å®Œæˆæ‰¹æ”¹çš„å­¦ç”Ÿæ•°")
    unidentified_pages: int = Field(0, description="æœªè¯†åˆ«å­¦ç”Ÿçš„é¡µæ•°")
    results: Optional[List[dict]] = Field(None, description="æ‰¹æ”¹ç»“æœ")


class ActiveRunItem(BaseModel):
    batch_id: str
    status: str
    class_id: Optional[str] = None
    homework_id: Optional[str] = None
    created_at: Optional[str] = None
    updated_at: Optional[str] = None
    started_at: Optional[str] = None
    completed_at: Optional[str] = None
    total_pages: Optional[int] = None
    progress: Optional[float] = None
    current_stage: Optional[str] = None


class ActiveRunsResponse(BaseModel):
    teacher_id: str
    runs: List[ActiveRunItem]


class RubricReviewContextResponse(BaseModel):
    """å‰ç«¯ rubric review ä¸Šä¸‹æ–‡"""

    batch_id: str
    status: Optional[str] = None
    current_stage: Optional[str] = None
    parsed_rubric: Optional[dict] = None
    rubric_images: List[str] = []


class ResultsReviewContextResponse(BaseModel):
    """å‰ç«¯ results review ä¸Šä¸‹æ–‡"""

    batch_id: str
    status: Optional[str] = None
    current_stage: Optional[str] = None
    student_results: List[dict] = []
    answer_images: List[str] = []
    parsed_rubric: Optional[dict] = None  # æ·»åŠ  parsed_rubric å­—æ®µ


def _pdf_to_images(pdf_path: str, dpi: int = 150) -> List[bytes]:
    """å°† PDF è½¬æ¢ä¸ºå›¾åƒåˆ—è¡¨"""
    pdf_doc = fitz.open(pdf_path)
    images = []

    for page_num in range(len(pdf_doc)):
        page = pdf_doc[page_num]
        mat = fitz.Matrix(dpi / 72, dpi / 72)
        pix = page.get_pixmap(matrix=mat)
        img = Image.frombytes("RGB", [pix.width, pix.height], pix.samples)

        images.append(pil_to_jpeg_bytes(img))

    pdf_doc.close()
    return images


async def broadcast_progress(batch_id: str, message: dict):
    """å‘æ‰€æœ‰è¿æ¥çš„ WebSocket å®¢æˆ·ç«¯å¹¿æ’­è¿›åº¦"""
    msg_type = message.get("type", "unknown")
    if msg_type in ("images_ready", "rubric_images_ready", "review_required"):
        cached = _get_batch_cache_bucket(batch_id)
        cached[msg_type] = message
    if msg_type == "llm_stream_chunk":
        node_id = message.get("nodeId") or ""
        if node_id in ("rubric_parse", "rubric_self_review", "rubric_review"):
            cached = _get_batch_cache_bucket(batch_id)
            stream_cache = cached.setdefault("llm_stream_cache", {})
            cache_key = f"{node_id}:{message.get('agentId') or 'all'}:{message.get('streamType') or 'output'}"
            existing = stream_cache.get(cache_key, {})
            chunk_data = message.get("chunk", "") or ""
            if isinstance(chunk_data, list):
                chunk_data = "".join([str(c) for c in chunk_data])
            else:
                chunk_data = str(chunk_data)

            existing_chunk = existing.get("chunk", "") or ""
            combined = existing_chunk + chunk_data
            max_chars = 12000
            if len(combined) > max_chars:
                combined = combined[-max_chars:]
            stream_cache[cache_key] = {
                **message,
                "chunk": combined,
            }
    if msg_type in ("review_completed", "workflow_completed"):
        cached = batch_image_cache.get(batch_id)
        if cached and "review_required" in cached:
            cached.pop("review_required", None)
        if cached and "llm_stream_cache" in cached:
            cached.pop("llm_stream_cache", None)
    if msg_type in (
        "workflow_update",
        "grading_progress",
        "workflow_completed",
        "workflow_error",
        "batch_error",
    ):
        run_updates: Dict[str, Any] = {"updated_at": datetime.now().isoformat()}
        if msg_type == "workflow_update":
            status = message.get("status")
            if status == "pending":
                run_updates["status"] = "queued"
            elif status in ("running", "paused"):
                run_updates["status"] = "running"
            elif status == "completed":
                run_updates["status"] = "completed"
                run_updates["completed_at"] = run_updates["updated_at"]
            elif status == "failed":
                run_updates["status"] = "failed"
                run_updates["completed_at"] = run_updates["updated_at"]
            node_id = message.get("nodeId")
            if node_id:
                run_updates["current_stage"] = node_id
        elif msg_type == "grading_progress":
            percentage = message.get("percentage")
            if percentage is not None:
                try:
                    progress_value = float(percentage)
                    if progress_value > 1.0:
                        progress_value = progress_value / 100.0
                    run_updates["progress"] = max(0.0, min(progress_value, 1.0))
                except (TypeError, ValueError):
                    pass
            stage = message.get("currentStage")
            if stage:
                run_updates["current_stage"] = stage
        elif msg_type == "workflow_completed":
            run_updates.update(
                {
                    "status": "completed",
                    "completed_at": run_updates["updated_at"],
                    "progress": 1.0,
                }
            )
        else:
            run_updates.update(
                {
                    "status": "failed",
                    "completed_at": run_updates["updated_at"],
                }
            )
        run_controller = await get_run_controller()
        if run_controller:
            await run_controller.update_run(batch_id, run_updates)
    try:
        await _cache_progress_message(batch_id, message)
    except Exception as exc:
        logger.debug(f"Failed to cache progress message: {exc}")
    if batch_id in active_connections:
        disconnected = []
        for ws in active_connections[batch_id]:
            if not _is_ws_connected(ws):
                disconnected.append(ws)
                continue
            
            # ä½¿ç”¨é”ä¿æŠ¤å¹¶å‘å†™å…¥
            ws_id = id(ws)
            if ws_id not in ws_locks:
                ws_locks[ws_id] = asyncio.Lock()
            
            try:
                async with ws_locks[ws_id]:
                    await ws.send_json(message)
            except WebSocketDisconnect:
                disconnected.append(ws)
            except (RuntimeError, AssertionError):
                # AssertionError: websockets åº“å†…éƒ¨ keepalive ping ä¸åº”ç”¨å±‚å†™å…¥çš„ç«æ€æ¡ä»¶
                # RuntimeError: è¿æ¥å·²å…³é—­
                # è¿™ä¸¤ç§é”™è¯¯éƒ½æ˜¯é¢„æœŸçš„ï¼Œé™é»˜å¤„ç†
                disconnected.append(ws)
            except Exception:
                # å…¶ä»–é”™è¯¯ä¹Ÿé™é»˜å¤„ç†ï¼Œåªè®°å½•æ–­å¼€
                disconnected.append(ws)

        # ç§»é™¤æ–­å¼€çš„è¿æ¥
        for ws in disconnected:
            _discard_connection(batch_id, ws)
            # æ¸…ç†é”
            ws_id = id(ws)
            ws_locks.pop(ws_id, None)


async def _start_run_with_teacher_limit(
    *,
    teacher_key: str,
    batch_id: str,
    payload: Dict[str, Any],
    orchestrator: Orchestrator,
    class_id: Optional[str],
    homework_id: Optional[str],
    student_mapping: List[dict],
) -> Optional[str]:
    logger.info(f"[_start_run_with_teacher_limit] å¼€å§‹æ‰§è¡Œ: batch_id={batch_id}")
    
    try:
        run_controller = await get_run_controller()
    except Exception as e:
        logger.error(f"[_start_run_with_teacher_limit] get_run_controllerå¼‚å¸¸: {e}")
        run_controller = None
    
    if run_controller:
        try:
            acquired = await run_controller.try_acquire_slot(
                teacher_key,
                batch_id,
                TEACHER_MAX_ACTIVE_RUNS,
            )
        except Exception as e:
            logger.error(f"[_start_run_with_teacher_limit] try_acquire_slotå¼‚å¸¸: {e}")
            acquired = False
        
        if not acquired:
            await broadcast_progress(
                batch_id,
                {
                    "type": "workflow_update",
                    "nodeId": "rubric_parse",
                    "status": "pending",
                    "message": "Queued: waiting for grading slot",
                },
            )
            max_wait = RUN_QUEUE_TIMEOUT_SECONDS if RUN_QUEUE_TIMEOUT_SECONDS > 0 else None
            acquired = await run_controller.wait_for_slot(
                teacher_key,
                batch_id,
                TEACHER_MAX_ACTIVE_RUNS,
                RUN_QUEUE_POLL_SECONDS,
                max_wait,
            )
            if not acquired:
                # ç­‰å¾…è¶…æ—¶åï¼Œå¼ºåˆ¶æ¸…ç†è¯¥æ•™å¸ˆçš„æ‰€æœ‰æ´»åŠ¨æ§½ä½å¹¶é‡æ–°å°è¯•
                logger.info(f"[_start_run_with_teacher_limit] ç­‰å¾…è¶…æ—¶ï¼Œå°è¯•å¼ºåˆ¶æ¸…ç†æ—§æ§½ä½å¹¶é‡æ–°è·å–")
                try:
                    await run_controller.force_clear_teacher_slots(teacher_key)
                    acquired = await run_controller.try_acquire_slot(
                        teacher_key,
                        batch_id,
                        TEACHER_MAX_ACTIVE_RUNS,
                    )
                except Exception as cleanup_err:
                    logger.error(f"[_start_run_with_teacher_limit] æ¸…ç†æ§½ä½å¤±è´¥: {cleanup_err}")
                    acquired = False
                
                if not acquired:
                    logger.info(f"[_start_run_with_teacher_limit] æ¸…ç†åä»æ— æ³•è·å–æ§½ä½ï¼Œä»»åŠ¡å¤±è´¥")
                    await run_controller.update_run(
                        batch_id,
                        {"status": "failed", "updated_at": datetime.now().isoformat()},
                    )
                    await run_controller.remove_from_queue(teacher_key, batch_id)
                    await broadcast_progress(
                        batch_id,
                        {
                            "type": "workflow_update",
                            "nodeId": "rubric_parse",
                            "status": "failed",
                            "message": "Queued run timed out - please try again",
                        },
                    )
                    return None
        
        logger.info(f"[_start_run_with_teacher_limit] æˆåŠŸè·å– slotï¼Œæ›´æ–°è¿è¡ŒçŠ¶æ€")
        await run_controller.update_run(
            batch_id,
            {
                "status": "running",
                "started_at": datetime.now().isoformat(),
                "updated_at": datetime.now().isoformat(),
            },
        )
    
    run_id: Optional[str] = None
    try:
        logger.info(f"[_start_run_with_teacher_limit] å‡†å¤‡å¯åŠ¨ LangGraph run")
        run_id = await orchestrator.start_run(
            graph_name="batch_grading", payload=payload, idempotency_key=batch_id
        )
        logger.info(f"[_start_run_with_teacher_limit] LangGraph å¯åŠ¨æˆåŠŸ: batch_id={batch_id}, run_id={run_id}")
        
        stream_task = asyncio.create_task(
            stream_langgraph_progress(
                batch_id=batch_id,
                run_id=run_id,
                orchestrator=orchestrator,
                class_id=class_id,
                homework_id=homework_id,
                student_mapping=student_mapping,
                teacher_key=teacher_key,
            )
        )
        _active_stream_tasks[batch_id] = stream_task
        logger.info(f"[_start_run_with_teacher_limit] æµå¼ä»»åŠ¡å·²åˆ›å»º")
        return run_id
    except Exception as exc:
        logger.error(f"[_start_run_with_teacher_limit] å¯åŠ¨å¤±è´¥: {exc}", exc_info=True)
        if run_controller:
            await run_controller.release_slot(teacher_key, batch_id)
            await run_controller.update_run(
                batch_id,
                {"status": "failed", "updated_at": datetime.now().isoformat()},
            )
        await broadcast_progress(
            batch_id,
            {
                "type": "workflow_update",
                "nodeId": "rubric_parse",
                "status": "failed",
                "message": "Queued run failed to start",
            },
        )
        return None


@router.post("/submit", response_model=BatchSubmissionResponse)
async def submit_batch(
    exam_id: Optional[str] = Form(None, description="è€ƒè¯• ID"),
    rubrics: List[UploadFile] = File(default=[], description="è¯„åˆ†æ ‡å‡† PDFï¼ˆå¯é€‰ï¼‰"),
    files: List[UploadFile] = File(..., description="å­¦ç”Ÿä½œç­” PDF"),
    api_key: Optional[str] = Form(None, description="LLM API Key"),
    teacher_id: Optional[str] = Form(None, description="?? ID"),
    auto_identify: bool = Form(True, description="æ˜¯å¦è‡ªåŠ¨è¯†åˆ«å­¦ç”Ÿèº«ä»½"),
    student_boundaries: Optional[str] = Form(
        None, description="æ‰‹åŠ¨è®¾ç½®çš„å­¦ç”Ÿè¾¹ç•Œ (JSON List of page indices)"
    ),
    expected_students: Optional[int] = Form(
        None, description="é¢„æœŸå­¦ç”Ÿæ•°é‡ï¼ˆå¼ºçƒˆå»ºè®®æä¾›ï¼Œç”¨äºæ›´å‡†ç¡®çš„åˆ†å‰²ï¼‰"
    ),
    expected_total_score: Optional[float] = Form(None, description="Expected total score"),
    # æ–°å¢ï¼šç­çº§æ‰¹æ”¹ä¸Šä¸‹æ–‡
    class_id: Optional[str] = Form(None, description="ç­çº§ IDï¼ˆç”¨äºæˆç»©å†™å›ï¼‰"),
    homework_id: Optional[str] = Form(None, description="ä½œä¸š IDï¼ˆç”¨äºæˆç»©å†™å›ï¼‰"),
    student_mapping_json: Optional[str] = Form(
        None, description="å­¦ç”Ÿæ˜ å°„ JSON [{studentId, studentName, startIndex, endIndex}]"
    ),
    enable_review: bool = Form(True, description="æ˜¯å¦å¯ç”¨äººå·¥äº¤äº’"),
    grading_mode: Optional[str] = Form(
        None, description="grading mode: standard/assist_teacher/assist_student/auto"
    ),
    orchestrator: Orchestrator = Depends(get_orchestrator),
):
    """
    æ‰¹é‡æäº¤è¯•å·å¹¶è¿›è¡Œæ‰¹æ”¹ï¼ˆä½¿ç”¨ LangGraph Orchestratorï¼‰

    æ­£ç¡®çš„æ¶æ„ï¼š
    1. ä½¿ç”¨ LangGraph Orchestrator å¯åŠ¨ batch_grading Graph
    2. Graph è‡ªåŠ¨å¤„ç†ï¼šè¾¹ç•Œæ£€æµ‹ â†’ å¹¶è¡Œæ‰¹æ”¹ â†’ èšåˆ â†’ æŒä¹…åŒ– â†’ é€šçŸ¥
    3. é€šè¿‡ WebSocket å®æ—¶æ¨é€ LangGraph çš„æ‰§è¡Œè¿›åº¦

    Args:
        exam_id: è€ƒè¯• ID
        rubrics: è¯„åˆ†æ ‡å‡† PDF æ–‡ä»¶åˆ—è¡¨
        files: å­¦ç”Ÿä½œç­” PDF æ–‡ä»¶åˆ—è¡¨
        api_key: LLM API Key
        auto_identify: æ˜¯å¦å¯ç”¨è‡ªåŠ¨å­¦ç”Ÿè¯†åˆ«
        orchestrator: LangGraph Orchestratorï¼ˆä¾èµ–æ³¨å…¥ï¼‰

    Returns:
        BatchSubmissionResponse: æ‰¹æ¬¡ä¿¡æ¯
    """
    # æ£€æŸ¥ orchestrator æ˜¯å¦å¯ç”¨
    if not orchestrator:
        raise HTTPException(status_code=503, detail="æ‰¹æ”¹æœåŠ¡æœªåˆå§‹åŒ–ï¼Œè¯·ç¨åé‡è¯•æˆ–æ£€æŸ¥æœåŠ¡é…ç½®")

    if not api_key:
        api_key = os.getenv("LLM_API_KEY") or os.getenv("OPENROUTER_API_KEY")

    if not api_key:
        raise HTTPException(
            status_code=400,
            detail="æœªæä¾› API Keyï¼Œè¯·åœ¨è¯·æ±‚ä¸­æä¾›æˆ–é…ç½®ç¯å¢ƒå˜é‡ LLM_API_KEY/OPENROUTER_API_KEY",
        )

    # è§£æå­¦ç”Ÿè¾¹ç•Œ
    parsed_boundaries = []
    if student_boundaries:
        try:
            logger.debug(
                f"æ¥æ”¶åˆ°åŸå§‹ student_boundaries: {student_boundaries} (type: {type(student_boundaries)})"
            )
            import json

            parsed_boundaries = json.loads(student_boundaries)
            logger.debug(f"è§£æåçš„ manual_boundaries: {parsed_boundaries}")
        except Exception as e:
            logger.debug(f"è§£ææ‰‹åŠ¨å­¦ç”Ÿè¾¹ç•Œå¤±è´¥: {e}")

    if not exam_id:
        exam_id = str(uuid.uuid4())

    batch_id = str(uuid.uuid4())
    teacher_key = _normalize_teacher_key(teacher_id)
    try:
        run_controller = await get_run_controller()
    except Exception:
        run_controller = None
    await _enforce_submit_backpressure(run_controller, teacher_key)

    logger.info(
        f"æ”¶åˆ°æ‰¹é‡æäº¤ï¼ˆLangGraphï¼‰: "
        f"batch_id={batch_id}, "
        f"exam_id={exam_id}, "
        f"auto_identify={auto_identify}"
    )

    temp_dir = None
    try:
        # åˆ›å»ºä¸´æ—¶ç›®å½•
        temp_dir = tempfile.mkdtemp()
        temp_path = Path(temp_dir)

        # === å¤„ç†ç­”é¢˜æ–‡ä»¶ï¼ˆæ”¯æŒå›¾ç‰‡åˆ—è¡¨æˆ–å•ä¸ª PDFï¼‰===
        answer_images = []

        for idx, file in enumerate(files):
            file_name = file.filename or f"file_{idx}"
            content = await file.read()

            # æ£€æŸ¥æ–‡ä»¶ç±»å‹
            if file_name.lower().endswith((".png", ".jpg", ".jpeg", ".webp", ".gif")):
                # å›¾ç‰‡æ–‡ä»¶ï¼šç›´æ¥ä½¿ç”¨å†…å®¹
                answer_images.append(_safe_to_jpeg_bytes(content, file_name))
                logger.debug(f"è¯»å–å›¾ç‰‡æ–‡ä»¶: {file_name}, å¤§å°: {len(content)} bytes")
            elif file_name.lower().endswith(".pdf"):
                # PDF æ–‡ä»¶ï¼šè½¬æ¢ä¸ºå›¾åƒ
                pdf_path = temp_path / f"answer_{idx}.pdf"
                with open(pdf_path, "wb") as f:
                    f.write(content)
                loop = asyncio.get_event_loop()
                pdf_images = await loop.run_in_executor(None, _pdf_to_images, str(pdf_path), 150)
                answer_images.extend(pdf_images)
                logger.debug(f"PDF æ–‡ä»¶ {file_name} è½¬æ¢ä¸º {len(pdf_images)} é¡µå›¾ç‰‡")
            elif file_name.lower().endswith(".txt"):
                # æ–‡æœ¬æ–‡ä»¶ï¼šç›´æ¥ä½¿ç”¨å†…å®¹
                answer_images.append(content)
                logger.debug(f"æ–‡æœ¬æ–‡ä»¶å¤„ç†å®Œæˆ: {file_name}, å†…å®¹é•¿åº¦={len(content)}")
            else:
                # å°è¯•ä½œä¸ºå›¾ç‰‡å¤„ç†ï¼ˆå¯èƒ½æ²¡æœ‰æ‰©å±•åï¼‰
                answer_images.append(_safe_to_jpeg_bytes(content, file_name))
                logger.debug(f"æœªçŸ¥æ–‡ä»¶ç±»å‹ {file_name}ï¼Œå°è¯•ä½œä¸ºå›¾ç‰‡å¤„ç†")

        total_pages = len(answer_images)

        if run_controller:
            await run_controller.register_run(
                GradingRunSnapshot(
                    batch_id=batch_id,
                    teacher_id=teacher_key,
                    status="queued",
                    class_id=class_id,
                    homework_id=homework_id,
                    created_at=datetime.now().isoformat(),
                    updated_at=datetime.now().isoformat(),
                    total_pages=total_pages,
                )
            )
        logger.info(f"ç­”é¢˜æ–‡ä»¶å¤„ç†å®Œæˆ: batch_id={batch_id}, æ€»é¡µæ•°={total_pages}")

        # === å¤„ç†è¯„åˆ†æ ‡å‡†ï¼ˆå¯é€‰ï¼‰===
        # Convert images to base64 and cache them immediately
        # (Fix: Rubric images not displaying on frontend)
        if answer_images:
            try:
                base64_images = [base64.b64encode(img).decode("utf-8") for img in answer_images]

                # Cache for direct WebSocket connection
                _get_batch_cache_bucket(batch_id)["images_ready"] = {
                    "type": "images_ready",
                    "images": base64_images,
                }

                # Broadcast (though no clients connected yet usually)
                await broadcast_progress(
                    batch_id, {"type": "images_ready", "images": base64_images}
                )
                logger.info(f"å·²ç¼“å­˜ {len(base64_images)} å¼ å›¾ç‰‡ç”¨äºå‰ç«¯æ˜¾ç¤º")
            except Exception as e:
                logger.error(f"å›¾ç‰‡ Base64 è½¬æ¢å¤±è´¥: {e}")

        # === å¤„ç†è¯„åˆ†æ ‡å‡†ï¼ˆå¯é€‰ï¼‰===
        rubric_images = []
        if rubrics and len(rubrics) > 0:
            for idx, rubric_file in enumerate(rubrics):
                rubric_name = rubric_file.filename or f"rubric_{idx}"
                rubric_content = await rubric_file.read()

                if rubric_name.lower().endswith((".png", ".jpg", ".jpeg", ".webp", ".gif")):
                    rubric_images.append(_safe_to_jpeg_bytes(rubric_content, rubric_name))
                elif rubric_name.lower().endswith(".pdf"):
                    rubric_path = temp_path / f"rubric_{idx}.pdf"
                    with open(rubric_path, "wb") as f:
                        f.write(rubric_content)
                    loop = asyncio.get_event_loop()
                    pdf_rubric_images = await loop.run_in_executor(
                        None, _pdf_to_images, str(rubric_path), 150
                    )
                    rubric_images.extend(pdf_rubric_images)
                else:
                    rubric_images.append(_safe_to_jpeg_bytes(rubric_content, rubric_name))

            logger.info(f"è¯„åˆ†æ ‡å‡†å¤„ç†å®Œæˆ: batch_id={batch_id}, æ€»é¡µæ•°={len(rubric_images)}")
            if rubric_images:
                try:
                    base64_rubric_images = [
                        base64.b64encode(img).decode("utf-8") for img in rubric_images
                    ]
                    _get_batch_cache_bucket(batch_id)["rubric_images_ready"] = {
                        "type": "rubric_images_ready",
                        "images": base64_rubric_images,
                    }
                    await broadcast_progress(
                        batch_id, {"type": "rubric_images_ready", "images": base64_rubric_images}
                    )
                    logger.info(f"å·²ç¼“å­˜ {len(base64_rubric_images)} å¼ è¯„åˆ†æ ‡å‡†å›¾ç‰‡ç”¨äºå‰ç«¯æ˜¾ç¤º")
                except Exception as e:
                    logger.error(f"è¯„åˆ†æ ‡å‡† Base64 è½¬æ¢å¤±è´¥: {e}")
        else:
            logger.info(f"æœªæä¾›è¯„åˆ†æ ‡å‡†ï¼Œå°†ä½¿ç”¨é»˜è®¤è¯„åˆ†: batch_id={batch_id}")

        logger.info(
            f"æ–‡ä»¶å¤„ç†å®Œæˆ: "
            f"batch_id={batch_id}, "
            f"rubric_pages={len(rubric_images)}, "
            f"answer_pages={total_pages}"
        )

        # ğŸ“ æŒä¹…åŒ–å­˜å‚¨åŸå§‹æ–‡ä»¶åˆ° PostgreSQLï¼ˆé«˜æ€§èƒ½ï¼Œæ›¿ä»£æœ¬åœ°æ–‡ä»¶å­˜å‚¨ï¼‰
        # ä½¿ç”¨ PostgreSQL BYTEA å­˜å‚¨ï¼Œé¿å…æœ¬åœ°æ–‡ä»¶ç³»ç»Ÿç“¶é¢ˆ
        stored_files: List[StoredFile] = []
        use_pg_storage = os.getenv("USE_PG_IMAGE_STORAGE", "true").lower() == "true"
        
        if use_pg_storage:
            try:
                # å¹¶å‘ä¿å­˜å›¾ç‰‡åˆ° PostgreSQLï¼ˆæ¯”æœ¬åœ°æ–‡ä»¶å¿«å¾ˆå¤šï¼‰
                answer_count = await save_batch_images_concurrent(
                    batch_id=batch_id,
                    images=answer_images,
                    image_type="answer",
                    max_concurrent=20,  # é«˜å¹¶å‘å†™å…¥
                )
                logger.info(f"[PG-Storage] ç­”é¢˜å›¾ç‰‡ä¿å­˜å®Œæˆ: batch_id={batch_id}, count={answer_count}")
                
                # ä¿å­˜è¯„åˆ†æ ‡å‡†å›¾ç‰‡
                if rubric_images:
                    rubric_count = await save_batch_images_concurrent(
                        batch_id=batch_id,
                        images=rubric_images,
                        image_type="rubric",
                        max_concurrent=10,
                    )
                    logger.info(f"[PG-Storage] è¯„åˆ†æ ‡å‡†ä¿å­˜å®Œæˆ: batch_id={batch_id}, count={rubric_count}")
                
            except Exception as e:
                logger.warning(f"[PG-Storage] PostgreSQL å­˜å‚¨å¤±è´¥ï¼Œå›é€€åˆ°æœ¬åœ°å­˜å‚¨: {e}")
                use_pg_storage = False  # å›é€€æ ‡è®°
        
        # å›é€€åˆ°æœ¬åœ°æ–‡ä»¶å­˜å‚¨ï¼ˆå¦‚æœ PostgreSQL å­˜å‚¨å¤±è´¥æˆ–ç¦ç”¨ï¼‰
        if not use_pg_storage and os.getenv("ENABLE_FILE_STORAGE", "true").lower() == "true":
            try:
                file_storage = get_file_storage_service()

                # ä¿å­˜ç­”é¢˜æ–‡ä»¶ï¼ˆä»¥å¤„ç†åçš„å›¾ç‰‡å½¢å¼ï¼‰
                answer_filenames = [f"answer_page_{i+1}.jpg" for i in range(len(answer_images))]
                stored_answers = await file_storage.save_answer_files(
                    batch_id=batch_id,
                    files=answer_images,
                    filenames=answer_filenames,
                )
                stored_files.extend(stored_answers)

                # ä¿å­˜è¯„åˆ†æ ‡å‡†æ–‡ä»¶ï¼ˆå¦‚æœæœ‰ï¼‰
                if rubric_images:
                    rubric_filenames = [f"rubric_page_{i+1}.jpg" for i in range(len(rubric_images))]
                    stored_rubrics = await file_storage.save_rubric_files(
                        batch_id=batch_id,
                        files=rubric_images,
                        filenames=rubric_filenames,
                    )
                    stored_files.extend(stored_rubrics)

                logger.info(
                    f"[FileStorage] æ–‡ä»¶å­˜å‚¨å®Œæˆ: batch_id={batch_id}, "
                    f"å…±ä¿å­˜ {len(stored_files)} ä¸ªæ–‡ä»¶"
                )
            except Exception as e:
                logger.warning(f"[FileStorage] æ–‡ä»¶å­˜å‚¨å¤±è´¥ï¼ˆä¸å½±å“æ‰¹æ”¹æµç¨‹ï¼‰: {e}")

        file_index_by_page: Dict[int, Dict[str, Any]] = {}
        answer_image_refs: List[Dict[str, Any]] = []
        rubric_image_refs: List[Dict[str, Any]] = []
        if stored_files:
            public_base = (
                os.getenv("BACKEND_PUBLIC_URL")
                or os.getenv("PUBLIC_BACKEND_URL")
                or os.getenv("PUBLIC_API_BASE_URL")
                or ""
            )

            def _build_file_url(file_id: str) -> str:
                if public_base:
                    return public_base.rstrip("/") + f"/api/batch/files/{file_id}/download"
                return f"/api/batch/files/{file_id}/download"

            for item in stored_files:
                meta = item.metadata or {}
                if meta.get("type") == "answer":
                    page_idx = meta.get("page_index")
                    if page_idx is not None:
                        file_url = _build_file_url(item.file_id)
                        file_index_by_page[int(page_idx)] = {
                            "file_id": item.file_id,
                            "content_type": item.content_type,
                            "file_url": file_url,
                        }
                        answer_image_refs.append(
                            {
                                "artifact_id": f"answer_page_{int(page_idx)}",
                                "uri": file_url,
                                "metadata": {
                                    "page_index": int(page_idx),
                                    "content_type": item.content_type,
                                    "source": "file_storage",
                                },
                            }
                        )
                elif meta.get("type") == "rubric":
                    file_url = _build_file_url(item.file_id)
                    rubric_image_refs.append(
                        {
                            "artifact_id": f"rubric_page_{meta.get('page_index', len(rubric_image_refs))}",
                            "uri": file_url,
                            "metadata": {
                                "page_index": meta.get("page_index"),
                                "content_type": item.content_type,
                                "source": "file_storage",
                            },
                        }
                    )



        # ğŸš€ ä½¿ç”¨ LangGraph Orchestrator å¯åŠ¨æ‰¹æ”¹æµç¨‹

        # è§£æå­¦ç”Ÿæ˜ å°„ï¼ˆç­çº§æ‰¹æ”¹æ¨¡å¼ï¼‰
        student_mapping = []
        if student_mapping_json:
            try:
                import json

                student_mapping = json.loads(student_mapping_json)
                logger.info(
                    f"ç­çº§æ‰¹æ”¹æ¨¡å¼: class_id={class_id}, homework_id={homework_id}, å­¦ç”Ÿæ•°={len(student_mapping)}"
                )
            except Exception as e:
                logger.debug(f"è§£æå­¦ç”Ÿæ˜ å°„å¤±è´¥: {e}")

        resolved_expected_students = expected_students or 0
        if resolved_expected_students <= 0:
            if student_mapping:
                resolved_expected_students = len(student_mapping)
            elif parsed_boundaries:
                resolved_expected_students = len(parsed_boundaries)
            else:
                resolved_expected_students = 1

        payload = {
            "batch_id": batch_id,
            "exam_id": exam_id,
            "temp_dir": str(temp_path),  # ä¸´æ—¶ç›®å½•ï¼ˆç”¨äºæ¸…ç†ï¼‰
            "rubric_images": rubric_images,
            "answer_images": answer_images,
            "answer_image_refs": answer_image_refs,
            "rubric_image_refs": rubric_image_refs,
            "file_index_by_page": file_index_by_page,
            "api_key": api_key,
            "teacher_id": teacher_key,
            # ç­çº§æ‰¹æ”¹ä¸Šä¸‹æ–‡ï¼ˆå¯é€‰ï¼‰
            "class_id": class_id,
            "homework_id": homework_id,
            "student_mapping": student_mapping,
            "inputs": {
                "rubric": "rubric_content",  # TODO: è§£æ rubric
                "auto_identify": auto_identify,
                "manual_boundaries": parsed_boundaries,  # ä¼ é€’äººå·¥è¾¹ç•Œ
                "expected_students": resolved_expected_students,
                "expected_total_score": expected_total_score,
                "enable_review": enable_review,
                "grading_mode": grading_mode or "auto",
                "teacher_id": teacher_key,
            },
        }

        # å¯åŠ¨ LangGraph batch_grading Graph
        logger.info(f"å‡†å¤‡å¯åŠ¨æ‰¹æ”¹ä»»åŠ¡: batch_id={batch_id}, answer_images={len(answer_images)}, rubric_images={len(rubric_images)}")
        
        task = asyncio.create_task(
            _start_run_with_teacher_limit(
                teacher_key=teacher_key,
                batch_id=batch_id,
                payload=payload,
                orchestrator=orchestrator,
                class_id=class_id,
                homework_id=homework_id,
                student_mapping=student_mapping,
            )
        )
        
        # æ·»åŠ é”™è¯¯å¤„ç†å›è°ƒ
        def task_done_callback(t):
            try:
                t.result()
            except asyncio.CancelledError:
                logger.info(f"Batch grading task cancelled: batch_id={batch_id}")
            except BaseException as e:
                logger.error(f"Batch grading task failed: batch_id={batch_id}, error={e}", exc_info=True)
        
        task.add_done_callback(task_done_callback)
        logger.info(f"æ‰¹æ”¹ä»»åŠ¡å·²æäº¤åˆ°äº‹ä»¶å¾ªç¯: batch_id={batch_id}")

        return BatchSubmissionResponse(
            batch_id=batch_id,
            status=SubmissionStatus.UPLOADED,
            total_pages=total_pages,
            estimated_completion_time=total_pages * 3,  # Estimated: 3s per page
        )

    except Exception as e:
        logger.error(f"æ‰¹é‡æäº¤å¤±è´¥: {str(e)}", exc_info=True)
        raise HTTPException(status_code=500, detail=f"æ‰¹é‡æäº¤å¤±è´¥: {str(e)}")


async def stream_langgraph_progress(
    batch_id: str,
    run_id: str,
    orchestrator: Orchestrator,
    class_id: Optional[str] = None,
    homework_id: Optional[str] = None,
    student_mapping: Optional[List[dict]] = None,
    teacher_key: Optional[str] = None,
):
    """
    æµå¼ç›‘å¬ LangGraph æ‰§è¡Œè¿›åº¦å¹¶æ¨é€åˆ° WebSocket

    è¿™æ˜¯å®ç°å®æ—¶è¿›åº¦æ¨é€çš„å…³é”®å‡½æ•°ï¼

    Args:
        batch_id: æ‰¹æ¬¡ ID
        run_id: LangGraph è¿è¡Œ ID
        orchestrator: LangGraph Orchestrator
    """
    logger.info(f"å¼€å§‹æµå¼ç›‘å¬ LangGraph è¿›åº¦: batch_id={batch_id}, run_id={run_id}")

    try:
        # ğŸ”¥ ä½¿ç”¨ LangGraph çš„æµå¼ API
        async for event in orchestrator.stream_run(run_id):
            event_type = event.get("type")
            node_name = event.get("node")
            data = event.get("data", {})

            logger.debug(
                f"LangGraph äº‹ä»¶: batch_id={batch_id}, type={event_type}, node={node_name}"
            )

            # å°† LangGraph äº‹ä»¶è½¬æ¢ä¸ºå‰ç«¯ WebSocket æ¶ˆæ¯
            if event_type == "node_start":
                await broadcast_progress(
                    batch_id,
                    {
                        "type": "workflow_update",
                        "nodeId": _map_node_to_frontend(node_name),
                        "status": "running",
                        "message": f"Running {_get_node_display_name(node_name)}...",
                    },
                )

            elif event_type == "node_end":
                await broadcast_progress(
                    batch_id,
                    {
                        "type": "workflow_update",
                        "nodeId": _map_node_to_frontend(node_name),
                        "status": "completed",
                        "message": f"{_get_node_display_name(node_name)} completed",
                    },
                )

                if node_name in ("confession", "logic_review"):
                    output = data.get("output", {})
                    student_count = None
                    if isinstance(output, dict):
                        if node_name == "confession":
                            student_count = len(
                                output.get("confessed_results")
                                or output.get("student_results")
                                or []
                            )
                        else:
                            student_count = len(
                                output.get("reviewed_results")
                                or output.get("student_results")
                                or []
                            )
                    logger.info(
                        f"[{node_name}] completed: batch_id={batch_id}, students={student_count}"
                    )

                # å¤„ç†èŠ‚ç‚¹è¾“å‡º
                output = data.get("output", {})
                if isinstance(output, dict):
                    interrupt_payload = output.get("__interrupt__")
                    if interrupt_payload:
                        review_type = (
                            interrupt_payload.get("type")
                            if isinstance(interrupt_payload, dict)
                            else "review_required"
                        )
                        await broadcast_progress(
                            batch_id,
                            {
                                "type": "review_required",
                                "reviewType": review_type,
                                "message": (
                                    interrupt_payload.get("message")
                                    if isinstance(interrupt_payload, dict)
                                    else None
                                ),
                                "payload": interrupt_payload,
                                "nodeId": (
                                    _map_node_to_frontend("grade_batch")
                                    if "grading_retry" in str(review_type)
                                    else (
                                        _map_node_to_frontend("rubric_review")
                                        if "rubric" in str(review_type)
                                        else _map_node_to_frontend("review")
                                    )
                                ),
                            },
                        )
                    # è¯„åˆ†æ ‡å‡†è§£æå®Œæˆ
                    if node_name == "rubric_parse" and output.get("parsed_rubric"):
                        parsed = output["parsed_rubric"]
                        await broadcast_progress(
                            batch_id,
                            {
                                "type": "rubric_parsed",
                                "totalQuestions": parsed.get("total_questions", 0),
                                "totalScore": parsed.get("total_score", 0),
                                "generalNotes": parsed.get("general_notes", ""),
                                "rubricFormat": parsed.get("rubric_format", ""),
                                "questions": [
                                    {
                                        "questionId": q.get("question_id", ""),
                                        "maxScore": q.get("max_score", 0),
                                        "questionText": q.get("question_text", ""),
                                        "standardAnswer": q.get("standard_answer", ""),
                                        "gradingNotes": q.get("grading_notes", ""),
                                        "sourcePages": q.get("source_pages")
                                        or q.get("sourcePages")
                                        or [],
                                        "scoringPoints": [
                                            {
                                                "pointId": sp.get("point_id")
                                                or sp.get("pointId")
                                                or f"{q.get('question_id')}.{idx + 1}",
                                                "description": sp.get("description", ""),
                                                "expectedValue": sp.get("expected_value")
                                                or sp.get("expectedValue", ""),
                                                "keywords": sp.get("keywords") or [],
                                                "score": sp.get("score", 0),
                                                "isRequired": sp.get("is_required", True),
                                            }
                                            for idx, sp in enumerate(q.get("scoring_points", []))
                                        ],
                                        "deductionRules": [
                                            {
                                                "ruleId": dr.get("rule_id")
                                                or dr.get("ruleId")
                                                or f"{q.get('question_id')}.d{idx + 1}",
                                                "description": dr.get("description", ""),
                                                "deduction": dr.get(
                                                    "deduction", dr.get("score", 0)
                                                ),
                                                "conditions": dr.get("conditions")
                                                or dr.get("when")
                                                or "",
                                            }
                                            for idx, dr in enumerate(
                                                q.get("deduction_rules")
                                                or q.get("deductionRules")
                                                or []
                                            )
                                        ],
                                        "alternativeSolutions": [
                                            {
                                                "description": alt.get("description", ""),
                                                "scoringCriteria": alt.get("scoring_criteria", ""),
                                                "note": alt.get("note", ""),
                                            }
                                            for alt in q.get("alternative_solutions", [])
                                        ],
                                    }
                                    for q in parsed.get("questions", [])
                                ],
                            },
                        )

                    # æ‰¹æ”¹æ‰¹æ¬¡å®Œæˆ
                    if node_name == "grade_batch" and output.get("grading_results"):
                        results = output["grading_results"]
                        completed = sum(1 for r in results if r.get("status") == "completed")

                        await broadcast_progress(
                            batch_id,
                            {
                                "type": "batch_complete",
                                "batchSize": len(results),
                                "successCount": completed,
                                "totalScore": sum(
                                    r.get("score", 0)
                                    for r in results
                                    if r.get("status") == "completed"
                                ),
                                "pages": [r.get("page_index") for r in results],
                            },
                        )

                    # å­¦ç”Ÿè¯†åˆ«å®Œæˆ
                    if output.get("student_boundaries"):
                        boundaries = output["student_boundaries"]
                        await broadcast_progress(
                            batch_id,
                            {
                                "type": "students_identified",
                                "studentCount": len(boundaries),
                                "students": [
                                    {
                                        "studentKey": b.get("student_key", ""),
                                        "startPage": b.get("start_page", 0),
                                        "endPage": b.get("end_page", 0),
                                        "confidence": b.get("confidence", 0),
                                        "needsConfirmation": b.get("needs_confirmation", False),
                                    }
                                    for b in boundaries
                                ],
                            },
                        )

                    # å®¡æ ¸å®Œæˆ
                    if node_name == "review" and output.get("review_summary"):
                        await broadcast_progress(
                            batch_id,
                            {"type": "review_completed", "summary": output["review_summary"]},
                        )

                    # è·¨é¡µé¢˜ç›®åˆå¹¶å®Œæˆ
                    if node_name == "cross_page_merge":
                        cross_page_questions = output.get("cross_page_questions", [])
                        merged_questions = output.get("merged_questions", [])
                        if cross_page_questions:
                            await broadcast_progress(
                                batch_id,
                                {
                                    "type": "cross_page_detected",
                                    "questions": cross_page_questions,
                                    "mergedCount": len(merged_questions),
                                    "crossPageCount": len(cross_page_questions),
                                },
                            )

            elif event_type == "paused":
                # å¤„ç† Graph ä¸­æ–­/æš‚åœï¼ˆé€šå¸¸æ˜¯éœ€è¦äººå·¥å®¡æ ¸ï¼‰
                data = event.get("data", {})
                interrupt_value = data.get("interrupt_value")

                logger.info(
                    f"LangGraph æš‚åœ: batch_id={batch_id}, interrupt_value={interrupt_value}"
                )

                if interrupt_value:
                    # å¦‚æœæœ‰ä¸­æ–­ payloadï¼Œå¹¿æ’­ review_required
                    review_type = (
                        interrupt_value.get("type")
                        if isinstance(interrupt_value, dict)
                        else "review_required"
                    )
                    await broadcast_progress(
                        batch_id,
                        {
                            "type": "review_required",
                            "reviewType": review_type,
                            "message": (
                                interrupt_value.get("message")
                                if isinstance(interrupt_value, dict)
                                else None
                            ),
                            "payload": interrupt_value,
                            "nodeId": (
                                _map_node_to_frontend("grade_batch")
                                if "grading_retry" in str(review_type)
                                else (
                                    _map_node_to_frontend("rubric_review")
                                    if "rubric" in str(review_type)
                                    else _map_node_to_frontend("review")
                                )
                            ),
                        },
                    )
                else:
                    # å¦‚æœæ²¡æœ‰ payloadï¼Œè‡³å°‘é€šçŸ¥çŠ¶æ€å˜æ›´
                    await broadcast_progress(
                        batch_id,
                        {
                            "type": "workflow_update",
                            "status": "paused",
                            "message": "Workflow paused (awaiting input)",
                        },
                    )

            elif event_type == "state_update":
                # æ¨é€çŠ¶æ€æ›´æ–°
                state = data.get("state", {})

                # æ‰¹æ¬¡è¿›åº¦æ›´æ–°
                if state.get("progress"):
                    progress = state["progress"]
                    await broadcast_progress(
                        batch_id,
                        {
                            "type": "batch_progress",
                            "batchIndex": progress.get("current_batch", 0),
                            "totalBatches": progress.get("total_batches", 1),
                            "successCount": progress.get("success_count", 0),
                            "failureCount": progress.get("failure_count", 0),
                        },
                    )

                # ç™¾åˆ†æ¯”è¿›åº¦
                if state.get("percentage"):
                    await broadcast_progress(
                        batch_id,
                        {
                            "type": "grading_progress",
                            "percentage": state["percentage"],
                            "currentStage": state.get("current_stage", ""),
                        },
                    )

            elif event_type == "llm_stream":
                # Real-time LLM token streaming
                node_name = event.get("node") or data.get("node", "")
                chunk = data.get("chunk") or data.get("content") or ""
                await broadcast_progress(
                    batch_id,
                    {
                        "type": "llm_stream_chunk",
                        "nodeId": _map_node_to_frontend(node_name) if node_name else None,
                        "nodeName": _get_node_display_name(node_name) if node_name else None,
                        "chunk": chunk,
                    },
                )

            elif event_type == "error":
                await broadcast_progress(
                    batch_id,
                    {"type": "workflow_error", "message": data.get("error", "Unknown error")},
                )

            elif event_type == "completed":
                # å·¥ä½œæµå®Œæˆ - è·å–å®Œæ•´çš„æœ€ç»ˆçŠ¶æ€
                final_state = data.get("state", {})

                # ä» student_results è·å–ç»“æœ
                student_results = final_state.get("student_results", [])

                # å¦‚æœæ²¡æœ‰ student_resultsï¼Œå°è¯•ä» orchestrator è·å–æœ€ç»ˆè¾“å‡º
                if not student_results:
                    try:
                        final_output = await orchestrator.get_final_output(run_id)
                        if final_output:
                            student_results = final_output.get("student_results", [])
                            logger.info(f"ä» orchestrator è·å–åˆ° {len(student_results)} ä¸ªå­¦ç”Ÿç»“æœ")
                    except Exception as e:
                        logger.debug(f"è·å–æœ€ç»ˆè¾“å‡ºå¤±è´¥: {e}")

                if not student_results:
                    grading_results = final_state.get("grading_results") or []
                    if not grading_results:
                        try:
                            final_output = await orchestrator.get_final_output(run_id)
                            if final_output:
                                grading_results = final_output.get("grading_results") or []
                        except Exception as e:
                            logger.debug(f"é‘¾å³°å½‡ grading_results æ¾¶è¾«è§¦: {e}")
                    if grading_results:
                        student_results = _build_student_results_from_grading_results(grading_results)
                        logger.info(
                            f"æµ ?grading_results é­ãˆ î˜² {len(student_results)} æ¶“î„î„Ÿé¢ç†ºç²¨é‹?"
                        )

                formatted_results = _format_results_for_frontend(student_results)
                class_report = final_state.get("class_report")
                if not class_report and final_state.get("export_data"):
                    class_report = final_state.get("export_data", {}).get("class_report")

                # ä¿å­˜æ‰¹æ”¹å†å²ä¸å­¦ç”Ÿç»“æœï¼ˆæ”¯æŒç­çº§/éç­çº§æ¨¡å¼ï¼‰
                try:
                    logger.info(
                        f"ä¿å­˜æ‰¹æ”¹ç»“æœ: batch_id={batch_id}, class_id={class_id}, homework_id={homework_id}"
                    )

                    now = datetime.now().isoformat()
                    scores = [
                        s.get("score")
                        for s in formatted_results
                        if isinstance(s.get("score"), (int, float))
                    ]
                    average_score = None
                    if isinstance(class_report, dict):
                        average_score = class_report.get("average_score")
                    if average_score is None and scores:
                        average_score = round(sum(scores) / len(scores), 2)

                    # å…ˆæ£€æŸ¥æ˜¯å¦å·²ç»å­˜åœ¨æ‰¹æ”¹å†å²è®°å½•
                    existing_history = await get_grading_history(batch_id)
                    if existing_history:
                        history_id = existing_history.id
                        logger.info(f"ä½¿ç”¨å·²å­˜åœ¨çš„æ‰¹æ”¹å†å²: history_id={history_id}")
                    else:
                        history_id = str(uuid.uuid4())
                        history = GradingHistory(
                            id=history_id,
                            batch_id=batch_id,
                            teacher_id=teacher_key,
                            status="completed",
                            class_ids=[class_id] if class_id else None,
                            created_at=now,
                            completed_at=now,
                            total_students=len(formatted_results),
                            average_score=average_score,
                            result_data=(
                                {
                                    "summary": class_report,
                                    "class_id": class_id,
                                    "homework_id": homework_id,
                                }
                                if class_report or class_id or homework_id
                                else None
                            ),
                        )
                        await _maybe_await(save_grading_history(history))
                        logger.info(f"åˆ›å»ºæ–°çš„æ‰¹æ”¹å†å²: history_id={history_id}")

                    student_map_by_index = {}
                    student_map_by_name = {}
                    if student_mapping:
                        for idx, mapping in enumerate(student_mapping):
                            student_map_by_index[idx] = mapping
                            name_key = (mapping.get("studentName") or "").strip().lower()
                            if name_key:
                                student_map_by_name[name_key] = mapping

                    roster = list_class_students(class_id) if class_id else []
                    roster_by_name = {
                        (student.name or student.username or "").strip().lower(): student
                        for student in roster
                        if (student.name or student.username)
                    }

                    for idx, result in enumerate(formatted_results):
                        student_name = (
                            result.get("studentName")
                            or result.get("student_name")
                            or f"Student {idx + 1}"
                        )
                        student_id = result.get("studentId") or result.get("student_id")

                        if not student_id and student_map_by_index.get(idx):
                            mapping = student_map_by_index[idx]
                            student_id = mapping.get("studentId")
                            student_name = mapping.get("studentName") or student_name
                        if not student_id and student_name:
                            mapping = student_map_by_name.get(student_name.strip().lower())
                            if mapping:
                                student_id = mapping.get("studentId")
                                student_name = mapping.get("studentName") or student_name
                        if not student_id and student_name:
                            roster_hit = roster_by_name.get(student_name.strip().lower())
                            if roster_hit:
                                student_id = roster_hit.id
                                student_name = (
                                    roster_hit.name or roster_hit.username or student_name
                                )
                        if not student_id and class_id and idx < len(roster):
                            roster_hit = roster[idx]
                            student_id = roster_hit.id
                            student_name = roster_hit.name or roster_hit.username or student_name
                        if not student_id and class_id:
                            student_id = f"auto-{idx + 1}"

                        student_summary = (
                            result.get("studentSummary") or result.get("student_summary") or {}
                        )
                        confession_payload = result.get("confession")
                        # Keep student_id whenever available so student-side pages can match records
                        student_id_value = student_id or None
                        student_result = StudentGradingResult(
                            id=_make_student_result_id(history_id, student_name, student_id_value),
                            grading_history_id=history_id,
                            student_key=student_name,
                            score=result.get("score"),
                            max_score=result.get("maxScore") or result.get("max_score"),
                            class_id=class_id,
                            student_id=student_id_value,
                            summary=(
                                student_summary.get("overall")
                                if isinstance(student_summary, dict)
                                else None
                            ),
                            confession=confession_payload,
                            result_data=result,
                        )
                        await _maybe_await(save_student_result(student_result))

                        if class_id and homework_id and student_id:
                            feedback = None
                            if isinstance(student_summary, dict):
                                feedback = student_summary.get("overall")
                            upsert_homework_submission_grade(
                                class_id=class_id,
                                homework_id=homework_id,
                                student_id=student_id,
                                student_name=student_name,
                                score=result.get("score"),
                                feedback=feedback,
                                grading_batch_id=batch_id,
                            )

                    logger.info(f"æ‰¹æ”¹ç»“æœå·²ä¿å­˜: history_id={history_id}")
                except Exception as e:
                    logger.error(f"ä¿å­˜æ‰¹æ”¹ç»“æœå¤±è´¥: {e}", exc_info=True)

                await broadcast_progress(
                    batch_id,
                    {
                        "type": "workflow_completed",
                        "message": f"Grading completed, processed {len(formatted_results)} students",
                        "results": formatted_results,
                        "classReport": class_report,
                    },
                )

        logger.info(f"LangGraph è¿›åº¦æµå¼ä¼ è¾“å®Œæˆ: batch_id={batch_id}")

    except Exception as e:
        logger.error(f"æµå¼ä¼ è¾“å¤±è´¥: batch_id={batch_id}, error={str(e)}", exc_info=True)
        await broadcast_progress(
            batch_id, {"type": "workflow_error", "message": f"æµå¼ä¼ è¾“å¤±è´¥: {str(e)}"}
        )

    finally:
        run_controller = await get_run_controller()
        if run_controller and teacher_key:
            try:
                run_info = await orchestrator.get_status(run_id)
                status_value = (
                    run_info.status.value
                    if hasattr(run_info.status, "value")
                    else str(run_info.status)
                )
                if status_value in ("completed", "failed", "cancelled"):
                    await run_controller.update_run(
                        batch_id,
                        {
                            "status": "completed" if status_value == "completed" else "failed",
                            "completed_at": datetime.now().isoformat(),
                            "updated_at": datetime.now().isoformat(),
                        },
                    )
                    await run_controller.release_slot(teacher_key, batch_id)
            except Exception:
                await run_controller.release_slot(teacher_key, batch_id)
        current_task = asyncio.current_task()
        if current_task and _active_stream_tasks.get(batch_id) is current_task:
            _active_stream_tasks.pop(batch_id, None)


async def _ensure_stream_task(
    *,
    batch_id: str,
    run_id: str,
    orchestrator: Orchestrator,
    class_id: Optional[str] = None,
    homework_id: Optional[str] = None,
    student_mapping: Optional[List[dict]] = None,
    teacher_key: Optional[str] = None,
) -> None:
    existing = _active_stream_tasks.get(batch_id)
    if existing and not existing.done():
        return
    stream_task = asyncio.create_task(
        stream_langgraph_progress(
            batch_id=batch_id,
            run_id=run_id,
            orchestrator=orchestrator,
            class_id=class_id,
            homework_id=homework_id,
            student_mapping=student_mapping,
            teacher_key=teacher_key,
        )
    )
    _active_stream_tasks[batch_id] = stream_task


async def resume_orphaned_streams(orchestrator: Optional[Orchestrator]) -> None:
    if not orchestrator:
        return
    try:
        running_runs = await orchestrator.list_runs(
            graph_name="batch_grading", status=RunStatus.RUNNING, limit=50
        )
        pending_runs = await orchestrator.list_runs(
            graph_name="batch_grading", status=RunStatus.PENDING, limit=50
        )
    except Exception as exc:
        logger.debug("Failed to list runs for stream recovery: %s", exc)
        return

    run_controller = await get_run_controller()
    for run_info in [*running_runs, *pending_runs]:
        run_id = run_info.run_id
        if not run_id.startswith("batch_grading_"):
            continue
        batch_id = run_id.replace("batch_grading_", "", 1)
        teacher_key = None
        class_id = None
        homework_id = None
        if run_controller:
            snapshot = await run_controller.get_run(batch_id)
            if snapshot:
                teacher_key = snapshot.teacher_id
                class_id = snapshot.class_id
                homework_id = snapshot.homework_id
        await _ensure_stream_task(
            batch_id=batch_id,
            run_id=run_id,
            orchestrator=orchestrator,
            class_id=class_id,
            homework_id=homework_id,
            student_mapping=None,
            teacher_key=teacher_key,
        )


def _map_node_to_frontend(node_name: str) -> str:
    """å°† LangGraph èŠ‚ç‚¹åç§°æ˜ å°„åˆ°å‰ç«¯èŠ‚ç‚¹ ID

    å‰ç«¯å·¥ä½œæµèŠ‚ç‚¹ï¼ˆconsoleStore.ts initialNodesï¼‰ï¼š
    - intake: æ¥æ”¶æ–‡ä»¶
    - rubric_parse: è§£æè¯„åˆ†æ ‡å‡†
    - grade_batch: åˆ†æ‰¹å¹¶è¡Œæ‰¹æ”¹ï¼ˆisParallelContainerï¼‰
    - cross_page_merge: è·¨é¡µé¢˜ç›®åˆå¹¶
    - index: æ‰¹æ”¹å‰ç´¢å¼•
    - index_merge: ç´¢å¼•èšåˆ
    - export: å¯¼å‡ºç»“æœ
    """
    mapping = {
        # ä¸»è¦èŠ‚ç‚¹ï¼ˆä¸åç«¯ batch_grading.py å®Œå…¨å¯¹åº”ï¼‰
        "intake": "intake",
        "rubric_parse": "rubric_parse",
        "rubric_review": "rubric_review",
        "grade_batch": "grade_batch",
        "cross_page_merge": "cross_page_merge",
        "index": "index",
        "index_merge": "index_merge",
        "segment": "index_merge",
        "review": "review",
        "logic_review": "logic_review",
        "export": "export",
        # å…¼å®¹æ—§åç§°
        "detect_boundaries": "index",
        "grade_student": "grade_batch",
        "grading": "grade_batch",
        "aggregate": "review",
        "batch_persist": "export",
        "batch_notify": "export",
    }
    if node_name in mapping:
        return mapping[node_name]
    if ":" in node_name:
        base = node_name.split(":", 1)[0]
        if base in mapping:
            return mapping[base]
    if "." in node_name:
        base = node_name.split(".", 1)[0]
        if base in mapping:
            return mapping[base]
    return node_name


def _get_node_display_name(node_name: str) -> str:
    """è·å–èŠ‚ç‚¹çš„æ˜¾ç¤ºåç§°ï¼ˆä¸­æ–‡ï¼‰"""
    display_names = {
        "intake": "Ingest",
        "preprocess": "Preprocess",
        "index": "Index",
        "rubric_parse": "Rubric Parse",
        "rubric_self_review": "Auto Review",
        "rubric_review": "Rubric Review",
        "grading_fanout": "Batch Fanout",
        "grade_batch": "Batch Grading",
        "cross_page_merge": "Cross-Page Merge",
        "logic_review": "Logic Review",
        "index_merge": "Index Merge",
        "segment": "Index Merge",
        "review": "Final Review",
        "export": "Export",
    }
    if node_name in display_names:
        return display_names[node_name]
    if ":" in node_name:
        base = node_name.split(":", 1)[0]
        if base in display_names:
            return display_names[base]
    if "." in node_name:
        base = node_name.split(".", 1)[0]
        if base in display_names:
            return display_names[base]
    return node_name


def _safe_float(value: Any, default: float = 0.0) -> float:
    try:
        if value is None:
            return default
        return float(value)
    except (TypeError, ValueError):
        return default


def _make_student_result_id(
    grading_history_id: str,
    student_key: Optional[str],
    student_id: Optional[str] = None,
) -> str:
    seed = student_id or student_key or "unknown"
    return str(uuid.uuid5(uuid.NAMESPACE_URL, f"{grading_history_id}:{seed}"))


def _resolve_page_indices(question: Dict[str, Any], fallback_page_index: Optional[int] = None) -> List[int]:
    pages = question.get("page_indices") or question.get("pageIndices")
    if not pages:
        page_index = question.get("page_index")
        if page_index is None:
            page_index = question.get("pageIndex")
        if page_index is None:
            page_index = fallback_page_index
        if page_index is not None:
            pages = [page_index]
    if isinstance(pages, (list, tuple)):
        return [int(p) for p in pages if isinstance(p, (int, float, str)) and str(p).strip() != ""]
    if pages is None:
        return []
    try:
        return [int(pages)]
    except (TypeError, ValueError):
        return []


def _resolve_question_max_score(
    question: Dict[str, Any], scoring_results: List[Dict[str, Any]]
) -> float:
    max_score = _safe_float(
        question.get("max_score")
        or question.get("maxScore")
        or question.get("max_points")
        or question.get("maxPoints"),
        default=0.0,
    )
    if max_score > 0:
        return max_score
    total_points = 0.0
    for item in scoring_results or []:
        max_points = item.get("max_points") or item.get("maxPoints")
        if max_points is None:
            scoring_point = item.get("scoring_point") or item.get("scoringPoint") or {}
            max_points = scoring_point.get("score") or scoring_point.get("points")
        total_points += _safe_float(max_points, default=0.0)
    return total_points


def _resolve_question_confidence(
    question: Dict[str, Any],
    scoring_results: List[Dict[str, Any]],
    *,
    score: float,
    max_score: float,
) -> float:
    raw_confidence = question.get("confidence")
    if raw_confidence is None:
        raw_confidence = question.get("confidence_score")
        if raw_confidence is None:
            raw_confidence = question.get("confidenceScore")
    confidence = _safe_float(raw_confidence, default=0.0)
    has_signal = bool(
        scoring_results
        or question.get("student_answer")
        or question.get("studentAnswer")
        or question.get("feedback")
    )
    if confidence <= 0 and has_signal:
        if max_score > 0:
            confidence = max(0.1, min(1.0, score / max_score))
        elif scoring_results:
            total_points = 0.0
            awarded_points = 0.0
            for item in scoring_results:
                max_points = item.get("max_points") or item.get("maxPoints")
                if max_points is None:
                    scoring_point = item.get("scoring_point") or item.get("scoringPoint") or {}
                    max_points = scoring_point.get("score")
                total_points += _safe_float(max_points, default=0.0)
                awarded_points += _safe_float(item.get("awarded") or item.get("score"), default=0.0)
            if total_points > 0:
                confidence = max(0.1, min(1.0, awarded_points / total_points))
    if confidence <= 0 and has_signal:
        confidence = 0.6 if score > 0 else 0.35
    return max(0.0, min(1.0, confidence))


def _derive_confession_overall_status(confession: Dict[str, Any]) -> str:
    """Derive ok/caution/needs_review for ConfessionReport v1 (frontend convenience)."""
    try:
        items = confession.get("items") or []
        if not isinstance(items, list):
            items = []
        error_count = sum(
            1 for i in items if isinstance(i, dict) and str(i.get("severity", "")).lower() == "error"
        )
        warning_count = sum(
            1
            for i in items
            if isinstance(i, dict) and str(i.get("severity", "")).lower() == "warning"
        )
        risk = confession.get("risk_score")
        if risk is None:
            risk = confession.get("riskScore")
        risk_v = _safe_float(risk, default=0.0)
        if error_count > 0 or risk_v >= 0.6:
            return "needs_review"
        if warning_count >= 3 or risk_v >= 0.3:
            return "caution"
        return "ok"
    except Exception:
        return "caution"


def _build_student_results_from_grading_results(
    grading_results: List[Dict[str, Any]],
) -> List[Dict[str, Any]]:
    if not grading_results:
        return []
    try:
        from src.graphs.batch_grading import _build_student_results_from_page_results

        return _build_student_results_from_page_results(grading_results)
    except Exception as exc:
        logger.debug(f"Failed to rebuild student_results from grading_results: {exc}")
        return []


def _merge_question_results(existing: Dict[str, Any], incoming: Dict[str, Any]) -> Dict[str, Any]:
    merged = existing.copy()
    merged["score"] = max(_safe_float(existing.get("score", 0)), _safe_float(incoming.get("score", 0)))
    merged["maxScore"] = max(
        _safe_float(existing.get("maxScore", 0)),
        _safe_float(incoming.get("maxScore", 0)),
    )
    if not merged.get("feedback") and incoming.get("feedback"):
        merged["feedback"] = incoming.get("feedback")
    if not merged.get("studentAnswer") and incoming.get("studentAnswer"):
        merged["studentAnswer"] = incoming.get("studentAnswer")
    merged_conf = _safe_float(existing.get("confidence", 0))
    incoming_conf = _safe_float(incoming.get("confidence", 0))
    merged["confidence"] = max(merged_conf, incoming_conf)
    pages = set(existing.get("page_indices") or existing.get("pageIndices") or [])
    pages.update(incoming.get("page_indices") or incoming.get("pageIndices") or [])
    if pages:
        merged["page_indices"] = sorted(pages)
        merged["pageIndices"] = sorted(pages)
    if not merged.get("scoring_point_results") and incoming.get("scoring_point_results"):
        merged["scoring_point_results"] = incoming.get("scoring_point_results")
    if not merged.get("steps") and incoming.get("steps"):
        merged["steps"] = incoming.get("steps")
    return merged


def _dedupe_formatted_results(results: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
    grouped: Dict[str, Dict[str, Any]] = {}
    for result in results:
        student_id = result.get("studentId") or result.get("student_id")
        student_name = result.get("studentName") or result.get("student_name") or ""
        student_key = result.get("studentKey") or result.get("student_key")
        start_page = result.get("startPage")
        end_page = result.get("endPage")
        key: Optional[str] = None
        if student_id:
            key = f"id:{student_id}"
        elif student_name and (start_page is not None or end_page is not None):
            key = f"name:{student_name}:{start_page}-{end_page}"
        elif student_name:
            key = f"name:{student_name}"
        elif student_key:
            key = f"key:{student_key}"
        if not key:
            key = str(len(grouped))
        entry = grouped.get(key)
        if not entry:
            grouped[key] = result
            continue

        merged = entry.copy()
        merged_questions: Dict[str, Dict[str, Any]] = {}
        for q in entry.get("questionResults", []) or []:
            qid = str(q.get("questionId") or q.get("question_id") or "")
            if not qid:
                qid = f"idx:{len(merged_questions)}"
            merged_questions[qid] = q
        for q in result.get("questionResults", []) or []:
            qid = str(q.get("questionId") or q.get("question_id") or "")
            if not qid:
                qid = f"idx:{len(merged_questions)}"
            if qid in merged_questions:
                merged_questions[qid] = _merge_question_results(merged_questions[qid], q)
            else:
                merged_questions[qid] = q
        merged["questionResults"] = list(merged_questions.values())

        # Prefer newer post-processing fields when the current entry is missing them.
        for field in (
            "confession",
            "logicReview",
            "logicReviewedAt",
            "draftQuestionDetails",
            "draftTotalScore",
            "draftMaxScore",
        ):
            if not merged.get(field) and result.get(field):
                merged[field] = result.get(field)

        merged_start = merged.get("startPage")
        merged_end = merged.get("endPage")
        candidate_start = result.get("startPage")
        candidate_end = result.get("endPage")
        if merged_start is not None and candidate_start is not None:
            merged["startPage"] = min(merged_start, candidate_start)
        elif merged_start is None and candidate_start is not None:
            merged["startPage"] = candidate_start
        if merged_end is not None and candidate_end is not None:
            merged["endPage"] = max(merged_end, candidate_end)
        elif merged_end is None and candidate_end is not None:
            merged["endPage"] = candidate_end

        if merged.get("startPage") is not None:
            if merged.get("endPage") is not None and merged["endPage"] != merged["startPage"]:
                merged["pageRange"] = f"{merged['startPage'] + 1}-{merged['endPage'] + 1}"
            else:
                merged["pageRange"] = str(merged["startPage"] + 1)

        question_scores = [
            _safe_float(q.get("score", 0)) for q in merged.get("questionResults", []) or []
        ]
        question_max = [
            _safe_float(q.get("maxScore", 0)) for q in merged.get("questionResults", []) or []
        ]
        if question_scores:
            merged["score"] = sum(question_scores)
        if question_max:
            merged["maxScore"] = sum(question_max)

        grouped[key] = merged
    return list(grouped.values())


def _format_results_for_frontend(results: List[Dict]) -> List[Dict]:
    """æ ¼å¼åŒ–æ‰¹æ”¹ç»“æœä¸ºå‰ç«¯æ ¼å¼"""
    formatted = []
    for r in results:
        # å¤„ç† question_details æ ¼å¼
        question_results = []

        # ä¼˜å…ˆä½¿ç”¨ question_details
        if r.get("question_details"):
            for q in r.get("question_details", []):
                scoring_results = q.get("scoring_point_results") or q.get("scoring_results") or []
                score_value = _safe_float(q.get("score", 0))
                max_score_value = _resolve_question_max_score(q, scoring_results)
                page_indices = _resolve_page_indices(q)
                confidence = _resolve_question_confidence(
                    q,
                    scoring_results,
                    score=score_value,
                    max_score=max_score_value,
                )
                question_results.append(
                    {
                        "questionId": str(q.get("question_id", "")),
                        "score": score_value,
                        "maxScore": max_score_value,
                        "feedback": q.get("feedback", ""),
                        "confidence": confidence,
                        "confidence_reason": q.get("confidence_reason")
                        or q.get("confidenceReason"),
                        "rubric_refs": q.get("rubric_refs") or q.get("rubricRefs"),
                        "review_summary": q.get("review_summary") or q.get("reviewSummary"),
                        "review_corrections": q.get("review_corrections")
                        or q.get("reviewCorrections"),
                        "needsReview": (
                            q.get("needs_review")
                            if q.get("needs_review") is not None
                            else q.get("needsReview", False)
                        ),
                        "reviewReasons": (
                            q.get("review_reasons")
                            if q.get("review_reasons") is not None
                            else q.get("reviewReasons") or []
                        ),
                        "confessionItems": q.get("confession_items")
                        or q.get("confessionItems")
                        or [],
                        "typo_notes": q.get("typo_notes") or q.get("typoNotes"),
                        "studentAnswer": q.get("student_answer", ""),
                        "question_type": q.get("question_type") or q.get("questionType"),
                        "isCorrect": q.get("is_correct", False),
                        "scoring_point_results": scoring_results,
                        "page_indices": page_indices,
                        "is_cross_page": q.get("is_cross_page", False),
                        "merge_source": q.get("merge_source"),
                        # ğŸ”¥ æ‰¹æ³¨åæ ‡å­—æ®µï¼ˆåç«¯ä¸å†è¿”å›ï¼Œæ”¹ä¸ºå‰ç«¯æ¸²æŸ“/æŒ‰éœ€ç”Ÿæˆï¼‰
                        "annotations": [],
                        "steps": q.get("steps") or [],
                        "answerRegion": q.get("answer_region") or q.get("answerRegion"),
                    }
                )
        # å…¼å®¹æ—§æ ¼å¼ grading_results
        elif r.get("grading_results"):
            for q in r.get("grading_results", []):
                scoring_results = q.get("scoring_point_results") or q.get("scoring_results") or []
                score_value = _safe_float(q.get("score", 0))
                max_score_value = _resolve_question_max_score(q, scoring_results)
                page_indices = _resolve_page_indices(q)
                confidence = _resolve_question_confidence(
                    q,
                    scoring_results,
                    score=score_value,
                    max_score=max_score_value,
                )
                question_results.append(
                    {
                        "questionId": str(q.get("question_id", "")),
                        "score": score_value,
                        "maxScore": max_score_value,
                        "feedback": q.get("feedback", ""),
                        "confidence": confidence,
                        "confidence_reason": q.get("confidence_reason")
                        or q.get("confidenceReason"),
                        "rubric_refs": q.get("rubric_refs") or q.get("rubricRefs"),
                        "review_summary": q.get("review_summary") or q.get("reviewSummary"),
                        "review_corrections": q.get("review_corrections")
                        or q.get("reviewCorrections"),
                        "needsReview": (
                            q.get("needs_review")
                            if q.get("needs_review") is not None
                            else q.get("needsReview", False)
                        ),
                        "reviewReasons": (
                            q.get("review_reasons")
                            if q.get("review_reasons") is not None
                            else q.get("reviewReasons") or []
                        ),
                        "confessionItems": q.get("confession_items")
                        or q.get("confessionItems")
                        or [],
                        "typo_notes": q.get("typo_notes") or q.get("typoNotes"),
                        "studentAnswer": q.get("student_answer", ""),
                        "question_type": q.get("question_type") or q.get("questionType"),
                        "scoring_point_results": scoring_results,
                        "page_indices": page_indices,
                        "is_cross_page": q.get("is_cross_page", False),
                        "merge_source": q.get("merge_source"),
                        # ğŸ”¥ æ‰¹æ³¨åæ ‡å­—æ®µï¼ˆåç«¯ä¸å†è¿”å›ï¼Œæ”¹ä¸ºå‰ç«¯æ¸²æŸ“/æŒ‰éœ€ç”Ÿæˆï¼‰
                        "annotations": [],
                        "steps": q.get("steps") or [],
                        "answerRegion": q.get("answer_region") or q.get("answerRegion"),
                    }
                )
        # Ã¥â€¦Â¼Ã¥Â®Â¹ export_data Ã§Å¡â€ question_results
        elif r.get("question_results") or r.get("questionResults"):
            raw_question_results = r.get("question_results") or r.get("questionResults") or []
            for q in raw_question_results:
                scoring_results = (
                    q.get("scoring_point_results")
                    or q.get("scoring_results")
                    or q.get("scoringPointResults")
                    or []
                )
                score_value = _safe_float(q.get("score", 0))
                max_score_value = _resolve_question_max_score(q, scoring_results)
                page_indices = _resolve_page_indices(q)
                confidence = _resolve_question_confidence(
                    q,
                    scoring_results,
                    score=score_value,
                    max_score=max_score_value,
                )
                question_results.append(
                    {
                        "questionId": str(q.get("question_id") or q.get("questionId") or ""),
                        "score": score_value,
                        "maxScore": max_score_value,
                        "feedback": q.get("feedback", ""),
                        "confidence": confidence,
                        "confidence_reason": q.get("confidence_reason")
                        or q.get("confidenceReason"),
                        "rubric_refs": q.get("rubric_refs") or q.get("rubricRefs"),
                        "review_summary": q.get("review_summary") or q.get("reviewSummary"),
                        "review_corrections": q.get("review_corrections")
                        or q.get("reviewCorrections"),
                        "needsReview": (
                            q.get("needs_review")
                            if q.get("needs_review") is not None
                            else q.get("needsReview", False)
                        ),
                        "reviewReasons": (
                            q.get("review_reasons")
                            if q.get("review_reasons") is not None
                            else q.get("reviewReasons") or []
                        ),
                        "confessionItems": q.get("confession_items")
                        or q.get("confessionItems")
                        or [],
                        "typo_notes": q.get("typo_notes") or q.get("typoNotes"),
                        "studentAnswer": q.get("student_answer") or q.get("studentAnswer") or "",
                        "question_type": q.get("question_type") or q.get("questionType"),
                        "isCorrect": q.get("is_correct", False),
                        "scoring_point_results": scoring_results,
                        "page_indices": page_indices,
                        "is_cross_page": q.get("is_cross_page", False),
                        "merge_source": q.get("merge_source"),
                        # ğŸ”¥ æ‰¹æ³¨åæ ‡å­—æ®µï¼ˆåç«¯ä¸å†è¿”å›ï¼Œæ”¹ä¸ºå‰ç«¯æ¸²æŸ“/æŒ‰éœ€ç”Ÿæˆï¼‰
                        "annotations": [],
                        "steps": q.get("steps") or [],
                        "answerRegion": q.get("answer_region") or q.get("answerRegion"),
                    }
                )
        # ä» page_results æå–
        elif r.get("page_results"):
            for page in r.get("page_results", []):
                if page.get("status") == "completed":
                    # ä»é¡µé¢ç»“æœä¸­æå–é¢˜ç›®è¯¦æƒ…
                    for q in page.get("question_details", []):
                        scoring_results = (
                            q.get("scoring_point_results") or q.get("scoring_results") or []
                        )
                        score_value = _safe_float(q.get("score", 0))
                        max_score_value = _resolve_question_max_score(q, scoring_results)
                        page_indices = _resolve_page_indices(q, page.get("page_index"))
                        confidence = _resolve_question_confidence(
                            q,
                            scoring_results,
                            score=score_value,
                            max_score=max_score_value,
                        )
                        question_results.append(
                            {
                                "questionId": str(q.get("question_id", "")),
                                "score": score_value,
                                "maxScore": max_score_value,
                                "feedback": q.get("feedback", ""),
                                "confidence": confidence,
                                "confidence_reason": q.get("confidence_reason")
                                or q.get("confidenceReason"),
                                "rubric_refs": q.get("rubric_refs") or q.get("rubricRefs"),
                                "review_summary": q.get("review_summary") or q.get("reviewSummary"),
                                "review_corrections": q.get("review_corrections")
                                or q.get("reviewCorrections"),
                                "needsReview": (
                                    q.get("needs_review")
                                    if q.get("needs_review") is not None
                                    else q.get("needsReview", False)
                                ),
                                "reviewReasons": (
                                    q.get("review_reasons")
                                    if q.get("review_reasons") is not None
                                    else q.get("reviewReasons") or []
                                ),
                                "confessionItems": q.get("confession_items")
                                or q.get("confessionItems")
                                or [],
                                "typo_notes": q.get("typo_notes") or q.get("typoNotes"),
                                "studentAnswer": q.get("student_answer", ""),
                                "question_type": q.get("question_type") or q.get("questionType"),
                                "isCorrect": q.get("is_correct", False),
                                "scoring_point_results": scoring_results,
                                "page_indices": page_indices or [],
                                "is_cross_page": q.get("is_cross_page", False),
                                "merge_source": q.get("merge_source"),
                                # ğŸ”¥ æ‰¹æ³¨åæ ‡å­—æ®µï¼ˆåç«¯ä¸å†è¿”å›ï¼Œæ”¹ä¸ºå‰ç«¯æ¸²æŸ“/æŒ‰éœ€ç”Ÿæˆï¼‰
                                "annotations": [],
                                "steps": q.get("steps") or [],
                                "answerRegion": q.get("answer_region") or q.get("answerRegion"),
                            }
                        )

        computed_score = sum(_safe_float(q.get("score", 0)) for q in question_results)
        computed_max = sum(_safe_float(q.get("maxScore", 0)) for q in question_results)
        raw_score = _safe_float(r.get("total_score", r.get("score", 0)))
        raw_max = _safe_float(r.get("max_total_score", r.get("max_score", 0)))
        final_score = raw_score if raw_score > 0 or computed_score <= 0 else computed_score
        final_max = raw_max if raw_max > 0 or computed_max <= 0 else computed_max

        student_summary = r.get("student_summary") or r.get("studentSummary")
        confession_raw = r.get("confession")
        if isinstance(confession_raw, str):
            try:
                confession_raw = json.loads(confession_raw)
            except Exception:
                confession_raw = None

        logic_review_raw = r.get("logic_review") or r.get("logicReview")
        if isinstance(logic_review_raw, str):
            try:
                logic_review_raw = json.loads(logic_review_raw)
            except Exception:
                logic_review_raw = None

        # æ ‡å‡†åŒ– confession æ ¼å¼ï¼Œç¡®ä¿å‰ç«¯èƒ½æ­£ç¡®æ˜¾ç¤º
        confession = None
        if confession_raw and isinstance(confession_raw, dict):
            confession = {}
            # å¤åˆ¶æ‰€æœ‰åŸå§‹å­—æ®µ
            confession.update(confession_raw)
            # ConfessionReport v1: add frontend convenience fields.
            if confession_raw.get("version") == "confession_report_v1":
                if "overallStatus" not in confession:
                    confession["overallStatus"] = _derive_confession_overall_status(confession_raw)
                if "overallConfidence" not in confession and "overall_confidence" in confession_raw:
                    confession["overallConfidence"] = confession_raw.get("overall_confidence")
                if "riskScore" not in confession and "risk_score" in confession_raw:
                    confession["riskScore"] = confession_raw.get("risk_score")

        # ğŸ”¥ ç¬¬ä¸€æ¬¡æ‰¹æ”¹è®°å½•ï¼ˆé€»è¾‘å¤æ ¸å‰çš„åŸå§‹ç»“æœï¼‰
        draft_question_details = r.get("draft_question_details") or r.get("draftQuestionDetails")
        draft_question_results = []
        if draft_question_details:
            for dq in draft_question_details:
                draft_scoring_results = (
                    dq.get("scoring_point_results") or dq.get("scoring_results") or []
                )
                draft_score_value = _safe_float(dq.get("score", 0))
                draft_max_score_value = _resolve_question_max_score(
                    dq, draft_scoring_results
                )
                draft_page_indices = _resolve_page_indices(dq)
                draft_confidence = _resolve_question_confidence(
                    dq,
                    draft_scoring_results,
                    score=draft_score_value,
                    max_score=draft_max_score_value,
                )
                draft_question_results.append(
                    {
                        "questionId": str(dq.get("question_id", "")),
                        "score": draft_score_value,
                        "maxScore": draft_max_score_value,
                        "feedback": dq.get("feedback", ""),
                        "confidence": draft_confidence,
                        "studentAnswer": dq.get("student_answer", ""),
                        "question_type": dq.get("question_type") or dq.get("questionType"),
                        "scoring_point_results": draft_scoring_results,
                        "page_indices": draft_page_indices,
                    }
                )

        # è®¡ç®—é¡µé¢èŒƒå›´æ˜¾ç¤ºå­—ç¬¦ä¸²
        start_page = r.get("start_page") if r.get("start_page") is not None else r.get("startPage")
        end_page = r.get("end_page") if r.get("end_page") is not None else r.get("endPage")
        page_range = ""
        if start_page is not None:
            if end_page is not None and end_page != start_page:
                page_range = f"{start_page + 1}-{end_page + 1}"
            else:
                page_range = str(start_page + 1)

        student_id = r.get("student_id") or r.get("studentId")
        student_key = r.get("student_key") or r.get("studentKey")
        formatted.append(
            {
                "studentName": r.get("student_name")
                or r.get("studentName")
                or student_key
                or student_id
                or "Unknown",
                "studentId": student_id,
                "studentKey": student_key,
                "score": final_score,
                "maxScore": final_max if final_max > 0 else 0,
                "startPage": start_page,
                "endPage": end_page,
                "pageRange": page_range,
                "questionResults": question_results,
                "confidence": r.get("confidence", 0),
                "needsConfirmation": r.get("needs_confirmation", False),
                "gradingMode": r.get("grading_mode") or r.get("gradingMode"),
                "studentSummary": student_summary,
                # ğŸ”¥ æ–°å¢ï¼šæ‰¹æ”¹é€æ˜åº¦å­—æ®µ
                "confession": confession,
                "logicReview": logic_review_raw,
                "draftQuestionDetails": draft_question_results if draft_question_results else None,
                "draftTotalScore": r.get("draft_total_score") or r.get("draftTotalScore"),
                "draftMaxScore": r.get("draft_max_score") or r.get("draftMaxScore"),
                "logicReviewedAt": r.get("logic_reviewed_at") or r.get("logicReviewedAt"),
                
            }
        )
    formatted = _dedupe_formatted_results(formatted)
    return formatted


@router.websocket("/ws/{batch_id}")
async def websocket_endpoint(websocket: WebSocket, batch_id: str):
    """
    WebSocket ç«¯ç‚¹ï¼Œç”¨äºå®æ—¶æ¨é€æ‰¹æ”¹è¿›åº¦

    å‰ç«¯é€šè¿‡æ­¤ç«¯ç‚¹æ¥æ”¶ LangGraph çš„å®æ—¶æ‰§è¡Œè¿›åº¦
    """
    await websocket.accept()
    
    # ä¸ºæ–°è¿æ¥åˆ›å»ºé”
    ws_id = id(websocket)
    if ws_id not in ws_locks:
        ws_locks[ws_id] = asyncio.Lock()

    redis_client = await _get_redis_client()
    use_redis_cache = redis_client is not None

    cached_images = batch_image_cache.get(batch_id, {})
    if cached_images:
        try:
            for key, message in cached_images.items():
                if key == "llm_stream_cache":
                    continue
                if use_redis_cache and key not in ("images_ready", "rubric_images_ready"):
                    continue
                async with ws_locks[ws_id]:
                    await websocket.send_json(message)
        except Exception as e:
            logger.debug(f"å‘é€ç¼“å­˜å›¾ç‰‡å¤±è´¥: {e}")

    if use_redis_cache:
        try:
            cached_progress = await _load_cached_progress_messages(batch_id)
            for message in cached_progress:
                # ä¸é‡æ”¾ workflow_completedï¼Œé¿å…é”™è¯¯è·³è½¬åˆ°ç»“æœé¡µ
                if message.get("type") == "workflow_completed":
                    logger.debug(f"è·³è¿‡é‡æ”¾ workflow_completed ç¼“å­˜: batch_id={batch_id}")
                    continue
                async with ws_locks[ws_id]:
                    await websocket.send_json(message)
        except Exception as e:
            logger.debug(f"å‘é€ç¼“å­˜è¿›åº¦å¤±è´¥: {e}")

    if cached_images:
        try:
            stream_cache = cached_images.get("llm_stream_cache")
            if isinstance(stream_cache, dict):
                for stream_message in stream_cache.values():
                    async with ws_locks[ws_id]:
                        await websocket.send_json(
                            {
                                "type": "llm_stream_chunk",
                                **stream_message,
                            }
                        )
        except Exception as e:
            logger.debug(f"å‘é€æµå¼ç¼“å­˜å¤±è´¥: {e}")

    # æ³¨å†Œè¿æ¥
    if batch_id not in active_connections:
        active_connections[batch_id] = []
    active_connections[batch_id].append(websocket)

    # æ£€æŸ¥è¯¥æ‰¹æ¬¡æ˜¯å¦æœ‰æ´»è·ƒçš„ LangGraph è¿è¡Œ
    orchestrator_check = await get_orchestrator()
    run_exists = False
    if orchestrator_check:
        try:
            run_info = await orchestrator_check.get_run_info(f"batch_grading_{batch_id}")
            run_exists = run_info is not None
        except Exception:
            pass
    
    logger.debug(f"WebSocket è¿æ¥å»ºç«‹: batch_id={batch_id}, run_exists={run_exists}")
    
    # å¦‚æœæ‰¹æ¬¡ä¸å­˜åœ¨æ´»è·ƒçš„è¿è¡Œï¼Œé™é»˜å…³é—­è¿æ¥
    # è¿™æ˜¯æ­£å¸¸æƒ…å†µï¼ˆå‰ç«¯å¯èƒ½è¿æ¥åˆ°å·²å®Œæˆçš„æ‰¹æ¬¡ï¼‰ï¼Œä¸éœ€è¦è®°å½•é”™è¯¯
    if not run_exists:
        try:
            async with ws_locks[ws_id]:
                await websocket.send_json({
                    "type": "batch_not_found",
                    "message": f"Batch {batch_id} has no active run. It may have completed or does not exist.",
                    "batchId": batch_id,
                })
        except Exception:
            pass  # é™é»˜å¤„ç† - è¿æ¥å¯èƒ½å·²å…³é—­ï¼Œè¿™æ˜¯é¢„æœŸçš„
        # æ¸…ç†è¿æ¥
        _discard_connection(batch_id, websocket)
        ws_locks.pop(ws_id, None)
        try:
            await websocket.close(code=1000, reason="Batch not found")
        except Exception:
            pass
        return  # ç›´æ¥è¿”å›ï¼Œä¸è¿›å…¥ while å¾ªç¯

    # è¿æ¥å»ºç«‹åå°è¯•å‘é€å½“å‰çŠ¶æ€å¿«ç…§ï¼Œé¿å…å‰ç«¯é”™è¿‡æ—©æœŸäº‹ä»¶å¯¼è‡´å¡ä½
    try:
        orchestrator = await get_orchestrator()
        if orchestrator:
            run_id = f"batch_grading_{batch_id}"
            run_info = await orchestrator.get_run_info(run_id)
            if run_info and run_info.state:
                state = run_info.state or {}
                current_stage = state.get("current_stage", "")
                percentage = state.get("percentage", 0)
                if current_stage or percentage:
                    async with ws_locks[ws_id]:
                        await websocket.send_json(
                            {
                                "type": "grading_progress",
                                "percentage": percentage or 0,
                                "currentStage": current_stage,
                            }
                        )
                if state.get("student_boundaries"):
                    boundaries = state.get("student_boundaries", [])
                    async with ws_locks[ws_id]:
                        await websocket.send_json(
                            {
                                "type": "students_identified",
                                "studentCount": len(boundaries),
                                "students": [
                                    {
                                        "studentKey": b.get("student_key", ""),
                                        "startPage": b.get("start_page", 0),
                                        "endPage": b.get("end_page", 0),
                                        "confidence": b.get("confidence", 0),
                                        "needsConfirmation": b.get("needs_confirmation", False),
                                    }
                                    for b in boundaries
                                ],
                            }
                        )
                if state.get("parsed_rubric"):
                    parsed = state.get("parsed_rubric", {})
                    async with ws_locks[ws_id]:
                        await websocket.send_json(
                            {
                                "type": "rubric_parsed",
                                "totalQuestions": parsed.get("total_questions", 0),
                                "totalScore": parsed.get("total_score", 0),
                                "generalNotes": parsed.get("general_notes", ""),
                                "rubricFormat": parsed.get("rubric_format", ""),
                                "questions": [
                                    {
                                        "questionId": q.get("question_id", ""),
                                        "maxScore": q.get("max_score", 0),
                                        "questionText": q.get("question_text", ""),
                                        "standardAnswer": q.get("standard_answer", ""),
                                        "gradingNotes": q.get("grading_notes", ""),
                                        "scoringPoints": [
                                            {
                                                "pointId": sp.get("point_id")
                                                or sp.get("pointId")
                                                or f"{q.get('question_id')}.{idx + 1}",
                                                "description": sp.get("description", ""),
                                                "expectedValue": sp.get("expected_value")
                                                or sp.get("expectedValue", ""),
                                                "keywords": sp.get("keywords") or [],
                                                "score": sp.get("score", 0),
                                                "isRequired": sp.get("is_required", True),
                                            }
                                            for idx, sp in enumerate(q.get("scoring_points", []))
                                        ],
                                        "deductionRules": [
                                            {
                                                "ruleId": dr.get("rule_id")
                                                or dr.get("ruleId")
                                                or f"{q.get('question_id')}.d{idx + 1}",
                                                "description": dr.get("description", ""),
                                                "deduction": dr.get("deduction", dr.get("score", 0)),
                                                "conditions": dr.get("conditions")
                                                or dr.get("when")
                                                or "",
                                            }
                                            for idx, dr in enumerate(
                                                q.get("deduction_rules")
                                                or q.get("deductionRules")
                                                or []
                                            )
                                        ],
                                        "alternativeSolutions": [
                                            {
                                                "description": alt.get("description", ""),
                                                "scoringCriteria": alt.get("scoring_criteria", ""),
                                                "note": alt.get("note", ""),
                                            }
                                            for alt in q.get("alternative_solutions", [])
                                        ],
                                    }
                                    for q in parsed.get("questions", [])
                                ],
                            }
                        )
                if run_info.status and run_info.status.value == "completed":
                    student_results = state.get("student_results", [])
                    formatted_results = _format_results_for_frontend(student_results)
                    # ğŸ”¥ FIX: åªæœ‰åœ¨æœ‰å®é™…ç»“æœæ•°æ®æ—¶æ‰å‘é€ workflow_completed
                    # é¿å…åœ¨å·¥ä½œæµå¼‚å¸¸å®Œæˆï¼ˆå¦‚è·³è¿‡æ‰¹æ”¹ï¼‰æ—¶é”™è¯¯å‘é€å®Œæˆäº‹ä»¶
                    if formatted_results and len(formatted_results) > 0:
                        class_report = state.get("class_report")
                        if not class_report and state.get("export_data"):
                            class_report = state.get("export_data", {}).get("class_report")
                        async with ws_locks[ws_id]:
                            await websocket.send_json(
                                {
                                    "type": "workflow_completed",
                                    "message": f"Grading completed, processed {len(formatted_results)} students",
                                    "results": formatted_results,
                                    "cross_page_questions": state.get("cross_page_questions", []),
                                    "classReport": class_report,
                                }
                            )
                    else:
                        logger.warning(f"è·³è¿‡å‘é€ workflow_completed: çŠ¶æ€ä¸º completed ä½†æ²¡æœ‰ç»“æœæ•°æ®, batch_id={batch_id}")
            if run_info and run_info.status and run_info.status.value in ("running", "pending"):
                run_controller = await get_run_controller()
                teacher_key = None
                class_id = None
                homework_id = None
                if run_controller:
                    snapshot = await run_controller.get_run(batch_id)
                    if snapshot:
                        teacher_key = snapshot.teacher_id
                        class_id = snapshot.class_id
                        homework_id = snapshot.homework_id
                await _ensure_stream_task(
                    batch_id=batch_id,
                    run_id=run_id,
                    orchestrator=orchestrator,
                    class_id=class_id,
                    homework_id=homework_id,
                    student_mapping=None,
                    teacher_key=teacher_key,
                )
    except Exception:
        pass  # çŠ¶æ€å¿«ç…§å‘é€å¤±è´¥æ˜¯æ­£å¸¸æƒ…å†µï¼Œé™é»˜å¤„ç†

    try:
        # ä¿æŒè¿æ¥ï¼Œç­‰å¾…å®¢æˆ·ç«¯æ¶ˆæ¯æˆ–æ–­å¼€
        while True:
            if not _is_ws_connected(websocket):
                break
            data = await websocket.receive_text()
            logger.debug(f"æ”¶åˆ° WebSocket æ¶ˆæ¯: batch_id={batch_id}, data={data}")

    except (WebSocketDisconnect, RuntimeError, AssertionError):
        # WebSocketDisconnect: æ­£å¸¸æ–­å¼€
        # RuntimeError: è¿æ¥å·²å…³é—­æ—¶çš„æ“ä½œ
        # AssertionError: websockets åº“å†…éƒ¨ keepalive ping ç«æ€æ¡ä»¶
        # è¿™äº›éƒ½æ˜¯é¢„æœŸçš„æ–­å¼€æƒ…å†µï¼Œé™é»˜å¤„ç†
        pass
    except Exception:
        # å…¶ä»–å¼‚å¸¸ä¹Ÿé™é»˜å¤„ç†
        pass
    finally:
        # ğŸ”¥ FIX: æ— è®ºå¦‚ä½•éƒ½è¦æ¸…ç†è¿æ¥ï¼Œé˜²æ­¢è¿æ¥æ³„æ¼
        _discard_connection(batch_id, websocket)
        ws_locks.pop(ws_id, None)


@router.get("/active", response_model=ActiveRunsResponse)
async def list_active_runs(teacher_id: Optional[str] = None) -> ActiveRunsResponse:
    teacher_key = _normalize_teacher_key(teacher_id)
    run_controller = await get_run_controller()
    if not run_controller:
        return ActiveRunsResponse(teacher_id=teacher_key, runs=[])
    snapshots = await run_controller.list_runs(teacher_key)
    runs = [
        ActiveRunItem(
            batch_id=item.batch_id,
            status=item.status,
            class_id=item.class_id,
            homework_id=item.homework_id,
            created_at=item.created_at,
            updated_at=item.updated_at,
            started_at=item.started_at,
            completed_at=item.completed_at,
            total_pages=item.total_pages,
            progress=item.progress,
            current_stage=item.current_stage,
        )
        for item in snapshots
    ]
    return ActiveRunsResponse(teacher_id=teacher_key, runs=runs)


@router.get("/status/{batch_id}", response_model=BatchStatusResponse)
async def get_batch_status(batch_id: str, orchestrator: Orchestrator = Depends(get_orchestrator)):
    """
    æŸ¥è¯¢æ‰¹æ¬¡çŠ¶æ€ï¼ˆä» LangGraph Orchestratorï¼‰

    Args:
        batch_id: æ‰¹æ¬¡ ID
        orchestrator: LangGraph Orchestrator

    Returns:
        BatchStatusResponse: æ‰¹æ¬¡çŠ¶æ€
    """
    try:
        if not orchestrator:
            raise HTTPException(status_code=503, detail="ç¼–æ’å™¨æœªåˆå§‹åŒ–")

        # æ„å»º run_idï¼ˆä¸ start_run ä¸­çš„æ ¼å¼ä¸€è‡´ï¼‰
        run_id = f"batch_grading_{batch_id}"

        # ä» LangGraph Orchestrator æŸ¥è¯¢çŠ¶æ€
        run_info = await orchestrator.get_run_info(run_id)

        if not run_info:
            raise HTTPException(status_code=404, detail="æ‰¹æ¬¡ä¸å­˜åœ¨")

        state = run_info.state or {}

        return BatchStatusResponse(
            batch_id=batch_id,
            exam_id=state.get("exam_id", ""),
            status=run_info.status.value,
            current_stage=state.get("current_stage"),
            error=run_info.error,
            total_students=len(state.get("student_boundaries", [])),
            completed_students=len(state.get("student_results", [])),
            unidentified_pages=0,
            results=state.get("student_results"),
        )

    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"æŸ¥è¯¢æ‰¹æ¬¡çŠ¶æ€å¤±è´¥: {str(e)}", exc_info=True)
        raise HTTPException(status_code=500, detail=f"æŸ¥è¯¢å¤±è´¥: {str(e)}")


@router.get("/rubric/{batch_id}", response_model=RubricReviewContextResponse)
async def get_rubric_review_context(
    batch_id: str, orchestrator: Orchestrator = Depends(get_orchestrator)
):
    """è·å– rubric review é¡µé¢ä¸Šä¸‹æ–‡ï¼ˆæ”¯æŒä»æ–‡ä»¶å­˜å‚¨å’Œæ•°æ®åº“è¯»å–ï¼‰"""
    
    async def _load_rubric_images_from_pg() -> List[str]:
        """ä» PostgreSQL batch_images è¡¨åŠ è½½ rubric å›¾ç‰‡ï¼ˆä¼˜å…ˆï¼‰"""
        try:
            images_bytes = await get_batch_images_as_bytes_list(batch_id, "rubric")
            if not images_bytes:
                return []
            images = [base64.b64encode(img).decode("utf-8") for img in images_bytes]
            logger.info(f"[PG-Storage] ä» PostgreSQL åŠ è½½äº† {len(images)} å¼  rubric å›¾ç‰‡")
            return images
        except Exception as exc:
            logger.debug(f"ä» PostgreSQL åŠ è½½ rubric å›¾ç‰‡å¤±è´¥: {exc}")
            return []
    
    async def _load_rubric_images_from_storage() -> List[str]:
        """ä»æ–‡ä»¶å­˜å‚¨åŠ è½½ rubric å›¾ç‰‡ï¼ˆå¤‡ç”¨ï¼‰"""
        try:
            file_storage = get_file_storage_service()
            stored_files = await file_storage.list_batch_files(batch_id)
            if not stored_files:
                return []
            
            rubric_files = [
                item for item in stored_files
                if item.metadata.get("type") == "rubric" or item.filename.startswith("rubric_page")
            ]
            if not rubric_files:
                return []
            
            rubric_files.sort(key=lambda f: f.filename)
            images: List[str] = []
            for item in rubric_files:
                data = await file_storage.get_file(item.file_id)
                if not data:
                    continue
                images.append(base64.b64encode(data).decode("utf-8"))
            
            logger.info(f"ä»æ–‡ä»¶å­˜å‚¨åŠ è½½äº† {len(images)} å¼  rubric å›¾ç‰‡")
            return images
        except Exception as exc:
            logger.debug(f"ä»æ–‡ä»¶å­˜å‚¨åŠ è½½ rubric å›¾ç‰‡å¤±è´¥: {exc}")
            return []
    
    async def _load_from_db() -> RubricReviewContextResponse:
        """ä»æ•°æ®åº“åŠ è½½ rubric æ•°æ®"""
        history = await _maybe_await(get_grading_history(batch_id))
        if not history:
            raise HTTPException(status_code=404, detail="æ‰¹æ¬¡ä¸å­˜åœ¨")
        
        parsed_rubric = None
        if history.rubric_data:
            try:
                if isinstance(history.rubric_data, str):
                    parsed_rubric = json.loads(history.rubric_data)
                elif isinstance(history.rubric_data, dict):
                    parsed_rubric = history.rubric_data
            except Exception as exc:
                logger.debug(f"è§£ææ•°æ®åº“ä¸­çš„ rubric_data å¤±è´¥: {exc}")
        
        # ä¼˜å…ˆä» PostgreSQL åŠ è½½å›¾ç‰‡
        rubric_images = await _load_rubric_images_from_pg()
        if not rubric_images:
            rubric_images = await _load_rubric_images_from_storage()
        
        logger.info(f"ä»æ•°æ®åº“åŠ è½½ rubric ä¸Šä¸‹æ–‡: parsed_rubric={'æœ‰' if parsed_rubric else 'æ— '}, images={len(rubric_images)}")
        
        return RubricReviewContextResponse(
            batch_id=batch_id,
            status=history.status,
            current_stage=history.current_stage,
            parsed_rubric=parsed_rubric,
            rubric_images=rubric_images,
        )
    
    try:
        # 1. ä¼˜å…ˆä» LangGraph state è¯»å–ï¼ˆå®æ—¶æ•°æ®ï¼‰
        if orchestrator:
            run_id = f"batch_grading_{batch_id}"
            run_info = await orchestrator.get_run_info(run_id)
            
            if run_info:
                state = run_info.state or {}
                parsed_rubric = state.get("parsed_rubric")
                
                # å°è¯•ä»ç¼“å­˜è¯»å–å›¾ç‰‡
                cached = batch_image_cache.get(batch_id, {})
                cached_images = cached.get("rubric_images_ready", {}).get("images") if cached else None
                rubric_images: List[str] = cached_images or []
                
                # ä» state è¯»å–å›¾ç‰‡
                if not rubric_images and state.get("rubric_images"):
                    try:
                        rubric_images = []
                        for img in state.get("rubric_images", []):
                            if isinstance(img, (bytes, bytearray)):
                                rubric_images.append(base64.b64encode(img).decode("utf-8"))
                            elif isinstance(img, str) and img:
                                rubric_images.append(img)
                    except Exception as exc:
                        logger.debug(f"ä» state è½¬æ¢ rubric å›¾ç‰‡å¤±è´¥: {exc}")
                
                # å¦‚æœä» state è·å–åˆ°äº†æ•°æ®ï¼Œç›´æ¥è¿”å›
                if parsed_rubric or rubric_images:
                    logger.info(f"ä» LangGraph state åŠ è½½ rubric: parsed_rubric={'æœ‰' if parsed_rubric else 'æ— '}, images={len(rubric_images)}")
                    return RubricReviewContextResponse(
                        batch_id=batch_id,
                        status=run_info.status.value if run_info.status else None,
                        current_stage=state.get("current_stage"),
                        parsed_rubric=parsed_rubric,
                        rubric_images=rubric_images,
                    )
        
        # 2. Fallback: ä»æ•°æ®åº“å’Œæ–‡ä»¶å­˜å‚¨è¯»å–
        logger.info(f"LangGraph state æ— æ•°æ®ï¼Œå°è¯•ä»æ•°æ®åº“åŠ è½½ rubric: batch_id={batch_id}")
        return await _load_from_db()
        
    except HTTPException:
        raise
    except Exception as exc:
        logger.error(f"è·å– rubric ä¸Šä¸‹æ–‡å¤±è´¥: {exc}", exc_info=True)
        raise HTTPException(status_code=500, detail=f"è·å–å¤±è´¥: {str(exc)}")


@router.get("/results-review/{batch_id}", response_model=ResultsReviewContextResponse)
async def get_results_review_context(
    batch_id: str, orchestrator: Orchestrator = Depends(get_orchestrator)
):
    """è·å– results review é¡µé¢ä¸Šä¸‹æ–‡"""

    async def _load_answer_images_from_pg() -> List[str]:
        """ä» PostgreSQL batch_images è¡¨åŠ è½½ç­”é¢˜å›¾ç‰‡ï¼ˆä¼˜å…ˆï¼‰"""
        try:
            images_bytes = await get_batch_images_as_bytes_list(batch_id, "answer")
            if not images_bytes:
                return []
            # è½¬æ¢ä¸º base64 å­—ç¬¦ä¸²
            images = [base64.b64encode(img).decode("utf-8") for img in images_bytes]
            logger.info(f"[PG-Storage] ä» PostgreSQL åŠ è½½äº† {len(images)} å¼ ç­”é¢˜å›¾ç‰‡")
            return images
        except Exception as exc:
            logger.debug(f"Failed to load answer images from PostgreSQL: {exc}")
            return []

    async def _load_answer_images_from_storage() -> List[str]:
        """ä»æœ¬åœ°æ–‡ä»¶å­˜å‚¨åŠ è½½ç­”é¢˜å›¾ç‰‡ï¼ˆå¤‡ç”¨ï¼‰"""
        try:
            file_storage = get_file_storage_service()
            stored_files = await file_storage.list_batch_files(batch_id)
            if not stored_files:
                return []
            answer_files = [
                item
                for item in stored_files
                if item.metadata.get("type") == "answer"
                or item.filename.startswith("answer_page")
            ]
            if not answer_files:
                return []
            answer_files.sort(key=lambda f: f.filename)
            images: List[str] = []
            for item in answer_files:
                data = await file_storage.get_file(item.file_id)
                if not data:
                    continue
                images.append(base64.b64encode(data).decode("utf-8"))
            return images
        except Exception as exc:
            logger.debug(f"Failed to load answer images from storage: {exc}")
            return []

    async def _load_answer_images_from_db(history_id: str) -> List[str]:
        try:
            images = await _maybe_await(get_page_images(history_id))
            if not images:
                return []
            return [img.file_url for img in images if img.file_url]
        except Exception as exc:
            logger.debug(f"Failed to load answer images from DB: {exc}")
            return []

    def _has_post_confession_fields(results: List[Dict[str, Any]]) -> bool:
        for item in results:
            if not isinstance(item, dict):
                continue
            if item.get("confession") or item.get("logic_review") or item.get("logicReview"):
                return True
        return False

    async def _load_from_db() -> ResultsReviewContextResponse:
        """ä»æ•°æ®åº“åŠ è½½æ‰¹æ”¹ç»“æœ"""
        history = await _maybe_await(get_grading_history(batch_id))
        if not history:
            raise HTTPException(status_code=404, detail="æ‰¹æ¬¡ä¸å­˜åœ¨")

        raw_results: List[Dict[str, Any]] = []
        student_rows = await _maybe_await(get_student_results(history.id))
        for row in student_rows:
            data = row.result_data
            if isinstance(data, str):
                try:
                    data = json.loads(data)
                except Exception:
                    data = {}
            if not isinstance(data, dict):
                data = {}
            if not data:
                data = {
                    "studentName": row.student_key,
                    "score": row.score,
                    "maxScore": row.max_score,
                }
            confession_value = row.confession
            if confession_value and not data.get("confession"):
                if isinstance(confession_value, str):
                    try:
                        confession_value = json.loads(confession_value)
                    except Exception:
                        confession_value = None
                if confession_value:
                    data["confession"] = confession_value
            raw_results.append(data)

        # ä» history.result_data ä¸­è·å– parsed_rubric
        parsed_rubric = None
        if history.result_data and isinstance(history.result_data, dict):
            parsed_rubric = history.result_data.get("parsed_rubric")

        answer_images = await _load_answer_images_from_db(history.id)
        if not answer_images:
            answer_images = await _load_answer_images_from_storage()
        return ResultsReviewContextResponse(
            batch_id=batch_id,
            status=history.status,
            current_stage=None,
            student_results=_format_results_for_frontend(raw_results),
            answer_images=answer_images,
            parsed_rubric=parsed_rubric,
        )

    try:
        if not orchestrator:
            return await _load_from_db()

        run_id = f"batch_grading_{batch_id}"
        run_info = await orchestrator.get_run_info(run_id)
        if not run_info:
            return await _load_from_db()

        state = run_info.state or {}
        student_results = (
            state.get("reviewed_results")
            or state.get("confessed_results")
            or state.get("student_results", [])
        )
        if not student_results:
            try:
                final_output = await orchestrator.get_final_output(run_id)
                if final_output:
                    student_results = final_output.get("student_results", [])
            except Exception as exc:
                logger.debug(f"è·å–æœ€ç»ˆè¾“å‡ºå¤±è´¥: {exc}")

        if not student_results:
            export_students = (state.get("export_data") or {}).get("students", [])
            if export_students:
                student_results = export_students
            else:
                grading_results = state.get("grading_results") or []
                if not grading_results:
                    try:
                        final_output = await orchestrator.get_final_output(run_id)
                        if final_output:
                            grading_results = final_output.get("grading_results") or []
                    except Exception as exc:
                        logger.debug(f"Failed to load grading_results from orchestrator: {exc}")
                if grading_results:
                    student_results = _build_student_results_from_grading_results(grading_results)
                if not student_results:
                    try:
                        return await _load_from_db()
                    except HTTPException:
                        student_results = []

        # Attach logic review payloads from state when missing on student_results.
        logic_review_by_student: Dict[str, Any] = {}
        for item in state.get("logic_review_results") or []:
            if not isinstance(item, dict):
                continue
            key = item.get("student_key") or item.get("studentKey")
            if key:
                logic_review_by_student[key] = item
        if logic_review_by_student:
            for student in student_results:
                if not isinstance(student, dict):
                    continue
                if student.get("logic_review") or student.get("logicReview"):
                    continue
                key = (
                    student.get("student_key")
                    or student.get("studentKey")
                    or student.get("student_name")
                    or student.get("studentName")
                )
                payload = logic_review_by_student.get(key)
                if payload:
                    student["logic_review"] = payload
                    student["logicReview"] = payload
                    if not student.get("logic_reviewed_at") and payload.get("reviewed_at"):
                        student["logic_reviewed_at"] = payload.get("reviewed_at")

        # If the run completed but confession/logic_review is missing, prefer DB results.
        if (
            student_results
            and run_info.status
            and run_info.status.value == "completed"
            and not _has_post_confession_fields(student_results)
        ):
            try:
                return await _load_from_db()
            except HTTPException:
                pass

        cached = batch_image_cache.get(batch_id, {})
        cached_images = cached.get("images_ready", {}).get("images") if cached else None
        answer_images: List[str] = cached_images or []

        if not answer_images:
            raw_images = state.get("processed_images") or state.get("answer_images") or []
            try:
                answer_images = []
                for img in raw_images:
                    if isinstance(img, (bytes, bytearray)):
                        answer_images.append(base64.b64encode(img).decode("utf-8"))
                    elif isinstance(img, str) and img:
                        answer_images.append(img)
            except Exception as exc:
                logger.debug(f"Failed to convert answer images: {exc}")
        if not answer_images:
            history = await _maybe_await(get_grading_history(batch_id))
            if history:
                answer_images = await _load_answer_images_from_db(history.id)
        else:
            # Prefer DB image URLs over large base64 blobs when available.
            if any(
                isinstance(img, str) and img.startswith("data:")
                for img in answer_images
            ):
                history = await _maybe_await(get_grading_history(batch_id))
                if history:
                    db_images = await _load_answer_images_from_db(history.id)
                    if db_images:
                        answer_images = db_images
        if not answer_images:
            # ä¼˜å…ˆä» PostgreSQL batch_images è¡¨åŠ è½½
            answer_images = await _load_answer_images_from_pg()
        if not answer_images:
            # å›é€€åˆ°æœ¬åœ°æ–‡ä»¶å­˜å‚¨
            answer_images = await _load_answer_images_from_storage()
        
        # ä» state ä¸­è·å– parsed_rubric
        parsed_rubric = state.get("parsed_rubric")
        
        return ResultsReviewContextResponse(
            batch_id=batch_id,
            status=run_info.status.value if run_info.status else None,
            current_stage=state.get("current_stage"),
            student_results=_format_results_for_frontend(student_results),
            answer_images=answer_images,
            parsed_rubric=parsed_rubric,
        )
    except HTTPException:
        raise
    except Exception as exc:
        logger.error(f"è·å– results ä¸Šä¸‹æ–‡å¤±è´¥: {exc}", exc_info=True)
        raise HTTPException(status_code=500, detail=f"è·å–å¤±è´¥: {str(exc)}")


@router.get("/results/{batch_id}")
async def get_batch_results(batch_id: str, orchestrator: Orchestrator = Depends(get_orchestrator)):
    """
    è·å–æ‰¹æ¬¡æ‰¹æ”¹ç»“æœï¼ˆä» LangGraph Orchestratorï¼‰

    Args:
        batch_id: æ‰¹æ¬¡ ID
        orchestrator: LangGraph Orchestrator

    Returns:
        æ‰¹æ”¹ç»“æœ
    """
    try:
        if not orchestrator:
            raise HTTPException(status_code=503, detail="ç¼–æ’å™¨æœªåˆå§‹åŒ–")

        # æ„å»º run_idï¼ˆä¸ start_run ä¸­çš„æ ¼å¼ä¸€è‡´ï¼‰
        run_id = f"batch_grading_{batch_id}"

        run_info = await orchestrator.get_run_info(run_id)

        if not run_info:
            raise HTTPException(status_code=404, detail="æ‰¹æ¬¡ä¸å­˜åœ¨")

        state = run_info.state or {}

        # ä¼˜å…ˆä» student_results è·å–ç»“æœ
        # æ³¨æ„ï¼šconfessed_results å·²ç§»é™¤ï¼ˆæ‰¹æ”¹å’Œå®¡è®¡ä¸€ä½“åŒ–æ”¹é€ ï¼‰
        student_results = (
            state.get("reviewed_results")
            or state.get("student_results", [])
        )

        # å¦‚æœæ²¡æœ‰ student_resultsï¼Œå°è¯•ä» orchestrator è·å–æœ€ç»ˆè¾“å‡º
        if not student_results:
            try:
                final_output = await orchestrator.get_final_output(run_id)
                if final_output:
                    student_results = (
                        final_output.get("reviewed_results")
                        or final_output.get("student_results", [])
                    )
            except Exception as e:
                logger.debug(f"è·å–æœ€ç»ˆè¾“å‡ºå¤±è´¥: {e}")

        
        # Attach logic review payloads from state when missing on student_results.
        logic_review_by_student: Dict[str, Any] = {}
        for item in state.get("logic_review_results") or []:
            if not isinstance(item, dict):
                continue
            key = item.get("student_key") or item.get("studentKey")
            if key:
                logic_review_by_student[key] = item
        if logic_review_by_student:
            for student in student_results:
                if not isinstance(student, dict):
                    continue
                if student.get("logic_review") or student.get("logicReview"):
                    continue
                key = (
                    student.get("student_key")
                    or student.get("studentKey")
                    or student.get("student_name")
                    or student.get("studentName")
                )
                payload = logic_review_by_student.get(key)
                if payload:
                    student["logic_review"] = payload
                    student["logicReview"] = payload
                    if not student.get("logic_reviewed_at") and payload.get("reviewed_at"):
                        student["logic_reviewed_at"] = payload.get("reviewed_at")
        return {
            "batch_id": batch_id,
            "status": run_info.status.value,
            "results": _format_results_for_frontend(student_results),
            "class_report": state.get("class_report"),
        }

    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"è·å–æ‰¹æ”¹ç»“æœå¤±è´¥: {str(e)}", exc_info=True)
        raise HTTPException(status_code=500, detail=f"è·å–å¤±è´¥: {str(e)}")


@router.get("/full-results/{batch_id}")
async def get_full_batch_results(
    batch_id: str, orchestrator: Orchestrator = Depends(get_orchestrator)
):
    """
    è·å–æ‰¹æ¬¡å®Œæ•´æ‰¹æ”¹ç»“æœï¼ˆåŒ…å«è·¨é¡µé¢˜ç›®ä¿¡æ¯ï¼‰

    Args:
        batch_id: æ‰¹æ¬¡ ID
        orchestrator: LangGraph Orchestrator

    Returns:
        å®Œæ•´æ‰¹æ”¹ç»“æœï¼ˆåŒ…å«è·¨é¡µé¢˜ç›®ä¿¡æ¯ï¼‰
    """

    async def _load_answer_images_from_db(history_id: str) -> List[str]:
        try:
            images = await _maybe_await(get_page_images(history_id))
            if not images:
                return []
            return [img.file_url for img in images if img.file_url]
        except Exception as exc:
            logger.debug(f"Failed to load answer images from DB: {exc}")
            return []

    async def _load_from_db() -> Dict[str, Any]:
        history = await get_grading_history(batch_id)
        if not history:
            raise HTTPException(status_code=404, detail="æ‰¹æ¬¡ä¸å­˜åœ¨")

        raw_results: List[Dict[str, Any]] = []
        for row in await get_student_results(history.id):
            data = row.result_data
            if isinstance(data, str):
                try:
                    data = json.loads(data)
                except Exception:
                    data = {}
            if not isinstance(data, dict):
                data = {}
            if not data:
                data = {
                    "studentName": row.student_key,
                    "score": row.score,
                    "maxScore": row.max_score,
                }
            raw_results.append(data)

        class_report = None
        history_data = history.result_data
        if history_data:
            if isinstance(history_data, str):
                try:
                    history_data = json.loads(history_data)
                except Exception:
                    history_data = {}
            if isinstance(history_data, dict):
                class_report = history_data.get("summary") or history_data.get("class_report")

        formatted_results = _format_results_for_frontend(raw_results)
        total_max = 0.0
        for item in formatted_results:
            try:
                total_max = max(total_max, float(item.get("maxScore") or 0))
            except (TypeError, ValueError):
                continue

        return {
            "batch_id": batch_id,
            "status": history.status or "completed",
            "results": formatted_results,
            "cross_page_questions": [],
            "parsed_rubric": {},
            "class_report": class_report,
            "total_students": len(formatted_results),
            "total_score": total_max or 100,
        }

    try:
        if not orchestrator:
            return await _load_from_db()

        run_id = f"batch_grading_{batch_id}"
        run_info = await orchestrator.get_run_info(run_id)

        if not run_info:
            return await _load_from_db()

        state = run_info.state or {}
        student_results = (
            state.get("reviewed_results")
            or state.get("confessed_results")
            or state.get("student_results", [])
        ) or []
        cross_page_questions = state.get("cross_page_questions", []) or []
        parsed_rubric = state.get("parsed_rubric", {}) or {}
        class_report = state.get("class_report") or state.get("export_data", {}).get("class_report")
        final_output: Optional[Dict[str, Any]] = None

        # Attach logic review payloads from state when missing on student_results.
        logic_review_by_student: Dict[str, Any] = {}
        for item in state.get("logic_review_results") or []:
            if not isinstance(item, dict):
                continue
            key = item.get("student_key") or item.get("studentKey")
            if key:
                logic_review_by_student[key] = item
        if logic_review_by_student:
            for student in student_results:
                if not isinstance(student, dict):
                    continue
                if student.get("logic_review") or student.get("logicReview"):
                    continue
                key = (
                    student.get("student_key")
                    or student.get("studentKey")
                    or student.get("student_name")
                    or student.get("studentName")
                )
                payload = logic_review_by_student.get(key)
                if payload:
                    student["logic_review"] = payload
                    student["logicReview"] = payload
                    if not student.get("logic_reviewed_at") and payload.get("reviewed_at"):
                        student["logic_reviewed_at"] = payload.get("reviewed_at")

        if not student_results or not parsed_rubric:
            final_output = await orchestrator.get_final_output(run_id)
            if final_output:
                student_results = (
                    student_results
                    or final_output.get("reviewed_results")
                    or final_output.get("confessed_results")
                    or final_output.get("student_results")
                    or final_output.get("results")
                    or []
                )
                parsed_rubric = parsed_rubric or final_output.get("parsed_rubric", {}) or {}
                cross_page_questions = cross_page_questions or final_output.get("cross_page_questions", []) or []

        if not student_results:
            grading_results = state.get("grading_results") or []
            if not grading_results and final_output:
                grading_results = final_output.get("grading_results") or []
            if grading_results:
                student_results = _build_student_results_from_grading_results(grading_results)

        # å¦‚æœ orchestrator è¿”å›ç©ºç»“æœï¼Œå›é€€åˆ°æ•°æ®åº“æŸ¥è¯¢
        if not student_results:
            logger.info(f"Orchestrator è¿”å›ç©ºç»“æœï¼Œå›é€€åˆ°æ•°æ®åº“æŸ¥è¯¢: batch_id={batch_id}")
            return await _load_from_db()

        return {
            "batch_id": batch_id,
            "status": run_info.status.value,
            "results": _format_results_for_frontend(student_results),
            "cross_page_questions": cross_page_questions,
            "parsed_rubric": parsed_rubric,
            "class_report": class_report,
            "total_students": len(student_results),
            "total_score": parsed_rubric.get("total_score", 100) if parsed_rubric else 100,
        }

    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"è·å–å®Œæ•´æ‰¹æ”¹ç»“æœå¤±è´¥: {str(e)}", exc_info=True)
        raise HTTPException(status_code=500, detail=f"è·å–å¤±è´¥: {str(e)}")


@router.get("/cross-page-questions/{batch_id}")
async def get_cross_page_questions(
    batch_id: str, orchestrator: Orchestrator = Depends(get_orchestrator)
):
    """
    è·å–è·¨é¡µé¢˜ç›®ä¿¡æ¯

    Args:
        batch_id: æ‰¹æ¬¡ ID
        orchestrator: LangGraph Orchestrator

    Returns:
        è·¨é¡µé¢˜ç›®ä¿¡æ¯åˆ—è¡¨
    """
    try:
        if not orchestrator:
            raise HTTPException(status_code=503, detail="ç¼–æ’å™¨æœªåˆå§‹åŒ–")

        run_id = f"batch_grading_{batch_id}"
        run_info = await orchestrator.get_run_info(run_id)

        if not run_info:
            raise HTTPException(status_code=404, detail="æ‰¹æ¬¡ä¸å­˜åœ¨")

        state = run_info.state or {}
        cross_page_questions = state.get("cross_page_questions", [])

        return cross_page_questions

    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"è·å–è·¨é¡µé¢˜ç›®ä¿¡æ¯å¤±è´¥: {str(e)}", exc_info=True)
        raise HTTPException(status_code=500, detail=f"è·å–å¤±è´¥: {str(e)}")


class ConfirmBoundaryRequest(BaseModel):
    """ç¡®è®¤å­¦ç”Ÿè¾¹ç•Œè¯·æ±‚"""

    batch_id: str = Field(..., description="æ‰¹æ¬¡ ID")
    student_key: str = Field(..., description="å­¦ç”Ÿæ ‡è¯†")
    confirmed_pages: List[int] = Field(..., description="ç¡®è®¤çš„é¡µé¢ç´¢å¼•åˆ—è¡¨")


class RubricReviewRequest(BaseModel):
    """æäº¤è¯„åˆ†æ ‡å‡†äººå·¥ç¡®è®¤ç»“æœ"""

    batch_id: str = Field(..., description="æ‰¹æ¬¡ ID")
    action: str = Field(..., description="approve/update/reparse")
    parsed_rubric: Optional[Dict[str, Any]] = Field(None, description="ä¿®æ­£åçš„è¯„åˆ†æ ‡å‡†")
    selected_question_ids: Optional[List[str]] = Field(None, description="ä»…é‡ä¿®æ­£çš„é—®é¢˜ ID åˆ—è¡¨")
    notes: Optional[str] = Field(None, description="è¡¥å……è¯´æ˜")


class ResultsReviewRequest(BaseModel):
    """æäº¤æ‰¹æ”¹ç»“æœäººå·¥ç¡®è®¤ç»“æœ"""

    batch_id: str = Field(..., description="æ‰¹æ¬¡ ID")
    action: str = Field(..., description="approve/update/regrade")
    results: Optional[List[Dict[str, Any]]] = Field(None, description="ä¿®æ­£åçš„ç»“æœ")
    regrade_items: Optional[List[Dict[str, Any]]] = Field(
        None,
        description="éœ€è¦é‡æ–°æ‰¹æ”¹çš„é¢˜ç›®é¡¹",
    )
    notes: Optional[str] = Field(None, description="è¡¥å……è¯´æ˜")


class GradingRetryRequest(BaseModel):
    """æ‰¹æ”¹æ–­ç‚¹é‡è¯•è¯·æ±‚ï¼ˆç”¨äºæ¢å¤è¢«æš‚åœçš„ grade_batch èŠ‚ç‚¹ï¼‰ã€‚"""

    batch_id: str = Field(..., description="æ‰¹æ¬¡ ID")
    action: str = Field(..., description="retry/abort")
    notes: Optional[str] = Field(None, description="è¡¥å……è¯´æ˜")


@router.post("/review/rubric")
async def submit_rubric_review(
    request: RubricReviewRequest, orchestrator: Orchestrator = Depends(get_orchestrator)
):
    """æäº¤è¯„åˆ†æ ‡å‡†å¤æ ¸ç»“æœï¼Œæ¢å¤ workflow"""
    try:
        if not orchestrator:
            raise HTTPException(status_code=503, detail="ç¼–æ’å™¨æœªåˆå§‹åŒ–")

        action = request.action.lower().strip()
        if action not in ("approve", "update", "override", "reparse"):
            raise HTTPException(status_code=400, detail="æ— æ•ˆçš„ review action")

        run_id = f"batch_grading_{request.batch_id}"
        run_info = await orchestrator.get_run_info(run_id)
        if not run_info:
            raise HTTPException(status_code=404, detail="æ‰¹æ¬¡ä¸å­˜åœ¨")

        payload: Dict[str, Any] = {
            "action": action,
        }
        if request.parsed_rubric is not None:
            payload["parsed_rubric"] = request.parsed_rubric
        if request.selected_question_ids:
            payload["selected_question_ids"] = request.selected_question_ids
        if request.notes:
            payload["notes"] = request.notes

        success = await orchestrator.send_event(run_id, "review_signal", payload)
        if not success:
            raise HTTPException(status_code=409, detail="æ‰¹æ¬¡æœªå¤„äºå¯å¤æ ¸çŠ¶æ€")

        # After resuming a paused graph, restart the stream forwarder so the frontend
        # continues to receive subsequent workflow updates.
        await _ensure_stream_task(
            batch_id=request.batch_id,
            run_id=run_id,
            orchestrator=orchestrator,
        )

        cached = batch_image_cache.get(request.batch_id)
        if cached and "review_required" in cached:
            cached.pop("review_required", None)

        return {"success": True, "message": "è¯„åˆ†æ ‡å‡†å¤æ ¸å·²æäº¤"}
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"æäº¤è¯„åˆ†æ ‡å‡†å¤æ ¸å¤±è´¥: {str(e)}", exc_info=True)
        raise HTTPException(status_code=500, detail=f"æäº¤å¤±è´¥: {str(e)}")


@router.post("/review/results")
async def submit_results_review(
    request: ResultsReviewRequest, orchestrator: Orchestrator = Depends(get_orchestrator)
):
    """æäº¤æ‰¹æ”¹ç»“æœå¤æ ¸ï¼Œæ¢å¤ workflow"""
    try:
        if not orchestrator:
            raise HTTPException(status_code=503, detail="ç¼–æ’å™¨æœªåˆå§‹åŒ–")

        action = request.action.lower().strip()
        if action not in ("approve", "update", "override", "regrade"):
            raise HTTPException(status_code=400, detail="æ— æ•ˆçš„ review action")

        run_id = f"batch_grading_{request.batch_id}"
        run_info = await orchestrator.get_run_info(run_id)
        if not run_info:
            raise HTTPException(status_code=404, detail="æ‰¹æ¬¡ä¸å­˜åœ¨")

        payload: Dict[str, Any] = {
            "action": action,
        }
        if request.results is not None:
            payload["results"] = request.results
        if request.regrade_items is not None:
            payload["regrade_items"] = request.regrade_items
        if request.notes:
            payload["notes"] = request.notes

        success = await orchestrator.send_event(run_id, "review_signal", payload)
        if not success and not request.results:
            raise HTTPException(status_code=409, detail="æ‰¹æ¬¡æœªå¤„äºå¯å¤æ ¸çŠ¶æ€")

        # Restart stream forwarder after resume so the frontend can see subsequent nodes.
        await _ensure_stream_task(
            batch_id=request.batch_id,
            run_id=run_id,
            orchestrator=orchestrator,
        )

        cached = batch_image_cache.get(request.batch_id)
        if cached and "review_required" in cached:
            cached.pop("review_required", None)

        if request.results:
            try:
                history = await get_grading_history(request.batch_id)
                if history:
                    raw_history_data = history.result_data
                    history_data: Dict[str, Any] = {}
                    if isinstance(raw_history_data, dict):
                        history_data = raw_history_data
                    elif isinstance(raw_history_data, str):
                        try:
                            parsed_history = json.loads(raw_history_data)
                            if isinstance(parsed_history, dict):
                                history_data = parsed_history
                        except Exception:
                            history_data = {}

                    class_id = history_data.get("class_id") if history_data else None
                    homework_id = history_data.get("homework_id") if history_data else None

                    existing_results = {
                        row.student_key: row for row in await get_student_results(history.id)
                    }

                    updated_scores: List[float] = []
                    updated_keys: set[str] = set()

                    for incoming in request.results:
                        student_key = (
                            incoming.get("studentKey")
                            or incoming.get("student_key")
                            or incoming.get("studentName")
                            or incoming.get("student_name")
                        )
                        if not student_key:
                            continue

                        existing_row = existing_results.get(student_key)
                        existing_data = existing_row.result_data if existing_row else None
                        if isinstance(existing_data, str):
                            try:
                                existing_data = json.loads(existing_data)
                            except Exception:
                                existing_data = {}
                        if not isinstance(existing_data, dict):
                            existing_data = {}

                        question_results = (
                            existing_data.get("questionResults")
                            or existing_data.get("question_results")
                            or []
                        )
                        if not isinstance(question_results, list):
                            question_results = []
                        original_scores = {
                            str(q.get("questionId") or q.get("question_id")): _safe_float(
                                q.get("score")
                            )
                            for q in question_results
                            if q.get("questionId") or q.get("question_id")
                        }

                        updates = (
                            incoming.get("questionResults")
                            or incoming.get("question_results")
                            or []
                        )
                        if not isinstance(updates, list):
                            updates = []

                        for update in updates:
                            question_id = str(
                                update.get("questionId") or update.get("question_id") or ""
                            )
                            if not question_id:
                                continue
                            updated_score = update.get("score")
                            updated_feedback = update.get("feedback")
                            target = next(
                                (
                                    q
                                    for q in question_results
                                    if str(q.get("questionId") or q.get("question_id"))
                                    == question_id
                                ),
                                None,
                            )
                            if not target:
                                target = {
                                    "questionId": question_id,
                                    "score": updated_score or 0,
                                    "feedback": updated_feedback or "",
                                }
                                question_results.append(target)
                                continue
                            original_score = original_scores.get(
                                question_id, target.get("score") or 0
                            )
                            target["score"] = (
                                updated_score if updated_score is not None else original_score
                            )
                            if updated_feedback is not None:
                                target["feedback"] = updated_feedback

                        existing_data["questionResults"] = question_results

                        total_score = sum(_safe_float(q.get("score")) for q in question_results)
                        total_max = sum(
                            _safe_float(q.get("maxScore") or q.get("max_score"))
                            for q in question_results
                        )
                        existing_data["score"] = total_score
                        if total_max > 0:
                            existing_data["maxScore"] = total_max
                            existing_data.pop("gradingAnnotations", None)
                            existing_data.pop("grading_annotations", None)

                        updated_scores.append(total_score)
                        updated_keys.add(student_key)

                        student_id_value = existing_row.student_id if existing_row else None
                        student_result = StudentGradingResult(
                            id=existing_row.id if existing_row else _make_student_result_id(history.id, student_key, student_id_value),
                            grading_history_id=history.id,
                            student_key=student_key,
                            score=total_score,
                            max_score=total_max or None,
                            class_id=existing_row.class_id if existing_row else None,
                            student_id=student_id_value,
                            summary=existing_row.summary if existing_row else None,
                            confession=existing_row.confession if existing_row else None,
                            result_data=existing_data,
                        )
                        await save_student_result(student_result)

                        if class_id and homework_id and student_result.student_id:
                            upsert_homework_submission_grade(
                                class_id=class_id,
                                homework_id=homework_id,
                                student_id=student_result.student_id,
                                student_name=student_key,
                                score=total_score,
                                feedback=(
                                    existing_data.get("studentSummary", {}).get("overall")
                                    if isinstance(existing_data.get("studentSummary"), dict)
                                    else None
                                ),
                                grading_batch_id=request.batch_id,
                            )

                    if updated_scores:
                        remaining_scores = [
                            row.score or 0
                            for key, row in existing_results.items()
                            if key not in updated_keys
                        ]
                        all_scores = [*updated_scores, *remaining_scores]
                        history.average_score = (
                            round(sum(all_scores) / len(all_scores), 2) if all_scores else None
                        )
                        history.total_students = len(all_scores)
                        if history_data:
                            summary = history_data.get("summary")
                            if isinstance(summary, dict):
                                summary["average_score"] = history.average_score
                            history.result_data = history_data
                        await save_grading_history(history)
            except Exception as exc:
                logger.error("ä¿å­˜å¤æ ¸ç»“æœå¤±è´¥: %s", exc, exc_info=True)

        if success:
            return {"success": True, "message": "æ‰¹æ”¹ç»“æœå¤æ ¸å·²æäº¤"}
        return {"success": True, "message": "æ‰¹æ”¹ç»“æœå·²ä¿å­˜ï¼Œæµç¨‹æœªæ¢å¤"}
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"æäº¤æ‰¹æ”¹å¤æ ¸å¤±è´¥: {str(e)}", exc_info=True)
        raise HTTPException(status_code=500, detail=f"æäº¤å¤±è´¥: {str(e)}")


@router.post("/review/grading")
async def submit_grading_retry(
    request: GradingRetryRequest, orchestrator: Orchestrator = Depends(get_orchestrator)
):
    """æäº¤æ‰¹æ”¹æ–­ç‚¹é‡è¯•/ç»ˆæ­¢ä¿¡å·ï¼Œæ¢å¤è¢«æš‚åœçš„ workflowã€‚"""
    try:
        if not orchestrator:
            raise HTTPException(status_code=503, detail="ç¼–æ’å™¨æœªåˆå§‹åŒ–")

        action = request.action.lower().strip()
        if action not in ("retry", "abort"):
            raise HTTPException(status_code=400, detail="æ— æ•ˆçš„ action")

        run_id = f"batch_grading_{request.batch_id}"
        run_info = await orchestrator.get_run_info(run_id)
        if not run_info:
            raise HTTPException(status_code=404, detail="æ‰¹æ¬¡ä¸å­˜åœ¨")

        payload: Dict[str, Any] = {"action": action}
        if request.notes:
            payload["notes"] = request.notes

        success = await orchestrator.send_event(run_id, "review_signal", payload)
        if not success:
            raise HTTPException(status_code=409, detail="æ‰¹æ¬¡æœªå¤„äºå¯ç»§ç»­çŠ¶æ€")

        await _ensure_stream_task(
            batch_id=request.batch_id,
            run_id=run_id,
            orchestrator=orchestrator,
        )

        cached = batch_image_cache.get(request.batch_id)
        if cached and "review_required" in cached:
            cached.pop("review_required", None)

        return {"success": True, "message": "Grading retry signal submitted"}
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"æäº¤æ‰¹æ”¹æ–­ç‚¹é‡è¯•å¤±è´¥: {str(e)}", exc_info=True)
        raise HTTPException(status_code=500, detail=f"æäº¤å¤±è´¥: {str(e)}")


@router.post("/confirm-boundary")
async def confirm_student_boundary(
    request: ConfirmBoundaryRequest, orchestrator: Orchestrator = Depends(get_orchestrator)
):
    """
    ç¡®è®¤å­¦ç”Ÿè¾¹ç•Œ

    å½“ AI è¯†åˆ«çš„å­¦ç”Ÿè¾¹ç•Œä¸å‡†ç¡®æ—¶ï¼Œå…è®¸ç”¨æˆ·æ‰‹åŠ¨ç¡®è®¤

    Args:
        request: ç¡®è®¤è¾¹ç•Œè¯·æ±‚
        orchestrator: LangGraph Orchestrator

    Returns:
        ç¡®è®¤ç»“æœ
    """
    try:
        if not orchestrator:
            raise HTTPException(status_code=503, detail="ç¼–æ’å™¨æœªåˆå§‹åŒ–")

        run_id = f"batch_grading_{request.batch_id}"
        run_info = await orchestrator.get_run_info(run_id)

        if not run_info:
            raise HTTPException(status_code=404, detail="æ‰¹æ¬¡ä¸å­˜åœ¨")

        # æ›´æ–°çŠ¶æ€ä¸­çš„å­¦ç”Ÿè¾¹ç•Œ
        state = run_info.state or {}
        student_boundaries = state.get("student_boundaries", [])

        # æŸ¥æ‰¾å¹¶æ›´æ–°å¯¹åº”å­¦ç”Ÿçš„è¾¹ç•Œ
        updated = False
        for boundary in student_boundaries:
            if boundary.get("student_key") == request.student_key:
                boundary["pages"] = request.confirmed_pages
                boundary["confirmed"] = True
                updated = True
                break

        if not updated:
            # å¦‚æœæ²¡æœ‰æ‰¾åˆ°ï¼Œæ·»åŠ æ–°çš„è¾¹ç•Œ
            student_boundaries.append(
                {
                    "student_key": request.student_key,
                    "pages": request.confirmed_pages,
                    "confirmed": True,
                }
            )

        logger.info(
            f"å­¦ç”Ÿè¾¹ç•Œå·²ç¡®è®¤: batch_id={request.batch_id}, student_key={request.student_key}"
        )

        return {"success": True, "message": f"å­¦ç”Ÿ {request.student_key} çš„è¾¹ç•Œå·²ç¡®è®¤"}

    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"ç¡®è®¤å­¦ç”Ÿè¾¹ç•Œå¤±è´¥: {str(e)}", exc_info=True)
        raise HTTPException(status_code=500, detail=f"ç¡®è®¤å¤±è´¥: {str(e)}")


# ==================== å¯¼å‡º API ====================


class ExportAnnotatedImagesRequest(BaseModel):
    """å¯¼å‡ºå¸¦æ‰¹æ³¨å›¾ç‰‡è¯·æ±‚"""

    include_original: bool = Field(default=False, description="æ˜¯å¦åŒ…å«åŸå§‹å›¾ç‰‡")


class ExportExcelRequest(BaseModel):
    """å¯¼å‡º Excel è¯·æ±‚"""

    columns: Optional[List[Dict[str, Any]]] = Field(None, description="è‡ªå®šä¹‰åˆ—é…ç½®")


class SmartExcelRequest(BaseModel):
    """æ™ºèƒ½ Excel ç”Ÿæˆè¯·æ±‚"""

    prompt: str = Field(..., description="ç”¨æˆ·æè¿°çš„æ ¼å¼éœ€æ±‚")
    template_base64: Optional[str] = Field(None, description="æ¨¡æ¿ Excel Base64")


@router.post("/export/annotated-images/{batch_id}")
async def export_annotated_images(
    batch_id: str,
    request: ExportAnnotatedImagesRequest = ExportAnnotatedImagesRequest(),
    orchestrator: Orchestrator = Depends(get_orchestrator),
):
    """
    å¯¼å‡ºå¸¦æ‰¹æ³¨çš„å­¦ç”Ÿä½œç­”å›¾ç‰‡ (ZIP)

    å°†æ‰€æœ‰å­¦ç”Ÿçš„ä½œç­”å›¾ç‰‡æ¸²æŸ“æ‰¹æ³¨åæ‰“åŒ…ä¸º ZIP ä¸‹è½½
    """
    raise HTTPException(
        status_code=410,
        detail="åç«¯æ‰¹æ³¨æ¸²æŸ“å·²ç¦ç”¨ï¼Œè¯·ä½¿ç”¨å‰ç«¯ Canvas æ¸²æŸ“ä¸å¯¼å‡ºã€‚",
    )


@router.post("/export/excel/{batch_id}")
async def export_excel(
    batch_id: str,
    request: ExportExcelRequest = ExportExcelRequest(),
    orchestrator: Orchestrator = Depends(get_orchestrator),
):
    """
    å¯¼å‡º Excel ç»Ÿè®¡æ•°æ®

    åŒ…å«å­¦ç”Ÿæˆç»©ã€é¢˜ç›®ç»Ÿè®¡ã€ç­çº§æŠ¥å‘Šç­‰å¤šä¸ª Sheet
    """
    from fastapi.responses import Response
    from src.services.export_service import ExcelExporter

    try:
        if not orchestrator:
            raise HTTPException(status_code=503, detail="ç¼–æ’å™¨æœªåˆå§‹åŒ–")

        run_id = f"batch_grading_{batch_id}"
        run_info = await orchestrator.get_run_info(run_id)

        if not run_info:
            raise HTTPException(status_code=404, detail="æ‰¹æ¬¡ä¸å­˜åœ¨")

        state = run_info.state or {}
        student_results = (
            state.get("reviewed_results")
            or state.get("confessed_results")
            or state.get("student_results", [])
        )
        class_report = state.get("class_report") or state.get("export_data", {}).get("class_report")

        if not student_results:
            raise HTTPException(status_code=404, detail="æ— æ‰¹æ”¹ç»“æœ")

        # æ ¼å¼åŒ–ç»“æœ
        formatted_results = _format_results_for_frontend(student_results)

        # å¯¼å‡º
        exporter = ExcelExporter()
        excel_bytes = exporter.export_basic(formatted_results, class_report, request.columns)

        filename = f"grading_report_{batch_id}_{datetime.now().strftime('%Y%m%d_%H%M%S')}.xlsx"

        return Response(
            content=excel_bytes,
            media_type="application/vnd.openxmlformats-officedocument.spreadsheetml.sheet",
            headers={
                "Content-Disposition": f'attachment; filename="{filename}"',
            },
        )

    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"å¯¼å‡º Excel å¤±è´¥: {str(e)}", exc_info=True)
        raise HTTPException(status_code=500, detail=f"å¯¼å‡ºå¤±è´¥: {str(e)}")


@router.post("/export/smart-excel/{batch_id}")
async def export_smart_excel(
    batch_id: str,
    request: SmartExcelRequest,
    orchestrator: Orchestrator = Depends(get_orchestrator),
):
    """
    LLM æ™ºèƒ½ Excel ç”Ÿæˆ

    æ”¯æŒï¼š
    - ç”¨æˆ·å¯¹è¯æè¿°æ ¼å¼éœ€æ±‚
    - å¯¼å…¥å·²æœ‰ Excel æ¨¡æ¿å¹¶å¡«å……æ•°æ®
    """
    from fastapi.responses import Response
    from src.services.export_service import SmartExcelGenerator
    from src.services.llm_client import get_llm_client

    try:
        if not orchestrator:
            raise HTTPException(status_code=503, detail="ç¼–æ’å™¨æœªåˆå§‹åŒ–")

        run_id = f"batch_grading_{batch_id}"
        run_info = await orchestrator.get_run_info(run_id)

        if not run_info:
            raise HTTPException(status_code=404, detail="æ‰¹æ¬¡ä¸å­˜åœ¨")

        state = run_info.state or {}
        student_results = state.get("student_results", [])
        class_report = state.get("class_report") or state.get("export_data", {}).get("class_report")

        if not student_results:
            raise HTTPException(status_code=404, detail="æ— æ‰¹æ”¹ç»“æœ")

        # æ ¼å¼åŒ–ç»“æœ
        formatted_results = _format_results_for_frontend(student_results)

        # è§£ç æ¨¡æ¿
        template_bytes = None
        if request.template_base64:
            import base64

            try:
                if request.template_base64.startswith("data:"):
                    request.template_base64 = request.template_base64.split(",", 1)[1]
                template_bytes = base64.b64decode(request.template_base64)
            except Exception as e:
                raise HTTPException(status_code=400, detail=f"æ¨¡æ¿è§£ç å¤±è´¥: {e}")

        # è·å– LLM å®¢æˆ·ç«¯
        llm_client = None
        try:
            llm_client = get_llm_client()
        except Exception as e:
            logger.debug(f"è·å– LLM å®¢æˆ·ç«¯å¤±è´¥: {e}")

        # ç”Ÿæˆ Excel
        generator = SmartExcelGenerator(llm_client)
        excel_bytes, explanation = await generator.generate_from_prompt(
            formatted_results,
            class_report,
            request.prompt,
            template_bytes,
        )

        filename = f"grading_smart_{batch_id}_{datetime.now().strftime('%Y%m%d_%H%M%S')}.xlsx"

        return Response(
            content=excel_bytes,
            media_type="application/vnd.openxmlformats-officedocument.spreadsheetml.sheet",
            headers={
                "Content-Disposition": f'attachment; filename="{filename}"',
                "X-LLM-Explanation": explanation.encode("utf-8").decode("latin-1"),
            },
        )

    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"æ™ºèƒ½ Excel ç”Ÿæˆå¤±è´¥: {str(e)}", exc_info=True)
        raise HTTPException(status_code=500, detail=f"ç”Ÿæˆå¤±è´¥: {str(e)}")


@router.post("/render/batch/{batch_id}")
async def render_batch_annotations(
    batch_id: str,
    page_indices: Optional[List[int]] = None,
    orchestrator: Orchestrator = Depends(get_orchestrator),
):
    """
    æ‰¹é‡æ¸²æŸ“æ‰¹æ³¨åˆ°å›¾ç‰‡

    è¿”å›æŒ‡å®šé¡µé¢çš„å¸¦æ‰¹æ³¨å›¾ç‰‡ Base64 åˆ—è¡¨
    """
    raise HTTPException(
        status_code=410,
        detail="åç«¯æ‰¹æ³¨æ¸²æŸ“å·²ç¦ç”¨ï¼Œè¯·ä½¿ç”¨å‰ç«¯ Canvas æ¸²æŸ“ã€‚",
    )


# ==================== Confession API (Task 11) ====================


class ConfessionResponse(BaseModel):
    """Confession report response"""

    batch_id: str
    overall_status: str
    overall_confidence: float
    issues: List[Dict[str, Any]]
    warnings: List[str]
    summary: str
    memory_updates: List[Dict[str, Any]]
    generated_at: str


@router.get("/confession/{batch_id}", response_model=ConfessionResponse)
async def get_batch_confession(
    batch_id: str,
    include_memory_updates: bool = True,
    orchestrator: Orchestrator = Depends(get_orchestrator),
):
    """
    Get batch confession report (enhanced).

    Returns:
        Confession report with issues, warnings, and memory updates.
    """
    try:
        if not orchestrator:
            raise HTTPException(status_code=503, detail="Orchestrator not initialized")

        run_id = f"batch_grading_{batch_id}"
        run_info = await orchestrator.get_run_info(run_id)

        if not run_info:
            raise HTTPException(status_code=404, detail="Batch not found")

        state = run_info.state or {}
        student_results = (
            state.get("reviewed_results")
            or state.get("confessed_results")
            or state.get("student_results", [])
        )

        if not student_results:
            try:
                final_output = await orchestrator.get_final_output(run_id)
                if final_output:
                    student_results = (
                        final_output.get("reviewed_results")
                        or final_output.get("confessed_results")
                        or final_output.get("student_results", [])
                    )
            except Exception:
                student_results = []
        if not student_results:
            raise HTTPException(status_code=404, detail="No grading results")

        all_issues: List[Dict[str, Any]] = []
        all_warnings: List[str] = []
        total_confidence = 0.0
        student_count = 0

        for student in student_results:
            confession = student.get("confession") or {}
            if isinstance(confession, str):
                try:
                    confession = json.loads(confession)
                except Exception:
                    confession = {}
            if not isinstance(confession, dict):
                confession = {}

            items = confession.get("items") or []
            if isinstance(items, list):
                student_key = student.get("student_key") or student.get("studentKey") or "Unknown"
                for item in items:
                    if not isinstance(item, dict):
                        continue
                    issue_copy = dict(item)
                    issue_copy["student_key"] = student_key
                    all_issues.append(issue_copy)

            conf = confession.get("overall_confidence") or confession.get("overallConfidence")
            if conf is not None:
                total_confidence += float(conf)
                student_count += 1

        avg_confidence = total_confidence / student_count if student_count > 0 else 0.5

        error_count = sum(
            1 for i in all_issues if str(i.get("severity") or "").lower() == "error"
        )
        warning_count = sum(
            1 for i in all_issues if str(i.get("severity") or "").lower() == "warning"
        )

        if error_count > 0:
            overall_status = "needs_review"
        elif warning_count > 3:
            overall_status = "caution"
        else:
            overall_status = "ok"

        memory_updates: List[Dict[str, Any]] = []
        if include_memory_updates:
            try:
                for student in student_results:
                    confession = student.get("confession") or {}
                    updates = confession.get("memory_updates") or []
                    if not isinstance(updates, list):
                        continue
                    student_key = (
                        student.get("student_key")
                        or student.get("studentKey")
                        or student.get("student_name")
                        or student.get("studentName")
                        or "Unknown"
                    )
                    for update in updates:
                        if isinstance(update, dict):
                            update_copy = dict(update)
                            update_copy.setdefault("student_key", student_key)
                            memory_updates.append(update_copy)
            except Exception as exc:
                logger.debug(f"Failed to collect memory updates: {exc}")

        return {
            "batch_id": batch_id,
            "overall_status": overall_status,
            "overall_confidence": round(avg_confidence, 3),
            "issues": all_issues,
            "warnings": list(set(all_warnings)),
            "summary": f"Batch {batch_id}: {len(student_results)} students, avg confidence {avg_confidence:.1%}",
            "memory_updates": memory_updates,
            "generated_at": datetime.now().isoformat(),
        }

    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"Failed to get confession report: {str(e)}", exc_info=True)
        raise HTTPException(status_code=500, detail=f"Failed to get confession: {str(e)}")


@router.get("/{batch_id}/files")
async def list_batch_files(batch_id: str):
    """è·å–æ‰¹æ¬¡çš„æ‰€æœ‰å­˜å‚¨æ–‡ä»¶åˆ—è¡¨"""
    try:
        file_storage = get_file_storage_service()
        files = await file_storage.list_batch_files(batch_id)
        
        return {
            "batch_id": batch_id,
            "files": [f.to_dict() for f in files],
            "total_count": len(files),
        }
    except Exception as e:
        logger.error(f"è·å–æ–‡ä»¶åˆ—è¡¨å¤±è´¥: {str(e)}", exc_info=True)
        raise HTTPException(status_code=500, detail=f"è·å–æ–‡ä»¶åˆ—è¡¨å¤±è´¥: {str(e)}")


@router.get("/files/{file_id}")
async def get_file(file_id: str):
    """è·å–æ–‡ä»¶ä¿¡æ¯"""
    try:
        file_storage = get_file_storage_service()
        file_info = await file_storage.get_file_info(file_id)
        
        if not file_info:
            raise HTTPException(status_code=404, detail="æ–‡ä»¶ä¸å­˜åœ¨")
        
        return file_info.to_dict()
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"è·å–æ–‡ä»¶ä¿¡æ¯å¤±è´¥: {str(e)}", exc_info=True)
        raise HTTPException(status_code=500, detail=f"è·å–æ–‡ä»¶ä¿¡æ¯å¤±è´¥: {str(e)}")


@router.get("/files/{file_id}/download")
async def download_file(file_id: str):
    """ä¸‹è½½æ–‡ä»¶"""
    from fastapi.responses import Response
    
    try:
        file_storage = get_file_storage_service()
        file_info = await file_storage.get_file_info(file_id)
        
        if not file_info:
            raise HTTPException(status_code=404, detail="æ–‡ä»¶ä¸å­˜åœ¨")
        
        content = await file_storage.get_file(file_id)
        if not content:
            raise HTTPException(status_code=404, detail="æ–‡ä»¶å†…å®¹ä¸å­˜åœ¨")
        
        return Response(
            content=content,
            media_type=file_info.content_type,
            headers={
                "Content-Disposition": f'attachment; filename="{file_info.filename}"',
            },
        )
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"ä¸‹è½½æ–‡ä»¶å¤±è´¥: {str(e)}", exc_info=True)
        raise HTTPException(status_code=500, detail=f"ä¸‹è½½æ–‡ä»¶å¤±è´¥: {str(e)}")


# ====================== ç®¡ç† API ======================

@router.post("/admin/clear-slots")
async def admin_clear_all_slots(teacher_id: Optional[str] = None):
    """
    ç®¡ç†ç«¯ç‚¹ï¼šæ¸…ç†æ•™å¸ˆçš„æ´»åŠ¨æ§½ä½ï¼ˆç”¨äºè§£å†³æ§½ä½å¡æ­»é—®é¢˜ï¼‰
    
    å¦‚æœæä¾› teacher_idï¼Œåªæ¸…ç†è¯¥æ•™å¸ˆçš„æ§½ä½ï¼›
    å¦åˆ™æ¸…ç†æ‰€æœ‰å·²çŸ¥æ•™å¸ˆçš„æ§½ä½ã€‚
    """
    try:
        run_controller = await get_run_controller()
        if not run_controller:
            return {"success": False, "message": "Redis ä¸å¯ç”¨ï¼Œæ— æ³•æ¸…ç†æ§½ä½"}
        
        cleared_teachers = []
        
        if teacher_id:
            # æ¸…ç†æŒ‡å®šæ•™å¸ˆçš„æ§½ä½
            await run_controller.force_clear_teacher_slots(teacher_id)
            cleared_teachers.append(teacher_id)
        else:
            # æ¸…ç†æ‰€æœ‰å·²çŸ¥æ•™å¸ˆçš„æ§½ä½
            # ä» Redis ä¸­è·å–æ‰€æœ‰æ•™å¸ˆçš„ active æ§½ä½ key
            redis_client = run_controller._redis
            pattern = "grading_run:*:active"
            async for key in redis_client.scan_iter(match=pattern):
                key_str = key.decode("utf-8") if isinstance(key, bytes) else key
                # ä» key ä¸­æå– teacher_key
                parts = key_str.split(":")
                if len(parts) >= 3:
                    teacher_key = parts[1]
                    await run_controller.force_clear_teacher_slots(teacher_key)
                    cleared_teachers.append(teacher_key)
        
        logger.info(f"[admin_clear_all_slots] å·²æ¸…ç†æ§½ä½: {cleared_teachers}")
        return {
            "success": True,
            "message": f"å·²æ¸…ç† {len(cleared_teachers)} ä¸ªæ•™å¸ˆçš„æ§½ä½",
            "cleared_teachers": cleared_teachers,
        }
    except Exception as e:
        logger.error(f"æ¸…ç†æ§½ä½å¤±è´¥: {str(e)}", exc_info=True)
        raise HTTPException(status_code=500, detail=f"æ¸…ç†æ§½ä½å¤±è´¥: {str(e)}")


@router.post("/admin/clear-progress-cache")
async def admin_clear_progress_cache(batch_id: Optional[str] = None):
    """
    ç®¡ç†ç«¯ç‚¹ï¼šæ¸…ç†æ‰¹æ”¹è¿›åº¦ç¼“å­˜ï¼ˆç”¨äºè§£å†³æ—§æ‰¹æ¬¡ç¼“å­˜é‡æ”¾é—®é¢˜ï¼‰
    
    å¦‚æœæä¾› batch_idï¼Œåªæ¸…ç†è¯¥æ‰¹æ¬¡çš„ç¼“å­˜ï¼›
    å¦åˆ™æ¸…ç†æ‰€æœ‰æ‰¹æ¬¡çš„ç¼“å­˜ã€‚
    """
    try:
        redis_client = await _get_redis_client()
        if not redis_client:
            return {"success": False, "message": "Redis ä¸å¯ç”¨ï¼Œæ— æ³•æ¸…ç†ç¼“å­˜"}
        
        cleared_count = 0
        
        if batch_id:
            # æ¸…ç†æŒ‡å®šæ‰¹æ¬¡çš„ç¼“å­˜
            cache_key = _progress_cache_key(batch_id)
            await redis_client.delete(cache_key)
            cleared_count = 1
        else:
            # æ¸…ç†æ‰€æœ‰æ‰¹æ¬¡çš„ç¼“å­˜
            pattern = f"{REDIS_PROGRESS_KEY_PREFIX}:*"
            async for key in redis_client.scan_iter(match=pattern):
                await redis_client.delete(key)
                cleared_count += 1
        
        logger.info(f"[admin_clear_progress_cache] å·²æ¸…ç† {cleared_count} ä¸ªæ‰¹æ¬¡çš„è¿›åº¦ç¼“å­˜")
        return {
            "success": True,
            "message": f"å·²æ¸…ç† {cleared_count} ä¸ªæ‰¹æ¬¡çš„è¿›åº¦ç¼“å­˜",
            "cleared_count": cleared_count,
        }
    except Exception as e:
        logger.error(f"æ¸…ç†è¿›åº¦ç¼“å­˜å¤±è´¥: {str(e)}", exc_info=True)
        raise HTTPException(status_code=500, detail=f"æ¸…ç†è¿›åº¦ç¼“å­˜å¤±è´¥: {str(e)}")
