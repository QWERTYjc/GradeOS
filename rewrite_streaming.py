from pathlib import Path
path = Path('ai_correction/functions/langgraph/agents/grading_worker_agent.py')
text = path.read_text(encoding='utf-8')
start = text.index('    def _grade_with_streaming(')
end = text.index('    def _build_grading_prompt')
lines = [
"    def _grade_with_streaming(",
"        self,",
"        messages: List[Dict],",
"        answer_file_path: str,",
"        student_name: str,",
"        state: Dict",
"    ) -> str:",
"        \"\"\"",
"        Stream thoughts for UI, then fetch final result without streaming",
"        \"\"\"",
"        callback = state.get('streaming_callback') if state else None",
"        if callable(callback):",
"            self._stream_thoughts_preview(",
"                messages,",
"                answer_file_path,",
"                student_name,",
"                callback",
"            )",
"        else:",
"            logger.info(\"Streaming preview skipped: streaming_callback is missing or not callable\")",
"",
"        return self._grade_non_streaming(messages, answer_file_path, student_name)",
"",
"    def _stream_thoughts_preview(",
"        self,",
"        messages: List[Dict],",
"        answer_file_path: str,",
"        student_name: str,",
"        callback",
"    ) -> None:",
"        \"\"\"Stream only thought content for real-time display\"\"\"",
"        thought_buffer = \"\"",
"        text_preview = \"\"",
"        try:",
"            stream = self.llm_client.chat(",
"                messages,",
"                temperature=0.2,",
"                files=[answer_file_path] if answer_file_path else None,",
"                thinking_level=\"high\",",
"                stream=True,",
"                include_thoughts=True,",
"                timeout=self.llm_timeout",
"            )",
"",
"            for chunk in stream:",
"                chunk_type = chunk.get(\"type\", \"text\")",
"                chunk_content = chunk.get(\"content\", \"\")",
"",
"                if chunk_type == \"thought\":",
"                    thought_buffer += chunk_content",
"                    callback({",
"                        \"type\": \"thought\",",
"                        \"content\": chunk_content,",
"                        \"student\": student_name",
"                    })",
"                    logger.debug(f\"[thought] {chunk_content[:50]}...\")",
"                elif chunk_type == \"text\":",
"                    text_preview += chunk_content",
"",
"            logger.info(f\"Streaming preview finished: thoughts {len(thought_buffer)} chars, text preview {len(text_preview)} chars\")",
"        except Exception as e:",
"            logger.error(f\"Streaming preview failed: {e}\", exc_info=True)",
"",
"    def _grade_non_streaming(",
"        self,",
"        messages: List[Dict],",
"        answer_file_path: str,",
"        student_name: str",
"    ) -> str:",
"        \"\"\"Fetch full grading result without streaming to ease JSON parsing\"\"\"",
"        response = self.llm_client.chat(",
"            messages,",
"            temperature=0.2,",
"            files=[answer_file_path] if answer_file_path else None,",
"            thinking_level=\"high\",",
"            stream=False,",
"            timeout=self.llm_timeout",
"        )",
"        logger.info(f\"LLM non-stream response length: {len(response)}\")",
"        return response",
""]
new_block = "\n".join(lines) + "\n"
path.write_text(text[:start] + new_block + text[end:], encoding='utf-8')
